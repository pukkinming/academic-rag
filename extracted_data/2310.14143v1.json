{
  "paper_id": "2310.14143v1",
  "title": "Mmtf-Des: A Fusion Of Multimodal Transformer Models For Desire, Emotion, And Sentiment Analysis Of Social Media Data",
  "published": "2023-10-22T00:43:06Z",
  "authors": [
    "Abdul Aziz",
    "Nihad Karim Chowdhury",
    "Muhammad Ashad Kabir",
    "Abu Nowshed Chy",
    "Md. Jawad Siddique"
  ],
  "keywords": [
    "Human desire understanding",
    "desire analysis",
    "sentiment analysis",
    "emotion analysis",
    "multimodal transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Desire is a set of human aspirations and wishes that comprise verbal and cognitive aspects that drive human feelings and behaviors, distinguishing humans from other animals. Understanding human desire has the potential to be one of the most fascinating and challenging research domains. It is tightly coupled with sentiment analysis and emotion recognition tasks. It is beneficial for increasing human-computer interactions, recognizing human emotional intelligence, understanding interpersonal relationships, and making decisions. However, understanding human desire is challenging and under-explored because ways of eliciting desire might be different among humans. The task gets more difficult due to the diverse cultures, countries, and languages. Prior studies overlooked the use of image-text pairwise feature representation, which is crucial for the task of human desire understanding. In this research, we have proposed a unified multimodal transformer-based framework with image-text pair settings to identify human desire, sentiment, and emotion. The core of our proposed method lies in the encoder module, which is built using two state-of-the-art multimodal transformer models. These models allow us to extract diverse features. To effectively extract visual and contextualized embedding features from social media image and text pairs, we conducted joint fine-tuning of two pre-trained multimodal transformer models: Vision-and-Language Transformer (ViLT) and Vision-and-Augmented-Language Transformer (VAuLT). Subsequently, we use an early fusion strategy on these embedding features to obtain combined diverse feature representations of the image-text pair. This consolidation incorporates diverse information about this task, enabling us to robustly perceive the context and image pair from multiple perspectives. Moreover, we leverage a multi-sample dropout mechanism to enhance the generalization ability and expedite the training process of our proposed method. To evaluate our proposed approach, we used the multimodal dataset MSED for the human desire understanding task. Through our experimental evaluation, we demonstrate that our method excels in capturing both visual and contextual information, resulting in superior performance compared to other state-of-the-art techniques. Specifically, our method outperforms existing approaches by 3% for sentiment analysis, 2.2% for emotion analysis, and approximately 1% for desire analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Social media platforms such as Twitter, Reddit, Facebook, and Instagram, have increasingly become the most widely used means of information sharing due to their practical features and real-time behavior  (Camacho et al., 2020) . People typically present their thoughts, opinions, breaking news, and concepts using various information modalities such as text, visual, and audio  (Afyouni et al., 2022) . The reliability of social media makes it an engaging source of information for researchers and business organizations to discover knowledge  (Zhang et al., 2022b) . Over the last decade, text-based sentiment analysis  (Xu et al., 2019; Alamoodi et al., 2021; Rani et al., 2022)  and emotion recognition  (Sailunaz and Alhajj, 2019; Estrada et al., 2020; Khoshnam and Baraani-Dastjerdi, 2022 ) from social media have advanced quickly and drawn a lot of interest from both academia and industry. Multimodal sentiment analysis  (Morency et al., 2011; Soleymani et al., 2017; Zhu et al., 2022)  and emotion recognition  (Poria et al., 2016; Ranganathan et al., 2016; Middya et al., 2022)  gained greater interest because of their multimodal nature which makes them more understandable. However, understanding human sentiments and emotions across several modalities is challenging because of the complexity of human feelings and expressions.\n\nDesire, a fundamental aspect of human nature, encompasses a strong feeling of craving or longing for something  (Dong et al., 2010) . It distinguishes humans from other animals, as they are uniquely driven by the desire to acquire or possess something, and can exhibit an unquenchable thirst. This desire motivates individuals to act and work toward their goals  (Wilt and Revelle, 2015) . It is an innate sense that permeates all human beings and holds power to shape and influence various aspects of human behavior.\n\nDesires often drive human sentiment and emotions, whether it be the desire for inspiration, anticipation, letdown, jealousy, or obsession. These emotions significantly shape our practical life experiences, influencing our thoughts, actions, and decisions. There is a close connection between human desire, sentiment, and emotion. Desire implicitly governs sentiment and emotion, while sentiment and emotion, in turn, can be influenced by desire. Together, desire, sentiment, and emotion form interconnected and essential components of the human experience, driving our actions and decisions.\n\nMultimodal desire understanding is an emerging task that plays an important role in recognizing human emotional intelligence and personalized and effective human-computer interaction, leading to improved customer satisfaction and experience in e-commerce. However, the multifaceted nature of human desire makes it difficult to understand it across several modalities.  Jia et al. (2022)  introduced the first benchmark dataset MSED focusing on the multimodal human desire understanding task. They designed the task to consist of three subtasks -desire analysis, sentiment analysis, and emotion analysis. The examples of the human desire understanding task in Figure  1  indicate that three classification labels are required for a given image, title, and caption. A couple climbing a mountain is depicted by the image and text combination in this figure from example A. To satisfy their curiosity, they climb to the top of the Negative In recent years, multimodal sentiment analysis  (Birjali et al., 2021; Chan et al., 2023)  and emotion recognition  (Zhang et al., 2020a; Ezzameli and Mahersia, 2023)  have gained significant attention due to the growing use of multimedia data in a variety of applications, including marketing, customer service, dialogue analysis and generation, and human-computer interaction. Most sentiment and emotion analysis studies  (Li et al., 2020; Tu et al., 2022; Zhang et al., 2022a Zhang et al., , 2023b ) have used the deep learning-based approach. However, they do not focus on the multimodal human desire understanding task. We proposed a multimodal transformer fusion-based approach to fill the multimodal human desire understanding research gaps. The key contributions of this paper are:\n\n• We propose a unified architecture of multimodal transformer models, called MMTF-DES (Multimodal Transformers Fusion for Desire, Emotion, and Sentiment analysis) for the multimodal human desire understanding task. In this proposed architecture, we jointly fine-tune two state-of-the-art pre-trained multimodal transformer models, Vison-and-Language Transformer (ViLT)  (Kim et al., 2021)  and Vision-and-Augmented-Language Transformer (VAuLT)  (Chochlakis et al., 2022) , creating a unified architecture.\n\n• We adopt a training strategy known as multi-sample dropout to enhance the generalizability and training efficiency of the base model. This strategy helps improve the overall performance of the MMTF-DES model.\n\n• We conduct a comparative analysis among other state-of-the-art models to evaluate the effectiveness of our proposed model. Also, we present a performance evaluation of the early and late fusion strategies for each of the three tasks. To ensure precise results, we propose a component-based analysis scheme that aids in determining the final task-specific labels.\n\n• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This task aims to deepen our understanding of human desire and facilitate more effective analysis in this domain.\n\nThe rest of the paper is organized as follows: Section 2 provides an overview of prior research that motivates us to work in this domain. We raise some research questions that have driven our work on this task in Section 3. In Section 4, we introduce our proposed fusion of a multimodal transformer-based approach for the multimodal human desire understanding task. We discuss experimental settings in Section 5. Section 6 includes the detailed experiments and evaluation and a comparative performance analysis of the related methods. We provide some precious insights for this task in Section 7. We conclude our work and discuss future directions in Section 8.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal sentiment analysis and emotion recognition pertain to analyzing a person's or group's emotions and sentiments using modalities, including text, speech, facial expressions, and body language. Because of its potential uses in several industries, including healthcare, education, and social media monitoring, this discipline has recently attracted increasing interest  (Araño et al., 2021) .\n\nIn earlier stages, textual sentiment analysis  (Zhou et al., 2013; Kiritchenko et al., 2014)  and emotion recognition  (Lee et al., 2012; Shaheen et al., 2014; Kratzwald et al., 2018)  were very popular. For example,  Zhou et al. (2013)  and  Gamallo et al. (2014)  used lexicon-based features and statistical machine learning classifiers, including support vector machine (SVM) and Naïve-Bayes, for the sentiment analysis of English tweets.  Oberländer and Klinger (2018)  introduced a dataset and provided insight into different models for emotion classification on text. A recent study used the BERT model for sentiment analysis on tweet text of coronavirus data  (Singh et al., 2021) . Recently, multimodal sentiment analysis and emotion recognition have gained the attention of researchers because of their multimodal nature.  Truong and Lauw (2019)  explored the use of deep learning models for multimodal sentiment analysis. They proposed a visual aspect attention network (VistaNet) that could effectively incorporate multiple modalities, including text and image, for sentiment analysis.  Mai et al. (2022)  proposed a hybrid contrastive learning framework of trimodal representation, including text, image, and speech, for multimodal sentiment analysis. To reduce the modality gap, they conducted intra-inter-modal contrastive learning and semi-contrastive learning so that the model can explore cross-modal interactions.  Yang et al. (2022)  proposed a multimodal translation for sentiment analysis (MTSA) leveraging text, visual, and audio modalities. To improve the quality of visual and audio features, they translate them into text features using BERT.  Peng et al. (2023)  represented a modal label-based multi-stage method for sentiment analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent tasks for multi-stage training.\n\nFor a long time, emotion recognition has been a topic of ongoing research. With the growth of image-text data owing to the emergence of social media platforms, multimodal emotion recognition on social media data has recently gained popularity.  Soleymani et al. (2011)  applied a modality fusion strategy, using a support vector machine as the classifier.  Poria et al. (2016)  used deep convolutional neural networks to extract features of visual and textual modalities.  Ranganathan et al. (2016)  tried convolutional deep belief networks (CDBNs) that generate robust multimodal features of emotion expressions.  Nemati et al. (2019)  introduced a hybrid approach to latent space data fusion in which the textual modality is combined with the auditory and visual modalities using a Dempster-Shafer (DS) theory-based evidentiary fusion methodology, and their projected characteristics are combined using a latent space linear map. Recently,  Zhang et al. (2022c)  used the manifold learning method to extract low-dimensional embedding features from high-dimensional features. Then, these features are fed into the deep convolutional neural network for emotion recognition. To learn a joint multimodal representation for emotion recognition,  Le et al. (2023)  fuse multiple modalities including video frames, audio signals, and text subtitles through a unified transformer architecture.\n\nDesire is inherently related to human sentiment and emotion, and understanding these relationships can provide insights into human behavior and decision-making, which drive the sentiments and emotions of humans.  Jia et al. (2022)  argue that there exists a significant interrelation between human desire, sentiment, and emotion, wherein desire holds a surreptitious hegemony over sentiment and emotion, while sentiment and emotion are subject to the influence of desire. However, human desire understanding is an under-explored task. Human desire involves both linguistic expression and visual expression.  Lim et al. (2012)  developed a system for desire understanding based on four types of emotions -speed, intensity, regularity, and extent from human emotional voices. To investigate the variations and similarities in the association between sexual desire and love,  Cacioppo et al. (2012)  introduced a multilevel kernel density fMRI analytic technique.  Yavuz et al. (2019)  proposed a data mining approach for desire and intention using neural and Bayesian networks. However, those works have some limitations in representing the human desire understanding task effectively. To break the research gap in the understanding and detection of human desire,  Jia et al. (2022)  proposed the first multimodal dataset MSED for human desire understanding. They represent three tasks -desire analysis, sentiment analysis, and emotion analysis in this dataset which contains textual and visual modalities. This facilitates a new window in the human desire understanding research. They also provide various strong baselines based on different combinations of feature representations using various visual and textual encoders.\n\nThe unified architecture of the BERT and ResNet model achieves top performance for them.\n\nHowever, most of the systems struggled to achieve intra-and-inter relationships between modalities as they fused different modalities externally. These systems do not properly capture the semantic orientation of the textual information and visual representation to estimate human desire understanding. To learn the intra-and-inter relationship between modalities, pairwise training of different data, including visual and textual, is crucial for the human desire understanding task. Transformer-based systems distill contextual and visual information effectively. However, the v fusion of a pre-trained vision and text transformer-based multimodal model without the pairwise training of the text inputs and joint training of image-text input struggles to distill intra-relationships between modalities. To mitigate this issue, we use multimodal transformer models with the pairwise training of text and images, which enables us to achieve the intra-relationship between image and text. We fuse two multimodal transformer models to extract diverse integrated visual-contextual features representation, which helps us achieve the inter-relationship of two modalities.\n\nMoreover, we use a multi-sample dropout training strategy to improve the generalizability of our proposed model.",
      "page_start": 4,
      "page_end": 21
    },
    {
      "section_name": "Research Questions",
      "text": "To contribute to the state-of-the-art in the newly introduced multimodal human desire understanding task, we propose a unified multimodal neural network architecture to understand the human desires from the given image and the associated text. Also, our work involves identifying which integrated visual-contextual representation and techniques perform better. Therefore, we articulate some research questions (RQs) related to the task of human desire understanding.\n\n• RQ1: Can a unified multimodal neural network model capture better visual-contextual features than a single model for image-text data in the human desire understanding task? -We use two SOTA multimodal transformer models which give diverse visual-contextual representations and improve the generalizability of our proposed method. The corresponding details are available in Section 6.4.\n\n• RQ2: What is the effect of different fusion techniques on the MSED benchmark dataset?\n\n-We fuse two model feature vectors to get the unified architecture of our proposed method for the human desire understanding task. We answer this question in Section 6.2.\n\n• RQ3: How much does our proposed approach improve performance compared to the SOTA human desire understanding methods? -We evaluate the performance of our proposed system with SOTA human desire understanding methods in the experiments and evaluation section. The discussion is available in Section 6.6.\n\n• RQ4: Can different training strategies improve the base model performance for the human desire understanding task? -To enhance the performance of the base method, we use a training strategy. The significance of this training strategy is given in the discussion section. The analysis is available in Section 7.2.\n\n• RQ5: How can the task be further extended to improve its effectiveness and applicability in the desire understanding task? -To further extend the desire understanding task, we proposed a new binary desire analysis task. The details of this task description and performance are analyzed in 6.5. In addition, we present a summarized analysis of these five RQs in Section 7.3 to analyze the potency of our proposed method.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Proposed Framework",
      "text": "Vision-and-language pre-training (VLP) transformers performed better in the vision-and-language multimodal task. To take advantage of VLP, we jointly fine-tuned two SOTA multimodal transformer models for the human desire understanding task. An overview of our proposed model for the human desire detection framework is depicted in Figure  2 . The model is trained on the MSED dataset.\n\nProblem definition: Given a text pair contains title t and caption c and a corresponding image i of a tweet, classify the desire as d of the tweet as ϵ {f amily, romance, vengeance, curiosity, tranquility, socialcontact, none}, emotion as e ϵ {happiness, sad, neutral, disgust, anger, f ear}, and sentiment as s ϵ {positive, negative, neutral}.  1 that take up a new instance denoted as it k (which contains t k , c k , i k ) to fitting desire d k , emotion e k , and sentiment s k labels, respectively. vii\n\nWhere it k is the input texts-image pair whose desire label d k , emotion label e k , and sentiment label s k are to be predicted. P denotes the log-likelihood function and the λ symbol denotes the method's parameters we intend to optimize.\n\nGiven an input image and texts are paired and transformed to the correct data formats, we feed them into the two multimodal transformer models' processors. We use two SOTA multimodal transformer models, ViLT and VAuLT, tuned on image-text pair settings to generate the image-text pair encoding. Each encoding is fed into the corresponding multimodal transformer model to get integrated visual-contextual feature vectors of each model. To get the diverse features representations of human desire understanding, we use a concatenation-based early fusion method on top of the two models' feature vectors. We jointly fine-tune this unified architecture to capture the domain-specific information of human desire understanding explicitly. To improve the generalizability and speed up the training of our proposed method, we use a multi-sample dropout training strategy. Then, a classification head is plugged on top of the multimodal unified architecture to predict the appropriate task-specific class label of the input image and texts.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Transformer Models",
      "text": "The BERT-based transformer model facilitates the learning of long-term dependency by handling the sentence as a whole rather than word by word. A combination of multi-head attention and positional embedding processes provides the necessary information regarding the relationship between words. To enable deep bidirectional representations of context, BERT uses masked language modeling. This model incorporates the feed-forward neural network architecture with a self-attention mechanism which helps distill contextual information more efficiently. In Vision The Vision-and-Language Transformer (ViLT)  (Kim et al., 2021)  processes visual inputs like text inputs, which implies a convolution-free method of processing visual data. The modal-specific components of ViLT are a class of VLPs that are less intensive to compute for multimodal interactions than the transformer component. ViLT is a parameter-efficient model that is much faster than other VLP models with region features. While it performs similarly or is even superior on downstream tasks, including VQA, image-text matching, multimodal classification, and multimodal entailment of vision-and-language, it is also at least four times faster than those models with grid features. To take advantage of the linear modality interaction, we use ViLT for the multimodal human desire understanding task. It helps ViLT to distill the relationships between image and text, which are crucial for the human desire detection task.\n\nTo tokenize input text, ViLT uses the bert-base-uncased 1  tokenizer. To encode the input image, ViLT uses the ViT-B/32 2  vison transformer model. It uses weights from ViT-B/32 pre-trained on ImageNet. It consists of 12 layers of transformer, with MLP size 3072, hidden size 768, patch size 32, and 12 attention head layers. We use HuggingFace's implementation of the vilt-b32-mlm  (Kim et al., 2021 ) checkpoint 3  of the ViLT model.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Vision-And-Augmented-Language Transformer (Vault)",
      "text": "The Vision-and-Augmented-Language Transformer (VAuLT)  (Chochlakis et al., 2022)  expands the well-known VLP model ViLT. To improve the model's capacity to capture complex text features, the VAuLT model uses a modified version of the ViLT architecture that incorporates an extra text encoder module. Because ViLT was initialized from ViT, it lacks language understanding capabilities. VAuLT addresses this issue by substituting ViLT's language embedding inputs with linguistic features taken from a large LM pre-trained on more linguistic data varieties, which may also be chosen to better meet the demands of the downstream application. Training the LM and ViLT together in VAuLT improves the results over ViLT and achieves state-of-the-art or equivalent performance on various VL tasks incorporating richer language inputs and effective constructs. This motivates us to use the VAuLT model in our proposed framework for the human desire understanding task. That is why we use VAuLT along with ViLT to -capture the diversity of features. In VAuLT, we can use any language model to produce the sequence of embeddings which may be input into the ViLT architecture. To tokenize input text, we employ the bertweet-base tokenizer. Due to the social media data used in MSED, the BERTweet may learn contextual information effectively. Thus, we use BERTweet to get diverse textual features representation in VAuLT 4 for human desire understanding task. We utilize HuggingFace's Vision-and-Language Transformer (ViLT)\n\n.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Title Caption",
      "text": "Taking their date to the top of the mountain Shot of a couple going for a hike up the mountain\n\n….\n\nPooler Output",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Input Representation Of Multimodal Transformer Model",
      "text": "In our proposed MMTF-DES method, we use two SOTA multimodal transformer models -ViLT and VAuLT.\n\nWe now describe the input representation of these models. MSED contains two text inputs, a title and caption, and one visual input, a corresponding image. Figure  3  illustrates the input representation used in our proposed method for representing the input data of MSED. We use an image-text pair training concept in the multimodal transformer models to train the human desire understanding task. The idea behind this concept is to combine the input image, title, and caption into one sequence in the multimodal transformer model's processor. Essentially, the caption and title pairs in the example are represented as a single sequence in which a special classification token [CLS] is inserted at the beginning of the first input (the caption), and a special separation token  [SEP]  is added between the caption and title to separate them. Therefore, the text-pair input sequence of multimodal transformer for our human desire understanding task is like \"[CLS] title [SEP] caption [SEP]\" and the input image is sliced into patches where the patches denote as PTOK (patch token) and are flattened into the processor with a special classification token [CLS] at the beginning.\n\n5 https://huggingface.co/vinai/bertweet-base x",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multimodal Transformers Early Fusion",
      "text": "The fusion of the models is an effective technique and common practice in multimodal learning that could improve the model's learning accuracy and robustness more than individual models. To benefit from the diversity of feature representations, we jointly fine-tune two SOTA VLP transformers, the ViLT and VAuLT models on the MSED dataset. In the case of multimodal models, an early fusion technique improves performance and generalizability. The advantage of early fusion is that it facilitates the exchange of information between the models at an earlier stage in the network, resulting in a better representation of the input data as a whole. We concatenate the pooler outputs of the ViLT and VAuLT models at an early stage, allowing the subsequent layers of the network to refine and combine these features in a more integrated and optimized manner. This enables us to get the two models integrated with visualcontextual feature vectors. With the fusion strategy, we have consolidated the diverse information of the human desire understanding task from these models. The same taxonomy of two multimodal transformer models with different text encoders improves the robustness and strength of our proposed model. This allows us to perceive the context and image pair from all aspects in a robust manner.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Multi-Sample Dropout Training Strategy",
      "text": "With a multi-sample dropout-based training strategy, the base model is trained faster and generalizability is improved, leading to the enhanced overall performance of the system  (Inoue, 2019; Hahn and Choi, 2020; Pan et al., 2021) . During learning, the model is forced to learn a more generalizable representation that different sets of neurons are dropped out randomly for every input example. Therefore, it helps the base model avoid overfitting. To optimize the model, it calculates the loss for some dropout samples and averages them, resulting in a smaller loss. In our proposed method, the multi-sample dropout training strategy is used on top of the unified multimodal transformer-based architecture, where we employ a sample size of three dropout layers in the training process. In essence, we are duplicating the features vector of the unified architecture after the dropout layer is applied and sharing the weights among these duplicated layers which are fully connected. Training can be accelerated since more models are evaluated in the same time, resulting in faster convergence. To obtain the final loss, we aggregate the loss obtained from each sample and take their average. This way, the model can more quickly converge to a better model that generalizes well to new data as it learns more generalizable features. Later, a task-specific classification head on top predicts the appropriate desire, sentiment, and emotion labels for the corresponding image-text pair.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Fine-Tuning Procedure",
      "text": "Fine-tuning multimodal transformer models may improve performance because it allows the model to be adapted to domain-specific knowledge. To achieve this, we jointly fine-tune the two multimodal transformer models described in Section 4.1 using the domain-specific datasets. For diverse integrated contextual-visual feature vectors, a single concatenate layer is all we need. We train the human desire understanding task as an image-text-pair task where the title becomes the first text and the caption the second text in the input sequence. Additionally, the image is xi the third input of our proposed method. During fine-tuning, all other hyper-parameters remain the same as during the multimodal transformer model training, except for batch size, learning rates, epochs, max length, and dropout.\n\nThe final hidden states of each model are then used to obtain a fixed-dimensional feature representation of the input sequence. Upon constructing the combined features vector of our method, we use an early fusion concatenation technique and then apply a multi-sample dropout technique to obtain the task-specific predicted class label.\n\nHere, T and I denote the input text pair and image in Equation  2 , respectively. T contains two text inputs -the title and caption paired with each other with a special classification token, t CLS for text inserted at the beginning of the first input. t 1...M represents the input title whereas t i represents each word in the title. The special separator token t S EP is in between the title and caption to separate them. Then, c 1...M represents the input caption whereas c i denotes each word in the caption. p 1...M represents the input image with a special classification token, p CLS for the image inserted at the beginning whereas p i indicates each patch in the input image. T and I are then passed into the corresponding model processor (V pr and V A pr ) to get the encoding of the text pair and image. Those encodings are then fed into the corresponding model (V m and V A m ) to extract the contextual-visual representation feature vector of the texts-image pair. In our proposed method, we use the pooler output of the ViLT and VAuLT models represented as V pool and V A pool in equations, respectively. To get the integrated features vector, IFV we concatenate two feature vectors V pool and V A pool . The ⊕ symbol denotes the concatenation in the equation. Later, IFV, is fed into a dropout layer, D 0 and the obtained output passes into the multi-sample dropout layer, MS D, to improve training. MS D is then connected to the output layer, OL, and averages the obtained output to get the final prediction, O, of our method. This architecture is fine-tuned to learn task-specific knowledge by our proposed method.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dataset Description",
      "text": "To demonstrate the efficacy of our proposed multi-modal transformers fusion model, we evaluate our model on a publicly available multimodal benchmark dataset MSED  (Jia et al., 2022) . MSED 6 contains 9,190 text-image 6 https://github.com/MSEDdataset/MSED xii combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset, the authors featured three multi-class classification tasks -sentiment analysis, emotion analysis, and desire analysis.\n\nIn the sentiment analysis task, we need to determine positive, neutral, or negative sentiment classes in the text and corresponding image. Based on the image-text pair, we need to classify each instance as one of six classes -happiness, sad, neutral, disgust, anger and fear in the emotion analysis task. The six typical human desires -family, romance, vengeance, curiosity, tranquility, and social contact feature in the desire analysis task. The MSED dataset contains 6,127 training instances, 1,021 validation instances, and 2,024 test instances. The detailed statistics of the MSED dataset are presented in Table  1 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "To evaluate the system performance on the MSED dataset  (Jia et al., 2022) , the authors used different strategies and metrics for the sentiment analysis, emotion analysis, and desire analysis tasks. For all three tasks of MSED, standard evaluation metrics, including Precision (P), Recall (R), and F1-score, are used to evaluate a system. To determine how well the model predicts a specific category in understanding human desire, the precision metric is used. The recall xiii\n\nmetric is used to understand how many times the model could detect a specific desire/emotion/sentiment category.\n\nThe macro average category of those evaluation metrics was used because MSED is an imbalanced dataset that helps treat each class equally. The macro average F1-score is the primary evaluation measure in our analysis because it gives summarized information on both precision and recall metrics. We also report the results based on other evaluation measures too.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model Configuration",
      "text": "We describe the hyper-parameter settings for our proposed MMTF-DES model in this section. To enhance the efficiency of parallel computation of tensor image-text pairs of training, we used PyTorch to design and implement our proposed method. As part of this study, all experiments were carried out on the Google Colaboratory platform  (Bisong, 2019) . The optimal hyper-parameters were selected using a grid search method based on the validation dataset. The hyper-parameters search space is illustrated in Table  2 .\n\nWe used two pre-trained multimodal transformer models  (Wolf et al., 2020) , VAuLT and ViLT, as multimodal encoders to extract effective integrated contextual-visual features in our proposed MMTF-DES model. Prior research  (Aziz et al., 2021)  reveals that fine-tuning the hyper-parameters of those models makes them consistently outperform the pre-trained models for downstream tasks. Several hyper-parameters, including training batch size, test batch size, learning rate, dropout, and epochs, were fine-tuned to get the optimal value. We train our proposed method with the provided training data with the validation data for efficient training. We trained VAuLT and ViLT models using 5 epochs and set the learning rate to nearly the same as 3e-5. The best settings of these hyper-parameters are reported in Table  3 . We used the vilt-b32-mlm checkpoint of the ViLT model and vilt-b32-mlm and the vinai/bertweetbase checkpoint for the VAuLT model. We used the CUDA-enabled GPU and set the fixed manual seed to generate reproducible results. During training, we saved our model based on the best validation loss by evaluating the validation set. To reduce the noisy features and avoid overfitting, we fine-tuned the dropout rate hyper-parameter.\n\nxiv",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "We now evaluate the performance of our proposed MMTF-DES approach. The objectives of our experimental design are six-fold: (1) we analyze different modality baseline models to choose the best model (  2 ) we examine the performance of our used fusion strategies to pick the most effective one that we used in all the later experiments (RQ2); (  3 ) we analyze the overall performance and task-wise performances of our proposed method on the MSED dataset; and (4) we determine the performance of individual multimodal transformer models used in our proposed approach (RQ1). (  5 ) We proposed a new desire analysis task where we describe the task and evaluate our method on this task (RQ5). (  6 ) We provide a comparative performance analysis between our proposed method and other current SOTA (RQ3).",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Analysis Of Different Modality Baseline Models",
      "text": "We present different modality baseline methods analysis on the human desire understanding task that drove this work. We consider three categories of modality in the baseline analysis discussion. From the textual modality, we consider the BERTweet transformer model since text data are annotated from tweet data. In the visual modality, we consider the ResNet model as it has outstanding performance in many image classification tasks. In the multimodal category, we use the ViLT multimodal transformer model for the human desire understanding task to take advantage of its linear modality interaction (text and image modality treated with the same priority).\n\nThe results of the various modality baseline experiments are presented in Table  4 . To show the baseline methods performance on the MSED dataset, we show the performance of all three subtasks -sentiment analysis, emotion",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Performance Of Fusion Techniques",
      "text": "To identify the best fusion technique for our proposed MMTF-DES, we apply two categories of fusion techniques -early fusion and late fusion  (Snoek et al., 2005) . In early fusion, we concatenate the ViLT and VAuLT multimodal transformer models' feature vectors together at an early stage of the network, typically after the initial feature extraction layers of each model. In concatenate-based late fusion, the feature vectors from the two multimodal transformer models are combined later in the network, typically after passing through various dense layers. That means each xvi model feature vector is fed into many dense layers and concatenated together. In these two fusion strategies, the early fusion concatenation strategy achieved the best performance across all three tasks. Table  5  illustrates the impact of both fusion strategies based on the primary evaluation metrics macro average F1-score across all three tasks (RQ2).\n\nHere, early fusion results in a better feature representation of the input image-text pair data, which helps the model improve the performance of the human desire understanding task. It enables the model to capture complex and subtle relationships in the image-text pair data, which can improve the model's ability to generalize to new and unseen data. Thus, the network learns a more integrated and optimized set of features by fusing the features at an earlier stage in the network. In the sentiment analysis task, the early fusion-based approach achieved an 88.44% macro average F1-score, which is 1.2% higher than the late fusion-based model's-performance. The early fusion-based method shows significant performance improvement over the late fusion-based method on the emotion analysis and desire analysis tasks, with a 5.1% and 5.6% higher macro average F1-score, respectively. It validates the efficacy of the early fusion-based strategy. Therefore, we choose this fusion strategy for our proposed MMTF-DES method and the rest of the results are reported following this setting. The summarized results of our proposed MMTF-DES method based on different tasks are presented in Table  6 .\n\nThe overall performance for the MSED dataset is 85.27% and 86.52% based on the macro averaged F1-score and accuracy score, respectively. Here, our proposed method performs better for the sentiment analysis task than for the emotion analysis and desire analysis tasks. A possible reason for this difference in performance may be related to the nature of the three tasks. Sentiment analysis is concerned with identifying the overall sentiment or opinion expressed in text and images. In contrast, emotion analysis and desire analysis are concerned with identifying the specific emotional states and desires expressed by an individual that are inherently embedded in images and text.\n\nTherefore, sentiment analysis may be a more straightforward task that is easier to model, while emotion analysis and desire analysis may be more complicated and nuanced.\n\nxvii",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Impact Of Individual Multimodal Transformer Models",
      "text": "We further analyze the performance of our proposed MMTF-DES method by evaluating the performance of individual multimodal transformer models. We only retain one multimodal transformer model at a time and discard the other model to do this. To examine the component analysis of our proposed method, we used the MSED dataset for all three tasks, and the evaluation results are illustrated in Table  7 . To capture diverse integrated contextual-visual representation, we incorporate two multimodal transformer models -ViLT and VAuLT. We apply an early fusion at the very early stage of the network i.e., the feature's label fusion. Such fusion is crucial for learning pixels-based information in visual content and semantic information in context from two modalities in an integrated manner. Jointly trained our proposed unified multimodal architecture performed better than the individual models across all three tasks (RQ1).\n\nIn the sentiment analysis task, the ViLT model performs better than the VAuLT model. However, MMTF-DES shows a 2.97% performance improvement over ViLT. The VAuLT model performs better than the ViLT model in the emotion analysis and desire analysis tasks. Nevertheless, our proposed MMTF-DES method achieved a 3.32% and 2.0% higher macro averaged F1-score on the emotion analysis and desire analysis tasks, respectively. This highlights the importance of using diverse multimodal transformer encoders in our proposed method.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Binary Desire Analysis",
      "text": "To further extend this task, we propose a new task for the human desire understanding task (RQ5). It is important to work on the human desire analysis task -we first need to identify whether the image-text pair contains human xviii   1 ). According to this, we distribute those data in the binary label. Our proposed binary-type desire data statistics are presented in Table  8 .\n\nTo know how well our proposed method identifies desire image-text pair data, we train our proposed method for the binary desire classification task. For binary desire classification, our proposed MMTF-DSE method achieved 90.21% precision, 90.19% recall, and a 90.21% macro averaged F1-score. Although, our proposed method achieved an 83.11% macro average F1-score in the multiclass desire analysis task. A higher macro F1-score demonstrated the applicability and generalizability of our proposed early fusion of a multimodal transformers-based approach for the human desire understanding tasks.\n\n6.6. Comparative performance analysis 6.6.1. Comparing with state-of-the-art study on MSED\n\nTo evaluate the performance of our proposed method against the current state-of-the-art techniques, we compared the performance with the top-performing systems on the MSED dataset. The authors proposed three tasks -desire analysis, sentiment analysis, and emotion analysis -to evaluate the MSED dataset and present various strong baselines by combining diverse features fusion  (Jia et al., 2022) . They used two categories of encoders to extract the text and image features, respectively. To represent the text, they use three well-known text encoders -deep CNN (DCNN), bidirectional LSTM (BiLSTM)  (Zhang et al., 2020b) , and the pre-trained language model BERT  (Devlin et al., 2019) .\n\nTo encode images, they use two widely used visual encoders, i.e., AlexNet  (Alom et al., 2018)  and ResNet  (Szegedy et al., 2017) . Among their modality analysis, the BERT model achieved the best macro averaged F1-score in the textual modality and ResNet achieved better performance in the visual modality. The fusion of BERT and ResNet learns the visual and textual information effectively and achieves the best performance among all modalities. Also, they used a xix   (Gabeur et al., 2020) , as one of the baselines.\n\nIt performs better than other models except for the fusion of the BERT-ResNet model. Recently, M3GAT  (Zhang et al., 2023a)  proposed a graph attention network-based method focusing on the sentiment and emotion analysis task.\n\nHowever, they did not report their result on the desire analysis task.",
      "page_start": 18,
      "page_end": 20
    },
    {
      "section_name": "Comparative Performance Across All Three Tasks",
      "text": "To evaluate the performance of our proposed method against the current SOTA, we compared the performance with the top-performing systems (RQ3) on the MSED dataset. We fuse two pre-trained multimodal transformer models -ViLT and VAuLT -to get the unified multimodal architecture for the human desire understanding task. To compare the performance of our proposed MMTF-DES model, we used the MSED dataset, which contains three tasks -sentiment analysis, emotion analysis, and desire analysis. The comparative performance of our proposed MMTF-DES system on test data against other SOTA systems is presented in Table  9 . Our proposed multimodal early fusion-based transformer xx approach outperforms other SOTA models across all three tasks (RQ3). Our proposed method achieves 88.44% on the sentiment analysis task, 84.26% on the emotion analysis task, and 83.11% on the desire analysis task, based on the primary evaluation metric macro average F1-score on the MSED dataset. Our proposed MMTF-DES model outperforms the prior best-performing fusion of the BERT-ResNet model by 3% on the sentiment analysis task, 2.2% on the emotion analysis task, and 1% on the desire analysis task (as denoted by ∇ in the Table  9 ). The comparative performance analysis confirms that an approach that involving the fusion of various multimodal transformers encoders can achieve good performance for the human desire understanding task from image-text across different associated tasks. This validates the effectiveness and applicability of our proposed method on the multimodal human desire understanding task.\n\nThose models do not perform as well as our proposed method because they struggle to achieve intra-and-inter relationships between modalities as they fuse different modalities externally. To learn the intra-and-inter relationship between modalities, the pairwise training of different data, including visual and textual, is crucial for the desire understanding task. To mitigate this issue, we use multimodal transformer models with pairwise training of the text and image. This helps us achieve the intra-relationship between image and text. We fuse two multimodal transformer models, to extract diverse integrated visual-contextual features representation which helps us to achieve the interrelationship of two modalities. The early fusion of two multimodal transformer models results in a better feature representation of the input image-text pair data, which helps the model to improve the performance of the human desire understanding tasks. It enables the model to capture more complex and subtle relationships in the image-text pair data, which can help improve the model's generalizability to unseen data. Moreover, we use a multi-sample dropout training strategy to improve the generalizability of our proposed model.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we proffer some research findings into the multimodal human desire understanding research. We provide modality dominance analysis, multi-sample dropout impact, research analysis, and error analysis to analyze the efficacy of our proposed MMTF-DES method in the human desire understanding domain.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Modality Dominance",
      "text": "To analyze the modality dominance in the multimodal human desire understanding task, we discuss the individual modality model's result across all three tasks. Table  10 shows",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Impact Of Multi-Sample Dropout",
      "text": "To analyze the impact of multi-sample dropout (MSD) in our proposed method, we experiment with the MSED dataset. In this experiment, we remove the multi-sample dropout module from our proposed MMTF-DES method to discover the significance of the MSD in the human desire understanding task. The results without MSD and with MSD (our proposed method) are reported in Table  11 . We have seen that the method without MSD results in significantly lower performance than our proposed method (with MSD) across all three tasks in terms of the macro averaged F1score. With the MSD, our proposed method improves the performance by 1.32% on the sentiment analysis task, xxii 1.72% on the emotion analysis task, and 2.22% on the desire analysis task, based on the primary evaluation measure macro averaged F1-score (RQ4). This highlighted the importance of adding multi-sample dropout layers to the unified multimodal transformer-based model architecture.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Research Analysis",
      "text": "We presented five research questions in Section 3 that motivated us to perform the human desire understanding research. The first question (RQ1) concerned capturing better integrated visual-contextual features from different image-text pair data. We used two multimodal transformer encoders -ViLT and VAuLT -to extract diverse visualcontextual features. The analysis results of Section 6.4 demonstrated the efficacy of exploiting diverse multimodal transformer encoders in the human desire understanding task. In the second question (RQ2), we focused on effective fusion techniques for leveraging the above-mentioned multimodal transformer models to get a unified multimodal architecture. We showed the comparative performance between the early fusion-based concatenation and late fusionbased concatenation techniques in Section 6.2. The early fusion-based concatenation achieved a macro averaged F1score 3.97% higher than the late fusion-based concatenation technique on the MSED dataset for our proposed method.\n\nNext, in the third question (RQ3), we investigated a comparative performance analysis of our proposed MMTF-DES method with other state-of-the-art methods (i.e., BERT-ResNet, Multimodal transformer, etc.), as described in Section 6.6. The comparative analysis based on the MSED dataset demonstrates that our proposed MMTF-DES method achieved a 3% improvement on the sentiment analysis task, a 2.22% improvement on the emotion analysis task, and a ∼1% improvement on the desire analysis task compared to the existing best-performing model fusion of BERT and ResNet method. This model struggles to learn the inter and intra-relational structure of the image-text pair relationship to identify human desire. To improve the base model performance, we use a training strategy called multi-sample dropout. It improves the base method performance by an average of 1.7% on the MSED dataset (RQ4).\n\nThe performance analysis in Section 7.2 demonstrated the efficacy of using multi-sample dropout in our proposed MMTF-DES method. Moreover, we propose a new binary desire analysis task to effectively understand human desire analysis (RQ5). The task description and performance of our method on this task are presented in Section 6.5",
      "page_start": 23,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: indicate that three",
      "page": 2
    },
    {
      "caption": "Figure 1: Example of the multimodal human desire understanding task.",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of our proposed framework for the human desire understanding task. To generate the integrated visual-contextualized feature",
      "page": 7
    },
    {
      "caption": "Figure 2: The model is trained on the MSED dataset.",
      "page": 7
    },
    {
      "caption": "Figure 3: Input representation of our proposed framework for the human desire understanding task.",
      "page": 10
    },
    {
      "caption": "Figure 3: illustrates the input representation used in our proposed method",
      "page": 10
    },
    {
      "caption": "Figure 4: Performance analysis and error analysis of our proposed framework for the human desire understanding task. A ✓indicates correctly",
      "page": 24
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "Abstract"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "Desire is a set of human aspirations and wishes that comprise verbal and cognitive aspects that drive human feelings"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "and behaviors, distinguishing humans from other animals. Understanding human desire has the potential\nto be one"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "of the most fascinating and challenging research domains.\nIt\nis tightly coupled with sentiment analysis and emotion"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "recognition tasks.\nIt\nis beneficial\nfor\nincreasing human-computer\ninteractions,\nrecognizing human emotional\nintel-"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "ligence, understanding interpersonal relationships, and making decisions. However, understanding human desire is"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "challenging and under-explored because ways of eliciting desire might be different among humans. The task gets more"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "difficult due to the diverse cultures, countries, and languages. Prior studies overlooked the use of image-text pairwise"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "feature representation, which is crucial for the task of human desire understanding. In this research, we have proposed"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "a unified multimodal transformer-based framework with image-text pair settings to identify human desire, sentiment,"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "and emotion. The core of our proposed method lies in the encoder module, which is built using two state-of-the-art"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "multimodal\ntransformer models. These models allow us to extract diverse features. To effectively extract visual and"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "contextualized embedding features from social media image and text pairs, we conducted joint fine-tuning of\ntwo"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "pre-trained multimodal\ntransformer models: Vision-and-Language Transformer (ViLT) and Vision-and-Augmented-"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "Language Transformer (VAuLT). Subsequently, we use an early fusion strategy on these embedding features to obtain"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "combined diverse feature representations of the image-text pair. This consolidation incorporates diverse information"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "about\nthis task, enabling us to robustly perceive the context and image pair from multiple perspectives. Moreover,"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "we leverage a multi-sample dropout mechanism to enhance the generalization ability and expedite the training pro-"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "cess of our proposed method. To evaluate our proposed approach, we used the multimodal dataset MSED for\nthe"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "human desire understanding task. Through our experimental evaluation, we demonstrate that our method excels in"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "capturing both visual and contextual information, resulting in superior performance compared to other state-of-the-art"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "techniques. Specifically, our method outperforms existing approaches by 3% for sentiment analysis, 2.2% for emotion"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "analysis, and approximately 1% for desire analysis."
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "Keywords: Human desire understanding, desire analysis, sentiment analysis, emotion analysis, multimodal"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "transformer"
        },
        {
          "cDepartment of Computer Science, Southern Illinois University, Carbondale, Illinois, 62901, USA": "Preprint submitted to arxiv\nOctober 24, 2023"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.\nIntroduction": "Social media platforms such as Twitter, Reddit, Facebook, and Instagram, have increasingly become the most"
        },
        {
          "1.\nIntroduction": "widely used means of\ninformation sharing due to their practical\nfeatures and real-time behavior\n(Camacho et al.,"
        },
        {
          "1.\nIntroduction": "2020).\nPeople typically present\ntheir\nthoughts, opinions, breaking news, and concepts using various information"
        },
        {
          "1.\nIntroduction": "modalities such as text, visual, and audio (Afyouni et al., 2022). The reliability of social media makes it an engaging"
        },
        {
          "1.\nIntroduction": "source of information for researchers and business organizations to discover knowledge (Zhang et al., 2022b). Over"
        },
        {
          "1.\nIntroduction": "the last decade, text-based sentiment analysis (Xu et al., 2019; Alamoodi et al., 2021; Rani et al., 2022) and emotion"
        },
        {
          "1.\nIntroduction": "recognition (Sailunaz and Alhajj, 2019; Estrada et al., 2020; Khoshnam and Baraani-Dastjerdi, 2022)\nfrom social"
        },
        {
          "1.\nIntroduction": "media have advanced quickly and drawn a lot of interest from both academia and industry. Multimodal sentiment"
        },
        {
          "1.\nIntroduction": "analysis (Morency et al., 2011; Soleymani et al., 2017; Zhu et al., 2022) and emotion recognition (Poria et al., 2016;"
        },
        {
          "1.\nIntroduction": "Ranganathan et al., 2016; Middya et al., 2022) gained greater\ninterest because of\ntheir multimodal nature which"
        },
        {
          "1.\nIntroduction": "makes them more understandable. However, understanding human sentiments and emotions across several modalities"
        },
        {
          "1.\nIntroduction": "is challenging because of the complexity of human feelings and expressions."
        },
        {
          "1.\nIntroduction": "Desire, a fundamental aspect of human nature, encompasses a strong feeling of craving or\nlonging for some-"
        },
        {
          "1.\nIntroduction": "thing (Dong et al., 2010).\nIt distinguishes humans from other animals, as they are uniquely driven by the desire to"
        },
        {
          "1.\nIntroduction": "acquire or possess something, and can exhibit an unquenchable thirst. This desire motivates individuals to act and"
        },
        {
          "1.\nIntroduction": "work toward their goals (Wilt and Revelle, 2015).\nIt\nis an innate sense that permeates all human beings and holds"
        },
        {
          "1.\nIntroduction": "power to shape and influence various aspects of human behavior."
        },
        {
          "1.\nIntroduction": "Desires often drive human sentiment and emotions, whether it be the desire for inspiration, anticipation, letdown,"
        },
        {
          "1.\nIntroduction": "jealousy, or obsession. These emotions significantly shape our practical\nlife experiences,\ninfluencing our thoughts,"
        },
        {
          "1.\nIntroduction": "actions, and decisions. There is a close connection between human desire, sentiment, and emotion. Desire implicitly"
        },
        {
          "1.\nIntroduction": "governs sentiment and emotion, while sentiment and emotion,\nin turn, can be influenced by desire. Together, desire,"
        },
        {
          "1.\nIntroduction": "sentiment, and emotion form interconnected and essential components of the human experience, driving our actions"
        },
        {
          "1.\nIntroduction": "and decisions."
        },
        {
          "1.\nIntroduction": "Multimodal desire understanding is an emerging task that plays an important role in recognizing human emotional"
        },
        {
          "1.\nIntroduction": "intelligence and personalized and effective human-computer\ninteraction,\nleading to improved customer satisfaction"
        },
        {
          "1.\nIntroduction": "and experience in e-commerce. However,\nthe multifaceted nature of human desire makes it difficult\nto understand it"
        },
        {
          "1.\nIntroduction": "across several modalities. Jia et al. (2022) introduced the first benchmark dataset MSED focusing on the multimodal"
        },
        {
          "1.\nIntroduction": "human desire understanding task. They designed the task to consist of\nthree subtasks - desire analysis, sentiment"
        },
        {
          "1.\nIntroduction": "analysis, and emotion analysis. The examples of the human desire understanding task in Figure 1 indicate that three"
        },
        {
          "1.\nIntroduction": "classification labels are required for a given image,\ntitle, and caption. A couple climbing a mountain is depicted by"
        },
        {
          "1.\nIntroduction": "the image and text combination in this figure from example A. To satisfy their curiosity,\nthey climb to the top of the"
        },
        {
          "1.\nIntroduction": "∗Corresponding author"
        },
        {
          "1.\nIntroduction": "Email addresses: aziz.abdul.cu@gmail.com (Abdul Aziz), nihad@cu.ac.bd (Nihad Karim Chowdhury), akabir@csu.edu.au"
        },
        {
          "1.\nIntroduction": "(Muhammad Ashad Kabir), nowshed@cu.ac.bd (Abu Nowshed Chy), mdjawad.siddique@siu.edu (Md. Jawad Siddique)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "• We propose a unified architecture of multimodal\ntransformer models, called MMTF-DES (Multimodal Trans-"
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "formers Fusion for Desire, Emotion, and Sentiment analysis) for the multimodal human desire understanding"
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "task. In this proposed architecture, we jointly fine-tune two state-of-the-art pre-trained multimodal transformer"
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "models, Vison-and-Language Transformer\n(ViLT)\n(Kim et al., 2021) and Vision-and-Augmented-Language"
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "Transformer (VAuLT) (Chochlakis et al., 2022), creating a unified architecture."
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "• We adopt a training strategy known as multi-sample dropout\nto enhance the generalizability and training effi-"
        },
        {
          "human desire understanding research gaps. The key contributions of this paper are:": "ciency of the base model. This strategy helps improve the overall performance of the MMTF-DES model."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "task aims to deepen our understanding of human desire and facilitate more effective analysis in this domain."
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "The rest of\nthe paper\nis organized as follows: Section 2 provides an overview of prior\nresearch that motivates us"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "to work in this domain. We raise some research questions that have driven our work on this task in Section 3.\nIn"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "Section 4, we introduce our proposed fusion of a multimodal transformer-based approach for the multimodal human"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "desire understanding task. We discuss experimental settings in Section 5. Section 6 includes the detailed experiments"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "and evaluation and a comparative performance analysis of the related methods. We provide some precious insights"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "for this task in Section 7. We conclude our work and discuss future directions in Section 8."
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "2. Related work"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "Multimodal sentiment analysis and emotion recognition pertain to analyzing a person’s or group’s emotions and"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "sentiments using modalities,\nincluding text, speech, facial expressions, and body language. Because of its potential"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "uses in several\nindustries,\nincluding healthcare, education, and social media monitoring,\nthis discipline has recently"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "attracted increasing interest (Ara˜no et al., 2021)."
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "In earlier stages,\ntextual sentiment analysis (Zhou et al., 2013; Kiritchenko et al., 2014) and emotion recogni-"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "tion (Lee et al., 2012; Shaheen et al., 2014; Kratzwald et al., 2018) were very popular. For example, Zhou et al. (2013)"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "and Gamallo et al. (2014) used lexicon-based features and statistical machine learning classifiers,\nincluding support"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "vector machine (SVM) and Na¨ıve-Bayes, for the sentiment analysis of English tweets. Oberl¨ander and Klinger (2018)"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "introduced a dataset and provided insight into different models for emotion classification on text. A recent study used"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "the BERT model for sentiment analysis on tweet\ntext of coronavirus data (Singh et al., 2021). Recently, multimodal"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "sentiment analysis and emotion recognition have gained the attention of researchers because of their multimodal na-"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "ture. Truong and Lauw (2019) explored the use of deep learning models for multimodal sentiment analysis. They"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "proposed a visual aspect attention network (VistaNet) that could effectively incorporate multiple modalities, including"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "text and image,\nfor sentiment analysis. Mai et al.\n(2022) proposed a hybrid contrastive learning framework of\ntri-"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "modal representation,\nincluding text,\nimage, and speech, for multimodal sentiment analysis. To reduce the modality"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "gap,\nthey conducted intra-inter-modal contrastive learning and semi-contrastive learning so that\nthe model can ex-"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "plore cross-modal interactions. Yang et al. (2022) proposed a multimodal translation for sentiment analysis (MTSA)"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "leveraging text, visual, and audio modalities. To improve the quality of visual and audio features, they translate them"
        },
        {
          "• We introduce a new task as an extension of human desire understanding research on the MSED dataset. This": "into text features using BERT. Peng et al. (2023) represented a modal\nlabel-based multi-stage method for sentiment"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "multi-stage training."
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "For a long time, emotion recognition has been a topic of ongoing research. With the growth of image-text data"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "owing to the emergence of social media platforms, multimodal emotion recognition on social media data has recently"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "gained popularity.\nSoleymani et al.\n(2011) applied a modality fusion strategy, using a support vector machine as"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "the classifier.\nPoria et al.\n(2016) used deep convolutional neural networks to extract\nfeatures of visual and textual"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "modalities. Ranganathan et al. (2016) tried convolutional deep belief networks (CDBNs) that generate robust multi-"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "modal features of emotion expressions. Nemati et al. (2019) introduced a hybrid approach to latent space data fusion"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "in which the textual modality is combined with the auditory and visual modalities using a Dempster-Shafer\n(DS)"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "theory-based evidentiary fusion methodology, and their projected characteristics are combined using a latent space"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "linear map. Recently, Zhang et al. (2022c) used the manifold learning method to extract low-dimensional embedding"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "features from high-dimensional features. Then,\nthese features are fed into the deep convolutional neural network for"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "emotion recognition. To learn a joint multimodal representation for emotion recognition, Le et al. (2023) fuse multiple"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "modalities including video frames, audio signals, and text subtitles through a unified transformer architecture."
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "Desire is inherently related to human sentiment and emotion, and understanding these relationships can provide"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "insights into human behavior and decision-making, which drive the sentiments and emotions of humans.\nJia et al."
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "(2022) argue that\nthere exists a significant\ninterrelation between human desire,\nsentiment, and emotion, wherein"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "desire holds a surreptitious hegemony over sentiment and emotion, while sentiment and emotion are subject\nto the"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "influence of desire. However, human desire understanding is an under-explored task. Human desire involves both"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "linguistic expression and visual expression.\nLim et al.\n(2012) developed a system for desire understanding based"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "on four\ntypes of emotions - speed,\nintensity,\nregularity, and extent\nfrom human emotional voices.\nTo investigate"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "the variations and similarities in the association between sexual desire and love, Cacioppo et al. (2012) introduced a"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "multilevel kernel density fMRI analytic technique. Yavuz et al. (2019) proposed a data mining approach for desire"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "and intention using neural and Bayesian networks. However,\nthose works have some limitations in representing the"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "human desire understanding task effectively. To break the research gap in the understanding and detection of human"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "desire, Jia et al. (2022) proposed the first multimodal dataset MSED for human desire understanding. They represent"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "three tasks - desire analysis, sentiment analysis, and emotion analysis in this dataset which contains textual and visual"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "modalities. This facilitates a new window in the human desire understanding research. They also provide various"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "strong baselines based on different combinations of feature representations using various visual and textual encoders."
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "The unified architecture of the BERT and ResNet model achieves top performance for them."
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "However, most of the systems struggled to achieve intra-and-inter relationships between modalities as they fused"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "different modalities externally. These systems do not properly capture the semantic orientation of the textual\ninfor-"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "mation and visual\nrepresentation to estimate human desire understanding. To learn the intra-and-inter\nrelationship"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "between modalities, pairwise training of different data,\nincluding visual and textual,\nis crucial for the human desire"
        },
        {
          "analysis. They consider different modalities including unimodal, bimodal, and multimodal, as independent\ntasks for": "understanding task. Transformer-based systems distill contextual and visual\ninformation effectively. However,\nthe"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "inputs and joint\ntraining of image-text\ninput struggles to distill\nintra-relationships between modalities. To mitigate"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "this issue, we use multimodal transformer models with the pairwise training of text and images, which enables us to"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "achieve the intra-relationship between image and text. We fuse two multimodal transformer models to extract diverse"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "integrated visual-contextual features representation, which helps us achieve the inter-relationship of two modalities."
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "Moreover, we use a multi-sample dropout training strategy to improve the generalizability of our proposed model."
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "3. Research questions"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "To contribute to the state-of-the-art\nin the newly introduced multimodal human desire understanding task, we"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "propose a unified multimodal neural network architecture to understand the human desires from the given image"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "and the associated text. Also, our work involves identifying which integrated visual-contextual\nrepresentation and"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "techniques perform better. Therefore, we articulate some research questions (RQs) related to the task of human desire"
        },
        {
          "fusion of a pre-trained vision and text\ntransformer-based multimodal model without\nthe pairwise training of the text": "understanding."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": "— We fuse two model"
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": "• RQ3:"
        },
        {
          "understanding.": "understanding methods?"
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": "task?"
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": "standing task?"
        },
        {
          "understanding.": ""
        },
        {
          "understanding.": "of this task description and performance are analyzed in 6.5."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Fusion"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Image-text"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Text Inputs"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Pair Input"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Predicted Label"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Figure 2: Overview of our proposed framework for the human desire understanding task. To generate the integrated visual-contextualized feature"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "vectors,\nthe multimodal\ntransformer model\ntakes the input of\nthe image-text pair.\nThe concatenation of\ntwo model\nfeature vectors creates the"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "multimodal fusion architecture to get the diverse features of the task. Then, a classification head predicts the desire, sentiment, and emotion label."
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "In addition, we present a summarized analysis of these five RQs in Section 7.3 to analyze the potency of our proposed"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "method."
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "4. Proposed framework"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Vision-and-language pre-training (VLP)\ntransformers performed better\nin the vision-and-language multimodal"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "task.\nTo take advantage of VLP, we jointly fine-tuned two SOTA multimodal\ntransformer models for\nthe human"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "desire understanding task. An overview of our proposed model for the human desire detection framework is depicted"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "in Figure 2. The model is trained on the MSED dataset."
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "Problem definition:\nGiven a text pair contains title t and caption c and a corresponding image i of a tweet,"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "classify the desire as d of the tweet as ϵ {− f amily, romance, vengeance, curiosity, tranquility, social − contact, none},"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "emotion as e ϵ {happiness,\nsad, neutral, disgust, anger,\nf ear}, and sentiment as s ϵ {positive, negative, neutral}."
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "{\nGiven a training set\n.\n.\n.\n,\n(tn, cn,\nin, dn, en,\nsn)} of n samples, our model’s objective is\n(t1, c1,\ni1, d1, e1,\ns1),"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "in Equation 1 that\n(which\nto maximize the function f (λd),\nf (λd), and f (λd)\ntake up a new instance denoted as itk"
        },
        {
          "TOK M\n[SEP]\nTOK 1\n[CLS]\nTOK N\n[SEP]\nTOK 1": "labels, respectively.\ncontains tk, ck, ik) to fitting desire dk, emotion ek, and sentiment sk"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "n": "(cid:89)"
        },
        {
          "n": "\n\nf (λs) = arg max\nP(sk\n| (tk, ck, ik)it; λ)"
        },
        {
          "n": "λ"
        },
        {
          "n": "k=0"
        },
        {
          "n": "is the input\nWhere itk\ntexts-image pair whose desire label dk, emotion label ek, and sentiment\nlabel sk are to be"
        },
        {
          "n": "predicted.\nP denotes the log-likelihood function and the λ symbol denotes the method’s parameters we intend to"
        },
        {
          "n": "optimize."
        },
        {
          "n": "Given an input image and texts are paired and transformed to the correct data formats, we feed them into the two"
        },
        {
          "n": "multimodal\ntransformer models’ processors. We use two SOTA multimodal\ntransformer models, ViLT and VAuLT,"
        },
        {
          "n": "tuned on image-text pair settings to generate the image-text pair encoding. Each encoding is fed into the correspond-"
        },
        {
          "n": "ing multimodal transformer model to get integrated visual-contextual feature vectors of each model. To get the diverse"
        },
        {
          "n": "features representations of human desire understanding, we use a concatenation-based early fusion method on top of"
        },
        {
          "n": "the two models’ feature vectors. We jointly fine-tune this unified architecture to capture the domain-specific infor-"
        },
        {
          "n": "mation of human desire understanding explicitly. To improve the generalizability and speed up the training of our"
        },
        {
          "n": "proposed method, we use a multi-sample dropout\ntraining strategy. Then, a classification head is plugged on top of"
        },
        {
          "n": "the multimodal unified architecture to predict the appropriate task-specific class label of the input image and texts."
        },
        {
          "n": "4.1. Multimodal transformer models"
        },
        {
          "n": "The BERT-based transformer model\nfacilitates the learning of\nlong-term dependency by handling the sentence"
        },
        {
          "n": "as a whole rather\nthan word by word. A combination of multi-head attention and positional embedding processes"
        },
        {
          "n": "provides the necessary information regarding the relationship between words. To enable deep bidirectional represen-"
        },
        {
          "n": "tations of context, BERT uses masked language modeling. This model incorporates the feed-forward neural network"
        },
        {
          "n": "architecture with a self-attention mechanism which helps distill contextual\ninformation more efficiently.\nIn Vision"
        },
        {
          "n": "Transformer (ViT), it divides an image into fixed-size patches, converts each patch into a linear embedding sequence,"
        },
        {
          "n": "adds position embeddings, and feeds this sequence of vectors to a transformer encoder.\nIt helps the ViT model\nto"
        },
        {
          "n": "learn pixel-label\ninformation of the visual content effectively. Multimodal\ntransformer models,\nincluding the ViLT"
        },
        {
          "n": "and VAuLT models, use the unified architecture of the BERT-based text encoder and ViT image encoder in their back-"
        },
        {
          "n": "bone. Thus,\nit helps the model recognize the long-term dependency of context and establishes an intra-relationship"
        },
        {
          "n": "between image and context. This relationship is critical for the human desire understanding task. We used two SOTA"
        },
        {
          "n": "VLP transformer models, ViLT and VAuLT,\nto extract\nthe integrated features vectors of the image-text pair for the"
        },
        {
          "n": "human desire understanding task on social media data. Our multimodal approach for the human desire understanding"
        },
        {
          "n": "task is based on ViLT as the backbone since it reflects the common design of multimodal transformers. Unlike other"
        },
        {
          "n": "multimodal transformer models (VisualBERT), ViLT does not depend on modality-specific submodels for extracting"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "Language Modeling (MLM)."
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "4.1.1. Vision-and-Language Transformer (ViLT)"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "The Vision-and-Language Transformer (ViLT) (Kim et al., 2021) processes visual\ninputs like text\ninputs, which"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "implies a convolution-free method of processing visual data.\nThe modal-specific components of ViLT are a class"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "of VLPs that are less intensive to compute for multimodal\ninteractions than the transformer component. ViLT is a"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "parameter-efficient model that is much faster than other VLP models with region features. While it performs similarly"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "or is even superior on downstream tasks, including VQA, image-text matching, multimodal classification, and multi-"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "modal entailment of vision-and-language, it is also at least four times faster than those models with grid features. To"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "take advantage of the linear modality interaction, we use ViLT for the multimodal human desire understanding task. It"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "helps ViLT to distill the relationships between image and text, which are crucial for the human desire detection task."
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "To tokenize input\ntext, ViLT uses the bert-base-uncased 1 tokenizer. To encode the input\nimage, ViLT uses the ViT-"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "B/32 2 vison transformer model.\nIt uses weights from ViT-B/32 pre-trained on ImageNet.\nIt consists of 12 layers of"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "transformer, with MLP size 3072, hidden size 768, patch size 32, and 12 attention head layers. We use HuggingFace’s"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "implementation of the vilt-b32-mlm (Kim et al., 2021) checkpoint 3 of the ViLT model."
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "4.1.2. Vision-and-Augmented-Language Transformer (VAuLT)"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "The Vision-and-Augmented-Language Transformer (VAuLT) (Chochlakis et al., 2022) expands the well-known"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "VLP model ViLT. To improve the model’s capacity to capture complex text features,\nthe VAuLT model uses a mod-"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "ified version of the ViLT architecture that\nincorporates an extra text encoder module. Because ViLT was initialized"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "from ViT,\nit\nlacks language understanding capabilities. VAuLT addresses this issue by substituting ViLT’s language"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "embedding inputs with linguistic features taken from a large LM pre-trained on more linguistic data varieties, which"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "may also be chosen to better meet the demands of the downstream application. Training the LM and ViLT together in"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "VAuLT improves the results over ViLT and achieves state-of-the-art or equivalent performance on various VL tasks"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "incorporating richer language inputs and effective constructs. This motivates us to use the VAuLT model\nin our pro-"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "posed framework for the human desire understanding task. That is why we use VAuLT along with ViLT to -capture the"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "diversity of features.\nIn VAuLT, we can use any language model to produce the sequence of embeddings which may"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "be input into the ViLT architecture. To tokenize input text, we employ the bertweet-base tokenizer. Due to the social"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "media data used in MSED,\nthe BERTweet may learn contextual\ninformation effectively. Thus, we use BERTweet\nto"
        },
        {
          "features, and multiple objectives are used to pre-train the model,\nincluding Image Text Matching (ITM) and Masked": "get diverse textual features representation in VAuLT 4 for human desire understanding task. We utilize HuggingFace’s"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "implementation of the vilt-b32-mlm (Kim et al., 2021) checkpoint of the ViLT model and vinai/bertweet-base (Nguyen"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "et al., 2020) checkpoint 5 as the language model in the VAuLT model training."
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "4.2.\nInput representation of multimodal transformer model"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "In our proposed MMTF-DES method, we use two SOTA multimodal\ntransformer models - ViLT and VAuLT."
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "We now describe the input representation of these models. MSED contains two text\ninputs, a title and caption, and"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "one visual\ninput, a corresponding image. Figure 3 illustrates the input representation used in our proposed method"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "for representing the input data of MSED. We use an image-text pair training concept\nin the multimodal\ntransformer"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "models to train the human desire understanding task. The idea behind this concept is to combine the input image, title,"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "and caption into one sequence in the multimodal transformer model’s processor. Essentially, the caption and title pairs"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "in the example are represented as a single sequence in which a special classification token [CLS] is inserted at\nthe"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "beginning of the first input (the caption), and a special separation token [SEP] is added between the caption and title to"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "separate them. Therefore, the text-pair input sequence of multimodal transformer for our human desire understanding"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "task is like “[CLS] title [SEP] caption [SEP]” and the input image is sliced into patches where the patches denote as"
        },
        {
          "Figure 3: Input representation of our proposed framework for the human desire understanding task.": "PTOK (patch token) and are flattened into the processor with a special classification token [CLS] at the beginning."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.3. Multimodal transformers early fusion": "The fusion of the models is an effective technique and common practice in multimodal\nlearning that could im-"
        },
        {
          "4.3. Multimodal transformers early fusion": "prove the model’s learning accuracy and robustness more than individual models. To benefit\nfrom the diversity of"
        },
        {
          "4.3. Multimodal transformers early fusion": "feature representations, we jointly fine-tune two SOTA VLP transformers, the ViLT and VAuLT models on the MSED"
        },
        {
          "4.3. Multimodal transformers early fusion": "dataset. In the case of multimodal models, an early fusion technique improves performance and generalizability. The"
        },
        {
          "4.3. Multimodal transformers early fusion": "advantage of early fusion is that\nit facilitates the exchange of information between the models at an earlier stage in"
        },
        {
          "4.3. Multimodal transformers early fusion": "the network, resulting in a better representation of the input data as a whole. We concatenate the pooler outputs of the"
        },
        {
          "4.3. Multimodal transformers early fusion": "ViLT and VAuLT models at an early stage, allowing the subsequent layers of the network to refine and combine these"
        },
        {
          "4.3. Multimodal transformers early fusion": "features in a more integrated and optimized manner. This enables us to get\nthe two models integrated with visual-"
        },
        {
          "4.3. Multimodal transformers early fusion": "contextual feature vectors. With the fusion strategy, we have consolidated the diverse information of the human desire"
        },
        {
          "4.3. Multimodal transformers early fusion": "understanding task from these models. The same taxonomy of two multimodal transformer models with different text"
        },
        {
          "4.3. Multimodal transformers early fusion": "encoders improves the robustness and strength of our proposed model. This allows us to perceive the context and"
        },
        {
          "4.3. Multimodal transformers early fusion": "image pair from all aspects in a robust manner."
        },
        {
          "4.3. Multimodal transformers early fusion": "4.4. Multi-sample dropout training strategy"
        },
        {
          "4.3. Multimodal transformers early fusion": "With a multi-sample dropout-based training strategy,\nthe base model\nis trained faster and generalizability is im-"
        },
        {
          "4.3. Multimodal transformers early fusion": "proved,\nleading to the enhanced overall performance of the system (Inoue, 2019; Hahn and Choi, 2020; Pan et al.,"
        },
        {
          "4.3. Multimodal transformers early fusion": "2021). During learning, the model is forced to learn a more generalizable representation that different sets of neurons"
        },
        {
          "4.3. Multimodal transformers early fusion": "are dropped out randomly for every input example. Therefore, it helps the base model avoid overfitting. To optimize"
        },
        {
          "4.3. Multimodal transformers early fusion": "the model, it calculates the loss for some dropout samples and averages them, resulting in a smaller loss.\nIn our pro-"
        },
        {
          "4.3. Multimodal transformers early fusion": "posed method, the multi-sample dropout training strategy is used on top of the unified multimodal transformer-based"
        },
        {
          "4.3. Multimodal transformers early fusion": "architecture, where we employ a sample size of three dropout layers in the training process. In essence, we are dupli-"
        },
        {
          "4.3. Multimodal transformers early fusion": "cating the features vector of the unified architecture after the dropout layer is applied and sharing the weights among"
        },
        {
          "4.3. Multimodal transformers early fusion": "these duplicated layers which are fully connected. Training can be accelerated since more models are evaluated in the"
        },
        {
          "4.3. Multimodal transformers early fusion": "same time, resulting in faster convergence. To obtain the final loss, we aggregate the loss obtained from each sample"
        },
        {
          "4.3. Multimodal transformers early fusion": "and take their average. This way, the model can more quickly converge to a better model that generalizes well to new"
        },
        {
          "4.3. Multimodal transformers early fusion": "data as it learns more generalizable features. Later, a task-specific classification head on top predicts the appropriate"
        },
        {
          "4.3. Multimodal transformers early fusion": "desire, sentiment, and emotion labels for the corresponding image-text pair."
        },
        {
          "4.3. Multimodal transformers early fusion": "4.5. Fine-tuning procedure"
        },
        {
          "4.3. Multimodal transformers early fusion": "Fine-tuning multimodal transformer models may improve performance because it allows the model to be adapted"
        },
        {
          "4.3. Multimodal transformers early fusion": "to domain-specific knowledge. To achieve this, we jointly fine-tune the two multimodal transformer models described"
        },
        {
          "4.3. Multimodal transformers early fusion": "in Section 4.1 using the domain-specific datasets. For diverse integrated contextual-visual feature vectors, a single"
        },
        {
          "4.3. Multimodal transformers early fusion": "concatenate layer\nis all we need. We train the human desire understanding task as an image-text-pair\ntask where"
        },
        {
          "4.3. Multimodal transformers early fusion": "the title becomes the first\ntext and the caption the second text\nin the input sequence. Additionally,\nthe image is"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the third input of our proposed method. During fine-tuning, all other hyper-parameters remain the same as during": "the multimodal"
        },
        {
          "the third input of our proposed method. During fine-tuning, all other hyper-parameters remain the same as during": "The final hidden states of each model are then used to obtain a fixed-dimensional feature representation of the input"
        },
        {
          "the third input of our proposed method. During fine-tuning, all other hyper-parameters remain the same as during": "sequence. Upon constructing the combined features vector of our method, we use an early fusion concatenation"
        },
        {
          "the third input of our proposed method. During fine-tuning, all other hyper-parameters remain the same as during": "technique and then apply a multi-sample dropout technique to obtain the task-specific predicted class label."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "the authors featured three multi-class classification tasks - sentiment analysis, emotion analysis, and desire analysis."
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "In the sentiment analysis task, we need to determine positive, neutral, or negative sentiment classes in the text and"
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "corresponding image. Based on the image-text pair, we need to classify each instance as one of six classes - happiness,"
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "sad, neutral, disgust, anger and fear in the emotion analysis task. The six typical human desires - family, romance,"
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "vengeance, curiosity,"
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "6,127 training instances, 1,021 validation instances, and 2,024 test"
        },
        {
          "combinations gathered from various of social media, including Twitter, Getty Image, and Flickr. In the MSED dataset,": "dataset are presented in Table 1."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Label"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Positive"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Neutral"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Negative"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Happiness"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Sad"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Neutral"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": ""
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Disgust"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Anger"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Fear"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Vengeance"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Curiosity"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Social-contract"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Family"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Tranquility"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Romance"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Label"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Positive"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Neutral"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Negative"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Happiness"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Sad"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Neutral"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": ""
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Disgust"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Anger"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Fear"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Vengeance"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Curiosity"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Social-contract"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Family"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Tranquility"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "Romance"
        },
        {
          "Table 1: Dataset statistics of MSED according to each label on the corresponding task.": "None"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Hyper-parameters"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Training batch size"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Test batch size"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Max length"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Learning rate"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Number of epochs"
        },
        {
          "Table 2: The hyper-parameters search space of our proposed MMTF-DES method.": "Dropout"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "metric is used to understand how many times the model could detect a specific desire/emotion/sentiment category."
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "The macro average category of those evaluation metrics was used because MSED is an imbalanced dataset that helps"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "treat each class equally. The macro average F1-score is the primary evaluation measure in our analysis because it gives"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "summarized information on both precision and recall metrics. We also report\nthe results based on other evaluation"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "measures too."
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "5.3. Model configuration"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "We describe the hyper-parameter settings for our proposed MMTF-DES model\nin this section. To enhance the"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "efficiency of parallel computation of tensor image-text pairs of training, we used PyTorch to design and implement our"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "proposed method. As part of this study, all experiments were carried out on the Google Colaboratory platform (Bisong,"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "2019). The optimal hyper-parameters were selected using a grid search method based on the validation dataset. The"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "hyper-parameters search space is illustrated in Table 2."
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "We used two pre-trained multimodal\ntransformer models (Wolf et al., 2020), VAuLT and ViLT, as multimodal"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "encoders\nto extract effective integrated contextual-visual\nfeatures\nin our proposed MMTF-DES model.\nPrior\nre-"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "search (Aziz et al., 2021)\nreveals that fine-tuning the hyper-parameters of\nthose models makes them consistently"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "outperform the pre-trained models for downstream tasks. Several hyper-parameters, including training batch size, test"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "batch size, learning rate, dropout, and epochs, were fine-tuned to get the optimal value. We train our proposed method"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "with the provided training data with the validation data for efficient\ntraining. We trained VAuLT and ViLT models"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "using 5 epochs and set\nthe learning rate to nearly the same as 3e-5. The best settings of these hyper-parameters are"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "reported in Table 3. We used the vilt-b32-mlm checkpoint of the ViLT model and vilt-b32-mlm and the vinai/bertweet-"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "base checkpoint for the VAuLT model. We used the CUDA-enabled GPU and set\nthe fixed manual seed to generate"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "reproducible results. During training, we saved our model based on the best validation loss by evaluating the validation"
        },
        {
          "Dropout\n{0.1, 0.2, · · ·\n, 0.8}": "set. To reduce the noisy features and avoid overfitting, we fine-tuned the dropout rate hyper-parameter."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "Sentiment Analysis"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "4"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "1"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "3e-3"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "40"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "0.5"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "0.1, 0.2, 0.3"
        },
        {
          "Table 3: The optimal value of hyper-parameters used in our proposed MMTF-DES system.": "5"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "Number of epochs\n5\n5\n5"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "6. Experimental results and analysis"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "We now evaluate the performance of our proposed MMTF-DES approach. The objectives of our experimental"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "design are six-fold:\n(1) we analyze different modality baseline models to choose the best model (2) we examine the"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "performance of our used fusion strategies to pick the most effective one that we used in all\nthe later experiments"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "(RQ2);\n(3) we analyze the overall performance and task-wise performances of our proposed method on the MSED"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "dataset; and (4) we determine the performance of\nindividual multimodal\ntransformer models used in our proposed"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "approach (RQ1). (5) We proposed a new desire analysis task where we describe the task and evaluate our method on"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "this task (RQ5). (6) We provide a comparative performance analysis between our proposed method and other current"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "SOTA (RQ3)."
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "6.1. Analysis of different modality baseline models"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "We present different modality baseline methods analysis on the human desire understanding task that drove this"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "work. We consider three categories of modality in the baseline analysis discussion. From the textual modality, we"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "consider the BERTweet\ntransformer model since text data are annotated from tweet data.\nIn the visual modality, we"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "consider the ResNet model as it has outstanding performance in many image classification tasks.\nIn the multimodal"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "category, we use the ViLT multimodal transformer model for the human desire understanding task to take advantage"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "of its linear modality interaction (text and image modality treated with the same priority)."
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "The results of the various modality baseline experiments are presented in Table 4. To show the baseline methods"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "performance on the MSED dataset, we show the performance of all\nthree subtasks - sentiment analysis, emotion"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "analysis, and desire analysis.\nIn sentiment analysis,\nthe BERTweet model achieves 82.49,\nthe ResNet model 70.64,"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "and the ViLT model 85.81 in terms of the primary evaluation measure macro-averaged F1 score. This indicates for the"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "multimodal\ntransformer model,\nthe ViLT performance is 3.87% and 17.68% higher than the BERTweet and ResNet"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "model performance, respectively. In emotion analysis, the BERTweet model achieves 78.34, the ResNet model 56.40,"
        },
        {
          "Multi-sample dropout\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3\n0.1, 0.2, 0.3": "and the ViLT model 80.81 in terms of the primary evaluation measure macro-averaged F1 score. This indicates for"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "Method"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "BERTweet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ResNet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": ""
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ViLT"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "BERTweet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ResNet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": ""
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ViLT"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "BERTweet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ResNet"
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": ""
        },
        {
          "Table 4: Performance of different modality baseline models on the MSED dataset.": "ViLT"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "Desire Analysis"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "ViLT\nMultimodal\n81.23\n75.16\n77.78"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "the multimodal transformer model, the ViLT performance is 3.05% and 30.2% higher than the BERTweet and ResNet"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "model performance,\nrespectively.\nIn desire analysis,\nthe BERTweet model achieves 78.86,\nthe ResNet model 49.2,"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "and the ViLT model 77.78 in terms of the primary evaluation measure macro-averaged F1 score. This indicates for"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "the multimodal transformer model, the ViLT gives a similar performance to the BERTweet and a 36.75% higher score"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "than the ResNet model performance.\nThis validates the significance of\nthe multimodal\ntransformers model\nin the"
        },
        {
          "ResNet\nImage\n49.97\n49.35\n49.20": "human desire understanding task."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 5: illustrates the impact of",
      "data": [
        {
          "the results are reported following this setting.": ""
        },
        {
          "the results are reported following this setting.": "Task"
        },
        {
          "the results are reported following this setting.": "Sentiment analysis"
        },
        {
          "the results are reported following this setting.": "Emotion analysis"
        },
        {
          "the results are reported following this setting.": "Desire analysis"
        },
        {
          "the results are reported following this setting.": "Average"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "Method"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "Sentiment analysis task"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "VAuLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "ViLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "MMTF-DES (proposed)"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "Emotion analysis task"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "VAuLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "ViLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "MMTF-DES (proposed)"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "Desire analysis task"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "VAuLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "ViLT"
        },
        {
          "Table 7: Individual component performance of our proposed MMTF-DES model for the MSED dataset.": "MMTF-DES (proposed)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "84.23\n83.11\n86.97\nMMTF-DES (proposed)\n82.01"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "6.4.\nImpact of individual multimodal transformer models"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "We further analyze the performance of our proposed MMTF-DES method by evaluating the performance of indi-"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "vidual multimodal\ntransformer models. We only retain one multimodal\ntransformer model at a time and discard the"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "other model to do this. To examine the component analysis of our proposed method, we used the MSED dataset for all"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "three tasks, and the evaluation results are illustrated in Table 7. To capture diverse integrated contextual-visual repre-"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "sentation, we incorporate two multimodal transformer models - ViLT and VAuLT. We apply an early fusion at the very"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "early stage of the network i.e., the feature’s label fusion. Such fusion is crucial for learning pixels-based information"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "in visual content and semantic information in context from two modalities in an integrated manner. Jointly trained our"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "proposed unified multimodal architecture performed better than the individual models across all\nthree tasks (RQ1)."
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "In the sentiment analysis task, the ViLT model performs better than the VAuLT model. However, MMTF-DES shows"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "a 2.97% performance improvement over ViLT. The VAuLT model performs better than the ViLT model\nin the emo-"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "tion analysis and desire analysis tasks. Nevertheless, our proposed MMTF-DES method achieved a 3.32% and 2.0%"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "higher macro averaged F1-score on the emotion analysis and desire analysis tasks, respectively. This highlights the"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "importance of using diverse multimodal transformer encoders in our proposed method."
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "6.5. Binary desire analysis"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "To further extend this task, we propose a new task for the human desire understanding task (RQ5). It is important"
        },
        {
          "ViLT\n81.23\n75.16\n77.78\n83.05": "to work on the human desire analysis task - we first need to identify whether\nthe image-text pair contains human"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Test\n1011\n1031": "desire. Then,\nif the pair contains desire,\nthe next\ntask is to identify which type of desire; prior research follows this"
        },
        {
          "Test\n1011\n1031": "trend (Jin et al., 2022). However, the MSED dataset is not designed as a binary desire analysis task together with the"
        },
        {
          "Test\n1011\n1031": "existing multiclass desire analysis task. Thus, we propose a new desire analysis task as a binary classification. To do"
        },
        {
          "Test\n1011\n1031": "this, we consider the Not Desire and Desire class labels in this task for the image-text pair.\nIn the Desire class label,"
        },
        {
          "Test\n1011\n1031": "we use vengeance, curiosity, social contract, family,\ntranquility, and romance label data as Desire class data because"
        },
        {
          "Test\n1011\n1031": "authors (Jia et al., 2022) consider Desire image-text pair data into these six categories, for Not Desire data we consider"
        },
        {
          "Test\n1011\n1031": "the same as authors provided (None class label data of desire analysis task in Table 1). According to this, we distribute"
        },
        {
          "Test\n1011\n1031": "those data in the binary label. Our proposed binary-type desire data statistics are presented in Table 8."
        },
        {
          "Test\n1011\n1031": "To know how well our proposed method identifies desire image-text pair data, we train our proposed method"
        },
        {
          "Test\n1011\n1031": "for the binary desire classification task. For binary desire classification, our proposed MMTF-DSE method achieved"
        },
        {
          "Test\n1011\n1031": "90.21% precision, 90.19% recall, and a 90.21% macro averaged F1-score. Although, our proposed method achieved"
        },
        {
          "Test\n1011\n1031": "an 83.11% macro average F1-score in the multiclass desire analysis task. A higher macro F1-score demonstrated the"
        },
        {
          "Test\n1011\n1031": "applicability and generalizability of our proposed early fusion of a multimodal\ntransformers-based approach for the"
        },
        {
          "Test\n1011\n1031": "human desire understanding tasks."
        },
        {
          "Test\n1011\n1031": "6.6. Comparative performance analysis"
        },
        {
          "Test\n1011\n1031": "6.6.1. Comparing with state-of-the-art study on MSED"
        },
        {
          "Test\n1011\n1031": "To evaluate the performance of our proposed method against the current state-of-the-art techniques, we compared"
        },
        {
          "Test\n1011\n1031": "the performance with the top-performing systems on the MSED dataset. The authors proposed three tasks - desire"
        },
        {
          "Test\n1011\n1031": "analysis, sentiment analysis, and emotion analysis - to evaluate the MSED dataset and present various strong baselines"
        },
        {
          "Test\n1011\n1031": "by combining diverse features fusion (Jia et al., 2022). They used two categories of encoders to extract\nthe text and"
        },
        {
          "Test\n1011\n1031": "image features,\nrespectively. To represent\nthe text,\nthey use three well-known text encoders - deep CNN (DCNN),"
        },
        {
          "Test\n1011\n1031": "bidirectional LSTM (BiLSTM) (Zhang et al., 2020b), and the pre-trained language model BERT (Devlin et al., 2019)."
        },
        {
          "Test\n1011\n1031": "To encode images, they use two widely used visual encoders, i.e., AlexNet (Alom et al., 2018) and ResNet (Szegedy"
        },
        {
          "Test\n1011\n1031": "et al., 2017). Among their modality analysis, the BERT model achieved the best macro averaged F1-score in the textual"
        },
        {
          "Test\n1011\n1031": "modality and ResNet achieved better performance in the visual modality. The fusion of BERT and ResNet learns the"
        },
        {
          "Test\n1011\n1031": "visual and textual information effectively and achieves the best performance among all modalities. Also, they used a"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "model with the other SOTA method on MSED dataset. ∇ denotes an improvement of our proposed method against current SOTA."
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Method"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Sentiment analysis task"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Multimodal Transformers (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "M3GAT (Zhang et al., 2023a)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "BERT+ResNet (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "MMTF-DES (proposed)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "∇ SOTA"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Emotion analysis task"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Multimodal Transformers (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "M3GAT (Zhang et al., 2023a)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "BERT+ResNet (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "MMTF-DES (proposed)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "∇ SOTA"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Desire analysis task"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "Multimodal Transformers (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "BERT+ResNet (Jia et al., 2022)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "MMTF-DES (proposed)"
        },
        {
          "Table 9: Comparative performance (precision (P), recall (R), Macro F1-score, and accuracy: higher is better) analysis of our proposed MMTF-DES": "∇ SOTA"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 10: shows the results of individual model performance on the",
      "data": [
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "the sentiment analysis task, 84.26% on the emotion analysis task, and 83.11% on the desire analysis task, based"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "on the primary evaluation metric macro average F1-score on the MSED dataset. Our proposed MMTF-DES model"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "outperforms the prior best-performing fusion of the BERT-ResNet model by 3% on the sentiment analysis task, 2.2%"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "on the emotion analysis task, and 1% on the desire analysis task (as denoted by ∇ in the Table 9). The comparative"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "performance analysis confirms that an approach that involving the fusion of various multimodal transformers encoders"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "can achieve good performance for the human desire understanding task from image-text across different associated"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "tasks.\nThis validates the effectiveness and applicability of our proposed method on the multimodal human desire"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "understanding task."
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "Those models do not perform as well as our proposed method because they struggle to achieve intra-and-inter"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "relationships between modalities as they fuse different modalities externally. To learn the intra-and-inter relationship"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "between modalities,\nthe pairwise training of different data,\nincluding visual and textual,\nis crucial\nfor\nthe desire"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "understanding task. To mitigate this issue, we use multimodal\ntransformer models with pairwise training of the text"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "and image. This helps us achieve the intra-relationship between image and text. We fuse two multimodal transformer"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "models,\nto extract diverse integrated visual-contextual\nfeatures representation which helps us to achieve the inter-"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "relationship of\ntwo modalities.\nThe early fusion of\ntwo multimodal\ntransformer models results in a better\nfeature"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "representation of\nthe input\nimage-text pair data, which helps the model\nto improve the performance of\nthe human"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "desire understanding tasks.\nIt enables the model\nto capture more complex and subtle relationships in the image-text"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "pair data, which can help improve the model’s generalizability to unseen data. Moreover, we use a multi-sample"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "dropout training strategy to improve the generalizability of our proposed model."
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "7. Discussion"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "In this section, we proffer some research findings into the multimodal human desire understanding research. We"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "provide modality dominance analysis, multi-sample dropout\nimpact, research analysis, and error analysis to analyze"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "the efficacy of our proposed MMTF-DES method in the human desire understanding domain."
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "7.1. Modality dominance"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "To analyze the modality dominance in the multimodal human desire understanding task, we discuss the individual"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "modality model’s result across all\nthree tasks. Table 10 shows the results of\nindividual model performance on the"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "MSED dataset. Here, we consider three text encoders - BiLSTM, BERT, and BERTweet, and two Visual encoders"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "including AlexNet, and ResNet in our modality dominance comparison. Across modalities, we have seen that all text"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "models perform better than all image models by a large margin. Hence, the multimodal human desire understanding"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "task is a text modality-dominant\ntask. This research finding is critical for working on the multimodal human desire"
        },
        {
          "approach outperforms other SOTA models across all\nthree tasks (RQ3). Our proposed method achieves 88.44% on": "understanding task. This modality dominance experiment motivates us to use the same taxonomic two multimodal"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "models with different": "and VAuLT -\nin a unified architecture for",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": "the human desire understanding task. The VAuLT model used the same"
        },
        {
          "models with different": "architecture as the ViLT model but generalized the text encoder. ViLT employs BERT as a text encoder in the model",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": ""
        },
        {
          "models with different": "backbone. Because of",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": "text modality dominance in the human desire understanding task, we try to get diversity in"
        },
        {
          "models with different": "text modality. Hence, we used BERTweet as a text encoder in VAuLT, as the MSED dataset used social media data",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": ""
        },
        {
          "models with different": "(Twitter data too).The BERTweet text encoder is pre-trained on Twitter data, which may help the model learn semantic",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": ""
        },
        {
          "models with different": "information in a robust way.",
          "text encoders.\nIn our proposed method, we use two multimodal\ntransformer models - ViLT": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": "Task"
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": "Sentiment Analysis"
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": "Emotion Analysis"
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": "Desire Analysis"
        },
        {
          "information in a robust way.": ""
        },
        {
          "information in a robust way.": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multimodal transformer-based model architecture.": ""
        },
        {
          "multimodal transformer-based model architecture.": "Task"
        },
        {
          "multimodal transformer-based model architecture.": ""
        },
        {
          "multimodal transformer-based model architecture.": "Sentiment Analysis"
        },
        {
          "multimodal transformer-based model architecture.": ""
        },
        {
          "multimodal transformer-based model architecture.": "Emotion Analysis"
        },
        {
          "multimodal transformer-based model architecture.": ""
        },
        {
          "multimodal transformer-based model architecture.": "Desire Analysis"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "Figure 4: Performance analysis and error analysis of our proposed framework for the human desire understanding task. A ✓indicates correctly"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "classified or matched with the gold label and a\nrepresents erroneously classified."
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "7.4. Performance analysis and error analysis"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "To enhance understanding of the potential of our proposed method, we conduct error and performance analyses. To"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "do this, we articulate a few multimodal error cases that were misclassified by the individual model and successfully"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "classified by our proposed method across all\nthree tasks from the MSED dataset, as depicted in Figure 4.\nIn this"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "figure, we show the predictions of three models, ViLT, VAuLT, and MMTF-DES where our proposed MMTF-DES"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "model’s predicted labels matched with the gold label\nfor each task and individual component\n(ViLT and VAuLT)"
        },
        {
          "ViLT ✕\nVAuLT ✓\nMMTF-DES ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓\nViLT ✓\nVAuLT ✕\nMMTF-DES  ✓": "were predicted erroneously. Across all\nthree tasks, our system correctly predicts the class label\nin E#1 and E#2,"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "models, early fusion helps the model to learn the image-text pair relationship effectively. Hence, our proposed method"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "correctly predicts the class label where individual components predict erroneously. However,\nin example E#3 of all"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "tasks, our approach correctly classifies the class label;\nthe ViLT model also predicts the correct\nlabel for sentiment"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "analysis and desire analysis,\nfor example, E#3;\nfor emotion analysis,\nthe VAuLT model predicts the correct class"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "label. Here, the individual models help the model to obtain the correct predicted label; if one individual model fails to"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "predict the correct label, the other model helps the overall model to predict the correct class label. This highlights the"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "effectiveness of using multiple multimodal\ntransformer encoders in our proposed MMTF-DES method. Despite the"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "uncertainty in the predicted class label for individual multimodal transformer models, ViLT and VAuLT, our proposed"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "method overcomes this limitation by using an effective early fusion strategy."
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "8. Conclusion and future work"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "In this research, we have proposed a unified multimodal transformer-based architecture for the multimodal human"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "desire understanding task where we fused two multimodal\ntransformer models, ViLT and VAuLT. Using pairwise"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "learning of\nthe image-text pair setting of\nthose multimodal\ntransformer models, we have exploited the contextual"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "relationship between title-caption pairs and the visual-contextual relationship between image-context pairs. That im-"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "proves the intra-and-inter relationship of each image-text pair data in the human desire understanding task. We have"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "also added multi-sample dropout\nlayers on top of the concatenation-based early fused unified architecture of multi-"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "modal transformer models, improving the generalizability and speeding up training in the human desire understanding"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "task. Experimental results on the benchmark dataset show that our proposed method outperforms all state-of-the-art"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "methods. In this research, we propose a new task to better understand the human desire for image-text pairs data. We"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "also discussed our system from various aspects, including modality dominance of image and text, early fusion impact,"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "multi-sample dropout\nimpact, and error analysis to validate the effectiveness of our proposed method. Fusing multi-"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "modal transformer-based architecture with a multi-sample dropout strategy on top of diverse multimodal transformer"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "models has contributed to more robust representations of the image-text pair input while ensuring robustness through-"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "out generating new representations. It is crucial for the human desire understanding task to improve performances by"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "encouraging the model to extract more robust features."
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "We plan to explore two other techniques. We intend to implement a bi-attention-based neural network, including"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "both image and text-based attention, for better performance and greater efficiency in capturing the image and context."
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "The second technique is domain-adaptive pre-training on the multimodal transformer models, where we need to feed"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "relevant\nimage-text pairs into the pre-trained multimodal\ntransformer model.\nIt may help to learn different human"
        },
        {
          "even if both individual components fail to predict the correct label. This is because with diverse features of multiple": "desire information effectively and improve the model performance."
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Afyouni,\nI., Aghbari, Z.A., Razack, R.A., 2022. Multi-feature, multi-modal, and multi-source social event detection: A comprehensive survey."
        },
        {
          "References": "Information Fusion 79, 279–308."
        },
        {
          "References": "Alamoodi, A.H., Zaidan, B.B., Zaidan, A.A., Albahri, O.S., Mohammed, K.I., Malik, R.Q., Almahdi, E.M., Chyad, M.A., Tareq, Z., Albahri, A.S.,"
        },
        {
          "References": "et al., 2021. Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review. Expert systems with"
        },
        {
          "References": "applications 167, 114155."
        },
        {
          "References": "Alom, M.Z., Taha, T.M., Yakopcic, C., Westberg, S., Sidike, P., Nasrin, M.S., Van Esesn, B.C., Awwal, A.A.S., Asari, V.K., 2018. The history"
        },
        {
          "References": "began from alexnet: A comprehensive survey on deep learning approaches. arXiv preprint arXiv:1803.01164 ."
        },
        {
          "References": "Ara˜no, K.A., Orsenigo, C., Soto, M., Vercellis, C., 2021. Multimodal sentiment and emotion recognition in hyperbolic space. Expert Systems with"
        },
        {
          "References": "Applications 184, 115507."
        },
        {
          "References": "Aziz, A., Hossain, M.A., Chy, A.N., 2021. CSECU-DSG at SemEval-2021 Task 1: Fusion of transformer models for lexical complexity prediction,"
        },
        {
          "References": "in: Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pp. 627–631."
        },
        {
          "References": "Birjali, M., Kasri, M., Beni-Hssane, A., 2021. A comprehensive survey on sentiment analysis: Approaches, challenges and trends. Knowledge-"
        },
        {
          "References": "Based Systems 226, 107134."
        },
        {
          "References": "Bisong, E., 2019. Google Colaboratory,\nin: Building Machine Learning and Deep Learning Models on Google Cloud Platform. Springer, pp."
        },
        {
          "References": "59–64."
        },
        {
          "References": "Cacioppo, S., Bianchi-Demicheli, F., Frum, C., Pfaus, J.G., Lewis, J.W., 2012.\nThe common neural bases between sexual desire and love:\na"
        },
        {
          "References": "multilevel kernel density fMRI analysis. The journal of sexual medicine 9, 1048–1054."
        },
        {
          "References": "´\nCamacho, D.,\nAngel Panizo-LLedot, Bello-Orgaz, G., Gonzalez-Pardo, A., Cambria, E., 2020. The four dimensions of social network analysis: An"
        },
        {
          "References": "overview of research methods, applications, and software tools.\nInformation Fusion 63, 88–120."
        },
        {
          "References": "Chan, J.Y.L., Bea, K.T., Leow, S.M.H., Phoong, S.W., Cheng, W.K., 2023.\nState of the art: a review of sentiment analysis based on sequential"
        },
        {
          "References": "transfer learning. Artificial Intelligence Review 56, 749–780."
        },
        {
          "References": "Chochlakis, G., Srinivasan, T., Thomason, J., Narayanan, S., 2022. VAuLT: Augmenting the vision-and-language transformer with the propagation"
        },
        {
          "References": "of deep language representations. arXiv preprint arXiv:2208.09021 ."
        },
        {
          "References": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"
        },
        {
          "References": "in: Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
        },
        {
          "References": "(NAACL:HLT), pp. 4171–4186."
        },
        {
          "References": "Dong, J., Yang, H.I., Oyama, K., Chang, C.K., 2010. Human desire inference process based on affective computing,\nin: 2010 IEEE 34th Annual"
        },
        {
          "References": "Computer Software and Applications Conference, IEEE. pp. 347–350."
        },
        {
          "References": "Estrada, M.L.B., Cabada, R.Z., Bustillos, R.O., Graff, M., 2020. Opinion mining and emotion recognition applied to learning environments. Expert"
        },
        {
          "References": "Systems with Applications 150, 113265."
        },
        {
          "References": "Ezzameli, K., Mahersia, H., 2023. Emotion recognition from unimodal to multimodal analysis: A review.\nInformation Fusion , 101847."
        },
        {
          "References": "Gabeur, V., Sun, C., Alahari, K., Schmid, C., 2020. Multi-modal transformer for video retrieval, in: Computer Vision–ECCV 2020: 16th European"
        },
        {
          "References": "Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16, Springer. pp. 214–229."
        },
        {
          "References": "Gamallo, P., Garcia, M., et al., 2014. Citius: A Naive-Bayes strategy for sentiment analysis on English tweets., in: Semeval@ coling, pp. 171–175."
        },
        {
          "References": "Hahn, S., Choi, H., 2020. Understanding dropout as an optimization trick. Neurocomputing 398, 64–70."
        },
        {
          "References": "Inoue, H., 2019. Multi-sample dropout for accelerated training and better generalization. arXiv preprint arXiv:1905.09788 ."
        },
        {
          "References": "Jia, A., He, Y., Zhang, Y., Uprety, S., Song, D., Lioma, C., 2022.\nBeyond emotion: A multi-modal dataset\nfor human desire understanding,"
        },
        {
          "References": "in: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language"
        },
        {
          "References": "Technologies, pp. 1512–1522."
        },
        {
          "References": "Jin, M., Preot¸iuc-Pietro, D., Do˘gru¨oz, A., Aletras, N., 2022. Automatic identification and classification of bragging in social media, in: Proceedings"
        },
        {
          "References": "of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3945–3959."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "computational linguistics. Expert Systems with Applications 198, 116686."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Kim, W., Son, B., Kim, I., 2021. Vilt: Vision-and-language transformer without convolution or region supervision, in: International Conference on"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Machine Learning, PMLR. pp. 5583–5594."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Kiritchenko, S., Zhu, X., Mohammad, S.M., 2014.\nSentiment analysis of short\ninformal\ntexts.\nJournal of Artificial\nIntelligence Research 50,"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "723–762."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Kratzwald, B., Ili´c, S., Kraus, M., Feuerriegel, S., Prendinger, H., 2018. Deep learning for affective computing: Text-based emotion recognition in"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "decision support. Decision Support Systems 115, 24–35."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Le, H.D., Lee, G.S., Kim, S.H., Kim, S., Yang, H.J., 2023. Multi-label multimodal emotion recognition with transformer-based fusion and"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "emotion-level representation learning.\nIEEE Access ."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Lee, H., Choi, Y.S., Lee, S., Park, I., 2012. Towards unobtrusive emotion recognition for affective social communication, in: 2012 IEEE Consumer"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Communications and Networking Conference (CCNC), IEEE. pp. 260–264."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Li, C., Bao, Z., Li, L., Zhao, Z., 2020. Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "emotion recognition.\nInformation Processing & Management 57, 102185."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Lim, A., Ogata, T., Okuno, H.G., 2012. The desire model: Cross-modal emotion analysis and expression for robots, in: 74th National Convention,"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Information Processing Society of Japan. pp. 59–60."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Mai, S., Zeng, Y., Zheng, S., Hu, H., 2022. Hybrid contrastive learning of\ntri-modal\nrepresentation for multimodal sentiment analysis.\nIEEE"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Transactions on Affective Computing ."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Middya, A.I., Nag, B., Roy, S., 2022. Deep learning based multimodal emotion recognition using model-level fusion of audio–visual modalities."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Knowledge-Based Systems 244, 108580."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Morency, L.P., Mihalcea, R., Doshi, P., 2011. Towards multimodal sentiment analysis: Harvesting opinions from the web,\nin: Proceedings of the"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "13th international conference on multimodal interfaces, pp. 169–176."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Nemati, S., Rohani, R., Basiri, M.E., Abdar, M., Yen, N.Y., Makarenkov, V., 2019. A hybrid latent space data fusion method for multimodal"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "emotion recognition.\nIEEE Access 7, 172948–172964."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Nguyen, D.Q., Vu, T., Nguyen, A.T., 2020. BERTweet: A pre-trained language model for English tweets, in: Proceedings of the 2020 Conference"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 9–14."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Oberl¨ander, L.A.M., Klinger, R., 2018. An analysis of annotated corpora for emotion classification in text, in: Proceedings of the 27th International"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Conference on Computational Linguistics, pp. 2104–2119."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Pan, C., Song, B., Wang, S., Luo, Z., 2021. DeepBlueAI at SemEval-2021 Task 1: Lexical complexity prediction with a deep ensemble approach,"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "in: Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pp. 578–584."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Peng, J., Wu, T., Zhang, W., Cheng, F., Tan, S., Yi, F., Huang, Y., 2023. A fine-grained modal\nlabel-based multi-stage network for multimodal"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "sentiment analysis. Expert Systems with Applications , 119721."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Poria, S., Chaturvedi, I., Cambria, E., Hussain, A., 2016. Convolutional MKL based multimodal emotion recognition and sentiment analysis,\nin:"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "2016 IEEE 16th international conference on data mining (ICDM), IEEE. pp. 439–448."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Ranganathan, H., Chakraborty, S., Panchanathan, S., 2016. Multimodal emotion recognition using deep learning architectures,\nin: 2016 IEEE"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "winter conference on applications of computer vision (WACV), IEEE. pp. 1–9."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Rani, S., Bashir, A.K., Alhudhaif, A., Koundal, D., Gunduz, E.S., et al., 2022.\nAn efficient CNN-LSTM model\nfor\nsentiment detection in"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "#BlackLivesMatter. Expert Systems with Applications 193, 116256."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Sailunaz, K., Alhajj, R., 2019. Emotion and sentiment analysis from Twitter text. Journal of Computational Science 36, 101003."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Shaheen, S., El-Hajj, W., Hajj, H., Elbassuoni, S., 2014. Emotion recognition from text based on automatically generated rules,\nin: 2014 IEEE"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "International Conference on Data Mining Workshop, IEEE. pp. 383–392."
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Singh, M., Jakhar, A.K., Pandey, S., 2021. Sentiment analysis on the impact of coronavirus in social life using the BERT model. Social Network"
        },
        {
          "Khoshnam, F., Baraani-Dastjerdi, A., 2022. A dual framework for implicit and explicit emotion recognition: An ensemble of language models and": "Analysis and Mining 11, 33."
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "international conference on Multimedia, pp. 399–402."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Soleymani, M., Garcia, D., Jou, B., Schuller, B., Chang, S.F., Pantic, M., 2017. A survey of multimodal sentiment analysis.\nImage and Vision"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Computing 65, 3–14."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Soleymani, M., Pantic, M., Pun, T., 2011. Multimodal emotion recognition in response to videos.\nIEEE transactions on affective computing 3,"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "211–223."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Szegedy, C.,\nIoffe, S., Vanhoucke, V., Alemi, A., 2017.\nInception-v4,\ninception-resnet and the impact of\nresidual connections on learning,\nin:"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Proceedings of the AAAI conference on artificial intelligence."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Truong, Q.T., Lauw, H.W., 2019. Vistanet: Visual aspect attention network for multimodal sentiment analysis,\nin: Proceedings of\nthe AAAI"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "conference on artificial intelligence, pp. 305–312."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Tu, G., Liang, B., Jiang, D., Xu, R., 2022.\nSentiment-emotion-and context-guided knowledge selection framework for emotion recognition in"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "conversations.\nIEEE Transactions on Affective Computing ."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Wilt, J., Revelle, W., 2015. Affect, behaviour, cognition and desire in the big five: An analysis of item content and structure. European journal of"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "personality 29, 478–497."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S., et al., 2020. Transformers:"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "State-of-the-art Natural Language Processing, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "System Demonstrations, pp. 38–45."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Xu, G., Meng, Y., Qiu, X., Yu, Z., Wu, X., 2019. Sentiment analysis of comment texts based on BiLSTM.\nIEEE Access 7, 51522–51532."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Yang, B., Shao, B., Wu, L., Lin, X., 2022. Multimodal sentiment analysis with unidirectional modality translation. Neurocomputing 467, 130–137."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "¨\nYavuz,\nO., Karahoca, A., Karahoca, D., 2019. A data mining approach for desire and intention to participate in virtual communities.\nInternational"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Journal of Electrical and Computer Engineering (IJECE) 9, 3714–3719."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, F., Li, X.C., Lim, C.P., Hua, Q., Dong, C.R., Zhai, J.H., 2022a. Deep emotional arousal network for multimodal sentiment analysis and"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "emotion recognition.\nInformation Fusion 88, 296–304."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, H., Zang, Z., Zhu, H., Uddin, M.I., Amin, M.A., 2022b. Big data-assisted social media analytics for business model for business decision"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "making system competitive analysis.\nInformation Processing & Management 59, 102762."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, J., Yin, Z., Chen, P., Nichele, S., 2020a. Emotion recognition using multi-modal data and machine learning techniques: A tutorial and"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "review.\nInformation Fusion 59, 103–126."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, Y., Cheng, C., Zhang, Y., 2022c. Multimodal emotion recognition based on manifold learning and convolution neural network. Multimedia"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Tools and Applications 81, 33253–33268."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, Y., Jia, A., Wang, B., Zhang, P., Zhao, D., Li, P., Hou, Y., Jin, X., Song, D., Qin, J., 2023a. M3GAT: A multi-modal multi-task interactive"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "graph attention network for conversational sentiment analysis and emotion recognition. ACM Transactions on Information Systems ."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, Y., Song, D., Li, X., Zhang, P., Wang, P., Rong, L., Yu, G., Wang, B., 2020b. A quantum-like multimodal network framework for modeling"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "interaction dynamics in multiparty conversational sentiment analysis.\nInformation Fusion 62, 14–31."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhang, Y., Wang, J., Liu, Y., Rong, L., Zheng, Q., Song, D., Tiwari, P., Qin, J., 2023b. A multitask learning model\nfor multimodal sarcasm,"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "sentiment and emotion recognition in conversations.\nInformation Fusion ."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhou, X., Tao, X., Yong, J., Yang, Z., 2013. Sentiment analysis on tweets for social events,\nin: Proceedings of the 2013 IEEE 17th international"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "conference on computer supported cooperative work in design (CSCWD), IEEE. pp. 557–562."
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "Zhu, T., Li, L., Yang, J., Zhao, S., Liu, H., Qian, J., 2022. Multimodal sentiment analysis with image-text interaction network.\nIEEE Transactions"
        },
        {
          "Snoek, C.G., Worring, M., Smeulders, A.W., 2005. Early versus late fusion in semantic video analysis,\nin: Proceedings of the 13th annual ACM": "on Multimedia ."
        }
      ],
      "page": 28
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-feature, multi-modal, and multi-source social event detection: A comprehensive survey",
      "authors": [
        "I Afyouni",
        "Z Aghbari",
        "R Razack"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review",
      "authors": [
        "A Alamoodi",
        "B Zaidan",
        "A Zaidan",
        "O Albahri",
        "K Mohammed",
        "R Malik",
        "E Almahdi",
        "M Chyad",
        "Z Tareq",
        "A Albahri"
      ],
      "year": "2021",
      "venue": "Expert systems with applications"
    },
    {
      "citation_id": "3",
      "title": "The history began from alexnet: A comprehensive survey on deep learning approaches",
      "authors": [
        "M Alom",
        "T Taha",
        "C Yakopcic",
        "S Westberg",
        "P Sidike",
        "M Nasrin",
        "B Van Esesn",
        "A Awwal",
        "V Asari"
      ],
      "year": "2018",
      "venue": "The history began from alexnet: A comprehensive survey on deep learning approaches",
      "arxiv": "arXiv:1803.01164"
    },
    {
      "citation_id": "4",
      "title": "Multimodal sentiment and emotion recognition in hyperbolic space",
      "authors": [
        "K Araño",
        "C Orsenigo",
        "M Soto",
        "C Vercellis"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "CSECU-DSG at SemEval-2021 Task 1: Fusion of transformer models for lexical complexity prediction",
      "authors": [
        "A Aziz",
        "M Hossain",
        "A Chy"
      ],
      "year": "2021",
      "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)"
    },
    {
      "citation_id": "6",
      "title": "A comprehensive survey on sentiment analysis: Approaches, challenges and trends",
      "authors": [
        "M Birjali",
        "M Kasri",
        "A Beni-Hssane"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "7",
      "title": "Google Colaboratory",
      "authors": [
        "E Bisong"
      ],
      "year": "2019",
      "venue": "Building Machine Learning and Deep Learning Models on Google Cloud Platform"
    },
    {
      "citation_id": "8",
      "title": "The common neural bases between sexual desire and love: a multilevel kernel density fMRI analysis",
      "authors": [
        "S Cacioppo",
        "F Bianchi-Demicheli",
        "C Frum",
        "J Pfaus",
        "J Lewis"
      ],
      "year": "2012",
      "venue": "The journal of sexual medicine"
    },
    {
      "citation_id": "9",
      "title": "The four dimensions of social network analysis: An overview of research methods, applications, and software tools",
      "authors": [
        "D Camacho",
        "Ángel Panizo-Lledot",
        "G Bello-Orgaz",
        "A Gonzalez-Pardo",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "State of the art: a review of sentiment analysis based on sequential transfer learning",
      "authors": [
        "J Chan",
        "K Bea",
        "S Leow",
        "S Phoong",
        "W Cheng"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "11",
      "title": "VAuLT: Augmenting the vision-and-language transformer with the propagation of deep language representations",
      "authors": [
        "G Chochlakis",
        "T Srinivasan",
        "J Thomason",
        "S Narayanan"
      ],
      "year": "2022",
      "venue": "VAuLT: Augmenting the vision-and-language transformer with the propagation of deep language representations",
      "arxiv": "arXiv:2208.09021"
    },
    {
      "citation_id": "12",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL:HLT)"
    },
    {
      "citation_id": "13",
      "title": "Human desire inference process based on affective computing",
      "authors": [
        "J Dong",
        "H Yang",
        "K Oyama",
        "C Chang"
      ],
      "year": "2010",
      "venue": "IEEE 34th Annual Computer Software and Applications Conference"
    },
    {
      "citation_id": "14",
      "title": "Opinion mining and emotion recognition applied to learning environments",
      "authors": [
        "M Estrada",
        "R Cabada",
        "R Bustillos",
        "M Graff"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "16",
      "title": "Multi-modal transformer for video retrieval",
      "authors": [
        "V Gabeur",
        "C Sun",
        "K Alahari",
        "C Schmid"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "17",
      "title": "Citius: A Naive-Bayes strategy for sentiment analysis on English tweets",
      "authors": [
        "P Gamallo",
        "M Garcia"
      ],
      "year": "2014",
      "venue": "Semeval@ coling"
    },
    {
      "citation_id": "18",
      "title": "Understanding dropout as an optimization trick",
      "authors": [
        "S Hahn",
        "H Choi"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Multi-sample dropout for accelerated training and better generalization",
      "authors": [
        "H Inoue"
      ],
      "year": "2019",
      "venue": "Multi-sample dropout for accelerated training and better generalization",
      "arxiv": "arXiv:1905.09788"
    },
    {
      "citation_id": "20",
      "title": "Beyond emotion: A multi-modal dataset for human desire understanding",
      "authors": [
        "A Jia",
        "Y He",
        "Y Zhang",
        "S Uprety",
        "D Song",
        "C Lioma"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "21",
      "title": "Automatic identification and classification of bragging in social media",
      "authors": [
        "M Jin",
        "D Preot ¸iuc-Pietro",
        "A Dogruöz",
        "N Aletras"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "A dual framework for implicit and explicit emotion recognition: An ensemble of language models and computational linguistics",
      "authors": [
        "F Xxvi Khoshnam",
        "A Baraani-Dastjerdi"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "23",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "W Kim",
        "B Son",
        "I Kim"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Sentiment analysis of short informal texts",
      "authors": [
        "S Kiritchenko",
        "X Zhu",
        "S Mohammad"
      ],
      "year": "2014",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "25",
      "title": "Deep learning for affective computing: Text-based emotion recognition in decision support",
      "authors": [
        "B Kratzwald",
        "S Ilić",
        "M Kraus",
        "S Feuerriegel",
        "H Prendinger"
      ],
      "year": "2018",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "26",
      "title": "Multi-label multimodal emotion recognition with transformer-based fusion and emotion-level representation learning",
      "authors": [
        "H Le",
        "G Lee",
        "S Kim",
        "S Kim",
        "H Yang"
      ],
      "year": "2023",
      "venue": "Multi-label multimodal emotion recognition with transformer-based fusion and emotion-level representation learning"
    },
    {
      "citation_id": "27",
      "title": "Towards unobtrusive emotion recognition for affective social communication",
      "authors": [
        "H Lee",
        "Y Choi",
        "S Lee",
        "I Park"
      ],
      "year": "2012",
      "venue": "2012 IEEE Consumer Communications and Networking Conference (CCNC)"
    },
    {
      "citation_id": "28",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "29",
      "title": "The desire model: Cross-modal emotion analysis and expression for robots",
      "authors": [
        "A Lim",
        "T Ogata",
        "H Okuno"
      ],
      "year": "2012",
      "venue": "The desire model: Cross-modal emotion analysis and expression for robots"
    },
    {
      "citation_id": "30",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "S Zheng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Deep learning based multimodal emotion recognition using model-level fusion of audio-visual modalities",
      "authors": [
        "A Middya",
        "B Nag",
        "S Roy"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "32",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "33",
      "title": "A hybrid latent space data fusion method for multimodal emotion recognition",
      "authors": [
        "S Nemati",
        "R Rohani",
        "M Basiri",
        "M Abdar",
        "N Yen",
        "V Makarenkov"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "34",
      "title": "BERTweet: A pre-trained language model for English tweets",
      "authors": [
        "D Nguyen",
        "T Vu",
        "A Nguyen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "35",
      "title": "An analysis of annotated corpora for emotion classification in text",
      "authors": [
        "L Oberländer",
        "R Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "DeepBlueAI at SemEval-2021 Task 1: Lexical complexity prediction with a deep ensemble approach",
      "authors": [
        "C Pan",
        "B Song",
        "S Wang",
        "Z Luo"
      ],
      "year": "2021",
      "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)"
    },
    {
      "citation_id": "37",
      "title": "A fine-grained modal label-based multi-stage network for multimodal sentiment analysis",
      "authors": [
        "J Peng",
        "T Wu",
        "W Zhang",
        "F Cheng",
        "S Tan",
        "F Yi",
        "Y Huang"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "38",
      "title": "Convolutional MKL based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "IEEE 16th international conference on data mining (ICDM)"
    },
    {
      "citation_id": "39",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "40",
      "title": "An efficient CNN-LSTM model for sentiment detection in #BlackLivesMatter",
      "authors": [
        "S Rani",
        "A Bashir",
        "A Alhudhaif",
        "D Koundal",
        "E Gunduz"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "41",
      "title": "Emotion and sentiment analysis from Twitter text",
      "authors": [
        "K Sailunaz",
        "R Alhajj"
      ],
      "year": "2019",
      "venue": "Journal of Computational Science"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition from text based on automatically generated rules",
      "authors": [
        "S Shaheen",
        "W El-Hajj",
        "H Hajj",
        "S Elbassuoni"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Data Mining Workshop"
    },
    {
      "citation_id": "43",
      "title": "Sentiment analysis on the impact of coronavirus in social life using the BERT model",
      "authors": [
        "M Singh",
        "A Jakhar",
        "S Pandey"
      ],
      "year": "2021",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "44",
      "title": "Early versus late fusion in semantic video analysis",
      "authors": [
        "C Snoek",
        "M Worring",
        "A Smeulders"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th annual ACM international conference on Multimedia"
    },
    {
      "citation_id": "45",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "46",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "47",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "48",
      "title": "Vistanet: Visual aspect attention network for multimodal sentiment analysis",
      "authors": [
        "Q Truong",
        "H Lauw"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "Sentiment-emotion-and context-guided knowledge selection framework for emotion recognition in conversations",
      "authors": [
        "G Tu",
        "B Liang",
        "D Jiang",
        "R Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Affect, behaviour, cognition and desire in the big five: An analysis of item content and structure",
      "authors": [
        "J Wilt",
        "W Revelle"
      ],
      "year": "2015",
      "venue": "European journal of personality"
    },
    {
      "citation_id": "51",
      "title": "Transformers: State-of-the-art Natural Language Processing",
      "authors": [
        "T Wolf",
        "J Chaumond",
        "L Debut",
        "V Sanh",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "M Funtowicz",
        "J Davison",
        "S Shleifer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "52",
      "title": "Sentiment analysis of comment texts based on BiLSTM",
      "authors": [
        "G Xu",
        "Y Meng",
        "X Qiu",
        "Z Yu",
        "X Wu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Multimodal sentiment analysis with unidirectional modality translation",
      "authors": [
        "B Yang",
        "B Shao",
        "L Wu",
        "X Lin"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "54",
      "title": "A data mining approach for desire and intention to participate in virtual communities",
      "authors": [
        "Ö Yavuz",
        "A Karahoca",
        "D Karahoca"
      ],
      "year": "2019",
      "venue": "International Journal of Electrical and Computer Engineering (IJECE)"
    },
    {
      "citation_id": "55",
      "title": "Deep emotional arousal network for multimodal sentiment analysis and emotion recognition",
      "authors": [
        "F Zhang",
        "X Li",
        "C Lim",
        "Q Hua",
        "C Dong",
        "J Zhai"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "56",
      "title": "Big data-assisted social media analytics for business model for business decision making system competitive analysis",
      "authors": [
        "H Zhang",
        "Z Zang",
        "H Zhu",
        "M Uddin",
        "M Amin"
      ],
      "year": "2022",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "58",
      "title": "Multimodal emotion recognition based on manifold learning and convolution neural network",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "59",
      "title": "2023a. M3GAT: A multi-modal multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition",
      "authors": [
        "Y Zhang",
        "A Jia",
        "B Wang",
        "P Zhang",
        "D Zhao",
        "P Li",
        "Y Hou",
        "X Jin",
        "D Song",
        "J Qin"
      ],
      "venue": "ACM Transactions on Information Systems"
    },
    {
      "citation_id": "60",
      "title": "A quantum-like multimodal network framework for modeling interaction dynamics in multiparty conversational sentiment analysis",
      "authors": [
        "Y Zhang",
        "D Song",
        "X Li",
        "P Zhang",
        "P Wang",
        "L Rong",
        "G Yu",
        "B Wang"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "61",
      "title": "2023b. A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "J Wang",
        "Y Liu",
        "L Rong",
        "Q Zheng",
        "D Song",
        "P Tiwari",
        "J Qin"
      ],
      "venue": "2023b. A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations"
    },
    {
      "citation_id": "62",
      "title": "Sentiment analysis on tweets for social events",
      "authors": [
        "X Zhou",
        "X Tao",
        "J Yong",
        "Z Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 IEEE 17th international conference on computer supported cooperative work in design (CSCWD)"
    },
    {
      "citation_id": "63",
      "title": "Multimodal sentiment analysis with image-text interaction network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "H Liu",
        "J Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}