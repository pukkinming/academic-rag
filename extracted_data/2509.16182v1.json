{
  "paper_id": "2509.16182v1",
  "title": "Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are Paralinguistic Pre-Trained Representations Sufficient?",
  "published": "2025-09-19T17:43:20Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Mohd Mujtaba Akhtar",
    "Girish",
    "Swarup Ranjan Behera",
    "Parabattina Bhagath",
    "Pailla Balakrishna Reddy",
    "Arun Balaji Buduru"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent benchmarks evaluating pre-trained models (PTMs) for cross-corpus speech emotion recognition (SER) have overlooked PTM pre-trained for paralinguistic speech processing (PSP), raising concerns about their reliability, since SER is inherently a paralinguistic task. We hypothesize that PSP-focused PTM will perform better in cross-corpus SER settings. To test this, we analyze state-of-the-art PTMs representations including paralinguistic, monolingual, multilingual, and speaker recognition. Our results confirm that TRILLsson (a paralinguistic PTM) outperforms others, reinforcing the need to consider PSP-focused PTMs in cross-corpus SER benchmarks. This study enhances benchmark trustworthiness and guides PTMs evaluations for reliable cross-corpus SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions significantly influence human behavior and interactions, and Speech Emotion Recognition (SER) identifies these cues by analyzing speech features such as pitch, tone, and intensity. This capability is valuable across many fields: it enables empathetic responses in human-computer interaction, supports mental health monitoring, enhances customer service through personalized interactions, and adapts learning environments in education. Additionally, in entertainment and security, SER improves user experience and safety by identifying emotional distress or threats. Thus, accurately recognizing emotions in speech is key for building intelligent, responsive systems. Early research in SER predominantly relied on handcrafted acoustic features, such as MFCCs, paired with traditional machine learning models [1],  [2] ,  [3] . With the advent of speech pre-trained models (PTMs), SER has seen significant advancements, as these models provide powerful, generalizable representations learned from large-scale, diverse datasets  [4] ,  [5] ,  [6] . This shift has alleviated both performance benefit and need for training models from scratch. PTMs, however, differ in various aspects, including their training data, which spans across diverse data distributions, and whether they are trained on monolingual or multilingual datasets. Additionally, these models vary in terms of architecture and the pre-training strategies used, such as self-supervised or supervised learning approaches. These differences in PTM nature and pre-training methods * Contributed equally as first author.\n\nhave direct implications on their downstream performance for SER. As a result, the choice of PTM can significantly affect the accuracy and robustness of SER and this variability in performance underscores the need for a deeper understanding of these PTMs. Several benchmarks have been proposed such as SUPERB  [7] , EMO-SUPERB  [8] , OPEN-EMOTION  [9]  and so on to better understand the performance of different PTMs for SER in monolingual or multilingual settings. These benchmarks also act as a reference for future research for selection of PTMs depending on their use case. However, these benchmarks evaluates PTMs for training and testing on the same corpus. As such there is a recent ongoing interest in the research community to access the cross-corpus SER capability of various SOTA speech PTMs  [10] ,  [11] . Here, \"cross-corpus\" encompasses two key scenarios: (1) the same language with varying data distributions and (2) cross-lingual settings. Crosscorpus SER presents greater challenges compared to samecorpus SER due to the domain shift that occurs between the training and test data. In such scenarios, the models must generalize across different data distributions, speaker demographics, recording environments, or even languages, which can lead to significant performance degradation. The variations in emotional expression, speaking styles, and acoustic conditions across corpora further complicate the task, making cross-corpus SER more demanding. In response, various benchmarks have been proposed in recent couple of years such as SER-evals  [12]  and EmoBox  [13] . However, these benchmarks haven't considered representations from PTM pre-trained primarily for paralinguistic speech processing (PSP). Such oversight raises concerns about the trustworthiness of the benchmarks as comprehensive references for future research, especially since SER fundamentally is a PSP task. Also, Phukan et al.  [14]  which has shown the topmost performance of paralinguistic PTM representations for SER in multiple languages, haven't evaluated paralinguistic PTM representations for cross-corpus SER. So, to solve this research gap and also to get better understanding, we explore paralinguistic PTM representations for cross-corpus SER. We hypothesize that representations from paralinguistic PTM arXiv:2509.16182v1 [eess.AS] 19 Sep 2025 representations are better suited for cross-corpus SER. Unlike general-purpose PTM representations, paralinguistic PTM representations captures speech characteristics cues such as intonation, pitch, rhythm, and prosody-features that are directly relevant for SER. These paralinguistic representations transcend linguistic boundaries and can generalize more effectively across different languages and data distributions. To validate our hypothesis, we perform a comprehensive comparative study of representations from SOTA PTMs comprising paralinguistic, monolingual, multilingual as well as speaker recognition. These PTMs are SOTA in their respective benchmarks. For example, Whisper reported the topmost performance for cross-corpus SER in SER-evals  [12]  and EmoBox  [13] . So, we are presenting the comparison of best of the best PTMs in our study to validate the performance and show the capability of paralinguistic PTM for cross-corpus SER. Key contributions of the paper are as follows:\n\n• We present a comprehensive comparative study of representations from SOTA PTMs including paralinguistic (TRILLsson), monolingual (WavLM, Unispeech-SAT, Wav2vec2), multilingual (XLS-R, Whisper, MMS) and speaker recognition (x-vector) for cross-corpus SER.\n\nWe experiment with CREMA-D (English), RAVDESS (English), emo-DB (English), MESD (Mexican Spanish), and AESDD (Greek) benchmark SER datasets. • Our findings demonstrate TRILLsson (paralinguistic PTM) representations superior performance in cross-corpus SER across all the datasets. • Our study will act as benchmark for future researchers for selection of PTM representations for cross-corpus SER. Also, our study calls for the inclusion of paralinguistic PTM representations in the previous benchmarks for crosscorpus SER. This will ensure reliability and trustworthiness of cross-corpus SER benchmarks, ensuring that future work can build upon more accurate and generalizable results. As monolingual PTMs, we use WavLM 1    [15] , Unispeech-SAT 2    [16] , and Wav2vec2 3    [17] . WavLM is a SOTA PTM in SUPERB and shows top performance compared to other PTMs in various speech processing tasks and trained to perform speech denoising and masked modeling together. Unispeech-SAT also shows SOTA performance in SUPERB and was trained in speaker-aware format. Wav2vec2 is not SOTA like WavLM and Unispeech-SAT in SUPERB, however, we consider it, as it shows relatably good performance in SER as shown by previous research  [18] . We use the base versions for WavLM, Unispeech-SAT, Wav2vec2 trained on librispeech 960 hours data with 94.70M, 94.70M, and 95.04M parameters respectively. As multilingual PTMs, we consider, XLS-R 4    [19] , Whisper 5    [20] , and MMS 6    [21] . XLS-R was trained on 128 languages while Whisper was trained on 96 languages. XLS-R is based on Wav2vec2 architecture and trained in a self-supervised fashion and in contrast, Whisper is based on vanilla transformer encoderdecoder architecture. We use XLS-R 300M parameters version and for whisper,we use the base variant with 74M. MMS is built on top of Wav2vec2 architecture and improves over Whisper in multilingual speech processing applications. It extends its pretraining to almost over 1400 languages. We use the 1B variant of MMS in our experiments. For paralinguistic PTM, we consider TRILLsson  [22]  and it is derived from SOTA paralinguistic Conformer (CAP12) through knowledge distillation. TRILLsson is open-sourced, however, CAP12 is not. It has demonstrated SOTA performance in different paralingusitic tasks such as SER, speaker identification, and so on in NOSS benchmark. It was trained on the speech samples of Audioset and Librilight datasets during its distillation phase. We use the 63.4 million parameters variant of TRILLsson. We also consider x-vector 7    [23] , a time delay-neural network trained for speaker recognition. It is trained on the combination of Voxceleb1 + Voxceleb2. x-vector has shown its potential for SER  [24] , so we included it in our study. For each frozen PTM, we extract the last hidden states through average pooling. We get representations of 768 from WavLM, Unispeech-SAT, wav2vec2, 1024 from TRILLsson, 1280 from XLS-R, MMS. For whisper, we discard the decoder and extract representations from its encoder with size 512 same as x-vector. We resample the audios to 16 KHz before passing it to the PTMs. TABLE I: Evaluation scores of different models trained with different PTM representations for both same corpus and cross-corpus SER; Scores are presented in Accuracy | Macro F1 format; P, C, M, E, A, R stands for PTM, CREMA-D, MESD, Emo-DB, AESDD, and RAVDESS; X(Train) -X(Test) represents the training and evaluation dataset where X = C, M, E, A, R. For example, C(Train) -M(Test) represents training on CREMA-D and testing on MESD; MMS (MM), Whisper (Wh), WavLM (W), XLSR (X), Wav2Vec2 (W2), UniSpeech (U), TRILLsson (T), and X-vector (XV) are PTMs; The intensity highlights the performance levels, where darker shades indicate higher values and lighter shades indicate lower values. The scores are highlighted block-wise i.e. X(Train) -X(Test) (where X = C, M, E, A, R) block based on its relative performance. SER and it is gender-balanced. It includes recordings in english from 48 male and 43 female actors, totaling 7,442 utterances. The dataset is valuable due to its representation of a variety of speaker ages and ethnic backgrounds. It covers six emotions: angry, happiness, sadness, fear, disgust, and a neutral state, with each actor delivering 12 distinct sentences.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ryerson Audio-Visual Database Of Emotional Speech And",
      "text": "Song (RAVDESS)  [26] : It contains 7,356 english clips covering eight emotions: neutral, calm, happiness, sadness, angry, fearful, disgust, and surprise for speech, and six emotions for song. The database provides high emotional validity, as validated by 319 raters who assessed recordings on emotional category, intensity, and genuineness.\n\nGerman Emotional Speech Database (Emo-DB)  [27] : It is recorded in German, contains 535 utterances from five male and five female actors. Each actor was assigned one of ten predefined scripts for the recordings. Emo-DB includes seven emotional categories: angry, fear, boredom, disgust, happiness, neutral, and sadness. Mexican Emotional Speech Database (MESD)  [28] : It is a culturally tailored emotional speech dataset designed for Mexican Spanish speakers. It includes 864 utterances featuring six emotional states: angry, disgust, fear, happiness, neutral, and sadness. The recordings are categorized into three demographic groups: male adults, female adults, and children, with nonprofessional actors delivering carefully selected single-word utterances.\n\nActed Emotional Speech Dynamic Database (AESDD)  [29] : It is a Greek-language speech emotion dataset consisting of approximately 600 utterances recorded by five actors. It includes five emotional states: angry, disgust, fear, happiness, and sadness. In our study, we consider the common emotions: happiness, fear, sadness, angry, and disgust as we are primarily interested in cross-corpus SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Downstream Modeling",
      "text": "We use FCN and CNN as our downstream models as used by previous research in SER  [14]  and related applications  [30] . As we are interested in understanding the PTMs implicit capability for cross-corpus SER, so we kept the downstream modeling as simple as possible. For CNN (Figure  1b ), we use 1D-CNN layer with a kernel size of 3 followed by max-pooling with a pool size of 2. The extracted features are then flattened and passed through a FCN block containing a dense layer of 90 neurons. Finally, a classification that contains the output layer with softmax activation function. For FCN (Figure  1a ), we use the same modeling as used by the FCN block in CNN modeling. The FCN models trainable parameters are between 0.4 to 0.8M and for CNN, it is between 1 to 2M. Training Details: We use Adam optimizer with batch size of 32 and cross-entropy as loss function. We train the all the models for 20 epochs. We use dropout and early stopping to prevent overfitting. We follow a five-fold cross-validation for training and evaluation of the models for both same-corpus and cross-corpus experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Results",
      "text": "Table I presents the evaluation results for the downstream models trained with different PTMs. For same-corpus evaluation, we present the average performance across five folds. For cross-corpus evaluation, we follow a similar five-fold approach: in each fold, we train on four folds of the training dataset and test on one fold of the testing dataset. This process is repeated five times, and we report the average performance across these folds to ensure robustness. For same-corpus experiments : TRILLsson consistently outperforms monolingual, multilingual, and speaker recognition PTMs across CREMA-D, MESD, Emo-DB, AESDD, and RAVDESS. This aligns with prior studies on evaluating TRILLsson for SER  [14] , reinforcing its strength in capturing In some instances, it reports top performance by a very large margin in comparison to other PTMs. This validates our hypothesis that paralinguistic PTM representations are inherently better suited for cross-corpus SER. Unlike other PTMs, TRILLsson focuses on capturing essential speech characteristics-such as intonation, pitch, rhythm, and prosody-that are fundamental for SER in a much better way. Because these features are largely independent of linguistic content, TRILLsson's representations exhibit greater robustness across diverse datasets, enabling superior generalization in cross-corpus settings. Among monolingual varies across datasets. WavLM excels in some cases, while Wav2vec2 achieves better in some, indicating that monolingual PTMs struggle with consistent cross-corpus generalization. Similarly, no single multilingual PTM dominates across all datasets among the multilingual PTMs. These results emphasize that PTM performance in cross-corpus settings is strongly influenced by the underlying data distribution. Despite these multilingual PTMs are pre-trained on multiple languages, they show poor cross-corpus generalization and thus, amplifying the dependence of SER on paralinguistic features and in which TRILLsson excels in. Interestingly, x-vector demonstrates mixed performance, occasionally surpassing both monolingual and multilingual PTMs in some datasets while underperforming in others. This suggests that paralinguistic characteristics embedded in x-vector representations can be beneficial for cross-corpus SER in certain contexts but are not universally effective. We also plot the t-SNE plot visualizations of TRILLsson representations for anger and sadness emotion across all the datasets in Figure  2  (a) and (b) respectively. We segmented the anger and sadness emotions for all the datasets, extracted representations from TRILLsson and visualized it using t-SNE plot. We observe that no clear clusters are present and through this, we can say that TRILLsson converts the same emotion samples to a joint representational space irrespective of linguistic difference. These plots further amplifies our obtained results. We also plot the confusion matrix of CNN model trained on CREMA-D with TRILLsson representations and evaluated on Emo-DB in Figure  3 (b) .\n\nIV. CONCLUSION\n\nIn this study, we investigate paralinguistic PTM (TRILLsson) for cross-corpus SER, which have been largely overlooked by previous benchmarks on evaluating PTMs for cross-corpus SER. We hypothesize that paralinguistic PTM would outperform other PTMs. By analyzing SOTA PTM representations, including paralinguistic, monolingual, multilingual, and speaker recognition, we show that TRILLsson, consistently outperforms other PTMs and validating our hypothesis. These results highlight the importance of our work and it will act as benchmark for future works on cross-corpus SER. It also calls for incorporating paralinguistic PTM in previous cross-corpus SER benchmarks, ultimately enhancing their trustworthiness and offering valuable insights for future PTM evaluations in SER tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: : Modeling",
      "page": 2
    },
    {
      "caption": "Figure 1: b), we use 1D-CNN",
      "page": 4
    },
    {
      "caption": "Figure 2: t-SNE plots of PTMs raw representations: (a) Anger",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices: (a) Trained and Tested on",
      "page": 4
    },
    {
      "caption": "Figure 2: (c) and (d), revealing",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), confirming its superior performance.",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) and (b) respectively. We",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "PTM Representation\nClassification\nOutput FCN\nHead"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "PTM Representation 1D-CNN\nOutput Classification FCN Maxpooling\nHead"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "76.00|75.82": "71.05|70.79",
          "76.48|76.37": "71.13|70.80",
          "76.39|76.19": "63.89|64.40",
          "87.50|87.55": "68.06|68.14",
          "86.95|83.45": "86.67|86.69",
          "93.33|92.76": "89.33|88.74",
          "24.15|14.58": "53.22|48.33",
          "25.62|20.54": "54.55|49.62",
          "61.98|60.84": "75.52|75.69",
          "77.08|76.79": "79.69|79.71"
        },
        {
          "76.00|75.82": "70.02|68.97",
          "76.48|76.37": "71.36|71.21",
          "76.39|76.19": "50.00|50.12",
          "87.50|87.55": "51.39|51.15",
          "86.95|83.45": "89.33|86.68",
          "93.33|92.76": "94.67|94.88",
          "24.15|14.58": "81.65|75.16",
          "25.62|20.54": "82.64|82.26",
          "61.98|60.84": "72.92|73.08",
          "77.08|76.79": "80.73|80.72"
        },
        {
          "76.00|75.82": "73.09|72.99",
          "76.48|76.37": "74.67|74.57",
          "76.39|76.19": "74.31|74.43",
          "87.50|87.55": "77.78|77.30",
          "86.95|83.45": "78.67|75.66",
          "93.33|92.76": "92.00|92.37",
          "24.15|14.58": "39.74|35.21",
          "25.62|20.54": "44.63|41.80",
          "61.98|60.84": "64.58|63.67",
          "77.08|76.79": "82.81|82.64"
        },
        {
          "76.00|75.82": "67.19|67.11",
          "76.48|76.37": "69.16|68.86",
          "76.39|76.19": "62.50|62.50",
          "87.50|87.55": "63.19|63.57",
          "86.95|83.45": "86.67|87.57",
          "93.33|92.76": "96.00|95.00",
          "24.15|14.58": "68.35|61.25",
          "25.62|20.54": "70.25|68.64",
          "61.98|60.84": "71.88|71.93",
          "77.08|76.79": "71.88|72.09"
        },
        {
          "76.00|75.82": "69.08|68.74",
          "76.48|76.37": "69.24|69.00",
          "76.39|76.19": "36.11|35.17",
          "87.50|87.55": "40.97|40.51",
          "86.95|83.45": "76.00|77.71",
          "93.33|92.76": "80.00|70.60",
          "24.15|14.58": "53.72|52.48",
          "25.62|20.54": "64.46|64.21",
          "61.98|60.84": "68.75|68.69",
          "77.08|76.79": "76.56|76.47"
        },
        {
          "76.00|75.82": "79.26|79.23",
          "76.48|76.37": "79.70|79.63",
          "76.39|76.19": "85.42|85.30",
          "87.50|87.55": "88.89|88.85",
          "86.95|83.45": "97.56|94.65",
          "93.33|92.76": "98.67|98.13",
          "24.15|14.58": "91.74|91.57",
          "25.62|20.54": "93.39|93.35",
          "61.98|60.84": "95.33|95.31",
          "77.08|76.79": "96.36|96.35"
        },
        {
          "76.00|75.82": "67.66|67.44",
          "76.48|76.37": "68.06|68.04",
          "76.39|76.19": "77.08|77.00",
          "87.50|87.55": "81.94|81.92",
          "86.95|83.45": "91.00|90.67",
          "93.33|92.76": "92.36|92.00",
          "24.15|14.58": "77.54|71.65",
          "25.62|20.54": "81.82|80.20",
          "61.98|60.84": "81.25|81.19",
          "77.08|76.79": "87.50|87.48"
        },
        {
          "76.00|75.82": "",
          "76.48|76.37": "",
          "76.39|76.19": "M(Train)-C(Test)",
          "87.50|87.55": "",
          "86.95|83.45": "E(Train)-M(Test)",
          "93.33|92.76": "",
          "24.15|14.58": "A(Train)-M(Test)",
          "25.62|20.54": "",
          "61.98|60.84": "",
          "77.08|76.79": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "30.56|25.54": "40.28|38.94",
          "32.64|28.51": "42.36|41.08",
          "19.43|9.33": "18.93|12.94",
          "20.77|10.48": "21.24|13.54",
          "30.56|25.06": "25.69|20.69",
          "31.94|29.57": "38.61|32.53",
          "17.89|6.51": "31.25|22.47",
          "19.44|13.56": "31.94|24.67",
          "22.92|11.22": "24.31|13.46",
          "29.17|16.49": "28.47|17.39"
        },
        {
          "30.56|25.54": "25.69|19.79",
          "32.64|28.51": "30.56|24.90",
          "19.43|9.33": "20.40|9.55",
          "20.77|10.48": "26.59|21.13",
          "30.56|25.06": "28.47|25.98",
          "31.94|29.57": "34.72|33.24",
          "17.89|6.51": "29.17|26.93",
          "19.44|13.56": "36.81|35.88",
          "22.92|11.22": "30.56|23.33",
          "29.17|16.49": "31.25|29.94"
        },
        {
          "30.56|25.54": "35.42|31.89",
          "32.64|28.51": "40.28|39.68",
          "19.43|9.33": "27.14|19.51",
          "20.77|10.48": "30.45|24.66",
          "30.56|25.06": "21.53|16.71",
          "31.94|29.57": "28.47|28.86",
          "17.89|6.51": "18.74|10.26",
          "19.44|13.56": "20.83|12.69",
          "22.92|11.22": "22.22|12.54",
          "29.17|16.49": "23.61|16.29"
        },
        {
          "30.56|25.54": "27.78|23.38",
          "32.64|28.51": "29.17|27.65",
          "19.43|9.33": "13.38|12.18",
          "20.77|10.48": "17.55|13.00",
          "30.56|25.06": "27.72|22.42",
          "31.94|29.57": "31.25|30.97",
          "17.89|6.51": "25.00|20.22",
          "19.44|13.56": "27.78|23.57",
          "22.92|11.22": "29.17|27.10",
          "29.17|16.49": "30.56|27.40"
        },
        {
          "30.56|25.54": "25.69|20.63",
          "32.64|28.51": "25.69|20.63",
          "19.43|9.33": "21.09|10.75",
          "20.77|10.48": "22.42|12.95",
          "30.56|25.06": "22.22|19.32",
          "31.94|29.57": "27.78|26.93",
          "17.89|6.51": "15.28|15.15",
          "19.44|13.56": "34.72|34.63",
          "22.92|11.22": "22.92|14.34",
          "29.17|16.49": "22.92|16.25"
        },
        {
          "30.56|25.54": "45.58|44.12",
          "32.64|28.51": "51.67|47.55",
          "19.43|9.33": "44.78|37.54",
          "20.77|10.48": "49.66|41.74",
          "30.56|25.06": "50.14|44.92",
          "31.94|29.57": "55.56|46.56",
          "17.89|6.51": "49.86|43.19",
          "19.44|13.56": "53.33|49.72",
          "22.92|11.22": "49.34|42.84",
          "29.17|16.49": "54.03|49.67"
        },
        {
          "30.56|25.54": "22.22|21.55",
          "32.64|28.51": "30.56|30.19",
          "19.43|9.33": "32.10|27.75",
          "20.77|10.48": "35.09|31.52",
          "30.56|25.06": "27.78|25.31",
          "31.94|29.57": "29.17|27.89",
          "17.89|6.51": "25.14|22.37",
          "19.44|13.56": "27.08|24.88",
          "22.92|11.22": "34.72|34.17",
          "29.17|16.49": "38.89|38.12"
        },
        {
          "30.56|25.54": "",
          "32.64|28.51": "",
          "19.43|9.33": "M(Train)-E(Test)",
          "20.77|10.48": "",
          "30.56|25.06": "E(Train)-C(Test)",
          "31.94|29.57": "",
          "17.89|6.51": "A(Train)-E(Test)",
          "19.44|13.56": "",
          "22.92|11.22": "",
          "29.17|16.49": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "33.33|10.42": "50.67|44.29",
          "60.00|55.17": "62.05|60.00",
          "25.33|17.19": "37.33|15.76",
          "49.33|49.33": "40.00|19.90",
          "22.82|11.67": "29.11|21.17",
          "23.84|13.67": "34.54|22.27",
          "29.85|19.11": "40.00|18.64",
          "38.67|24.69": "76.00|71.80",
          "45.33|44.67": "50.67|47.97",
          "52.32|52.00": "64.00|55.28"
        },
        {
          "33.33|10.42": "34.71|26.40",
          "60.00|55.17": "49.33|45.10",
          "25.33|17.19": "50.67|41.19",
          "49.33|49.33": "52.00|47.72",
          "22.82|11.67": "33.52|28.31",
          "23.84|13.67": "37.45|36.37",
          "29.85|19.11": "57.33|58.58",
          "38.67|24.69": "72.00|69.57",
          "45.33|44.67": "17.33|15.84",
          "52.32|52.00": "20.00|16.18"
        },
        {
          "33.33|10.42": "72.00|68.11",
          "60.00|55.17": "74.67|69.71",
          "25.33|17.19": "38.67|26.19",
          "49.33|49.33": "40.00|26.20",
          "22.82|11.67": "25.10|15.27",
          "23.84|13.67": "26.07|15.27",
          "29.85|19.11": "38.57|25.69",
          "38.67|24.69": "42.78|30.97",
          "45.33|44.67": "35.74|38.67",
          "52.32|52.00": "40.00|41.04"
        },
        {
          "33.33|10.42": "36.00|26.30",
          "60.00|55.17": "40.00|34.17",
          "25.33|17.19": "49.33|35.29",
          "49.33|49.33": "50.67|35.69",
          "22.82|11.67": "18.02|9.78",
          "23.84|13.67": "20.06|12.41",
          "29.85|19.11": "61.33|59.75",
          "38.67|24.69": "77.33|74.43",
          "45.33|44.67": "52.00|40.11",
          "52.32|52.00": "52.00|46.80"
        },
        {
          "33.33|10.42": "21.33|19.52",
          "60.00|55.17": "32.00|32.28",
          "25.33|17.19": "38.67|31.32",
          "49.33|49.33": "49.33|41.37",
          "22.82|11.67": "18.80|11.56",
          "23.84|13.67": "21.64|13.99",
          "29.85|19.11": "33.33|31.02",
          "38.67|24.69": "41.33|33.87",
          "45.33|44.67": "14.07|17.33",
          "52.32|52.00": "22.66|25.33"
        },
        {
          "33.33|10.42": "75.60|73.61",
          "60.00|55.17": "79.33|74.16",
          "25.33|17.19": "55.33|47.03",
          "49.33|49.33": "58.00|56.73",
          "22.82|11.67": "48.54|46.15",
          "23.84|13.67": "54.84|52.75",
          "29.85|19.11": "86.14|85.33",
          "38.67|24.69": "91.08|90.67",
          "45.33|44.67": "78.67|76.38",
          "52.32|52.00": "80.00|78.70"
        },
        {
          "33.33|10.42": "56.00|44.84",
          "60.00|55.17": "64.00|51.60",
          "25.33|17.19": "52.00|46.90",
          "49.33|49.33": "54.67|49.27",
          "22.82|11.67": "28.09|20.44",
          "23.84|13.67": "29.58|21.38",
          "29.85|19.11": "47.25|41.25",
          "38.67|24.69": "52.19|49.28",
          "45.33|44.67": "68.80|65.99",
          "52.32|52.00": "72.00|68.27"
        },
        {
          "33.33|10.42": "",
          "60.00|55.17": "",
          "25.33|17.19": "M(Train)-A(Test)",
          "49.33|49.33": "",
          "22.82|11.67": "E(Train)-A(Test)",
          "23.84|13.67": "",
          "29.85|19.11": "A(Train)-C(Test)",
          "38.67|24.69": "",
          "45.33|44.67": "",
          "52.32|52.00": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "35.54|26.33": "42.15|38.31",
          "45.45|35.56": "44.63|44.10",
          "19.83|6.62": "23.14|14.80",
          "23.14|11.80": "31.40|22.18",
          "47.11|41.94": "51.24|46.89",
          "65.29|62.80": "52.89|46.93",
          "18.65|8.60": "28.32|16.33",
          "20.61|15.45": "34.62|26.10",
          "36.36|32.85": "29.75|24.61",
          "45.45|45.09": "31.40|25.79"
        },
        {
          "35.54|26.33": "23.61|19.69",
          "45.45|35.56": "45.45|40.09",
          "19.83|6.62": "38.84|29.54",
          "23.14|11.80": "40.50|32.51",
          "47.11|41.94": "44.63|39.60",
          "65.29|62.80": "55.37|50.97",
          "18.65|8.60": "31.24|27.97",
          "20.61|15.45": "32.49|28.93",
          "36.36|32.85": "33.88|29.61",
          "45.45|45.09": "39.67|33.52"
        },
        {
          "35.54|26.33": "48.76|42.90",
          "45.45|35.56": "55.37|49.50",
          "19.83|6.62": "19.18|6.62",
          "23.14|11.80": "24.79|17.53",
          "47.11|41.94": "36.36|29.04",
          "65.29|62.80": "47.93|44.72",
          "18.65|8.60": "25.85|17.40",
          "20.61|15.45": "27.69|23.15",
          "36.36|32.85": "23.97|17.46",
          "45.45|45.09": "31.40|26.11"
        },
        {
          "35.54|26.33": "27.27|17.64",
          "45.45|35.56": "37.19|30.99",
          "19.83|6.62": "25.62|15.54",
          "23.14|11.80": "41.32|37.63",
          "47.11|41.94": "51.24|47.91",
          "65.29|62.80": "52.07|49.55",
          "18.65|8.60": "22.90|15.28",
          "20.61|15.45": "25.96|20.07",
          "36.36|32.85": "39.67|36.58",
          "45.45|45.09": "40.50|39.28"
        },
        {
          "35.54|26.33": "29.75|24.79",
          "45.45|35.56": "32.23|26.49",
          "19.83|6.62": "28.10|24.59",
          "23.14|11.80": "38.02|32.76",
          "47.11|41.94": "38.02|32.38",
          "65.29|62.80": "42.98|38.41",
          "18.65|8.60": "25.18|20.75",
          "20.61|15.45": "25.89|23.12",
          "36.36|32.85": "22.31|15.51",
          "45.45|45.09": "28.93|26.55"
        },
        {
          "35.54|26.33": "62.98|55.09",
          "45.45|35.56": "67.93|61.78",
          "19.83|6.62": "52.23|44.48",
          "23.14|11.80": "62.98|54.73",
          "47.11|41.94": "66.12|65.36",
          "65.29|62.80": "73.90|73.55",
          "18.65|8.60": "50.67|45.94",
          "20.61|15.45": "53.19|49.29",
          "36.36|32.85": "51.40|44.32",
          "45.45|45.09": "58.84|55.14"
        },
        {
          "35.54|26.33": "37.19|29.27",
          "45.45|35.56": "47.11|38.54",
          "19.83|6.62": "38.02|33.50",
          "23.14|11.80": "50.41|43.70",
          "47.11|41.94": "61.16|60.27",
          "65.29|62.80": "67.77|67.23",
          "18.65|8.60": "31.25|26.15",
          "20.61|15.45": "32.65|29.65",
          "36.36|32.85": "46.28|38.97",
          "45.45|45.09": "53.72|49.69"
        },
        {
          "35.54|26.33": "",
          "45.45|35.56": "",
          "19.83|6.62": "M(Train)-R(Test)",
          "23.14|11.80": "",
          "47.11|41.94": "E(train)-R(Test)",
          "65.29|62.80": "",
          "18.65|8.60": "A(Train)-R(Test)",
          "20.61|15.45": "",
          "36.36|32.85": "",
          "45.45|45.09": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "31.77|18.93": "62.50|60.84",
          "38.54|28.60": "64.58|61.40",
          "20.31|7.67": "20.31|8.69",
          "27.08|19.64": "21.30|8.75",
          "20.31|6.75": "25.00|15.80",
          "40.62|40.85": "37.50|34.46",
          "17.65|11.90": "22.40|10.57",
          "19.79|15.69": "29.17|27.07",
          "26.28|18.75": "36.90|32.64",
          "35.25|30.87": "38.79|36.73"
        },
        {
          "31.77|18.93": "29.17|23.17",
          "38.54|28.60": "32.29|27.46",
          "20.31|7.67": "30.73|23.06",
          "27.08|19.64": "35.94|27.42",
          "20.31|6.75": "32.29|21.84",
          "40.62|40.85": "32.29|25.27",
          "17.65|11.90": "26.56|19.30",
          "19.79|15.69": "28.80|23.38",
          "26.28|18.75": "25.89|17.06",
          "35.25|30.87": "33.28|23.24"
        },
        {
          "31.77|18.93": "40.62|28.50",
          "38.54|28.60": "42.19|29.18",
          "20.31|7.67": "20.31|7.67",
          "27.08|19.64": "25.52|18.09",
          "20.31|6.75": "22.92|11.65",
          "40.62|40.85": "40.62|28.50",
          "17.65|11.90": "24.98|18.64",
          "19.79|15.69": "26.04|21.66",
          "26.28|18.75": "31.00|24.70",
          "35.25|30.87": "36.66|30.48"
        },
        {
          "31.77|18.93": "21.35|8.93",
          "38.54|28.60": "26.56|18.47",
          "20.31|7.67": "29.17|21.53",
          "27.08|19.64": "30.73|26.00",
          "20.31|6.75": "34.90|27.62",
          "40.62|40.85": "34.90|29.65",
          "17.65|11.90": "29.17|27.07",
          "19.79|15.69": "32.29|32.24",
          "26.28|18.75": "19.75|10.54",
          "35.25|30.87": "20.61|17.97"
        },
        {
          "31.77|18.93": "41.15|32.45",
          "38.54|28.60": "43.75|37.60",
          "20.31|7.67": "19.79|6.61",
          "27.08|19.64": "19.79|6.61",
          "20.31|6.75": "21.35|8.81",
          "40.62|40.85": "21.35|8.81",
          "17.65|11.90": "27.08|19.87",
          "19.79|15.69": "31.25|26.72",
          "26.28|18.75": "28.87|26.77",
          "35.25|30.87": "32.42|31.13"
        },
        {
          "31.77|18.93": "71.88|71.33",
          "38.54|28.60": "72.40|72.29",
          "20.31|7.67": "45.52|34.70",
          "27.08|19.64": "48.65|38.98",
          "20.31|6.75": "66.15|66.12",
          "40.62|40.85": "72.40|72.29",
          "17.65|11.90": "64.06|63.17",
          "19.79|15.69": "69.52|69.27",
          "26.28|18.75": "57.20|56.57",
          "35.25|30.87": "62.23|61.59"
        },
        {
          "31.77|18.93": "35.52|27.30",
          "38.54|28.60": "41.15|32.68",
          "20.31|7.67": "34.38|29.53",
          "27.08|19.64": "39.58|35.03",
          "20.31|6.75": "37.84|31.91",
          "40.62|40.85": "38.54|36.40",
          "17.65|11.90": "34.89|31.45",
          "19.79|15.69": "37.50|34.43",
          "26.28|18.75": "37.84|34.80",
          "35.25|30.87": "37.92|36.40"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Test) M(Train) -E(Test) E(Train) -C(Test) A(Train) -E(Test) R(Train) -E",
      "authors": [
        ") -E C(train"
      ],
      "venue": "Test) M(Train) -E(Test) E(Train) -C(Test) A(Train) -E(Test) R(Train) -E"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Test) M(Train) -A(Test) E(Train) -A(Test) A(Train) -C(Test) R(Train) -A",
      "authors": [
        ") -A C(train"
      ],
      "venue": "Test) M(Train) -A(Test) E(Train) -A(Test) A(Train) -C(Test) R(Train) -A"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "Test) M(Train) -R(Test) E(train) -R(Test) A(Train) -R(Test) R",
      "authors": [
        ")-R C(train"
      ],
      "venue": "Test) M(Train) -R(Test) E(train) -R(Test) A(Train) -R(Test) R"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn Fcn Cnn"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Svm scheme for speech emotion recognition using mfcc feature",
      "authors": [
        "A Milton",
        "S Roy",
        "S Selvi"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "8",
      "title": "Speaker dependent speech emotion recognition using mfcc and support vector machine",
      "authors": [
        "P Dahake",
        "K Shaw",
        "P Malathi"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang"
      ],
      "venue": "Interspeech 2021, 2021",
      "doi": "10.21437/Interspeech.2021-1775"
    },
    {
      "citation_id": "14",
      "title": "Emo-superb: An in-depth look at speech emotion recognition",
      "authors": [
        "H Wu"
      ],
      "year": "2024",
      "venue": "Emo-superb: An in-depth look at speech emotion recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "15",
      "title": "Open-emotion: A reproducible emo-superb for speech emotion recognition systems",
      "authors": [
        "H Wu"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Generalization of self-supervised learning-based representations for cross-domain speech emotion recognition",
      "authors": [
        "A Naini",
        "M Kohler",
        "E Richerson",
        "D Robinson",
        "C Busso"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "authors": [
        "A Ibrahim",
        "S Shehata",
        "A Kulkarni",
        "M Mohamed",
        "M Abdul-Mageed"
      ],
      "venue": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "doi": "10.21437/Interspeech.2024-1983"
    },
    {
      "citation_id": "18",
      "title": "Ser evals: In-domain and out-of-domain benchmarking for speech emotion recognition",
      "authors": [
        "M Osman",
        "D Kaplan",
        "T Nadeem"
      ],
      "venue": "Ser evals: In-domain and out-of-domain benchmarking for speech emotion recognition",
      "doi": "10.21437/Interspeech.2024-2440"
    },
    {
      "citation_id": "19",
      "title": "Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Z Ma"
      ],
      "year": "2024",
      "venue": "Interspeech",
      "doi": "10.21437/Interspeech.2024-788"
    },
    {
      "citation_id": "20",
      "title": "Are paralinguistic representations all that is needed for speech emotion recognition?",
      "authors": [
        "O Phukan",
        "G Kashyap",
        "A Buduru",
        "R Sharma"
      ],
      "venue": "Are paralinguistic representations all that is needed for speech emotion recognition?",
      "doi": "10.21437/Interspeech.2024-2233"
    },
    {
      "citation_id": "21",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "S Chen"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2021-703"
    },
    {
      "citation_id": "25",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2022-143"
    },
    {
      "citation_id": "26",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "Scaling speech technology to 1,000+ languages",
      "authors": [
        "V Pratap"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "28",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "J Shor",
        "S Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2022-118"
    },
    {
      "citation_id": "29",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2018.8461375"
    },
    {
      "citation_id": "30",
      "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-2561"
    },
    {
      "citation_id": "31",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "32",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "33",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "34",
      "title": "The mexican emotional speech database (mesd): Elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "36",
      "title": "Whisper encoder features for infant cry classification",
      "authors": [
        "M Charola",
        "A Kachhi",
        "H Patil"
      ],
      "venue": "Whisper encoder features for infant cry classification",
      "doi": "10.21437/Interspeech.2023-1916"
    }
  ]
}