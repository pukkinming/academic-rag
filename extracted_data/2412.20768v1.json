{
  "paper_id": "2412.20768v1",
  "title": "Sample Correlation For Fingerprinting Deep Face Recognition",
  "published": "2024-12-30T07:37:06Z",
  "authors": [
    "Jiyang Guan",
    "Jian Liang",
    "Yanbo Wang",
    "Ran He"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-theshelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, remarkable advancements in face recognition have been largely attributable to the development of deep learning techniques  [1] . A common practice for model owners is to offer their models to clients through either cloud-based services or client-side software. Generally, training deep neural networks, especially deep face recognition models, is both resource-intensive and financially burdensome, requiring extensive data collection and significant computational resources. Therefore, welltrained models possess valuable intellectual property and necessitate protection  [2; 3] . Nonetheless, model stealing attacks can steal these well-trained models and evade the model owners' detection with only API access to the models  [4] , posing serious threats to the model owner's Intellectual Property (IP).\n\nModel stealing attacks are carried out with the goal of illegally obtaining functionally equivalent copies of the well-trained model owners' source model, with the white-box or even the black-box access to the source models. In the case of white-box access, the attacker can gain access to all the internal parameters of the source model. To avoid detection by the model owner, the attacker is able to employ source model modification, including pruning  [5] , fine-tuning  [6] , adversarial training  [7] , and knowledge distillation  [8] . Furthermore, attackers are also able to leverage the model extraction attack  [9; 10]  to steal the function of the source model, with only the black-box access to the source model. In such a paradigm of model stealing attack, the attacker can steal the function of the source model using only the model's outputs, without the need for access to the inner parameters, and thus is considered more general and threatening. Regarding deep face recognition models, we observe that model extraction attacks achieve an accuracy of up to 95.0% of the original accuracy of the source model on KDEF  [11]  in face emotion recognition with only output labels of the source model. Moreover, in face verification, we also observe attackers can evade most of the stealing detection easily because these models only output the verification results rather than labels. In total, deep face recognition is confronted with a pressing challenge posed by model stealing attacks.\n\nIn recent years, the growing concerns over model stealing attacks have led to the development of various methods aiming at protecting the intellectual property (IP) of the deep models. Generally, these methods can be categorized into two categories: the watermarking methods  [12; 13; 14; 15; 16; 17; 18; 2; 19]  and the fingerprinting methods  [4; 20; 21; 22; 23] . Watermarking techniques typically incorporate either weight regularization methods  [12; 13; 14]  or backdoor insertion strategies  [16; 17; 2]  during the model training phase to embed a distinct watermark into the model. However, these approaches need to manipulate the model's training process, often resulting in a trade-off where the model's performance on its main task is compromised. On the contrary, fingerprinting methods leverage the transferability of adversarial examples and identify stolen models by calculating the attack success rate on the suspected model. These methods do not interfere with the model's training procedure, which means they do not sacrifice the model's accuracy on its main task. However, it's important to note that adversarial-example-based fingerprinting methods can still be vulnerable to adversarial training  [24]  or transfer learning  [25]  and are resourceintensive as well as time-consuming for the model owner  [4] . Furthermore, when it comes to the threat of model stealing attacks on well-trained deep face recognition models, unfortunately, it is regrettable to note that no model fingerprinting methods have been proposed so far. Faced with these stealing threats within the field of deep face recognition, we propose a model fingerprinting method tailored specifically to this domain.\n\nTo overcome the weaknesses of existing methods and solve the model fingerprinting problem in deep face recognition, we propose a correlation-based model fingerprinting method called SAC. As mentioned above, existing model fingerprinting methods rely on the suspect model's output as a point-wise indicator to detect the stolen models, which neglects the information hidden behind pair-wise correlation. Intuitively, samples with similar outputs in the source model are more likely to have similar outputs in the stolen models  [26] . Specifically, we utilize the correlation difference between the source model and the suspect model as an indicator for detecting the stolen model. Nevertheless, calculating correlation among clean samples from the defender's dataset may be affected by the common knowledge shared by most models trained for the same task, on which most of the models produce identical labels. To get rid of the influence of common knowledge shared by most models, we leverage data augmentation to magnify the difference between models and have studied the influence of different augmentation methods used in Hendrycks and Dietterich  [27]  on SAC. Results demonstrate that SAC with JPEG Compression (SAC-JC) achieves the best results on different tasks and model architectures. Furthermore, on the task of face verification, because the model owner can only know whether two images are from the same identity and cannot get access to the exact label or probability of the images, we propose Feature from Reference Images (FRI). FRI gathers a batch of n + 1 images from the same identity and chooses one sample as the target for augmentation and the other samples as reference, and we leverage the results of the face verification model (whether 0 or 1) to get an n dimension vector to replace the model outputs used in SAC. To assess the effectiveness of SAC on face recognition, we conduct experiments involving five distinct types of attacks: fine-tuning, pruning, transfer learning, model extraction, and adversarial training on two common face recognition tasks face verification  [28]  and face emotion recognition  [11]  across different model architectures. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, also demonstrating that SAC-JC outperforms the previous methods.\n\nOur main contributions are summarized as follows:\n\n- Compared to the preliminary conference version  [26] , this manuscript has made significant improvements and extensions. The main differences can be summarized into five aspects: 1) We disclose the vulnerability of deep face recognition models to model stealing attacks and design a feature generation method FRI to fingerprint the face verification task in Section 3.4. 2) We study the influence of augmented samples on SAC and propose JPEG compression based SAC-JC in Section 3.3, which is more effective and converted. 3) We extend SAC-JC to two more face-related tasks including the face verification task and face emotion recognition task KDEF, and evaluate SAC-JC on two more model stealing attacks including adversarial training and knowledge distillation in Section 4.2. 4) We provide additional experimental results and in-depth analysis in Section 4. 5) We add analysis and related work about fingerprinting on deep face recognition in Section 1 and Section 2.2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Deep Ip Protection",
      "text": "Model stealing attacks present a significant risk to the proprietary rights of the model's owner. These attacks can be categorized into several distinct methods: 1. Finetuning  [29] : In this method, the attacker modifies the parameters of the source model with labeled training data for multiple epochs. The attackers can fine-tune the source model in all layers or only the last layer. 2. Pruning  [5; 6; 30] : Attackers employing this technique selectively prune less significant weights in the source model based on certain indicators, often involving activation values. 3. Transfer learning  [25] : In this setting, the attacker adapts the source model for similar tasks and utilizes the knowledge embedded in the source model to advance their own goals. 4. Model extraction  [10; 9] : Given the substantial expenses and time required for data labeling, attackers opt for this technique. It involves replicating the functionality of the source model using unlabeled data from the same distribution. Remarkably, this attack can be executed without access to the source model's internal parameters, relying solely on the model's outputs. 5. Adversarial training  [24] : Attackers employ a blend of normal and adversarial examples to train models, which helps circumvent most fingerprinting detection methods. To counter the threat of model stealing attacks, numerous methods for protecting model intellectual property (IP) have been proposed. These methods can generally be categorized into two main categories: watermarking methods and fingerprinting methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Watermarking Methods",
      "text": "Watermarking methods mainly focus on the training phase of the source model. They usually rely on weight regularization  [12; 13; 14; 15]  to add weight-related watermark into models, or train models on triggered set to leave the backdoor in them  [16; 17] . Nevertheless, methods mentioned above can not detect newer attacks, for example, model extraction which trains a surrogate model from scratch  [10; 4] . Even though some watermark methods such as VEF  [31]  or EWE  [2]  could handle model extraction, they unavoidably interfere in the training process, sacrificing model utility  [18; 2; 19]  for IP protection. For VEF, it also requires white-box access to the suspect model, which greatly limits the functionality, let alone some special circumstances under which the accuracy drop for IP protection is absolutely unacceptable.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fingerprinting Methods",
      "text": "Fingerprinting, in contrast, capitalizes on the transferability of adversarial examples and therefore authenticates model ownership without the need to manipulate the model training process. This approach ensures zero compromise in model accuracy. Lukas et al.  [4]  proposes conferrable adversarial examples, with the aim of optimizing their transferability to stolen models while minimizing the transferability to irrelevant models trained independently. Additionally, ModelDiff  [21] , FUAP  [22] , and DFA  [23]  employ various types of adversarial examples, such as DeepFool  [32]  and UAP  [33] , to fingerprint the source model. Except for this, DeepJudge  [34]  introduces a unified framework, utilizing various indicators to detect model stealing under both white-box and black-box settings. However, these techniques are vulnerable to adversarial defenses like adversarial training  [24]  or transfer learning, making them less effective in preserving model ownership. Moreover, these methods often necessitate the training of numerous surrogate and irrelevant models with diverse architectures to create robust fingerprints, which imposes a significant computational burden on the model owner. Unlike previous approaches, our method relies on the correlation between samples rather than simple instance-level differences, enabling faster and more robust model fingerprinting with data-augmented samples instead of adversarial examples. Fig.  1 : Framework of SAC-JC. We first generate JPEG compressed samples as model inputs, represented by the colored balls. For the face verification model, we leverage FRI to calculate the inputs' model-specific features from the reference-target pairs. Then we calculate the correlation difference and any suspect model with a similar correlation will be recognized as a stolen model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Fingerprinting On Deep Face Recognition",
      "text": "Deep face recognition has emerged as a prominent biometric technique for identity authentication and has been widely used in many areas, such as finance, public security, and daily life  [1] . Well-trained deep face recognition models hold significant value and can be deployed as either cloud-based services or client-side software. However, these well-trained deep face recognition models are threatened by the model stealing attacks  [35]  and an example is that, only with the black-box access to the source model, model extraction attacks can reach 95.0% of the original accuracy of the source model on KDEF  [11]  in face emotion recognition. Furthermore, model stealing attacks also threaten the face verification models. Face verification is an important task in face recognition, and computes the one-to-one similarity between faces and determines whether two faces belong to the same identity  [36] . Because the defenders can only get access to whether two faces belong to the same identity in the suspect model while no labels are available in the prevailing black-box fingerprinting setting, there are problems with existing model IP protection methods. As far as we know, no existing model IP protection methods are designed for protecting the model's intellectual property in verification tasks such as face verification. To solve the problem in face verification, we leverage FRI to generate samples' specific features from the 0-1 results of face verification models and calculate the correlation difference between the source model and the suspect models to detect the model stealing attacks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "In the context of the model IP protection scenario, there are two primary parties involved: the defender and the attacker. The defender is the entity that owns the wellperforming machine learning model by using a proprietary training dataset and a specific training algorithm  [20] . In a cloud service setting, the defender deploys their well-trained models as a cloud service or client-sided software  [20]  so that attackers could only get access to the model output. Another setting is called the client-sided software setting, where the attacker can get white-box access to all the inner parameters as well as the model structure.\n\nThe attacker's objective is to use their own dataset, which follows the same distribution as the defender's data, to reverse engineer a model that closely mimics the accuracy of the original model, while the defender aims at identifying the ownership of the suspect model. Generally speaking, cloud service deployment is the prevailing choice in the market. As such, our primary focus is on black-box IP protection, where the defender is constrained to accessing only the output of the suspect model while remaining oblivious to the architecture of the suspect model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sample Correlation For Neural Network Fingerprinting",
      "text": "Previous fingerprinting methods have traditionally focused solely on point-wise consistency between the source model and the suspect model, identifying model stealing behavior by determining whether the suspect model classifies the same adversarial examples into the same incorrect classes as the source model. However, these methods often prove to be less robust when it comes to scenarios involving adversarial training or transfer learning.\n\nTo enhance the robustness of model stealing detection, we shift our approach away from point-wise criteria to the pairwise relationship between model outputs. The intuition behind this is easy to understand: when two samples yield similar outputs in the source model, they are more likely to produce similar outputs in the stolen models as well. Therefore, we introduce SAC, a correlation-based fingerprinting method that leverages the previously mentioned correlation consistency to identify model stealing effectively. Additionally, to mitigate the impact of shared common knowledge among irrelevant models, we investigate the process of searching for suitable samples, as elaborated in Section 3.3. An overview of our framework is illustrated in Figure  1 .\n\nHere we elaborate on the details of model fingerprinting with sample correlation  [37; 38] . Assign\n\nwhere C i,j denotes the i, j entry of the model's correlation matrix C, which is computed by assessing the correlation between the i-th and j-th outputs of the model. Φ is the function for calculating the correlation matrix based on the output set. To precisely measure the correlation among the model's outputs, we introduce several functions to model the relationship of outputs  [38] . First is cosine similarity  [39] , which denotes the cosine of the angle between two vectors:\n\nAnother method for measuring the correlation of model outputs is Gaussian RBF  [40] . Gaussian RBF is a popular kernel function, computing the distance between two instances based on their Euclidean distances:\n\nOnce the correlation matrices have been computed for both the source model and the suspect model, we proceed to calculate the L1 distance between these matrices, which serves as our fingerprinting indicator. Any model whose distance to the source model falls below a specified threshold value, denoted as d, will be identified as a stolen model:\n\nwhere we denote C stolen and C source as the correlation matrix of the stolen model and the source model. In scenarios where defenders seek to determine the optimal threshold value d, the use of a validation set can be instrumental. For instance, defenders can utilize the average of the means of the correlation indicators obtained from the irrelevant models and the models created through adversarial extraction on the validation set as the threshold d.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "How To Find Suitable Samples?",
      "text": "Utilizing the correlation matrix stated above, SAC calculates the distance between the source models and the suspect models. In addition, it is crucial to have suitable samples as model inputs to support this fingerprinting process. Because models trained for the same task will output the same ground-truth labels on most of the clean samples, SAC's performance is affected by the common knowledge shared by these models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fingerprinting With Jpeg Compressed Samples",
      "text": "To get rid of the influence of common knowledge and amplify the difference between models, we leverage data augmentation on the randomly selected normal samples as the input for SAC. We first study the influence of 14 different types of corrupted methods from Hendrycks and Dietterich  [27]  as the augmented methods on SAC. Figure  2     [43]  to evade these fingerprinting methods' detection. Our experiments demonstrate that attackers can successfully evade detection by fine-tuning the extracted model for just a few epochs with unlabeled data and the predicted label from the source model in an adversarial training way, expressed as follows:\n\nwhere f stolen and f source denotes the stolen and the source model, δ denotes the adversarial noise smaller than the bound ϵ, which is generated by the attacker using adversarial attack methods such as FGSM  [44]  or PGD  [24]  and θ stolen denotes the parameters of the stolen model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Calculating Image Feature In Face Verification",
      "text": "In the context of face verification tasks, the model does not produce output labels. Instead, it only provides a binary result, typically denoting whether two images belong to the same identity. The lack of model output in label space from images causes problems in calculating the correlation matrix in SAC. To handle this issue as well as get the feature of the target image, we propose a method called Feature from Reference Images (FRI).\n\nAiming at forming the specific feature of the target image, FRI first gathers n reference images with the same identity as the target image, and then, makes use of JPEG compression to augment the target image to amplify the difference between the target image and the reference images. Finally, FRI forms n target-reference pairs as the input of the suspect models and gets an n dimension vector as the model-specific feature of the target image:\n\nwhere F t represents the feature of the target image, V represents the verification model, I t represents the target image, and I r(n) represents the n -th reference image. To be specific, we set n = 50 in our experiments and F t is a 50 dimension 0-1 feature for the target image.\n\nWe then replace the output of the model in SAC with the feature F t generated from FRI. The algorithm for employing SAC-JC with the feature generation method FRI in face verification is illustrated in Algorithm 1 The experiment results illustrated in Section 4.2 demonstrate the effectiveness of SAC-JC with FRI in face verification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Setup",
      "text": "In this section, we evaluate various methods for safeguarding model intellectual property (IP) against diverse model stealing attacks on a range of datasets and model structures, confirming the efficacy of SAC-JC. To be specific, we design our evaluation of different IP protection methods against five categories of stealing attacks as listed below:\n\n-Fine-tuning. Typically, there exist two prevalent fine-tuning techniques: fine-tuning the last layer\n\nfor n in 1 . . . num do 7:\n\nCalculate correlation matrix for the source model:\n\nCalculate correlation matrix for the suspect model: (Finetune-L) or fine-tuning all the layers (Finetune-A). As the names stated, Finetune-L indicates keeping the majority of the model's layers frozen and only training the final layers, while Finetune-A means finetuning the entire model, including all of its layers.\n\nIn our experiments, we assume that the attacker fine-tunes the source model with an SGD optimizer on the attacker's dataset.\n\n-Pruning. In our settings, we adopt Fine Pruning  [5]  as the pruning method. Fine Pruning, as a commonly used method that involves pruning neurons based on their activation values, removes less significant neurons from a neural network. As a common backdoor defense method, it could typically remove neurons that contribute to backdoor or malicious behaviors. Here it could serve as an attack to threat IP protection.\n\nwhere f T stolen (x) and f T source (x) are the soft probability from the stolen and source models:\n\nT ), in which T indicates the temperature, and KL(•) refers to the KL divergence. In the following experiments, we fix T = 20 as the temperature. In face verification, because no label is available, we leverage the white-box knowledge distillation to replace the model extraction in our experiments. Knowledge distillation  [46]  is one of the model compression methods, which utilizes the knowledge from teachers to train the student models.\n\n-Adversarial Model Extraction. Sharing the similar logic as adaptive model extraction from CAE  [4] , the attacker could evade fingerprint detection with adversarial training after the label-based model extraction. The slight difference between adaptive model extraction and this thread is that the attacker achieves the extraction by the predicted label from the source model in Equation  5 , rather than the ground-truth label. Therefore, after adversarial training, the attacker could evade adversarial-examplebased fingerprinting methods with negligible accuracy sacrifice. Additionally, in face verification, we leverage adversarial training as one of the attackers' methods to evade the attackers' detection. -Transfer Learning. The attacker may also utilize the transfer learning technique to repurpose the source model for other related tasks, taking advantage of the model's knowledge while escaping from potential fingerprint detection. To simulate this, we transfer the CIFAR10 model to CIFAR10-C  [27]  and CIFAR100 dataset, from which we choose the first 10 labels. In addition, we perform transfer learning on the Tiny-ImageNet model, which is originally trained on the first 100 labels in the Tiny-ImageNet dataset, to the remaining 100 labels in the same dataset.\n\nModel Architecture.\n\nWe evaluate different IP protection methods across a range of commonly used model architectures. All extraction models and irrelevant models are trained on VGG  [47] , ResNet  [48] , DenseNet  [49]  and MobileNet  [50]  on the multi-classification tasks, including KDEF, Tiny-ImageNet, and CIFAR10. Additionally, on the face verification task, we follow the experiment in a famous project Insightface 1 and leverage the commonly used model architecture in face recognition as irrelevant models and distillation models, including ResNet18, ResNet50, and MobileFace. Furthermore, to ensure the robustness of our results, for each attack and irrelevant model architecture, we train five models in case of randomness, in other words, 20 models for irrelevant models, extraction models, and fine-tuning models.\n\nModel IP Protection Methods.\n\nIn order to validate the effectiveness of our method, we conduct a comparative analysis against several existing approaches, including IPGuard  [20] , CAE  [4] , and EWE  [2] . IPGuard and CAE leverage the transferability of adversarial examples by testing the success rate of these adversarial examples when applied to the suspected models. If the attack success rate for any model exceeds a predefined threshold, it is identified as a stolen model. In the face verification task, we utilize adversarial attacks designed for face recognition  [51]  and calculate the attack success rate in pairs to adapt IPGuard and CAE to face verification tasks for a fair comparison. In contrast, EWE takes a different approach by training the source model using backdoor data  [52]  and embedding a watermark within the model. By employing a soft nearest neighbor loss to intertwine the watermark data with the training data, EWE aims to enhance the transferability of the watermark against model extraction. One thing to note is that we did not include the results of EWE on the face verification task and face emotion recognition task. In face verification, there is no output label from the verification model, and thus EWE fails. Additionally, in face emotion recognition, even if we try our best and use the official code of EWE, EWE causes the model to collapse in main tasks with accuracy dropping lower than 20%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets.",
      "text": "To assess the effectiveness and robustness of various fingerprinting methods, we perform experiments on different datasets and multiple tasks. Following previous works, there are two datasets used in our experiments, D def ender and D attacker , which belong to the defender and the attacker respectively. In the face verification task, we leverage MS1MV2  [53]  as the dataset of the defender, and CASIA-Webface  [54]  as the dataset of the attacker. Additionally, we leverage ArcFace  [55]  as our model training protocol. As for the multi-classification tasks, e.g. KDEF  [11] , Tiny-ImageNet [56], and CIFAR10 [57], according to previous methods, we split the training dataset into two equalsized subsets: D def ender and D attacker , which are owned by the attacker and defender, respectively. One point worth noting is that given the limitation of only 250 samples per label in Tiny-ImageNet, which leads to a source model accuracy drop to approximately 40%, we opt to curate a smaller dataset by selecting the initial 100 labels. This choice allows for a higher source model accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics.",
      "text": "To assess the effectiveness of different fingerprinting methods, similar to CAE  [4] , we employ the AUC-ROC curve [58] and calculate the AUC value, which quantifies the separation between the fingerprinting scores of the irrelevant models and the stolen models, serving as a measure of fingerprinting effectiveness. The ROC curve is a graphical representation of the True Positive Rate and False Positive Rate. AUC, the area under the ROC curve, ranges from 0 to 1, with a higher AUC indicating a superior fingerprinting method. For further evaluating the performance of different fingerprinting methods, we follow  [31; 22]  and introduce p-value as another evaluation metric. We leverage an independent two-sample T-test to calculate the p-value with the null hypothesis H 0 : µ suspect = µ irrelevant , where µ suspect and µ irrelevant represent the average of the fingerprinting scores of the suspect and irrelevant models. To be specific, the fingerprinting score represents the correlation distance in SAC-JC, and attack success rate in IPGuard, CAE, and EWE. A smaller p-value indicates a higher level of confidence and a better distinction between the suspect models and the irrelevant models.\n\nAUC and p-value are both metrics not related to the threshold. To evaluate the performance of the fingerprinting methods with a specific threshold, we introduce the F1 score as another metric. The F1 score, which is the harmonic mean of precision and recall, can be calculated using the formula: F 1 = 2 × P recision×Recall P recision+Recall . Additionally, we have provided the details of the specific threshold selection in the following paragraph.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Threshold Selection.",
      "text": "In general, thresholds can be determined using a small validation set. We follow the threshold decision method used in  [21]  and select the worst value found in the irrelevant models. To be specific, we choose the smallest correlation distance found in irrelevant models in SAC-JC and the highest attack success rate found in irrelevant models in IPGuard, CAE, and EWE as the threshold. To reduce the need for collecting irrelevant models, we only use four irrelevant models in our experiments across different datasets and source model architectures. Additionally, since model fingerprinting is a black-box detection method and we do not know the category of the model stealing attacks, we use the same threshold for all suspect models within one task for detection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Fingerprinting On Face Recognition",
      "text": "In this section, we evaluate SAC-JC on two common face recognition tasks: face verification and face emotion recognition. Tables  1  and 2    black-box access to both the source model and the suspect models, while the other fingerprinting methods such as IPGuard and CAE need white-box access to the source model to generate adversarial examples. SAC-JC offers a broader range of applicability, allowing a third party to detect model stealing attacks without access to the inner parameters of the source model.\n\nFrom our experiments, we observe that the average attack success rate of CAE is higher than that of IPGuard, suggesting that CAE exhibits superior transferability compared to IPGuard. Additionally, in Table  1 , our findings indicate that CAE outperforms IPGuard in identifying model extraction attacks, primarily owing to the introduction of conferrable scores. However, it's worth noting that the success rates of these methods still exhibit significant fluctuations across various model architectures, causing the low AUC in our multi-model architecture scenario. The success rate of attacks involving adversarial examples can be influenced by the robustness of the target model. Models designed with greater robustness or those that have undergone adversarial training may exhibit lower attack success rates when compared to irrelevant models. In contrast, the correlation difference metric is independent of model robustness and excels in its ability to detect stolen models consistently across various model architectures. Thus, our proposed method, SAC-JC, demonstrates a higher AUC compared to the other two fingerprinting methods. Furthermore, SAC-JC eliminates the need for the defender to train any surrogate models, which saves the defender's time on a large scale. We will discuss it in detail in Section 4.6.\n\nMoreover, we also compare the images used for fingerprinting in Figure  3 . Adversarial examples used in CAE or IPGuard are not only easy to attract the attention of attackers, but also easy to detect and remove by attackers  [4] . On the contrary, SAC-JC only leverages JPEG-compressed images as the input of the suspect models and can be hardly detected or observed by the owner of the suspect model. Additionally, in Table  3 , we present the average accuracy of both the source and stolen models. This table illustrates that the majority of model stealing attacks have the capability to successfully replicate the source model with only a minimal decrease in accuracy. To be specific, the attackers achieve 95.0% of the accuracy of the source model only by utilizing the unlabeled data and the hard label from the source model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fingerprinting On Object Classification",
      "text": "To better assess the performance of SAC-JC in object classification tasks, we conduct experiments on Tiny-ImageNet [56] and CIFAR10 [57] in Tables  4  and 5 . In these two tables, Finetune-A and Finetune-L represent fine-tuning the source model on all the layers and the last layer respectively, while Extract-L, Extract-P and Extract-Adv In addition, our experiments also reveal that another compared method EWE exhibits sensitivity to pruning, although its decline in AUC is less pronounced compared to other watermarking methods based on normal backdoors. One thing to note is that all three IP protection methods, IPGuard, CAE, and EWE, cannot detect the transfer-based model stealing attacks due to the label space change in transfer learning. These methods rely solely on the attack success rate as their identification criterion and consequently, they struggle to identify stolen models when dealing with transfer learning or label changes. Conversely, our methods utilize correlation differences as fingerprints, and this correlation consistency remains intact even when models are transferred to different tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sensitivity Analysis",
      "text": "To assess the efficacy of SAC-JC across varying numbers of augmented images, we conducted experiments",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we thoroughly examine the selection of input samples, and correlation criteria to assess the effectiveness of SAC-JC. We choose two tasks including face emotion recognition KDEF and face verification (FV) in face recognition as representatives of the multiclassification tasks and verification tasks and our results are illustrated in Tables  9  and 10 . In these tables, SAC-Clean represents fingerprinting with SAC using clean samples from the defender's dataset, SAC-CAE rep-   10 , utilizing the source model's inaccurately classified JPEG compressed images as model input and leveraging the accuracy of suspect models as a point-wise indicator for model fingerprinting proves to be ineffective. In addition, SAC-Clean outperforms both CAE and IPGuard in terms of AUC for both the face emotion recognition and face verification tasks. This highlights the superior performance of fingerprinting with correlation compared to fingerprinting with pointwise accuracy. Furthermore, we also investigate the impact of different correlation functions for model fingerprinting on face emotion recognition KDEF and face verification (FV). As shown in Table  11 , both the Gaussian RBF kernel and Cosine similarity effectively fingerprint the source model, with an AUC near 1. In the table, FT, Ex, and Adv are abbreviations for Finetune, Extract, and Adv-Train. Due to the similar performance of the Gaussian RBF kernel and Cosine similarity, we have opted for Cosine similarity as our correlation function in the aforementioned experiments.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we disclose the threat posed by model stealing attacks in deep face recognition. To protect the welltrained source models, we delve into model fingerprinting in face recognition and propose a correlation-based model fingerprinting framework SAC. To weaken the influence of common knowledge shared by the similar-task models, we propose SAC-JC, which leverages a common data-augmented method, JPEG compression on the images for fingerprinting. Furthermore, to overcome the lack of output labels in face verification, we propose FRI to generate specific features of images from the 0-1 verification results. In contrast to existing adversarialexample-based model fingerprinting methods, SAC-JC utilizes augmented samples and excels in two tasks where existing methods fall short: adversarial training and transfer learning. Additionally, SAC-JC eliminates the need for training surrogate models as well as generating adversarial examples, resulting in significantly faster processing speed compared to other fingerprinting methods (approximately 34,393 times faster than CAE). Extensive results validate that SAC effectively defends against various model stealing attacks in deep face recognition. This includes both face verification and face emotion recognition tasks, where it consistently exhibits the highest performance in terms of AUC, p-value, and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets, including Tiny-ImageNet and CIFAR10. The results also demonstrate the superior performance of SAC-JC compared to previous methods.\n\nData Availability Statement. All the datasets used in this paper are available online. MS1MV2 2  , CASIA-Webface 3  , KDEF 4  , CIFAR10 5  , Tiny-ImageNet 6  can be downloaded from their websites accordingly.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Acknowledgement",
      "text": "The authors would like to thank the reviewers and the associate editor for their valuable comments.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of SAC-JC. We first generate JPEG compressed samples as model inputs, represented by the",
      "page": 4
    },
    {
      "caption": "Figure 1: Here we elaborate on the details of model fin-",
      "page": 5
    },
    {
      "caption": "Figure 2: demonstrates the result of SAC with different",
      "page": 5
    },
    {
      "caption": "Figure 2: Different image corruption methods for SAC.",
      "page": 6
    },
    {
      "caption": "Figure 3: Images used for model fingerprinting in KDEF.",
      "page": 9
    },
    {
      "caption": "Figure 3: Adversarial examples used in",
      "page": 10
    },
    {
      "caption": "Figure 5: The results further illustrate that",
      "page": 11
    },
    {
      "caption": "Figure 4: Performance change of SAC-JC with different data amounts.",
      "page": 12
    },
    {
      "caption": "Figure 5: Average F1 score of SAC-JC on different source",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Accuracies (%) of the source model, irrelevant models, and different attack models on KDEF.",
      "data": [
        {
          "%\nIrrelevant\nFinetune\nPruning": "Source VGG ResNet Dense Mobile\nF-A\nF-L\np=0.1 p=0.2 p=0.25"
        },
        {
          "%\nIrrelevant\nFinetune\nPruning": "86.19\n82.67\n77.06\n74.27\n69.30\n90.41\n88.14\n80.96\n75.58\n69.19"
        },
        {
          "%\nIrrelevant\nFinetune\nPruning": "Extract-L\nExtract-P\nExtract-Adv"
        },
        {
          "%\nIrrelevant\nFinetune\nPruning": "VGG ResNet Dense Mobile VGG ResNet Dense Mobile VGG ResNet Dense Mobile"
        },
        {
          "%\nIrrelevant\nFinetune\nPruning": "81.86\n81.19\n81.77\n80.41\n82.88\n80.87\n82.47\n80.96\n73.02\n76.83\n74.27\n69.30"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep face recognition: A survey",
      "authors": [
        "Mei Wang",
        "Weihong Deng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "2",
      "title": "Entangled watermarks as a defense against model extraction",
      "authors": [
        "Hengrui Jia",
        "Christopher Choquette-Choo",
        "Varun Chandrasekaran",
        "Nicolas Papernot"
      ],
      "year": "2021",
      "venue": "Proc. USENIX"
    },
    {
      "citation_id": "3",
      "title": "Sensitive-sample fingerprinting of deep neural networks",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "Ruby Lee"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "4",
      "title": "Deep neural network fingerprinting by conferrable adversarial examples",
      "authors": [
        "Nils Lukas",
        "Yuxuan Zhang",
        "Florian Kerschbaum"
      ],
      "year": "2021",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "5",
      "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks",
      "authors": [
        "Kang Liu",
        "Brendan Dolan-Gavitt",
        "Siddharth Garg"
      ],
      "year": "2018",
      "venue": "Proc. RAID"
    },
    {
      "citation_id": "6",
      "title": "Importance estimation for neural network pruning",
      "authors": [
        "Pavlo Molchanov",
        "Arun Mallya",
        "Stephen Tyree",
        "Iuri Frosio",
        "Jan Kautz"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "7",
      "title": "Adversarial training for free",
      "authors": [
        "Ali Shafahi",
        "Mahyar Najibi",
        "Mohammad Amin Ghiasi",
        "Zheng Xu",
        "John Dickerson",
        "Christoph Studer",
        "Larry Davis",
        "Gavin Taylor",
        "Tom Goldstein"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "8",
      "title": "Evaluation-oriented knowledge distillation for deep face recognition",
      "authors": [
        "Yuge Huang",
        "Jiaxiang Wu",
        "Xingkun Xu",
        "Shouhong Ding"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "9",
      "title": "David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracy and high fidelity extraction of neural networks",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini"
      ],
      "year": "2020",
      "venue": "Proc. USENIX"
    },
    {
      "citation_id": "10",
      "title": "Knockoff nets: Stealing functionality of blackbox models",
      "authors": [
        "Tribhuvanesh Orekondy",
        "Bernt Schiele",
        "Mario Fritz"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "11",
      "title": "The karolinska directed emotional faces: a validation study",
      "authors": [
        "Ellen Goeleven",
        "Rudi De Raedt",
        "Lemke Leyman",
        "Bruno Verschuere"
      ],
      "year": "2008",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "12",
      "title": "Embedding watermarks into deep neural networks",
      "authors": [
        "Yusuke Uchida",
        "Yuki Nagai",
        "Shigeyuki Sakazawa",
        "Shin'ichi Satoh"
      ],
      "year": "2017",
      "venue": "Proc. ICMR"
    },
    {
      "citation_id": "13",
      "title": "Deepmarks: A digital fingerprinting framework for deep neural networks",
      "authors": [
        "Huili Chen",
        "Bita Darvish Rohani",
        "Farinaz Koushanfar"
      ],
      "year": "2018",
      "venue": "Deepmarks: A digital fingerprinting framework for deep neural networks",
      "arxiv": "arXiv:1804.03648"
    },
    {
      "citation_id": "14",
      "title": "Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks",
      "authors": [
        "Lixin Fan",
        "Kam Ng",
        "Chee Seng"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "15",
      "title": "Passport-aware normalization for deep model protection",
      "authors": [
        "Jie Zhang",
        "Dongdong Chen",
        "Jing Liao",
        "Weiming Zhang",
        "Gang Hua",
        "Nenghai Yu"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
      "authors": [
        "Yossi Adi",
        "Carsten Baum",
        "Moustapha Cisse",
        "Benny Pinkas",
        "Joseph Keshet"
      ],
      "year": "2018",
      "venue": "Proc. USENIX"
    },
    {
      "citation_id": "17",
      "title": "Marc Ph Stoecklin, Heqing Huang, and Ian Molloy. Protecting intellectual property of deep neural networks with watermarking",
      "authors": [
        "Jialong Zhang",
        "Zhongshu Gu",
        "Jiyong Jang",
        "Hui Wu"
      ],
      "year": "2018",
      "venue": "Proc. ASIACCS"
    },
    {
      "citation_id": "18",
      "title": "Deepip: Deep neural network intellectual property protection with passports",
      "authors": [
        "Lixin Fan",
        "Kam Ng",
        "Chee Seng Chan",
        "Qiang Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation",
      "authors": [
        "Yunjie Ge",
        "Qian Wang",
        "Baolin Zheng",
        "Xinlu Zhuang",
        "Qi Li",
        "Chao Shen",
        "Cong Wang"
      ],
      "year": "2021",
      "venue": "Proc. ACMMM"
    },
    {
      "citation_id": "20",
      "title": "Ipguard: Protecting intellectual property of deep neural networks via fingerprinting the classification boundary",
      "authors": [
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Neil Zhenqiang"
      ],
      "year": "2021",
      "venue": "Proc. ICCCS"
    },
    {
      "citation_id": "21",
      "title": "Modeldiff: testing-based dnn similarity comparison for model reuse detection",
      "authors": [
        "Yuanchun Li",
        "Ziqi Zhang",
        "Bingyan Liu",
        "Ziyue Yang",
        "Yunxin Liu"
      ],
      "year": "2021",
      "venue": "Proc. SIGSOFT"
    },
    {
      "citation_id": "22",
      "title": "Fingerprinting deep neural networks globally via universal adversarial perturbations",
      "authors": [
        "Zirui Peng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Cheng Zhang",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "year": "2022",
      "venue": "Fingerprinting deep neural networks globally via universal adversarial perturbations",
      "arxiv": "arXiv:2202.08602"
    },
    {
      "citation_id": "23",
      "title": "Fingerprinting deep neural networks-a deepfool approach",
      "authors": [
        "Si Wang",
        "Chip-Hong Chang"
      ],
      "year": "2021",
      "venue": "Proc. ISCAS"
    },
    {
      "citation_id": "24",
      "title": "Towards deep learning models resistant to adversarial attacks",
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "25",
      "title": "A survey of transfer learning",
      "authors": [
        "Karl Weiss",
        "Taghi Khoshgoftaar",
        "Dingding Wang"
      ],
      "year": "2016",
      "venue": "Journal of Big data"
    },
    {
      "citation_id": "26",
      "title": "Are you stealing my model? sample correlation for fingerprinting deep neural networks",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "Ran He"
      ],
      "year": "2022",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "27",
      "title": "Benchmarking neural network robustness to common corruptions and perturbations",
      "authors": [
        "Dan Hendrycks",
        "Thomas Dietterich"
      ],
      "year": "2018",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "28",
      "title": "Deepface: Closing the gap to human-level performance in face verification",
      "authors": [
        "Yaniv Taigman",
        "Ming Yang",
        "Marc'aurelio Ranzato",
        "Lior Wolf"
      ],
      "year": "2014",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "29",
      "title": "Convolutional neural networks for medical image analysis: Full training or fine tuning?",
      "authors": [
        "Nima Tajbakhsh",
        "Jae Shin",
        "Todd Suryakanth R Gurudu",
        "Christopher Hurst",
        "Michael Kendall",
        "Jianming Gotway",
        "Liang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "30",
      "title": "Few-shot backdoor defense using shapley estimation",
      "authors": [
        "Jiyang Guan",
        "Zhuozhuo Tu"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "31",
      "title": "Defending against model stealing via verifying embedded external features",
      "authors": [
        "Yiming Li",
        "Linghui Zhu",
        "Xiaojun Jia",
        "Yong Jiang",
        "Shu-Tao Xia",
        "Xiaochun Cao"
      ],
      "year": "2022",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "32",
      "title": "Deepfool: a simple and accurate method to fool deep neural networks",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Pascal Frossard"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "33",
      "title": "Universal adversarial perturbations",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Omar Fawzi",
        "Pascal Frossard"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "34",
      "title": "Copy, right? a testing framework for copyright protection of deep learning models",
      "authors": [
        "Jialuo Chen",
        "Jingyi Wang",
        "Tinglan Peng",
        "Youcheng Sun",
        "Peng Cheng",
        "Shouling Ji",
        "Xingjun Ma",
        "Bo Li",
        "Dawn Song"
      ],
      "year": "2022",
      "venue": "Proc. SP"
    },
    {
      "citation_id": "35",
      "title": "Cloudleak: Large-scale deep learning models stealing through adversarial examples",
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "year": "2020",
      "venue": "Proc. NDSS"
    },
    {
      "citation_id": "36",
      "title": "Surpassing humanlevel face verification performance on lfw with gaussianface",
      "authors": [
        "Chaochao Lu",
        "Xiaoou Tang"
      ],
      "year": "2015",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "37",
      "title": "Discriminant analysis in correlation similarity measure space",
      "authors": [
        "Yong Ma",
        "Shihong Lao",
        "Erina Takikawa",
        "Masato Kawade"
      ],
      "year": "2007",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "38",
      "title": "Correlation congruence for knowledge distillation",
      "authors": [
        "Baoyun Peng",
        "Xiao Jin",
        "Jiaheng Liu",
        "Dongsheng Li",
        "Yichao Wu",
        "Yu Liu",
        "Shunfeng Zhou",
        "Zhaoning Zhang"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "39",
      "title": "Cosine similarity metric learning for face verification",
      "authors": [
        "V Hieu",
        "Li Nguyen",
        "Bai"
      ],
      "year": "2010",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "40",
      "title": "An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels",
      "authors": [
        "Ingo Steinwart",
        "Don Hush",
        "Clint Scovel"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "41",
      "title": "Scale invariance and noise in natural images",
      "authors": [
        "Daniel Zoran",
        "Yair Weiss"
      ],
      "year": "2009",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "42",
      "title": "On detecting adversarial perturbations",
      "authors": [
        "Jan Hendrik Metzen",
        "Tim Genewein",
        "Volker Fischer",
        "Bastian Bischoff"
      ],
      "year": "2016",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "43",
      "title": "Recent advances in adversarial training for adversarial robustness",
      "authors": [
        "Tao Bai",
        "Jinqi Luo",
        "Jun Zhao",
        "Bihan Wen",
        "Qian Wang"
      ],
      "year": "2021",
      "venue": "Recent advances in adversarial training for adversarial robustness",
      "arxiv": "arXiv:2102.01356"
    },
    {
      "citation_id": "44",
      "title": "Adversarial machine learning at scale",
      "authors": [
        "Alexey Kurakin",
        "Ian Goodfellow",
        "Samy Bengio"
      ],
      "year": "2016",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "45",
      "title": "Data-free model extraction",
      "authors": [
        "Jean-Baptiste Truong",
        "Pratyush Maini",
        "Robert Walls",
        "Nicolas Papernot"
      ],
      "year": "2021",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "46",
      "title": "Knowledge distillation: A survey",
      "authors": [
        "Jianping Gou",
        "Baosheng Yu",
        "Stephen Maybank",
        "Dacheng Tao"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "48",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "49",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "50",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "51",
      "title": "Generating adversarial examples by makeup attacks on face recognition",
      "authors": [
        "Zheng-An Zhu",
        "Yun-Zhong Lu",
        "Chen-Kuo Chiang"
      ],
      "year": "2019",
      "venue": "Proc. ICIP"
    },
    {
      "citation_id": "52",
      "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
      "authors": [
        "Tianyu Gu",
        "Brendan Dolan-Gavitt",
        "Siddharth Garg"
      ],
      "year": "2017",
      "venue": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
      "arxiv": "arXiv:1708.06733"
    },
    {
      "citation_id": "53",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Yandong Guo",
        "Lei Zhang",
        "Yuxiao Hu",
        "Xiaodong He",
        "Jianfeng Gao"
      ],
      "year": "2016",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "54",
      "title": "Learning face representation from scratch",
      "authors": [
        "Dong Yi",
        "Zhen Lei",
        "Shengcai Liao",
        "Stan Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "55",
      "title": "Arcface: Additive angular margin loss for",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Niannan Xue",
        "Stefanos Zafeiriou"
      ],
      "venue": "Arcface: Additive angular margin loss for"
    }
  ]
}