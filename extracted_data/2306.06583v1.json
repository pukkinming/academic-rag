{
  "paper_id": "2306.06583v1",
  "title": "React2023: The First Multi-Modal Multiple Appropriate Facial Reaction Generation Challenge",
  "published": "2023-06-11T04:15:56Z",
  "authors": [
    "Siyang Song",
    "Micol Spitale",
    "Cheng Luo",
    "German Barquero",
    "Cristina Palmero",
    "Sergio Escalera",
    "Michel Valstar",
    "Tobias Baur",
    "Fabien Ringeval",
    "Elisabeth Andre",
    "Hatice Gunes"
  ],
  "keywords": [
    "CCS Concepts:",
    "Computer systems organization ‚Üí Embedded systems",
    "Redundancy",
    "Robotics",
    "‚Ä¢ Networks ‚Üí Network reliability datasets, neural networks, gaze detection, text tagging"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The Multi-modal Multiple Appropriate Facial Reaction Generation Challenge (REACT2023) is the first competition event focused on evaluating multimedia processing and machine learning techniques for generating human-appropriate facial reactions in various dyadic interaction scenarios, with all participants competing strictly under the same conditions. The goal of the challenge is to provide the first benchmark test set for multi-modal information processing and to foster collaboration among the audio, visual, and audio-visual affective computing communities, to compare the relative merits of the approaches to automatic appropriate facial reaction generation under different spontaneous dyadic interaction conditions. This paper presents: (i) novelties, contributions and guidelines of the REACT2023 challenge; (ii) the dataset utilized in the challenge; and (iii) the performance of baseline systems on the two proposed sub-challenges: Offline Multiple Appropriate Facial Reaction Generation and Online Multiple Appropriate Facial Reaction Generation, respectively. The challenge baseline code is publicly available at https://github.com/reactmultimodalchallenge/baseline_react2023.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Introduction",
      "text": "The Multi-modal Multiple Appropriate Facial Reaction Generation Challenge (REACT2023) is the first competition aimed at the comparison of multimedia processing and machine learning methods for automatic human appropriate facial reaction generation under different dyadic interaction scenarios, with all participants competing strictly under the same conditions.\n\nAs discussed in  [36] , the generation of human facial reactions in dyadic interactions poses uncertainties, as various (non-verbal) reactions may be deemed appropriate in response to specific speaker behaviours. Although some prior studies  [4, 5, 15, 16, 25, 33, 34, 36]  have already explored the task of automatically generating humanstyle facial and bodily reactions based on the conversational partner's behaviours, they mainly focused on reproducing a specific real facial reactions corresponding to the input speaker behaviour, which introduces challenges due to the potential divergence of non-verbal reaction labels for similar speaker behaviours at the training stage. Just a very recent work  [22]  presents a non-deterministic approach to generate multiple listener reactions from a speaker behaviour but without evaluating the appropriateness of the generated reactions. For an in-depth discussion on the multiple different appropriate facial reaction generation task, please refer to Song et al.  [36] . In other words, none of existing approaches can automatically generate multiple appropriate reactions in a dyadic interaction setting, which is a more realistic task, while lacking objective measures to evaluate the appropriateness of the generated facial reactions.\n\nThe main goal of the REACT2023 challenge is to facilitate collaboration among multiple communities from different disciplines, in particular, the affective computing and multimedia communities and researchers in the psychological and social sciences specializing in expressive facial behaviours. The challenge aims to encourage the initial development and benchmarking of Machine Learning (ML) models capable of generating appropriate facial reactions in response to a given stimulus, using three state-of-the-art datasets for dyadic interaction research, namely, RECOLA  [32] , NOXI  [6] , and UDIVA  [26] . As a part of the challenge, we will provide challenge participants with the REACT2023 Challenge Dataset, comprising segmented 30-second interaction video clips (video pairs) from the aforementioned three datasets, annotated with challenge-specific labels indicating the appropriateness of facial reactions. We will then invite the participating groups to submit their developed / trained ML models for evaluation, which will be benchmarked in terms of the appropriateness, diversity, and synchrony of the generated facial reactions.\n\nThe main contributions and novelties are introduced for the RE-ACT2023 with two separated sub-challenges focusing on online and offline appropriate facial reaction generations as: ùëÜ . Based on the predicted facial attributes, the challenge participants have to generate ùëÄ appropriate and realistic / naturalistic spatio-temporal facial reactions (2D face image sequences) given each input speaker behaviour.\n\nWhile participants are welcome to report their results obtained on the validation partition, they are restricted to a maximum of five submission attempts per sub-challenge for presenting their results on the test partition. Both sub-challenges allow participants to explore their own features and machine learning algorithms. We additionally provide standardized audio-visual feature sets (Sec. 5.1), along with the baseline system scripts available in a public repository  1  , to facilitate the reproduction of baseline features and facial reaction generation systems (Sec. 5). All participants are required to report their results achieved on the validation and test partitions.\n\nThe REACT2023 Challenge adopts the metrics defined in  [36]  to evaluate the performance of the submitted models in terms of generated facial reactions, namely: appropriateness, diversity, realism and synchrony. Participants are required to submit their developed model and checkpoints, which are evaluated and visualised based on the framework provided by  [36] . The ranking of the submitted model competing in the Challenge relies on the two metrics: Appropriate facial reaction distance (FRDist) and facial reactions' diverseness FRDiv, for both sub-challenges. In addition, Facial reaction correlation (FRCorr), Facial reaction realism (FRRea), Facial reaction variance (FRVar), Diversity among facial reactions generated from different speaker behaviours (FRDvs) and Synchrony between generated facial reactions and speaker behaviours (FRSyn) should also be reported.\n\nTo be eligible to participate in the Challenge, each team must fulfill specific criteria, including the submission of thoroughly explained source code, well-trained models and associated checkpoints, accompanied by a paper submitted to the REACT2023 Challenge describing the proposed methodology and the achieved results. These papers undergo a rigorous peer-review by the REACT2023 challenge technical program committee. Only contributions that meet the terms and conditions 2  requirements are eligible for participation. The organisers do not engage in active participation themselves, but instead undertake a re-evaluation of the findings from the best performing systems in each sub-challenge.\n\nThe remainder of this paper is organised as follows. The relevant related works are reviewed in Sec. 2. We then introduce the Challenge corpora in Sec. 3, and the evaluation metrics in Sec. 4. The baseline audio-visual feature sets, and baseline facial reaction generation systems are introduced in Sec. 5, respectively. We finally conclude the challenge in Sec. 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we first review previous works on automatic facial reaction generation in Sec. 2.1, and then further summarises common facial reaction visualization strategies in Sec. 2.2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Facial Reaction Generation",
      "text": "As discussed in  [36] , in dyadic interactions, human listeners could express a broad spectrum of appropriate non-verbal reactions for responding to a specific speaker behaviour. However, most prior works have attempted to reproduce the listener's real facial reaction that corresponds to the input speaker behaviour from a deterministic perspective  [4] . For example, Huang et al.  [15]  trained a conditional Generative Adversarial Network (GAN)  [13, 21]  and attempted to generate the listener's real facial reaction sketch from the corresponding speaker's facial action units (AUs). Similar frameworks  [14, 16, 23, 40, 41]  have been extended for the same purpose (i.e., reproducing the specific real facial reaction from each input speaker behaviour), where more modalities (e.g., low-level facial expression features and audio features) are employed as the input. In particular, Song et al.  [33, 34]  propose to explore a person-specific network for each listener, and thus each listener's person-specific facial reactions could be reproduced. Other works have explored the generation of other non-verbal behaviours, such as hand gestures, posture, and facial reaction altogether in face-to-face scenarios  [5, 25, 38] . They all highlighted that avoiding the convergence to a mean reaction is challenging with existing deterministic approaches. However, the training process of such deterministic approaches would face the illposed 'one-to-many mapping' problem (i.e., one speaker behaviour corresponds to one appropriate facial reaction distribution, and even the same listener can express different facial reactions in response to the same speaker behaviour under different contexts), making them theoretically impossible to learn good hypothesis  [36] .\n\nRecently, a few works have started to explore the non-deterministic perspective of this problem, which can predict different facial reactions from the same input. For example, Jone et al.  [17]  proposed an architecture that is able to sample multiple avatar's facial reaction to the interlocutor's speech and facial motion. Similarly,  [22]  presented a VQ-VAE-based multimodal method that leverages the speaker's behaviour (i.e., speech features and facial motion). This model can also generate multiple listener's facial reactions from the same input speaker behaviour, despite it does not consider the appropriateness of the generate facial reactions. Geng et al.  [12]  proposed to exploit pre-trained large language models and visionlanguage models together to retrieve the best listener's reaction to the speaker's speech. Their method allows the user to control the reaction retrieval process with textual instructions. However, reactions can only be retrieved from a pre-existing video database, and therefore limited to the identities and reactions available in the database. Xu et al.  [43]  and Luo et al.  [20]  recently proposed a reversible graph neural network-based and a transformer-based models, respectively. Both of them reformulated the 'one-to-many mapping' problems (i.e., one input speaker behaviour could corresponds to multiple appropriate facial reaction labels) occurring in the facial reaction generation models' training into 'one-to-one mapping' problem. Consequently, at the inference stage, multiple different but appropriate facial reactions could be sampled from the learned distribution.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Facial Reaction Visualization",
      "text": "A common strategy for visualising facial behaviours is through using 3D morphable models. For example, Ng et al.  [22]  proposed to use 3D Morphable Model (3DMM) coefficients to represent and visualize facial reactions, which were then transformed to a 2D image with a proprietary software. Xing et al.  [42]  proposed to discretize the continuous space of 3DMM coefficients using a codebook learnt by using a VQ-VAE. Thanks to the mapping to a finite discrete space, the uncertainty of the facial generation is significantly reduced, yielding higher quality results. The similar 3DMM coefficient-based strategy is also used by Zhou et al.  [46] , whose approach only aimed to reproduce the real facial reaction in terms of three emotional states (positive, neutral, and negative) rather than detailed facial muscle movements.\n\nMeanwhile, 2D facial behaviours are frequently visualised based on Generative Adversarial Networks (GANs)  [7]  conditioned on the predicted facial expression latent representations, where facial image sequences are generated based on manually defined conditions such pre-defined AUs  [29] , 2D facial landmarks  [24]  and audio signals  [11, 31]  without considering interaction scenarios (i.e., they do not predict reactions from speaker behaviours). Moreover, recent studies are also proposed to generate 2D facial image sequence from 3D facial behaviours. For example, the PIRender  [30]  and FaceVer-seV2  [39]  frameworks can translate 3DMM coefficient to 2D facial images conditioned on the portrait of reference identity, where the FaceVerseV2 framework is also employed in this paper to generate facial reaction image sequences.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Challenge Corpora",
      "text": "The REACT2023 Challenge relies on three corporas: NoXi  [6] , UDIVA  [26] , and RECOLA  [32]  datasets. We provide a short overview of each dataset below and recommend readers to check the original work for details.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Employed Datasets",
      "text": "3.1.1 NOvice eXpert Interaction dataset. The NOvice eXpert Interaction (NOXI) is a dyadic interaction dataset that is annotated during an information retrieval task targeting multiple languages, multiple topics, and the occurrence of unexpected situations. NoXi is a corpus of screen-mediated face-to-face interactions recorded at three locations (France, Germany and UK), spoken in seven languages (English, French, German, Spanish, Indonesian, Arabic and Italian) discussing a wide range of topics.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Understanding Dyadic",
      "text": "Interactions from Video and Audio signals dataset. The UDIVA dataset features face-to-face interactions between pairs of participants performing a set of collaborative and competitive tasks, using one of the three languages included (i.e., English, Spanish or Catalan). We rely on the UDIVA v0.5 data subset  [25] , composed of 145 dyadic interaction sessions between 135 participants, with a total of 80 hours of recordings. Each clip contains two audio-visual files that record the dyadic interaction between a pair of participants, as well as the conversation transcripts, and metadata about the participants, sessions, and tasks (e.g., sociodemographics, internal state, self-reported personality, relationship among participants, task difficulty).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Remote Collaborative And Affective Dataset. The Remote",
      "text": "COLlaborative and Affective (RECOLA) database consists of 9,5 hours of audio, visual, and physiological (electrocardiogram, and electrodermal activity) recordings of online dyadic interactions between 46 French speaking participants, who were solving a task in collaboration.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Appropriate Facial Reaction (Afr) Dataset",
      "text": "We first segmented the audio-video data of all the three datasets in 30-seconds long clips as in  [1] . Then, we cleaned the dataset by selecting only the dyadic interaction with complete data of both conversational partners (where both faces were in the frame of the camera). This resulted into 8616 clips of 30 seconds each (71,8 hours We divided the datasets into training, test and validation sets. Specifically, we split the datasets with a subject-independent strategy (i.e., the same subject was never included in the train and test sets). We also attempted to balance the language across the training, test and validation sets. However, since many users interacted in multiple sessions, we were not able to get a language-balance split. This results in a training composed by: 1030 video clips -8,6 hoursof UDIVA, 1585 video clips -13,2 hours -of NOXI, and 9 video clips -0,1 hour -of RECOLA. The test set was composed by: 39 video clips -0,3 hour -of UDIVA, 797 video clips -6,6 hours -of NoXI, and 9 video clips -0,1 hour -of RECOLA. While the validation set has the following interactions: 277 video clips -2,3 hours -of UDIVA, 553 video clips -4,6 hours -of NoXI, and 9 video clips -0,1 hour -of RECOLA. Tables  1, 2 , and 3 collect the details about the training, testing, and validation sets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In this challenge, we ask participants to develop models that can generate two types of outputs representing each generated facial reaction: (i) 25 facial attribute time-series (explained in Sec. 5.1); and (ii) 2D and 3D facial image sequence.\n\nWe follow  [36]  to comprehensively evaluate four aspects of the facial reactions generated by participant models. In particular, three aspects are assessed based on the 25 facial attribute time-series: (i) (ii) Diversities, which encompass inter-condition and inter-frame variations. Metrics such as FRVar, FRDiv, and FRDvs, as defined in  [36] , are employed to measure these diversities; and (iii) Synchrony, which examines the alignment between the generated facial reactions and the corresponding speaker behaviour. The Time Lagged Cross Correlation (TLCC) is employed as a metric for measuring this synchrony, referred to as FRSyn in this challenge. Based on the generated 2D facial image sequence, we also evaluate the (iv) Realism of the generated facial reactions, which is assessed using the Fr√©chet Inception Distance (FID) between the distribution of the generated facial reactions and the distribution of the corresponding appropriate real facial reactions, denoted as FRRea.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baselines",
      "text": "This section presents the baseline systems developed for the RE-ACT23 Challenge. First, we detail the audio and visual behavioural features extracted which are used for describing facial reactions in the evaluation protocol (Sec. 5.1). Then, we propose two baseline systems in Sec. 5.2. Finally, we report all results achieved by our baseline systems in Sec. 5.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Behavioural Features",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Features.",
      "text": "We follow  [36]  to provide three widely-used frame-level facial attribute features for each video frame as the baseline facial features. This includes the occurrence of 15 facial action units (AUs), 2 facial affect -valence and arousal intensitiesand the probabilities of 8 categorical facial expressions. Specifically, 15 AUs' occurrence (AU1, AU2, AU4, AU6, AU7, AU9, AU10, AU12, AU14, AU15, AU17, AU23, AU24, AU25 and AU26) are predicted by the state-of-the-art GraphAU model  [19, 35] , while the facial affects (i.e., valence and arousal intensities) and 8 facial expression probabilities (i.e., Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Contempt) are predicted using the approach proposed by  [37] . Then, a latent diffusion model is trained to conditionally sample reactions from it. The condition is either the past or the current speaker reaction, for the online and offline approaches, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Features.",
      "text": "We also apply OpenSmile  [9]  to extract cliplevel audio descriptors, including GEMAP and MFCC features. Consequently, we represent each speaker behaviour by combining all frame-level descriptors as a multi-channel audio-visual time-series behavioural signal.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Systems",
      "text": "In this challenge, we first establish a set of naive baselines, namely B_Random, B_Mime, and B_MeanSeq/B_MeanFr. Specifically, B_Random randomly samples ùõº = 10 facial reaction sequences from a Gaussian distribution. B_Mime generates facial reactions by mimicking the corresponding speaker's facial expressions. For B_MeanSeq and B_MeanFr, the generated facial reactions are decided by the sequence-and frame-wise average reaction in the training set, respectively. Despite their simplicity, these baselines illustrate the bounds of the metrics. For the implementation details and opensource code of all baselines, please refer to our GitHub page at https://github.com/reactmultimodalchallenge/baseline_react2023.\n\nTrans-VAE. The Trans-VAE baseline has a similar architecture as the TEACH proposed in  [2] , which consists of: (i) a CNN encoder that encodes the speaker facial image sequence (i.e., a short video) as a sequence-level embedding; (ii) a transformer encoder that first combines learned facial embeddings and audio embeddings (78dimnesional MFCC features) extracted by Torchaudio library  [44] , and then predicts a pair of tokens ùúá token and ùúé token representing the Gaussian Distribution of multiple appropriate facial reactions of the corresponding input speaker behaviour, based on not only the combined audio-visual embedding but also a pair of learnable tokens; and (iii) a transformer decoder that samples a set of representations describing an appropriate facial reaction based on the predicted distribution tokens, which include a set of 3D Morphable Model (3DMM) coefficients (i.e., 52 facial expression coefficients, 3 pose coefficients and 3 translation coefficients defined by  [39] ) and a emotion matrices (i.e., 25-channel time-series including 15 framelevel AUs' occurrence, 8 frame-level facial expression probabilities as well as frame-level valence and arousal intensities). Based on the learned 3DMM coefficients and the corresponding listener's portrait, FaceVerseV2  [39]  is finally employed to translate the learned 3DMM coefficients to the facial reaction image sequence.\n\nAs illustrated in Fig 1 , we apply the Trans-VAE model to both offline and online facial reaction generation sub-challenges. For the offline sub-challenge, it takes the entire sequence of speaker audiovisual behaviours (i.e., 750 frames corresponding to 30s clip in this challenge) as the input and generates a sequence of facial reactions consisting of 750 frames. The online Trans-VAE baseline follows  [20]  to iteratively predict a short segment consisting of ùë§ facial reaction frames corresponding to the time [ùë° -ùë§ + 1 : ùë°], where causal mask  [8, 10, 20, 28]  is employed to avoid future speaker behaviours to be used for the facial reaction prediction. In particular, the ùúè th facial reaction frame is predicted based on: (i) ùë° -ùë§ frames ([1 : ùë° -ùë§]) of past speaker behaviours; (ii) ùë° -ùë§ frames ([1 : ùë° -ùë§]) of previously predicted facial reactions; and (iii) ùúè frames ([ùë° -ùë§ + 1 : ùúè]) of the current speaker behaviour. The Trans-VAE models for both sub-challenges are trained end-to-end with maximum 50 epochs, where Mean Square Error (MSE) loss function is employed for the 2D facial frame reconstruction; a diversity loss  [20, 45]  is leveraged to increase sampling diversity; and a KL divergence loss is used to constrain the predicted distribution tokens. To optimize the model, AdamW optimizer  [18]  with a fixed learning rate of 1ùëí -5 is used.\n\nBeLFusion. We use BeLFusion as our second baseline  [3] , see Figure  2 . For the sake of simplicity, we use the version without behavioural disentanglement. BeLFusion is trained in two stages. First, a variational autoencoder (VAE) is trained to learn a lower representation of the visual features (e.g., AUs, facial affects, and expressions) of ùë§ frames. On the VAE's head, we include a regressor that transforms the decoded reaction to a sequence of 3DMM coefficients. The VAE losses consists of the KL divergence, the reaction MSE, and the 3DMM coefficients MSE, with weights of 1ùëí -5, 1, and 1, respectively. We chose ùë§ = 50, and the latent space has dimension 128. The model is trained with the AdamW optimizer  [18]  with a fixed learning rate of 0.001 and weight decay of 0.0005, for 1000 epochs. In  the second stage, a latent diffusion model (LDM) learns to, given the speaker's reaction, predict the lower-dimensional representation of the listener's appropriate facial reaction. Similarly to Trans-VAE, this baseline also adopts a window-based approach where ùëá /ùë§ reactions are predicted independently. Then, the ùë§-frames-long reactions are stacked to build the full reaction. For the online sub-challenge, the generation of the listener's visual features for the window [ùë°, ùë° + ùë§) is conditioned on the past speaker's features at [ùë° -ùë§, ùë°). It predicts all zeroes for segment [0, ùë§). For the offline sub-challenge, such generation is conditioned on the speaker's features on the same time period: [ùë°, ùë° + ùë§). The LDM's loss is the average of the MSE in the latent space and the MSE in the reconstructed space. The denoising chain has 10 steps, and every denoising step is implemented with a sequence of residual MLPs as in  [27] . It is also optimized with the AdamW  [18] , a learning rate of 0.0001, and a weight decay of 0.0005, for 100 epochs. We include two versions of the model: with ùëò = 1 and ùëò = 10. As explained in  [3] , higher values for ùëò lead to a stronger implicit diversity loss, and therefore more diverse reactions generation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5.",
      "text": "3.1 Offline facial reaction generation sub-challenge. Table  4  and Table  5  show that both baselines can generate facial reactions that positively correlate to the appropriate real facial reactions, where BeLFusion outperforms Trans-VAE in terms of the distance between the prediction and real appropriate facial reactions (FRD), as well as intra-sequence diversity (FRVar). In return, the intra-and intersubject diversities (S-MSE and FRDvs) for the non-binarized approaches are lower than Trans-VAE. Moreover, the results achieved by Trans-VAE suggest that both visual and audio modalities positively contribute to the diversity of generated facial reactions (FRDiv, FRVar, and FRDvs). As for the FRC metric, Trans-VAE shows the same limitations observed for the BeLFusion. For the latter, we observe that, as expected, higher values of ùëò boost all diversity metrics  [3] . Randomly sampled facial reaction (i.e., B_Random) are diverse but not appropriate (in terms of FRC and FRD), whereas  deterministic baselines (i.e., B_Mime, B_MeanSeq and B_MeanFr) achieved better appropriateness but much lower diversity. Unlike above baselines, deep learned probabilistic models (i.e., Trans-VAE and BeLFusion) can make a trade-off between the these two, which means they can generate multiple different but appropriate facial reactions (i.e., qualitative results achieved by the offline Trans-VAE are visualised in Figure  3 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Online Facial Reaction Generation Sub-Challenge.",
      "text": "As demonstrated in Table  6  and Table  7 , the results confirmed that both baselines can generate real-time facial reactions that are positively correlated to the appropriate face reactions. Also, Trans-VAE outperforms the BeLFusion approach in terms of diversity (FRDiv, FRVar, and FRDvs), synchrony (FRSync), and while BeLFusion outperforms in terms of DTW distances (FRD). Similarly to the offline task, the randomly sampled facial reactions (i.e., B_Random) are diverse but not appropriate, while the deterministic naive approaches (i.e., B_Mime, B_MeanSeq and B_MeanFR) achieved better results in terms of appropriateness but not diversity. Again, the proposed deep learning baselines can be better in trading-off between appropriateness and diversity. In this sub-challenge, the differences are magnified for the four metrics. This again suggests the existence of a trade-off between the appropriateness and diversity of the generated facial reactions. In fact, such trade-off is fairly observed after binarizing the predicted AUs: while the FRD worsens, all diversity metrics are doubled. Compared to offline setting, online generation is more challenging and cause jitters and inconsistency between windows, which are main reasons for the decrease in Realism (i.e., FRRea metric). However, online Trans-VAE approach outperformed the offline one. As a transformer-architecture network, it models a long-range relation between current and past reaction frames and attends to salient changes in speaker behaviours so as to achieve good synchrony in online scenario. Figure  4  visualises that Trans-VAE can give real-time facial expression feedback to speaker behaviour in the online scenario and given reactions can be also multiple and diverse. Our protocol strictly evaluates all participant models under the same settings by comprehensively considering four aspects of their generated facial reactions: appropriateness, diversity, realism and synchrony.\n\nThe results of our proposed baselines suggested that: (i) both Trans-VAE and BeLFusion baselines achieved better results in making a trade-off between appropriateness and diversity of the facial reactions with respect to the naive baselines; (ii) both visual and audio modalities in Trans-VAE positively contributed to the diversity and appropriateness of the generated facial reactions; (iii) the Trans-VAE achieved better results for online sub-challenge over the offline sub-challenge, while the opposite scenario has been observed in the BeLFusion approach;\n\nAs the first multiple appropriate facial reaction generation challenge, the used dataset is not specifically recorded, and thus some important behavioural cues (e.g., verbal texts, physiological signals) were not considered in this challenge. Our future work will focus on continue organizing REACT challenges while introducing a new dataset containing more modalities.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the Trans-VAE baseline.",
      "page": 5
    },
    {
      "caption": "Figure 2: Overview of offline and online BeLFusion baselines. The reaction",
      "page": 5
    },
    {
      "caption": "Figure 1: , we apply the Trans-VAE model to both",
      "page": 5
    },
    {
      "caption": "Figure 3: Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are",
      "page": 6
    },
    {
      "caption": "Figure 4: Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are",
      "page": 7
    },
    {
      "caption": "Figure 4: visualises that Trans-VAE can",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Generation Challenge"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "SIYANG SONG‚àó‚Ä†, University of Leicester & University of Cambridge, United Kingdom"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "MICOL SPITALE‚àó, University of Cambridge, United Kingdom"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "CHENG LUO, Shenzhen University, China"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "GERM√ÅN BARQUERO, Universitat de Barcelona & Computer Vision Center, Spain"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "CRISTINA PALMERO, Universitat de Barcelona & Computer Vision Center, Spain"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "SERGIO ESCALERA, Universitat de Barcelona & Computer Vision Center, Spain"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "MICHEL VALSTAR, University of Nottingham, United Kingdom"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "TOBIAS BAUR, University of Augsburg, Germany"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "FABIEN RINGEVAL, Universit√© Grenoble Alpes, France"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "ELISABETH ANDR√â, University of Augsburg, Germany"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "HATICE GUNES, University of Cambridge, United Kingdom"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "The Multi-modal Multiple Appropriate Facial Reaction Generation Chal-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "lenge (REACT2023)\nis the first competition event\nfocused on evaluating"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "multimedia processing and machine learning techniques for generating"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "human-appropriate facial reactions in various dyadic interaction scenarios,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "with all participants competing strictly under the same conditions. The goal"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "of the challenge is to provide the first benchmark test set for multi-modal in-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "formation processing and to foster collaboration among the audio, visual, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "audio-visual affective computing communities, to compare the relative merits"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "of the approaches to automatic appropriate facial reaction generation under"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "different spontaneous dyadic interaction conditions. This paper presents:"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "(i) novelties, contributions and guidelines of the REACT2023 challenge; (ii)"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "the dataset utilized in the challenge; and (iii) the performance of baseline"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "systems on the two proposed sub-challenges: Offline Multiple Appropriate"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Facial Reaction Generation and Online Multiple Appropriate Facial Reaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Generation, respectively. The challenge baseline code is publicly available"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "at https://github.com/reactmultimodalchallenge/baseline_react2023."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "CCS Concepts: ‚Ä¢ Computer systems organization ‚Üí Embedded systems;"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Redundancy; Robotics; ‚Ä¢ Networks ‚Üí Network reliability."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Additional Key Words and Phrases: datasets, neural networks, gaze detection,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "text tagging"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "ACM Reference Format:"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Siyang Song, Micol Spitale, Cheng Luo, Germ√°n Barquero, Cristina Palmero,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, Elisabeth"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Andr√©, and Hatice Gunes. 2018. REACT2023: the first Multi-modal Multiple"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Appropriate Facial Reaction Generation Challenge. In Woodstock ‚Äô18: ACM"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Symposium on Neural Gaze Detection, June 03‚Äì05, 2018, Woodstock, NY . ACM,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "‚àóBoth authors contributed equally to this research."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "‚Ä†Corresponding author."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Permission to make digital or hard copies of all or part of this work for personal or"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "classroom use is granted without fee provided that copies are not made or distributed"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "for profit or commercial advantage and that copies bear this notice and the full citation"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "on the first page. Copyrights for components of this work owned by others than ACM"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "to post on servers or to redistribute to lists, requires prior specific permission and/or a"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "fee. Request permissions from permissions@acm.org."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "¬© 2018 Association for Computing Machinery."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction": "https://doi.org/XXXXXXX.XXXXXXX"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "offline appropriate facial reaction generations as:",
          "model and checkpoints, which are evaluated and visualised based": "the submit-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "ted model competing in the Challenge relies on the two metrics:"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "Appropriate facial reaction distance (FRDist) and facial reactions‚Äô"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "‚Ä¢ Offline Multiple Appropriate Facial Reaction Genera-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "diverseness FRDiv, for both sub-challenges. In addition, Facial re-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "tion (Offline MAFRG) task focuses on generating multiple",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "action correlation (FRCorr), Facial reaction realism (FRRea), Facial"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "appropriate facial reaction videos from the input speaker be-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "reaction variance (FRVar), Diversity among facial reactions gen-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "haviour (i.e., audio-visual clip). Specifically, this task aims to",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "erated from different speaker behaviours (FRDvs) and Synchrony"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "develop a machine learning model H that takes the entire",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "speaker behaviour sequence ùêµùë°1,ùë°2\nas the input, and generates\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": "between generated facial reactions and speaker behaviours (FRSyn)"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "multiple (ùëÄ) appropriate and realistic / naturalistic spatio-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": ", ùëù ùëì (ùëèùë°1,ùë°2\ntemporal facial reactions ùëù ùëì (ùëèùë°1,ùë°2\n)ùëÄ ; where\n)1, ¬∑ ¬∑ ¬∑\nùëÜ\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": "To be eligible to participate in the Challenge, each team must"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "fulfill specific criteria, including the submission of thoroughly ex-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö is a multi-channel time-series ‚Äì consisting of AUs,\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "plained source code, well-trained models and associated checkpoints,"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "facial expressions, valence and arousal state ‚Äì which repre-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "accompanied by a paper submitted to the REACT2023 Challenge de-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "sent the ùëöùë°‚Ñé predicted appropriate facial reaction in response",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "scribing the proposed methodology and the achieved results. These"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "to ùêµùë°1,ùë°2\n. Based on the predicted facial attributes, the challenge\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "papers undergo a rigorous peer-review by the REACT2023 chal-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "participants have to generate ùëÄ appropriate and realistic /",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "lenge technical program committee. Only contributions that meet"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "naturalistic spatio-temporal facial reactions (2D face image",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "the terms and conditions2 requirements are eligible for participation."
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "sequences) given each input speaker behaviour.",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "The organisers do not engage in active participation themselves,"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "‚Ä¢ Online Multiple Appropriate Facial Reaction Genera-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "but instead undertake a re-evaluation of the findings from the best"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "tion (Online MAFRG) task focuses on the continuous gener-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "ation of facial reaction frames based on current and previous",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "The remainder of this paper is organised as follows. The rele-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "speaker behaviours. This task aims to develop a machine",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "vant related works are reviewed in Sec. 2. We then introduce the"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "learning model H that estimates multiple facial attributes",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "Challenge corpora in Sec. 3, and the evaluation metrics in Sec. 4."
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "(AUs, facial expressions, valence and arousal state) represent-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "The baseline audio-visual feature sets, and baseline facial reaction"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "ing each appropriate facial reaction frame (i.e., ùõæth ‚àà [ùë°1, ùë°2]",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "generation systems are introduced in Sec. 5, respectively. We finally"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "frame) by only considering the ùõæth frame and its previous",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "frames of the corresponding speaker behaviour (i.e., ùë°1th to ùõæth",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "frames in ùêµùë°1,ùë°2\n), rather than taking all frames from ùë°1 to ùë°2 into",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "ùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "account. The model is expected to gradually generate multi-",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "In this section, we first review previous works on automatic facial re-"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "ple multi-channel facial attribute time-series to represent all",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "action generation in Sec. 2.1, and then further summarises common"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "face frames of multiple appropriate and realistic / naturalistic",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": ", ùëù ùëì (ùëèùë°1,ùë°2\nspatio-temporal facial reactions ùëù ùëì (ùëèùë°1,ùë°2\n)ùëÄ ,\n)1, ¬∑ ¬∑ ¬∑\nùëÜ\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "where ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö, where ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö is a multi-channel time-\nùëÜ\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "series ‚Äì consisting of AUs, facial expressions, valence and",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "As discussed in [36], in dyadic interactions, human listeners could"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "arousal state ‚Äì representing the ùëöùë°‚Ñé predicted appropriate",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "express a broad spectrum of appropriate non-verbal reactions for"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "facial reaction in response to ùêµùë°1,ùë°2\n. Based on the predicted fa-\nùëÜ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "responding to a specific speaker behaviour. However, most prior"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "cial attributes, the challenge participants have to generate ùëÄ",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "works have attempted to reproduce the listener‚Äôs real facial reaction"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "appropriate and realistic / naturalistic spatio-temporal facial",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "that corresponds to the input speaker behaviour from a deterministic"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "reactions (2D face image sequences) given each input speaker",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "perspective [4]. For example, Huang et al. [15] trained a conditional"
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "behaviour.",
          "model and checkpoints, which are evaluated and visualised based": ""
        },
        {
          "ACT2023 with two separated sub-challenges focusing on online and": "",
          "model and checkpoints, which are evaluated and visualised based": "Generative Adversarial Network (GAN) [13, 21] and attempted to"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "segmented 30-second interaction video clips (video pairs) from the",
          "Trovato and Tobin, et al.": "along with the baseline system scripts available in a public reposi-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "aforementioned three datasets, annotated with challenge-specific la-",
          "Trovato and Tobin, et al.": "tory1, to facilitate the reproduction of baseline features and facial"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "bels indicating the appropriateness of facial reactions. We will then",
          "Trovato and Tobin, et al.": "reaction generation systems (Sec. 5). All participants are required"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "invite the participating groups to submit their developed / trained",
          "Trovato and Tobin, et al.": "to report their results achieved on the validation and test partitions."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ML models for evaluation, which will be benchmarked in terms",
          "Trovato and Tobin, et al.": "The REACT2023 Challenge adopts the metrics defined in [36] to"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "of the appropriateness, diversity, and synchrony of the generated",
          "Trovato and Tobin, et al.": "evaluate the performance of the submitted models in terms of gen-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "facial reactions.",
          "Trovato and Tobin, et al.": "erated facial reactions, namely: appropriateness, diversity, realism"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "The main contributions and novelties are introduced for the RE-",
          "Trovato and Tobin, et al.": "and synchrony. Participants are required to submit their developed"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ACT2023 with two separated sub-challenges focusing on online and",
          "Trovato and Tobin, et al.": "model and checkpoints, which are evaluated and visualised based"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "offline appropriate facial reaction generations as:",
          "Trovato and Tobin, et al.": "on the framework provided by [36]. The ranking of\nthe submit-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "ted model competing in the Challenge relies on the two metrics:"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "Appropriate facial reaction distance (FRDist) and facial reactions‚Äô"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "‚Ä¢ Offline Multiple Appropriate Facial Reaction Genera-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "diverseness FRDiv, for both sub-challenges. In addition, Facial re-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tion (Offline MAFRG) task focuses on generating multiple",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "action correlation (FRCorr), Facial reaction realism (FRRea), Facial"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "appropriate facial reaction videos from the input speaker be-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "reaction variance (FRVar), Diversity among facial reactions gen-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "haviour (i.e., audio-visual clip). Specifically, this task aims to",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "erated from different speaker behaviours (FRDvs) and Synchrony"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "develop a machine learning model H that takes the entire",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "speaker behaviour sequence ùêµùë°1,ùë°2\nas the input, and generates\nùëÜ",
          "Trovato and Tobin, et al.": "between generated facial reactions and speaker behaviours (FRSyn)"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "should also be reported."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "multiple (ùëÄ) appropriate and realistic / naturalistic spatio-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ", ùëù ùëì (ùëèùë°1,ùë°2\ntemporal facial reactions ùëù ùëì (ùëèùë°1,ùë°2\n)ùëÄ ; where\n)1, ¬∑ ¬∑ ¬∑\nùëÜ\nùëÜ",
          "Trovato and Tobin, et al.": "To be eligible to participate in the Challenge, each team must"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "fulfill specific criteria, including the submission of thoroughly ex-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö is a multi-channel time-series ‚Äì consisting of AUs,\nùëÜ",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "plained source code, well-trained models and associated checkpoints,"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "facial expressions, valence and arousal state ‚Äì which repre-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "accompanied by a paper submitted to the REACT2023 Challenge de-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sent the ùëöùë°‚Ñé predicted appropriate facial reaction in response",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "scribing the proposed methodology and the achieved results. These"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to ùêµùë°1,ùë°2\n. Based on the predicted facial attributes, the challenge\nùëÜ",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "papers undergo a rigorous peer-review by the REACT2023 chal-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "participants have to generate ùëÄ appropriate and realistic /",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "lenge technical program committee. Only contributions that meet"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "naturalistic spatio-temporal facial reactions (2D face image",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "the terms and conditions2 requirements are eligible for participation."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sequences) given each input speaker behaviour.",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "The organisers do not engage in active participation themselves,"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "‚Ä¢ Online Multiple Appropriate Facial Reaction Genera-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "but instead undertake a re-evaluation of the findings from the best"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tion (Online MAFRG) task focuses on the continuous gener-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "performing systems in each sub-challenge."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ation of facial reaction frames based on current and previous",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "The remainder of this paper is organised as follows. The rele-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "speaker behaviours. This task aims to develop a machine",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "vant related works are reviewed in Sec. 2. We then introduce the"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "learning model H that estimates multiple facial attributes",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "Challenge corpora in Sec. 3, and the evaluation metrics in Sec. 4."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "(AUs, facial expressions, valence and arousal state) represent-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "The baseline audio-visual feature sets, and baseline facial reaction"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ing each appropriate facial reaction frame (i.e., ùõæth ‚àà [ùë°1, ùë°2]",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "generation systems are introduced in Sec. 5, respectively. We finally"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "frame) by only considering the ùõæth frame and its previous",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "conclude the challenge in Sec. 6."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "frames of the corresponding speaker behaviour (i.e., ùë°1th to ùõæth",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "frames in ùêµùë°1,ùë°2\n), rather than taking all frames from ùë°1 to ùë°2 into",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ùëÜ",
          "Trovato and Tobin, et al.": "2\nRELATED WORK"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "account. The model is expected to gradually generate multi-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "In this section, we first review previous works on automatic facial re-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ple multi-channel facial attribute time-series to represent all",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "action generation in Sec. 2.1, and then further summarises common"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "face frames of multiple appropriate and realistic / naturalistic",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ", ùëù ùëì (ùëèùë°1,ùë°2\nspatio-temporal facial reactions ùëù ùëì (ùëèùë°1,ùë°2\n)ùëÄ ,\n)1, ¬∑ ¬∑ ¬∑\nùëÜ\nùëÜ",
          "Trovato and Tobin, et al.": "facial reaction visualization strategies in Sec. 2.2."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "where ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö, where ùëù ùëì (ùëèùë°1,ùë°2\n)ùëö is a multi-channel time-\nùëÜ\nùëÜ",
          "Trovato and Tobin, et al.": "2.1\nFacial reaction generation"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "series ‚Äì consisting of AUs, facial expressions, valence and",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "As discussed in [36], in dyadic interactions, human listeners could"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "arousal state ‚Äì representing the ùëöùë°‚Ñé predicted appropriate",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "express a broad spectrum of appropriate non-verbal reactions for"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "facial reaction in response to ùêµùë°1,ùë°2\n. Based on the predicted fa-\nùëÜ",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "responding to a specific speaker behaviour. However, most prior"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "cial attributes, the challenge participants have to generate ùëÄ",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "works have attempted to reproduce the listener‚Äôs real facial reaction"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "appropriate and realistic / naturalistic spatio-temporal facial",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "that corresponds to the input speaker behaviour from a deterministic"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reactions (2D face image sequences) given each input speaker",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "perspective [4]. For example, Huang et al. [15] trained a conditional"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "behaviour.",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "Generative Adversarial Network (GAN) [13, 21] and attempted to"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "generate the listener‚Äôs real facial reaction sketch from the corre-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "While participants are welcome to report their results obtained",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "sponding speaker‚Äôs facial action units (AUs). Similar frameworks"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "on the validation partition, they are restricted to a maximum of",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "[14, 16, 23, 40, 41] have been extended for the same purpose (i.e.,"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "five submission attempts per sub-challenge for presenting their re-",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "reproducing the specific real facial reaction from each input speaker"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sults on the test partition. Both sub-challenges allow participants",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to explore their own features and machine learning algorithms. We",
          "Trovato and Tobin, et al.": ""
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "1https://github.com/reactmultimodalchallenge/baseline_react2023/tree/main"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "additionally provide standardized audio-visual feature sets (Sec. 5.1),",
          "Trovato and Tobin, et al.": "2https://sites.google.com/cam.ac.uk/react2023/home"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "behaviour), where more modalities (e.g., low-level facial expression\nMeanwhile, 2D facial behaviours are frequently visualised based"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features and audio features) are employed as the input. In particular,\non Generative Adversarial Networks (GANs) [7] conditioned on"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Song et al. [33, 34] propose to explore a person-specific network for\nthe predicted facial expression latent representations, where facial"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "each listener, and thus each listener‚Äôs person-specific facial reactions\nimage sequences are generated based on manually defined condi-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "could be reproduced. Other works have explored the generation of\ntions such pre-defined AUs [29], 2D facial landmarks [24] and audio"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "other non-verbal behaviours, such as hand gestures, posture, and\nsignals [11, 31] without considering interaction scenarios (i.e., they"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "facial reaction altogether in face-to-face scenarios [5, 25, 38]. They\ndo not predict reactions from speaker behaviours). Moreover, recent"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "all highlighted that avoiding the convergence to a mean reaction is\nstudies are also proposed to generate 2D facial image sequence from"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "challenging with existing deterministic approaches. However, the\n3D facial behaviours. For example, the PIRender [30] and FaceVer-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "training process of such deterministic approaches would face the ill-\nseV2 [39] frameworks can translate 3DMM coefficient to 2D facial"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "posed ‚Äôone-to-many mapping‚Äô problem (i.e., one speaker behaviour\nimages conditioned on the portrait of reference identity, where the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "corresponds to one appropriate facial reaction distribution, and even\nFaceVerseV2 framework is also employed in this paper to generate"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the same listener can express different facial reactions in response\nfacial reaction image sequences."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to the same speaker behaviour under different contexts), making"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3\nCHALLENGE CORPORA"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "them theoretically impossible to learn good hypothesis [36]."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Recently, a few works have started to explore the non-deterministic"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "The REACT2023 Challenge relies on three corporas: NoXi [6], UDIVA"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "perspective of this problem, which can predict different facial reac-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[26], and RECOLA [32] datasets. We provide a short overview of"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tions from the same input. For example, Jone et al. [17] proposed"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "each dataset below and recommend readers to check the original"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "an architecture that is able to sample multiple avatar‚Äôs facial reac-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "work for details."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tion to the interlocutor‚Äôs speech and facial motion. Similarly, [22]"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "presented a VQ-VAE-based multimodal method that leverages the\n3.1\nEmployed datasets"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "speaker‚Äôs behaviour (i.e., speech features and facial motion). This"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3.1.1\nNOvice eXpert Interaction dataset. The NOvice eXpert Interac-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "model can also generate multiple listener‚Äôs facial reactions from"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tion (NOXI) is a dyadic interaction dataset that is annotated during"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the same input speaker behaviour, despite it does not consider the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "an information retrieval task targeting multiple languages, multiple"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "appropriateness of the generate facial reactions. Geng et al. [12]"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "topics, and the occurrence of unexpected situations. NoXi is a cor-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "proposed to exploit pre-trained large language models and vision-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "pus of screen-mediated face-to-face interactions recorded at three"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "language models together to retrieve the best listener‚Äôs reaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "locations (France, Germany and UK), spoken in seven languages"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to the speaker‚Äôs speech. Their method allows the user to control"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "(English, French, German, Spanish, Indonesian, Arabic and Italian)"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the reaction retrieval process with textual instructions. However,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "discussing a wide range of topics."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reactions can only be retrieved from a pre-existing video database,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "and therefore limited to the identities and reactions available in\n3.1.2\nUnderstanding Dyadic Interactions from Video and Audio sig-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the database. Xu et al. [43] and Luo et al. [20] recently proposed\nnals dataset. The UDIVA dataset features face-to-face interactions"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "a reversible graph neural network-based and a transformer-based\nbetween pairs of participants performing a set of collaborative and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "models, respectively. Both of them reformulated the ‚Äôone-to-many\ncompetitive tasks, using one of the three languages included (i.e.,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "mapping‚Äô problems (i.e., one input speaker behaviour could cor-\nEnglish, Spanish or Catalan). We rely on the UDIVA v0.5 data sub-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "responds to multiple appropriate facial reaction labels) occurring\nset [25], composed of 145 dyadic interaction sessions between 135"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "in the facial reaction generation models‚Äô training into ‚Äôone-to-one\nparticipants, with a total of 80 hours of recordings. Each clip contains"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "mapping‚Äô problem. Consequently, at the inference stage, multiple\ntwo audio-visual files that record the dyadic interaction between"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "different but appropriate facial reactions could be sampled from the\na pair of participants, as well as the conversation transcripts, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "learned distribution.\nmetadata about the participants, sessions, and tasks (e.g., sociode-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "mographics, internal state, self-reported personality, relationship"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "among participants, task difficulty)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "2.2\nFacial reaction visualization"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3.1.3\nREmote COLlaborative and Affective dataset. The REmote"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "A common strategy for visualising facial behaviours is through using"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "COLlaborative and Affective (RECOLA) database consists of 9,5"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3D morphable models. For example, Ng et al. [22] proposed to use"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "hours of audio, visual, and physiological (electrocardiogram, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3D Morphable Model (3DMM) coefficients to represent and visualize"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "electrodermal activity) recordings of online dyadic interactions be-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "facial reactions, which were then transformed to a 2D image with"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tween 46 French speaking participants, who were solving a task in"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "a proprietary software. Xing et al. [42] proposed to discretize the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "collaboration."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "continuous space of 3DMM coefficients using a codebook learnt by"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "using a VQ-VAE. Thanks to the mapping to a finite discrete space,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "3.2\nAppropriate Facial Reaction (AFR) dataset"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the uncertainty of\nthe facial generation is significantly reduced,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "yielding higher quality results. The similar 3DMM coefficient-based\nWe first segmented the audio-video data of all the three datasets"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "strategy is also used by Zhou et al. [46], whose approach only aimed\nin 30-seconds long clips as in [1]. Then, we cleaned the dataset by"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to reproduce the real facial reaction in terms of three emotional\nselecting only the dyadic interaction with complete data of both"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "states (positive, neutral, and negative) rather than detailed facial\nconversational partners (where both faces were in the frame of the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "muscle movements.\ncamera). This resulted into 8616 clips of 30 seconds each (71,8 hours"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Clips"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "English"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Catalan"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Spanish"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Arabic"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Italian"
        },
        {
          "RECOLA datasets.": "Indonesian"
        },
        {
          "RECOLA datasets.": "German"
        },
        {
          "RECOLA datasets.": "French"
        },
        {
          "RECOLA datasets.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Clips"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "English"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Catalan"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Spanish"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Arabic"
        },
        {
          "RECOLA datasets.": ""
        },
        {
          "RECOLA datasets.": "Italian"
        },
        {
          "RECOLA datasets.": "Indonesian"
        },
        {
          "RECOLA datasets.": "German"
        },
        {
          "RECOLA datasets.": "French"
        },
        {
          "RECOLA datasets.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "bounds of the metrics. For the implementation details and open-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "source code of all baselines, please refer to our GitHub page at"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoder\nCNN\nDistribution \nhttps://github.com/reactmultimodalchallenge/baseline_react2023."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tokens"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features\nEmotion\nTrans-VAE. The Trans-VAE baseline has a similar architecture as"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoder\nTransformer \nthe TEACH proposed in [2], which consists of: (i) a CNN encoder"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Decoder\nTransformer \nSpeaker visual \nthat encodes the speaker facial image sequence (i.e., a short video)"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "frames"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Projection\nAudio\nas a sequence-level embedding; (ii) a transformer encoder that"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features\nVisualization \nfirst combines learned facial embeddings and audio embeddings (78-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "dimnesional MFCC features) extracted by Torchaudio library [44],"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Speaker audio\nand then predicts a pair of tokens ùúátoken and ùúétoken representing"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "(a)\nIllustration of the offline Trans-VAE baseline"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the Gaussian Distribution of multiple appropriate facial reactions"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "of the corresponding input speaker behaviour, based on not only"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the combined audio-visual embedding but also a pair of learnable"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoder\nCNN\nDistribution \ntokens; and (iii) a transformer decoder that samples a set of rep-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tokens"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoder\n features\nemotion\nCurrent\nresentations describing an appropriate facial reaction based on the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Transformer \nPast\nCurrent\npredicted distribution tokens, which include a set of 3D Morphable"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reaction\nPast\nDecoder\nTransformer \n ‚àí"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "speaker \n speaker \nModel (3DMM) coefficients (i.e., 52 facial expression coefficients, 3"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Projection\nAudio\nvisual frames\nvisual frames"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features\nvisualization \nCurrent \npose coefficients and 3 translation coefficients defined by [39]) and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reaction\nPast"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Past\nCurrent\na emotion matrices (i.e., 25-channel time-series including 15 frame-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "speaker \n speaker \nlevel AUs‚Äô occurrence, 8 frame-level facial expression probabilities"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "audio\naudio\n(b)\nIllustration of the online Trans-VAE baseline"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "as well as frame-level valence and arousal intensities). Based on the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Fig. 1. Overview of the Trans-VAE baseline."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "learned 3DMM coefficients and the corresponding listener‚Äôs portrait,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "FaceVerseV2 [39] is finally employed to translate the learned 3DMM"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "coefficients to the facial reaction image sequence."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "if online"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features\nEmotion\nDenoising chain\nAs illustrated in Fig 1, we apply the Trans-VAE model to both"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Reaction \noffline and online facial reaction generation sub-challenges. For the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Decoder\n[ t - w, t )"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "offline sub-challenge, it takes the entire sequence of speaker audio-\nPast speaker reaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "features\nVisualization \nvisual behaviours (i.e., 750 frames corresponding to 30s clip in this\nReaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoder"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "challenge) as the input and generates a sequence of facial reactions"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Encoded \nconsisting of 750 frames. The online Trans-VAE baseline follows [20]"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "if offline\nspeaker"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reaction\n[ t, t + w )\nto iteratively predict a short segment consisting of ùë§ facial reaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Current speaker reaction\nframes corresponding to the time [ùë° ‚àí ùë§ + 1 : ùë°], where causal mask"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[8, 10, 20, 28] is employed to avoid future speaker behaviours to be"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Fig. 2. Overview of offline and online BeLFusion baselines. The reaction"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "used for the facial reaction prediction. In particular, the ùúèth facial"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "encoder and decoder are previously trained as a variational autoencoder to"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "reaction frame is predicted based on: (i) ùë° ‚àí ùë§ frames ([1 : ùë° ‚àí ùë§]) of"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "learn a lower-dimensional representation of reactions sequences of length ùë§."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "past speaker behaviours; (ii) ùë° ‚àí ùë§ frames ([1 : ùë° ‚àí ùë§]) of previously"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Then, a latent diffusion model is trained to conditionally sample reactions"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "predicted facial reactions; and (iii) ùúè\nframes ([ùë° ‚àí ùë§ + 1\n:\nùúè]) of\nfrom it. The condition is either the past or the current speaker reaction, for"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the current speaker behaviour. The Trans-VAE models for both\nthe online and offline approaches, respectively."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sub-challenges are trained end-to-end with maximum 50 epochs,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "where Mean Square Error (MSE) loss function is employed for the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "2D facial frame reconstruction; a diversity loss [20, 45] is leveraged"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "5.1.2\nAudio features. We also apply OpenSmile [9] to extract clip-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "to increase sampling diversity; and a KL divergence loss is used to"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "level audio descriptors, including GEMAP and MFCC features. Con-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "constrain the predicted distribution tokens. To optimize the model,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sequently, we represent each speaker behaviour by combining all"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "AdamW optimizer [18] with a fixed learning rate of 1ùëí ‚àí 5 is used."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "frame-level descriptors as a multi-channel audio-visual time-series"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "BeLFusion. We use BeLFusion as our second baseline [3], see Fig-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "behavioural signal."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ure 2. For the sake of simplicity, we use the version without be-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "havioural disentanglement. BeLFusion is trained in two stages. First,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "5.2\nBaseline systems"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "a variational autoencoder (VAE) is trained to learn a lower represen-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "In this challenge, we first establish a set of naive baselines, namely\ntation of the visual features (e.g., AUs, facial affects, and expressions)"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "B_Random, B_Mime, and B_MeanSeq/B_MeanFr. Specifically, B_Random\nof ùë§ frames. On the VAE‚Äôs head, we include a regressor that trans-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "randomly samples ùõº = 10 facial reaction sequences from a Gauss-\nforms the decoded reaction to a sequence of 3DMM coefficients. The"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ian distribution. B_Mime generates facial reactions by mimicking\nVAE losses consists of the KL divergence, the reaction MSE, and the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "the corresponding speaker‚Äôs facial expressions. For B_MeanSeq\n3DMM coefficients MSE, with weights of 1ùëí ‚àí 5, 1, and 1, respec-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "and B_MeanFr, the generated facial reactions are decided by the\ntively. We chose ùë§ = 50, and the latent space has dimension 128."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "sequence- and frame-wise average reaction in the training set, re-\nThe model is trained with the AdamW optimizer [18] with a fixed"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge\nConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "spectively. Despite their simplicity,\nthese baselines illustrate the\nlearning rate of 0.001 and weight decay of 0.0005, for 1000 epochs. In"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4.": "",
          "Baseline offline facial reaction generation results achieved on the validation set.": "Appropriateness"
        },
        {
          "Table 4.": "Method",
          "Baseline offline facial reaction generation results achieved on the validation set.": ""
        },
        {
          "Table 4.": "",
          "Baseline offline facial reaction generation results achieved on the validation set.": "FRC (‚Üë)"
        },
        {
          "Table 4.": "GT",
          "Baseline offline facial reaction generation results achieved on the validation set.": "8.42"
        },
        {
          "Table 4.": "B_Random",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.04"
        },
        {
          "Table 4.": "B_Mime",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.35"
        },
        {
          "Table 4.": "B_MeanSeq",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.01"
        },
        {
          "Table 4.": "B_MeanFr",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.00"
        },
        {
          "Table 4.": "Trans-VAE w/o visual modality",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.09"
        },
        {
          "Table 4.": "Trans-VAE w/o audio modality",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.10"
        },
        {
          "Table 4.": "Trans-VAE",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.12"
        },
        {
          "Table 4.": "BeLFusion (ùëò=1)",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.12"
        },
        {
          "Table 4.": "BeLFusion (ùëò=10)",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.14"
        },
        {
          "Table 4.": "BeLFusion (ùëò=10) + Binarized AUs",
          "Baseline offline facial reaction generation results achieved on the validation set.": "0.12"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "BeLFusion (ùëò=10) + Binarized AUs\n0.12\n97.17",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "0.0323\n0.0173\n0.0341\n-\n49.00"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "the second stage, a latent diffusion model (LDM) learns to, given the",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "stronger implicit diversity loss, and therefore more diverse reactions"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "speaker‚Äôs reaction, predict the lower-dimensional representation of",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "generation."
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "the listener‚Äôs appropriate facial reaction. Similarly to Trans-VAE, this",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "5.3\nBaseline results"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "baseline also adopts a window-based approach where ùëá /ùë§ reactions",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "5.3.1\nOffline facial reaction generation sub-challenge. Table 4 and"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "are predicted independently. Then, the ùë§-frames-long reactions are",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "Table 5 show that both baselines can generate facial reactions that"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "stacked to build the full reaction. For the online sub-challenge, the",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "positively correlate to the appropriate real facial reactions, where"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "generation of the listener‚Äôs visual features for the window [ùë°, ùë° + ùë§)",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "BeLFusion outperforms Trans-VAE in terms of the distance between"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "is conditioned on the past speaker‚Äôs features at [ùë° ‚àí ùë§, ùë°). It predicts",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "the prediction and real appropriate facial reactions (FRD), as well"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "all zeroes for segment [0, ùë§). For the offline sub-challenge, such gen-",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "as intra-sequence diversity (FRVar). In return, the intra- and inter-"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "eration is conditioned on the speaker‚Äôs features on the same time",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "subject diversities (S-MSE and FRDvs)\nfor the non-binarized ap-"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "period: [ùë°, ùë° + ùë§). The LDM‚Äôs loss is the average of the MSE in the",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "proaches are lower than Trans-VAE. Moreover, the results achieved"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "latent space and the MSE in the reconstructed space. The denoising",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "by Trans-VAE suggest that both visual and audio modalities posi-"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "chain has 10 steps, and every denoising step is implemented with",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "tively contribute to the diversity of generated facial reactions (FRDiv,"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "a sequence of residual MLPs as in [27].\nIt is also optimized with",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "FRVar, and FRDvs). As for the FRC metric, Trans-VAE shows the"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "the AdamW [18], a learning rate of 0.0001, and a weight decay of",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "same limitations observed for the BeLFusion. For the latter, we"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "0.0005, for 100 epochs. We include two versions of the model: with",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "observe that, as expected, higher values of ùëò boost all diversity"
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "ùëò = 1 and ùëò = 10. As explained in [3], higher values for ùëò lead to a",
          "0.0134\n0.0078\n0.0149\n-\n46.94": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.14\n94.26": "",
          "0.0134\n0.0078\n0.0149\n-\n46.94": "metrics [3]. Randomly sampled facial reaction (i.e., B_Random) are"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5.": "",
          "Baseline offline facial reaction generation results achieved on the test set.": "Appropriateness"
        },
        {
          "Table 5.": "Method",
          "Baseline offline facial reaction generation results achieved on the test set.": ""
        },
        {
          "Table 5.": "",
          "Baseline offline facial reaction generation results achieved on the test set.": "FRC (‚Üë)"
        },
        {
          "Table 5.": "GT",
          "Baseline offline facial reaction generation results achieved on the test set.": "8.74"
        },
        {
          "Table 5.": "B_Random",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.04"
        },
        {
          "Table 5.": "B_Mime",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.38"
        },
        {
          "Table 5.": "B_MeanSeq",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.01"
        },
        {
          "Table 5.": "B_MeanFr",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.00"
        },
        {
          "Table 5.": "Trans-VAE w/o visual modality",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.08"
        },
        {
          "Table 5.": "Trans-VAE w/o audio modality",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.09"
        },
        {
          "Table 5.": "Trans-VAE",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.10"
        },
        {
          "Table 5.": "BeLFusion (ùëò=1)",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.12"
        },
        {
          "Table 5.": "BeLFusion (ùëò=10)",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.13"
        },
        {
          "Table 5.": "BeLFusion (ùëò=10) + Binarized AUs",
          "Baseline offline facial reaction generation results achieved on the test set.": "0.12"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "generated by an online Trans-VAE model."
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "deterministic baselines (i.e., B_Mime, B_MeanSeq and B_MeanFr)"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "achieved better appropriateness but much lower diversity. Unlike"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "above baselines, deep learned probabilistic models (i.e., Trans-VAE"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "and BeLFusion) can make a trade-off between the these two, which"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "means they can generate multiple different but appropriate facial"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "reactions (i.e., qualitative results achieved by the offline Trans-VAE"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "are visualised in Figure 3)."
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "5.3.2\nOnline facial reaction generation sub-challenge. As demon-"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "strated in Table 6 and Table 7, the results confirmed that both base-"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "lines can generate real-time facial reactions that are positively corre-"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "lated to the appropriate face reactions. Also, Trans-VAE outperforms"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "the BeLFusion approach in terms of diversity (FRDiv, FRVar, and"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "FRDvs), synchrony (FRSync), and while BeLFusion outperforms in"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "terms of DTW distances (FRD). Similarly to the offline task, the ran-"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "domly sampled facial reactions (i.e., B_Random) are diverse but not"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "appropriate, while the deterministic naive approaches (i.e., B_Mime,"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": ""
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "B_MeanSeq and B_MeanFR) achieved better results in terms of ap-"
        },
        {
          "Fig. 4. Examples of generated multiple listener reactions to a given speaker behaviour (including the speaker‚Äôs audio and face frames). These reactions are": "propriateness but not diversity. Again, the proposed deep learning"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6.": "",
          "Baseline online facial reaction generation results achieved on the validation set.": "Appropriateness"
        },
        {
          "Table 6.": "Method",
          "Baseline online facial reaction generation results achieved on the validation set.": ""
        },
        {
          "Table 6.": "",
          "Baseline online facial reaction generation results achieved on the validation set.": "FRC (‚Üë)"
        },
        {
          "Table 6.": "GT",
          "Baseline online facial reaction generation results achieved on the validation set.": "8.42"
        },
        {
          "Table 6.": "B_Random",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.04"
        },
        {
          "Table 6.": "B_Mime",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.35"
        },
        {
          "Table 6.": "B_MeanSeq",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.01"
        },
        {
          "Table 6.": "B_MeanFr",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.00"
        },
        {
          "Table 6.": "Trans-VAE w/o visual modality",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.15"
        },
        {
          "Table 6.": "Trans-VAE w/o audio modality",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.13"
        },
        {
          "Table 6.": "Trans-VAE",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.14"
        },
        {
          "Table 6.": "BeLFusion (ùëò=1)",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.12"
        },
        {
          "Table 6.": "BeLFusion (ùëò=10)",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.14"
        },
        {
          "Table 6.": "BeLFusion (ùëò=10) + Binarized AUs",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.12"
        },
        {
          "Table 6.": "Table 7.",
          "Baseline online facial reaction generation results achieved on the validation set.": "Baseline online facial reaction generation results achieved on the test set."
        },
        {
          "Table 6.": "",
          "Baseline online facial reaction generation results achieved on the validation set.": "Appropriateness"
        },
        {
          "Table 6.": "Method",
          "Baseline online facial reaction generation results achieved on the validation set.": ""
        },
        {
          "Table 6.": "",
          "Baseline online facial reaction generation results achieved on the validation set.": "FRC (‚Üë)"
        },
        {
          "Table 6.": "GT",
          "Baseline online facial reaction generation results achieved on the validation set.": "8.74"
        },
        {
          "Table 6.": "B_Random",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.04"
        },
        {
          "Table 6.": "B_Mime",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.38"
        },
        {
          "Table 6.": "B_MeanSeq",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.01"
        },
        {
          "Table 6.": "B_MeanFr",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.00"
        },
        {
          "Table 6.": "Trans-VAE w/o visual modality",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.13"
        },
        {
          "Table 6.": "Trans-VAE w/o audio modality",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.13"
        },
        {
          "Table 6.": "Trans-VAE",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.13"
        },
        {
          "Table 6.": "BeLFusion (ùëò=1)",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.12"
        },
        {
          "Table 6.": "BeLFusion (ùëò=10)",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.13"
        },
        {
          "Table 6.": "BeLFusion (ùëò=10) + Binarized AUs",
          "Baseline online facial reaction generation results achieved on the validation set.": "0.12"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "BeLFusion (ùëò=10) + Binarized AUs\n0.12\n92.13",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "0.0306\n0.0164\n0.0317\n-\n49.00"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "6\nCONCLUSION",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "reactions with respect to the naive baselines; (ii) both visual and"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "audio modalities in Trans-VAE positively contributed to the diver-"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "In this paper, we introduced REACT2023 - the first Multiple Ap-",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "sity and appropriateness of the generated facial reactions; (iii) the"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "propriate Facial Reaction Generation challenge, which provides",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "Trans-VAE achieved better results for online sub-challenge over the"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "the very first attempt to bring together researchers from different",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "offline sub-challenge, while the opposite scenario has been observed"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "subjects to contribute a new challenging but promising affective",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "in the BeLFusion approach;"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "computing research direction. It comprises two sub-challenges: (i)",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "As the first multiple appropriate facial reaction generation chal-"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "Offline Multiple Appropriate Facial Reaction Generation challenge;",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "lenge, the used dataset is not specifically recorded, and thus some"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "and (ii) Online Multiple Appropriate Facial Reaction Generation",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "important behavioural cues (e.g., verbal texts, physiological signals)"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "challenge. Intentionally, we provide not only audio-visual dyadic",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "were not considered in this challenge. Our future work will focus"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "interaction clips that segmented from three different datasets with",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "on continue organizing REACT challenges while introducing a new"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "various interaction conditions, but also a set of audio-visual baseline",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "dataset containing more modalities."
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "features extracted from open-source software/code with the highest",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "possible transparency and realism for the baselines. Importantly,",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "ACKNOWLEDGEMENTS"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "we made all the code scripts for both features extraction and two",
          "0.0133\n0.0077\n0.0143\n-\n44.80": ""
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "facial reaction generation baselines (i.e., Trans-VAE and BeLFusion)",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "Funding: M. Spitale and H. Gunes are supported by the EPSRC/UKRI"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "to be publicly available, where both baselines can generate mul-",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "under grant ref. EP/R030782/1 (ARoEQ). This work has been par-"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "tiple different but appropriate and realistic facial reactions from",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "tially supported by the Spanish project PID2019-105093GB-I00 and"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "speaker audio-visual behaviours in both offline and online settings.",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "by ICREA under the ICREA Academia programme."
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "Our protocol strictly evaluates all participant models under the",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "Open Access: For open access purposes, the authors have applied"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "same settings by comprehensively considering four aspects of their",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "a Creative Commons Attribution (CC BY)\nlicence to any Author"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "generated facial reactions: appropriateness, diversity, realism and",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "Accepted Manuscript version arising."
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "synchrony.",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "Data access: Data related to this publication can be accessed upon"
        },
        {
          "BeLFusion (ùëò=10)\n0.13\n89.42": "The results of our proposed baselines suggested that:\n(i) both",
          "0.0133\n0.0077\n0.0143\n-\n44.80": "request following the terms and conditions of the datasets‚Äô owners."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "REFERENCES",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Facial Motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Pattern Recognition. 20395‚Äì20405."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[1] Nalini Ambady and Robert Rosenthal. 1992. Thin slices of expressive behavior as",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[23] Behnaz Nojavanasghari, Yuchi Huang, and Saad Khan. 2018. Interactive generative"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "predictors of interpersonal consequences: A meta-analysis. Psychological bulletin",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "adversarial networks for facial expression generation in dyadic interactions. arXiv"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "111, 2 (1992), 256.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "preprint arXiv:1801.09092 (2018)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[2] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and G√ºl Varol. 2022. TEACH:",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[24] Naima Otberdout, Mohamed Daoudi, Anis Kacem, Lahoucine Ballihi, and Stefano"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Temporal Action Composition for 3D Humans. In International Conference on 3D",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Berretti. 2020. Dynamic facial expression generation on hilbert hypersphere with"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Vision 2022.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "conditional wasserstein generative adversarial nets.\nIEEE Transactions on Pattern"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[3] German Barquero, Sergio Escalera, and Cristina Palmero. 2022. BeLFusion: La-",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Analysis and Machine Intelligence 44, 2 (2020), 848‚Äì863."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "tent Diffusion for Behavior-Driven Human Motion Prediction.\narXiv preprint",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[25] Cristina Palmero, German Barquero, Julio C. S. Jacques Junior, Albert Clap√©s,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "arXiv:2211.14304 (2022).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Johnny N√∫√±ez, David Curto, Sorina Smeureanu, Javier Selva, Zejian Zhang, David"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[4] German Barquero, Johnny N√∫nez, Sergio Escalera, Zhen Xu, Wei-Wei Tu, Isabelle",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Saeteros, David Gallardo-Pujol, Georgina Guilera, David Leiva, Feng Han, Xiaoxue"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Guyon, and Cristina Palmero. 2022. Didn‚Äôt see that coming: a survey on non-",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Feng, Jennifer He, Wei-Wei Tu, Thomas B. Moeslund, Isabelle Guyon, and Sergio"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "verbal social human behavior forecasting. In Understanding Social Behavior in",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Escalera. 2022. ChaLearn LAP Challenges on Self-Reported Personality Recog-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Dyadic and Small Group Interactions. PMLR, 139‚Äì178.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "nition and Non-Verbal Behavior Forecasting During Social Dyadic Interactions:"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[5] German Barquero, Johnny N√∫√±ez, Zhen Xu, Sergio Escalera, Wei-Wei Tu, Isabelle",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Dataset, Design, and Results. In Understanding Social Behavior in Dyadic and Small"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Guyon, and Cristina Palmero. 2022. Comparison of Spatio-Temporal Models for",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Group Interactions (Proceedings of Machine Learning Research, Vol. 173), Cristina"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Human Motion and Pose Forecasting in Face-to-Face Interaction Scenarios. In",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Palmero, Julio C. S. Jacques Junior, Albert Clap√©s, Isabelle Guyon, Wei-Wei Tu,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Understanding Social Behavior in Dyadic and Small Group Interactions (Proceedings",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Thomas B. Moeslund, and Sergio Escalera (Eds.). 4‚Äì52."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "of Machine Learning Research, Vol. 173), Cristina Palmero, Julio C. S. Jacques Junior,",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[26] Cristina Palmero, Javier Selva, Sorina Smeureanu, Julio Junior, CS Jacques, Albert"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Albert Clap√©s,\nIsabelle Guyon, Wei-Wei Tu, Thomas B. Moeslund, and Sergio",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Clap√©s, Alexa Mosegu√≠, Zejian Zhang, David Gallardo, Georgina Guilera, et al."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Escalera (Eds.). PMLR, 107‚Äì138.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "2021. Context-aware personality inference in dyadic scenarios: Introducing the"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[6] Angelo Cafaro, Johannes Wagner, Tobias Baur, Soumia Dermouche, Mercedes",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "udiva dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Torres Torres, Catherine Pelachaud, Elisabeth Andr√©, and Michel Valstar. 2017.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Computer Vision. 1‚Äì12."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "The NoXi database: multimodal recordings of mediated novice-expert interactions.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[27] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "In Proceedings of the 19th ACM International Conference on Multimodal Interaction.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Suwajanakorn. 2022. Diffusion autoencoders: Toward a meaningful and decodable"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "350‚Äì359.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[7] Zezhou Chen, Zhaoxiang Liu, Huan Hu, Jinqiang Bai, Shiguo Lian, Fuyuan Shi, and",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Pattern Recognition. 10619‚Äì10629."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Kai Wang. 2019. A realistic face-to-face conversation system based on deep neural",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[28] Ofir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short,\ntest\nlong: At-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "networks. In Proceedings of the IEEE/CVF International Conference on Computer",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tention with linear biases enables input\nlength extrapolation.\narXiv preprint"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Vision Workshops. 0‚Äì0.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "arXiv:2108.12409 (2021)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[8]\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[29] Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019.\nUnified language model pre-",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Francesc Moreno-Noguer. 2018.\nGanimation: Anatomically-aware facial ani-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "training for natural language understanding and generation. Advances in neural",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "mation from a single image. In Proceedings of the European conference on computer"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "information processing systems 32 (2019).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "vision (ECCV). 818‚Äì833."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[9]\nFlorian Eyben, Martin W√∂llmer, and Bj√∂rn Schuller. 2010. Opensmile: the munich",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[30]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan Liu. 2021. Pirenderer: Con-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "trollable portrait image generation via semantic neural rendering. In Proceedings"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "ACM international conference on Multimedia. 1459‚Äì1462.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "of the IEEE/CVF International Conference on Computer Vision. 13759‚Äì13768."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[10] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. 2022.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[31] Alexander Richard, Michael Zollh√∂fer, Yandong Wen, Fernando De la Torre, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers.\nIn Pro-",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Yaser Sheikh. 2021. Meshtalk: 3d face animation from speech using cross-modality"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "disentanglement. In Proceedings of the IEEE/CVF International Conference on Com-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "18770‚Äì18780.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "puter Vision. 1173‚Äì1182."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[11] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. 2022.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[32]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. 2013."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Joint audio-text model for expressive speech-driven 3d facial animation.\nPro-",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Introducing the RECOLA multimodal corpus of remote collaborative and affective"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "ceedings of the ACM on Computer Graphics and Interactive Techniques 5, 1 (2022),",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "interactions. In 2013 10th IEEE international conference and workshops on automatic"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "1‚Äì15.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "face and gesture recognition (FG). IEEE, 1‚Äì8."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[12]\nScott Geng, Revant Teotia, Purva Tendulkar, Sachit Menon, and Carl Vondrick.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[33] Zilong Shao, Siyang Song, Shashank Jaiswal, Linlin Shen, Michel Valstar, and Hat-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "2023. Affective Faces for Goal-Driven Dyadic Communication. arXiv preprint",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "ice Gunes. 2021. Personality recognition by modelling person-specific cognitive"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "arXiv:2301.10939 (2023).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "processes using graph representation. In proceedings of the 29th ACM international"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[13]\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "conference on multimedia. 357‚Äì366."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[34]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Siyang Song, Zilong Shao, Shashank Jaiswal, Linlin Shen, Michel Valstar, and"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "networks. Commun. ACM 63, 11 (2020), 139‚Äì144.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Hatice Gunes. 2022. Learning Person-specific Cognition from Facial Reactions"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Yuchi Huang and Saad Khan. 2018. A generative approach for dynamically varying\n[14]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "for Automatic Personality Recognition.\nIEEE Transactions on Affective Computing"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "photorealistic facial expressions in human-agent interactions. In Proceedings of",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "(2022)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "the 20th ACM International Conference on Multimodal Interaction. 437‚Äì445.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[35]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Siyang Song, Yuxin Song, Cheng Luo, Zhiyuan Song, Selim Kuzucu, Xi Jia, Zhijiang"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[15]\nYuchi Huang and Saad M Khan. 2017. Dyadgan: Generating facial expressions in",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Guo, Weicheng Xie, Linlin Shen, and Hatice Gunes. 2022. GRATIS: Deep Learning"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "dyadic interactions. In Proceedings of the IEEE Conference on Computer Vision and",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Graph Representation with Task-specific Topology and Multi-dimensional Edge"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Pattern Recognition Workshops. 11‚Äì18.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Features. arXiv preprint arXiv:2211.12482 (2022)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[16]\nYuchi Huang and Saad M Khan. 2018. Generating Photorealistic Facial Expressions",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[36]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Siyang Song, Micol Spitale, Yiming Luo, Batuhan Bal, and Hatice Gunes. 2023."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "in Dyadic Interactions.. In BMVC. 201.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Multiple Appropriate Facial Reaction Generation in Dyadic Interaction Settings:"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[17]\nPatrik Jonell, Taras Kucherenko, Gustav Eje Henter, and Jonas Beskow. 2020. Let‚Äôs",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "What, Why and How? arXiv e-prints (2023), arXiv‚Äì2302."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[37] Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos, and Maja"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "in dyadic settings.\nIn Proceedings of the 20th ACM International Conference on",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Pantic. 2021. Estimation of continuous valence and arousal levels from faces in"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Intelligent Virtual Agents. 1‚Äì8.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "naturalistic conditions. Nature Machine Intelligence 3, 1 (2021), 42‚Äì50."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[18]\nIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[38] Nguyen Tan Viet Tuyen and Oya Celiktutan. 2022. Context-Aware Human Be-"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "arXiv preprint arXiv:1711.05101 (2017).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "haviour Forecasting in Dyadic Interactions. In Understanding Social Behavior in"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[19] Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, and Hatice Gunes. 2022.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Dyadic and Small Group Interactions (Proceedings of Machine Learning Research,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Vol. 173), Cristina Palmero, Julio C. S. Jacques Junior, Albert Clap√©s, Isabelle Guyon,"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Action Unit Recognition.\nIn Proceedings of\nthe Thirty-First\nInternational Joint",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Wei-Wei Tu, Thomas B. Moeslund, and Sergio Escalera (Eds.). PMLR, 88‚Äì106."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Conference on Artificial Intelligence, IJCAI-22. 1239‚Äì1246.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[39]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[20] Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Linlin Shen, and Hatice",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "2022. Faceverse: a fine-grained and detail-controllable 3d face morphable model"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Gunes. 2023.\nReactFace: Multiple Appropriate Facial Reaction Generation in",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "from a hybrid dataset. In Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "Dyadic Interactions. arXiv preprint arXiv:2305.15748 (2023).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Vision and Pattern Recognition. 20333‚Äì20342."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[21] Mehdi Mirza and Simon Osindero. 2014. Conditional generative adversarial nets.",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[40]",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Jieyeon Woo, Mireille Fares, Catherine Pelachaud, and Catherine Achard. 2023."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "arXiv preprint arXiv:1411.1784 (2014).",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted"
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "[22]\nEvonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa,",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Behavior Synthesis. arXiv preprint arXiv:2305.11310 (2023)."
        },
        {
          "REACT2023: the first Multi-modal Multiple Appropriate Facial Reaction Generation Challenge": "and Shiry Ginosar. 2022. Learning to Listen: Modeling Non-Deterministic Dyadic",
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[41]\nJieyeon Woo, Catherine I Pelachaud, and Catherine Achard. 2021. Creating an",
          "Trovato and Tobin, et al.": "[44] Yao-Yuan Yang, Moto Hira, Zhaoheng Ni, Artyom Astafurov, Caroline Chen,"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "interactive human/agent loop using multimodal recurrent neural networks. In",
          "Trovato and Tobin, et al.": "Christian Puhrsch, David Pollack, Dmitriy Genzel, Donny Greenberg, Edward Z"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "WACAI 2021.",
          "Trovato and Tobin, et al.": "Yang, et al. 2022. Torchaudio: Building blocks for audio and speech processing. In"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[42]\nJinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-",
          "Trovato and Tobin, et al.": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Tsin Wong. 2023. CodeTalker: Speech-Driven 3D Facial Animation with Discrete",
          "Trovato and Tobin, et al.": "Processing (ICASSP). IEEE, 6982‚Äì6986."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Motion Prior. arXiv preprint arXiv:2301.02379 (2023).",
          "Trovato and Tobin, et al.": "[45]\nYe Yuan and Kris Kitani. 2020. Dlow: Diversifying latent flows for diverse human"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "[43] Tong Xu, Micol Spitale, Hao Tang, Lu Liu, Hatice Gunes, and Siyang Song. 2023.",
          "Trovato and Tobin, et al.": "motion prediction.\nIn Computer Vision‚ÄìECCV 2020: 16th European Conference,"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "Reversible Graph Neural Network-based Reaction Distribution Learning for Mul-",
          "Trovato and Tobin, et al.": "Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part IX 16. Springer, 346‚Äì364."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "tiple Appropriate Facial Reactions Generation. arXiv preprint arXiv:2305.15270",
          "Trovato and Tobin, et al.": "[46] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. 2022."
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "(2023).",
          "Trovato and Tobin, et al.": "Responsive listening head generation: a benchmark dataset and baseline. In Euro-"
        },
        {
          "Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY": "",
          "Trovato and Tobin, et al.": "pean Conference on Computer Vision. Springer, 124‚Äì142."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Thin slices of expressive behavior as predictors of interpersonal consequences: A meta-analysis",
      "authors": [
        "Nalini Ambady",
        "Robert Rosenthal"
      ],
      "year": "1992",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "2",
      "title": "TEACH: Temporal Action Composition for 3D Humans",
      "authors": [
        "Nikos Athanasiou",
        "Mathis Petrovich",
        "Michael Black",
        "G√ºl Varol"
      ],
      "year": "2022",
      "venue": "International Conference on 3D Vision"
    },
    {
      "citation_id": "3",
      "title": "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction",
      "authors": [
        "German Barquero",
        "Sergio Escalera",
        "Cristina Palmero"
      ],
      "year": "2022",
      "venue": "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction",
      "arxiv": "arXiv:2211.14304"
    },
    {
      "citation_id": "4",
      "title": "Didn't see that coming: a survey on nonverbal social human behavior forecasting",
      "authors": [
        "German Barquero",
        "Johnny N√∫nez",
        "Sergio Escalera",
        "Zhen Xu",
        "Wei-Wei Tu",
        "Isabelle Guyon",
        "Cristina Palmero"
      ],
      "year": "2022",
      "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions"
    },
    {
      "citation_id": "5",
      "title": "Comparison of Spatio-Temporal Models for Human Motion and Pose Forecasting in Face-to-Face Interaction Scenarios",
      "authors": [
        "German Barquero",
        "Johnny N√∫√±ez",
        "Zhen Xu",
        "Sergio Escalera",
        "Wei-Wei Tu",
        "Isabelle Guyon"
      ],
      "year": "2022",
      "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "6",
      "title": "The NoXi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "Angelo Cafaro",
        "Johannes Wagner",
        "Tobias Baur",
        "Soumia Dermouche",
        "Mercedes Torres",
        "Catherine Pelachaud",
        "Elisabeth Andr√©",
        "Michel Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "7",
      "title": "A realistic face-to-face conversation system based on deep neural networks",
      "authors": [
        "Zezhou Chen",
        "Zhaoxiang Liu",
        "Huan Hu",
        "Jinqiang Bai",
        "Shiguo Lian",
        "Fuyuan Shi",
        "Kai Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "8",
      "title": "Unified language model pretraining for natural language understanding and generation",
      "authors": [
        "Li Dong",
        "Nan Yang",
        "Wenhui Wang",
        "Furu Wei",
        "Xiaodong Liu",
        "Yu Wang",
        "Jianfeng Gao",
        "Ming Zhou",
        "Hsiao-Wuen Hon"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "FaceFormer: Speech-Driven 3D Facial Animation with Transformers",
      "authors": [
        "Yingruo Fan",
        "Zhaojiang Lin",
        "Jun Saito",
        "Wenping Wang",
        "Taku Komura"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Joint audio-text model for expressive speech-driven 3d facial animation",
      "authors": [
        "Yingruo Fan",
        "Zhaojiang Lin",
        "Jun Saito",
        "Wenping Wang",
        "Taku Komura"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques"
    },
    {
      "citation_id": "12",
      "title": "Affective Faces for Goal-Driven Dyadic Communication",
      "authors": [
        "Scott Geng",
        "Revant Teotia",
        "Purva Tendulkar",
        "Sachit Menon",
        "Carl Vondrick"
      ],
      "year": "2023",
      "venue": "Affective Faces for Goal-Driven Dyadic Communication",
      "arxiv": "arXiv:2301.10939"
    },
    {
      "citation_id": "13",
      "title": "Generative adversarial networks",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2020",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "14",
      "title": "A generative approach for dynamically varying photorealistic facial expressions in human-agent interactions",
      "authors": [
        "Yuchi Huang",
        "Saad Khan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Dyadgan: Generating facial expressions in dyadic interactions",
      "authors": [
        "Yuchi Huang",
        "Saad M Khan"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "16",
      "title": "Generating Photorealistic Facial Expressions in Dyadic Interactions",
      "authors": [
        "Yuchi Huang",
        "Saad M Khan"
      ],
      "year": "2018",
      "venue": "BMVC"
    },
    {
      "citation_id": "17",
      "title": "Let's face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings",
      "authors": [
        "Patrik Jonell",
        "Taras Kucherenko",
        "Gustav Henter",
        "Jonas Beskow"
      ],
      "year": "2020",
      "venue": "Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "18",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "19",
      "title": "Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "ReactFace: Multiple Appropriate Facial Reaction Generation in Dyadic Interactions",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Micol Spitale",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "year": "2023",
      "venue": "ReactFace: Multiple Appropriate Facial Reaction Generation in Dyadic Interactions",
      "arxiv": "arXiv:2305.15748"
    },
    {
      "citation_id": "21",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "Mehdi Mirza",
        "Simon Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "22",
      "title": "Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion",
      "authors": [
        "Evonne Ng",
        "Hanbyul Joo",
        "Liwen Hu",
        "Hao Li",
        "Trevor Darrell",
        "Angjoo Kanazawa",
        "Shiry Ginosar"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Interactive generative adversarial networks for facial expression generation in dyadic interactions",
      "authors": [
        "Behnaz Nojavanasghari",
        "Yuchi Huang",
        "Saad Khan"
      ],
      "year": "2018",
      "venue": "Interactive generative adversarial networks for facial expression generation in dyadic interactions",
      "arxiv": "arXiv:1801.09092"
    },
    {
      "citation_id": "24",
      "title": "Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative adversarial nets",
      "authors": [
        "Naima Otberdout",
        "Mohamed Daoudi",
        "Anis Kacem",
        "Lahoucine Ballihi",
        "Stefano Berretti"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "ChaLearn LAP Challenges on Self-Reported Personality Recognition and Non-Verbal Behavior Forecasting During Social Dyadic Interactions: Dataset, Design, and Results",
      "authors": [
        "Cristina Palmero",
        "German Barquero",
        "Julio Jacques Junior",
        "Albert Clap√©s",
        "Johnny N√∫√±ez",
        "David Curto",
        "Sorina Smeureanu",
        "Javier Selva",
        "Zejian Zhang",
        "David Saeteros",
        "David Gallardo-Pujol",
        "Georgina Guilera",
        "David Leiva",
        "Feng Han",
        "Xiaoxue Feng",
        "Jennifer He",
        "Wei-Wei Tu",
        "Thomas Moeslund",
        "Isabelle Guyon",
        "Sergio Escalera"
      ],
      "year": "2022",
      "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "26",
      "title": "Context-aware personality inference in dyadic scenarios: Introducing the udiva dataset",
      "authors": [
        "Cristina Palmero",
        "Javier Selva",
        "Sorina Smeureanu",
        "Julio Junior",
        "Albert Jacques",
        "Alexa Clap√©s",
        "Zejian Mosegu√≠",
        "David Zhang",
        "Georgina Gallardo",
        "Guilera"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Diffusion autoencoders: Toward a meaningful and decodable representation",
      "authors": [
        "Konpat Preechakul",
        "Nattanat Chatthee",
        "Suttisak Wizadwongsa",
        "Supasorn Suwajanakorn"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
      "authors": [
        "Ofir Press",
        "Noah Smith",
        "Mike Lewis"
      ],
      "year": "2021",
      "venue": "Train short, test long: Attention with linear biases enables input length extrapolation",
      "arxiv": "arXiv:2108.12409"
    },
    {
      "citation_id": "29",
      "title": "Ganimation: Anatomically-aware facial animation from a single image",
      "authors": [
        "Albert Pumarola",
        "Antonio Agudo",
        "M Aleix",
        "Alberto Martinez",
        "Francesc Sanfeliu",
        "Moreno-Noguer"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "30",
      "title": "Pirenderer: Controllable portrait image generation via semantic neural rendering",
      "authors": [
        "Ge Yurui Ren",
        "Yuanqi Li",
        "Thomas Chen",
        "Shan Li",
        "Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Meshtalk: 3d face animation from speech using cross-modality disentanglement",
      "authors": [
        "Alexander Richard",
        "Michael Zollh√∂fer",
        "Yandong Wen",
        "Fernando De La Torre",
        "Yaser Sheikh"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "32",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "33",
      "title": "Personality recognition by modelling person-specific cognitive processes using graph representation",
      "authors": [
        "Zilong Shao",
        "Siyang Song",
        "Shashank Jaiswal"
      ],
      "year": "2021",
      "venue": "proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "34",
      "title": "Learning Person-specific Cognition from Facial Reactions for Automatic Personality Recognition",
      "authors": [
        "Siyang Song",
        "Zilong Shao",
        "Shashank Jaiswal",
        "Linlin Shen",
        "Michel Valstar",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "GRATIS: Deep Learning Graph Representation with Task-specific Topology and Multi-dimensional Edge Features",
      "authors": [
        "Siyang Song",
        "Yuxin Song",
        "Cheng Luo",
        "Zhiyuan Song",
        "Selim Kuzucu",
        "Xi Jia",
        "Zhijiang Guo",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "GRATIS: Deep Learning Graph Representation with Task-specific Topology and Multi-dimensional Edge Features",
      "arxiv": "arXiv:2211.12482"
    },
    {
      "citation_id": "36",
      "title": "Multiple Appropriate Facial Reaction Generation",
      "authors": [
        "Siyang Song",
        "Micol Spitale",
        "Yiming Luo",
        "Batuhan Bal",
        "Hatice Gunes"
      ],
      "year": "2023",
      "venue": "Dyadic Interaction Settings: What, Why and How? arXiv e-prints"
    },
    {
      "citation_id": "37",
      "title": "Georgios Tzimiropoulos, and Maja Pantic. 2021. Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Context-Aware Human Behaviour Forecasting in Dyadic Interactions",
      "authors": [
        "Nguyen Tan",
        "Viet Tuyen",
        "Oya Celiktutan"
      ],
      "year": "2022",
      "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "Faceverse: a fine-grained and detail-controllable 3d face morphable model from a hybrid dataset",
      "authors": [
        "Lizhen Wang",
        "Zhiyuan Chen",
        "Tao Yu",
        "Chenguang Ma",
        "Liang Li",
        "Yebin Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis",
      "authors": [
        "Jieyeon Woo",
        "Mireille Fares",
        "Catherine Pelachaud",
        "Catherine Achard"
      ],
      "year": "2023",
      "venue": "AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis",
      "arxiv": "arXiv:2305.11310"
    },
    {
      "citation_id": "41",
      "title": "Creating an interactive human/agent loop using multimodal recurrent neural networks",
      "authors": [
        "Jieyeon Woo",
        "Catherine Pelachaud",
        "Catherine Achard"
      ],
      "year": "2021",
      "venue": "Creating an interactive human/agent loop using multimodal recurrent neural networks"
    },
    {
      "citation_id": "42",
      "title": "CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior",
      "authors": [
        "Jinbo Xing",
        "Menghan Xia",
        "Yuechen Zhang",
        "Xiaodong Cun",
        "Jue Wang",
        "Tien-Tsin Wong"
      ],
      "year": "2023",
      "venue": "CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior",
      "arxiv": "arXiv:2301.02379"
    },
    {
      "citation_id": "43",
      "title": "Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation",
      "authors": [
        "Tong Xu",
        "Micol Spitale",
        "Hao Tang",
        "Lu Liu",
        "Hatice Gunes",
        "Siyang Song"
      ],
      "year": "2023",
      "venue": "Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation",
      "arxiv": "arXiv:2305.15270"
    },
    {
      "citation_id": "44",
      "title": "Torchaudio: Building blocks for audio and speech processing. ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "authors": [
        "Yao-Yuan",
        "Moto Yang",
        "Zhaoheng Hira",
        "Artyom Ni",
        "Caroline Astafurov",
        "Christian Chen",
        "David Puhrsch",
        "Dmitriy Pollack",
        "Donny Genzel",
        "Edward Greenberg",
        "Yang"
      ],
      "year": "2022",
      "venue": "Torchaudio: Building blocks for audio and speech processing. ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Dlow: Diversifying latent flows for diverse human motion prediction",
      "authors": [
        "Ye Yuan",
        "Kris Kitani"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "46",
      "title": "Responsive listening head generation: a benchmark dataset and baseline",
      "authors": [
        "Mohan Zhou",
        "Yalong Bai",
        "Wei Zhang",
        "Ting Yao",
        "Tiejun Zhao",
        "Tao Mei"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    }
  ]
}