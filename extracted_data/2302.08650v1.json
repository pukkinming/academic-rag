{
  "paper_id": "2302.08650v1",
  "title": "Gaussian-Smoothed Imbalance Data Improves Speech Emotion Recognition",
  "published": "2023-02-17T01:50:46Z",
  "authors": [
    "Xuefeng Liang",
    "Hexin Jiang",
    "Wenxin Xu",
    "Ying Zhou"
  ],
  "keywords": [
    "Pairwise-emotion Data Distribution Smoothing",
    "Gaussian Smoothing",
    "Mixup Augmentation",
    "Model-modality Agnostic"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In speech emotion recognition tasks, models learn emotional representations from datasets. We find the data distribution in the IEMOCAP dataset is very imbalanced, which may harm models to learn a better representation. To address this issue, we propose a novel Pairwise-emotion Data Distribution Smoothing (PDDS) method. PDDS considers that the distribution of emotional data should be smooth in reality, then applies Gaussian smoothing to emotion-pairs for constructing a new training set with a smoother distribution. The required new data are complemented using the mixup augmentation. As PDDS is model and modality agnostic, it is evaluated with three SOTA models on the IEMOCAP dataset. The experimental results show that these models are improved by 0.2% ∼ 4.8% and 1.5% ∼ 5.9% in terms of WA and UA. In addition, an ablation study demonstrates that the key advantage of PDDS is the reasonable data distribution rather than a simple data augmentation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is of great significance to understanding human communication. SER techniques have been applied to many fields, such as video understanding  [1] , human-computer interaction  [2] , mobile services  [3]  and call centers  [4] . Up to now, plenty of deep learning based SER methods have been proposed  [5, 6] . Recently, multimodal emotion recognition attracted more attention  [7, 8, 9]  because of its richer representation and better performance. Most of these studies assigned one-hot labels to utterances. In fact, experts often have inconsistent cognition of emotional data, thus, the one-hot label is obtained by majority voting. Figure  1 (a) shows the emotional data distribution in the IEMOCAP dataset  [10] .\n\nLater, some studies argued that the one-hot label might not represent emotions well. They either addressed this problem through multi-task learning  [11] , soft-label  [12, 13]  and multi-label  [14] , or enhanced the model's learning ability of  ambiguous emotions by dynamic label correction  [15]  and label reconstruction through interactive learning  [16] . These works usually defined the samples with consistent experts' votes as clear samples and those with inconsistent votes as ambiguous samples. The statistics of clear and ambiguous samples in the IEMOCAP dataset are shown in Fig.  1(b) .\n\nAfter statistical analysis, we find that the data distribution is imbalanced, especially for ambiguous samples. For example, the quantity of ambiguous samples between anger and frustration is abundant, while that between anger and sadness is very few. Meanwhile, the quantity of clear samples of happiness counts much less than other clear emotion categories. We consider such distribution is unreasonable that may prevent the model from learning a better emotional representation. The possible reason is that the votes come from few experts, which is a rather sparse sampling of human population.\n\nWe think that clear and ambiguous emotional data shall follow a smooth statistical distribution in the real world.  of unreasonable data distribution. PDDS applies Gaussian smoothing on the data distribution between clear emotionpairs, which augments ambiguous samples up to reasonable quantities, meanwhile balances the quantities of clear samples in all categories to be close to each other. Figure  2  shows the data distributions before and after smoothing. To complement the missing data, we use a feature-level mixup between clear samples to augment the data.\n\nAs PDDS is model and modality agnostic, we evaluate it on three SOTA methods, as well as the unimodal and bimodal data on the IEMOCAP dataset. The results show that these models are improved by 0.2% ∼ 4.8% and 1.5% ∼ 5.9% in terms of WA and UA. Our proposed CLTNet achieves the best performance. The ablation study reveals that the nature of superior performance of PDDS is the reasonable data distribution rather than simply increasing the data size.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Figure  3  shows that our proposed framework includes three modules: (1) Preprocessing module. It extracts audio features using the pre-trained data2vec  [17]  and text features using the pre-trained Bert  [18] . (2) Pairwise-emotion Data Distribution Smoothing module (PDDS). It smooths the unreasonable data distribution, and is a plug-in module that can be applied Suppose there are c emotions in the dataset. As the label of an ambiguous sample often comes from two distinct emotions, we construct a clear-ambiguous-clear distribution for the population of four types of samples between every two emotions i and j, where i, j ∈ {1, . . . , c}, i = j. They are clear samples I only containing emotion i, ambiguous samples I J containing major emotion i and minor emotion j, ambiguous samples J I containing major emotion j and minor emotion i, and clear samples J only containing emotion j, as shown in Fig.  4 . We think that the quantity distribution of these four types of samples in every emotion-pair should be statistically smooth, so a Gaussian kernel is convolved with the distribution to have a smoothed version,\n\nwhere K = {k -1, k, k + 1} denotes the indexes of {I, I J , J I } or {I J , J I , J} when k is the index of I J or J I , n k is the quantity of samples in type k before smoothing, and n k is the quantity of samples after smoothing. For clear samples, if their quantity in an emotion category is too small, they are augmented until the quantity reaches that in other categories.\n\nThe smoothed quantity distribution of all emotion-pairs is shown in Fig.  2(b ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mixup Augmentation",
      "text": "After smoothing the data distribution, the quantities of some ambiguous samples in the original dataset are less than the expectation. To complete data, a feature-level data augmentation, Mixup  [19] , is applied to augment those samples. Mixup can improve the generalization ability of the model by generating new training samples by linearly combining two clear samples and their labels. It follows the rules\n\nwhere x α and x β are the features from a clear sample of the major emotion and a clear sample of the minor emotion, respectively. x mix is the feature of the new sample. y α and y β are the one-hot labels of x α and x β , and y mix is the label distribution of the new sample. To avoid undersampling, we use the original data when the original quantity of ambiguous samples has met or exceeded the smooth distribution.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cltnet",
      "text": "To further verify the effectiveness of PDDS, we design a simple but effective utterance-level multimodal fusion network, named CLTNet, which uses CNN, LSTM and Transformer to extract multimodal emotional features as shown in Fig.  3 . Firstly, for the audio modality, the acoustic features are fed into three convolutional blocks to capture the local patterns. Each of them has a 1D convolutional layer and a global pooling layer. To capture the temporal dependencies in the acoustic sequence, an LSTM layer and a pooling layer are employed. The obtained 4 encoded feature vectors are concatenated and fed into a fully-connected layer to get the audio representations as follows,\n\nwhere ConvBlock(•) = M axP ool(Relu(Conv1D(•))), LST M Block(•) = M axP ool(LST M (•)), x A ∈ R t A ×d A and x conv i A , x lstm A ∈ R d1 are input features and output features of convolution blocks and LSTM blocks, respectively, W A ∈ R 4d1×d and b A ∈ R d are trainable parameters. For the text modality, the text features are fed into a transformer encoder of N layers to capture the interactions between each pair of textual words. An attention mechanism  [20]  is applied to the outputs of the last block to focus on informative words and generate text representations,\n\nz T = T ransf ormerEncoder(x T ),\n\nwhere x T ∈ R t T ×d T and z T ∈ R t T ×d2 are input features and output features of TransformerEncoder,\n\nis the attention weight. Finally, the representations of the two modalities are concatenated and fed into three fully-connected layers with a residual connection, followed by a softmax layer. As using the label distribution to annotate emotional data, we choose the KL loss to optimize the model,\n\nwhere\n\n3. EXPERIMENT",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Evaluation Metrics",
      "text": "PDDS is evaluated on the most commonly used dataset (IEMOCAP) in SER  [10] . There are five sessions in the dataset, where each sentence is annotated by at least three experts. Following previous work  [21] , we choose six emotions: anger, happiness, sadness, neutral, excited and frustration. Session 1 to 4 are used as the training set and session 5 is used as the testing set, and PDDS is only applied on the training set. The weighted accuracy (WA, i.e., the overall accuracy) and unweighted accuracy (UA, i.e., the average accuracy over all emotion categories) are adopted as the evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "For audio, 512-dimensional features are extracted from the raw speech signal by the pre-trained data2vec  [17] . The frame size and frame shift are set to 25 ms and 20 ms, respectively. For text, the pre-trained Bert  [18]  is used to embed each word into a 768-dimensional vector.\n\nIn the original dataset, the quantity of clear samples belonging to happiness is very small, we then augment the data using mixup in order to balance the quantities of clear samples in all emotion categories. The augmentation settings are p = 0.5, α = β = happiness, and the quantity is increased from 43 to 215. Afterward, the data distribution smoothing is applied to the ambiguous samples of each emotion-pair with Gaussian kernel σ = 1.\n\nThe kernel sizes of three CNN blocks in CLTNet are 3, 4 and 5, and the number of hidden units is 300 in LSTM. The number of layers in the transformer encoder is 5 and each layer is with 5 attention heads. The embedding dimensions d 1 , d 2 and d in Eq. (  4 ) and (  5 ) are set to 300, 100 and 100, respectively. CLTNet is trained by the Adam optimizer. The learning rate, weight decay and batch size are set to 0.001, 0.00001 and 64, respectively. The training stops if the loss does not decrease for 10 consecutive epochs, or at most 30 epochs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Validation Experiment",
      "text": "As PDDS is model-modality agnostic, we select SpeechText  [7] , MAT  [8] , and MCSAN  [9] , which aim at utterance-level emotion recognition, to evaluate its effectiveness. Our proposed CLTNet and its unimodal branches are also tested. The experimental results are shown in Table  1 . We can observe that all models are significantly improved when the training data are processed by PDDS, with 0.2% ∼ 4.8% increase on WA and 1.5% ∼ 5.9% increase on UA. Among them, our proposed CLTNet achieves the best performance. This result demonstrates that a reasonable data distribution in the training set does help models learn a better emotional representation.\n\nA more detailed analysis, the confusion matrices of CLT-Net with and without PDDS, are shown in Fig.  5 . One can see that the classification accuracies in most emotion categories are increased. Only two more samples are misclassified in frustration. However, the rightmost columns in the confusion matrices illustrate that the model trained on the original dataset inclines to misclassify more samples as frustration. By contrast, PDDS considerably alleviates this tendency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "To verify the rationality of PDDS, two additional experiments are designed and tested on the CLTNet model: (a) Only balancing happiness: Mixup augmentation is only applied on The results are shown in Table  2 . We can observe that all three data augmentations boost the performance of the model. Compared to only balancing the happiness category, augmenting both ambiguous and clear samples can help the model perform better. Although Uniform balancing has the largest training dataset, PDDS performs best on WA and UA with 4.8% and 5.9% improvements. This result reveals the nature of advantage of PDDS is the reasonable data distribution instead of simply increasing the data size.\n\nWe further evaluate the effectiveness of p values in the mixup augmentation, and show the result in Fig.  6 . One can see the best results are achieved when p = 0.8.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we address the imbalanced data distribution in the IEMOCAP dataset by proposing the Pairwise-emotion Data Distribution Smoothing (PDDS) method. PDDS constructs a more reasonable training set with smoother distribution by applying Gaussian smoothing to emotion-pairs, and complements required data using a mixup augmentation. Experimental results show that PDDS considerably improves three SOTA methods. Meanwhile, our proposed CLTNet achieves the best result. More importantly, the ablation study verifies that the nature of superiority of PDDS is the reasonable data distribution instead of simply increasing the amount of data. In future work, we will explore a more reasonable data distribution for a better emotional representation learning.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) shows the emotional data distribution in the IEMOCAP",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) Data distribution in the IEMOCAP when the label",
      "page": 1
    },
    {
      "caption": "Figure 2: The data distributions before and after smoothing. The",
      "page": 2
    },
    {
      "caption": "Figure 3: The framework of our approach. It consists of three",
      "page": 2
    },
    {
      "caption": "Figure 3: shows that our proposed framework includes three",
      "page": 2
    },
    {
      "caption": "Figure 4: An example of smoothing the quantity distribution of",
      "page": 2
    },
    {
      "caption": "Figure 4: We think that the quantity distribution of these four",
      "page": 2
    },
    {
      "caption": "Figure 3: Firstly, for the audio modality, the acoustic features are",
      "page": 3
    },
    {
      "caption": "Figure 5: One can see",
      "page": 4
    },
    {
      "caption": "Figure 5: Confusion matrices of CLTNet without and with",
      "page": 4
    },
    {
      "caption": "Figure 6: Effect of different p values.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The comparison of models without and with PDDS.",
      "page": 4
    },
    {
      "caption": "Table 1: We can observe",
      "page": 4
    },
    {
      "caption": "Table 2: Evaluation of different data augmentation methods.",
      "page": 4
    },
    {
      "caption": "Table 2: We can observe that",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Pairwise emotional relationship recognition in drama videos: Dataset and benchmark",
      "authors": [
        "Xun Gao",
        "Yin Zhao",
        "Jie Zhang",
        "Longjun Cai"
      ],
      "venue": "ACM MM, 2021"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "4",
      "title": "Application of speech emotion recognition in intelligent household robot",
      "authors": [
        "Xu Huahu",
        "Gao Jue",
        "Yuan Jian"
      ],
      "year": "2010",
      "venue": "Application of speech emotion recognition in intelligent household robot"
    },
    {
      "citation_id": "5",
      "title": "Two-stream emotion recognition for call center monitoring",
      "authors": [
        "Purnima Gupta",
        "Nitendra Rajput"
      ],
      "year": "2007",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using speech feature and word embedding",
      "authors": [
        "Kiyoaki Bagus Tris Atmaja",
        "Masato Shirai",
        "Akagi"
      ],
      "year": "2019",
      "venue": "APSIPA ASC. IEEE"
    },
    {
      "citation_id": "9",
      "title": "Modulated fusion using transformer for linguisticacoustic emotion recognition",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "Proceedings of the First International Workshop on Natural Language Processing Beyond Text"
    },
    {
      "citation_id": "10",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "12",
      "title": "Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2016",
      "venue": "IJCNN"
    },
    {
      "citation_id": "14",
      "title": "Soft-target training with ambiguous emotional utterances for dnn-based speech emotion classification",
      "authors": [
        "Atsushi Ando",
        "Satoshi Kobashikawa",
        "Hosana Kamiyama",
        "Ryo Masumura",
        "Yusuke Ijima",
        "Yushi Aono"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
      "authors": [
        "Atsushi Ando",
        "Ryo Masumura",
        "Hosana Kamiyama",
        "Satoshi Kobashikawa",
        "Yushi Aono"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Meta-Learning for Speech Emotion Recognition Considering Ambiguity of Emotion Labels",
      "authors": [
        "Takuya Fujioka",
        "Takeshi Homma",
        "Kenji Nagamatsu"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Multi-classifier interactive learning for ambiguous speech emotion recognition",
      "authors": [
        "Ying Zhou",
        "Xuefeng Liang",
        "Yu Gu",
        "Yifei Yin",
        "Longshan Yao"
      ],
      "year": "2022",
      "venue": "TASLP"
    },
    {
      "citation_id": "18",
      "title": "data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "ICML"
    },
    {
      "citation_id": "19",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "20",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    },
    {
      "citation_id": "21",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "TASLP"
    },
    {
      "citation_id": "22",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "EMNLP"
    }
  ]
}