{
  "paper_id": "2508.20579v1",
  "title": "Glare: A Graph-Based Landmark Region Embedding Network For Emotion Recognition",
  "published": "2025-08-28T09:17:57Z",
  "authors": [
    "Debasis Maji",
    "Debaditya Barman"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Graph Neural Network",
    "Deep learning",
    "Human Computer Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression recognition (FER) is a crucial task in computer vision with wide range of applications including human-computer interaction, surveillance, and assistive technologies. However, challenges such as occlusion, expression variability, and lack of interpretability hinder the performance of traditional FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by modeling relational dependencies between facial landmarks, enabling structured and interpretable learning. In this paper, we propose GLaRE, a novel Graph-based Landmark Region Embedding network for emotion recognition. Facial landmarks are extracted using 3D facial alignment, and a quotient graph is constructed via hierarchical coarsening to preserve spatial structure while reducing complexity. Our method achieves ∼64.89% accuracy on AffectNet 1 and ∼94.24% on FERG 2 , outperforming several existing baselines. Additionally, ablation studies have demonstrated that region-level embeddings from quotient graphs have contributed to improved prediction performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition through facial expressions is one of the essential components of artificial intelligence (AI), particularly in the area of computer vision and human-computer interaction (HCI). Among the various modalities for emotion recognition, such as audio, video, and bio-signals, facial gestures offer a direct and intuitive cue to human emotions. Facial landmarks, which abstract away identifiable facial details, offer an efficient and privacy-preserving alternative to the processing of the entire image of the face by capturing only key facial positions. Their subtle movements are strongly associated with different affective states, making them well-suited for real-time applications in smart devices  [25, 26] , driver monitoring systems  [9, 14] , e-learning  [7] , and immersive gaming environments  [22] . This has resulted in development of Facial Landmark based Emotion Recognition, which holds promise for both accuracy and resource efficiency.\n\nAlthough facial landmarks  [4]  provide semantically rich and privacy preserving indicators for Facial Expression Recognition (FER), they have remained underutilized due to the lack of effective methods to model their spatial and temporal dependencies. While early studies have demonstrated the effectiveness of combining landmarks with appearance features or integrating them in multimodal pipelines (i.e., with EEG or video data)  [17, 12, 29]  , standalone landmark based methods require advanced geometric reasoning to reach full potential. The advancement of geometric deep learning and graph neural networks (GNNs) provides further opportunities to effectively utilize facial landmarks for FER. Convolutional neural networks (CNNs)  [20, 23, 28]  have achieved notable success in FER by learning local patterns from facial images. However, they often fall short in capturing the geometric relationships among facial landmarks. Most CNN-based methods focus on extracting deep appearance features, overlooking the underlying spatial structure of the face, which limits generalization and interpretability.\n\nTo overcome these limitations, GNNs  [13, 21, 1]  have emerged as a powerful alternative. Unlike CNNs, GNNs naturally operate on non-Euclidean data such as facial landmarks, modeling both the local and global structural relationships among keypoints. In the context of FER, each facial landmark is modeled as a node in a facial landmark graph. Edges in the graph encode anatomical or spatial connections, enabling the model to capture physiologically meaningful interactions. This facial landmark graph formulation enhances representation and generalization while improving robustness and interpretability by aligning with human cognitive mechanisms. Fig.  1  illustrates two example of faces and their corresponding facial landmark graphs, highlighting how expression information can be structurally encoded.\n\nThe key contributions of this work are as follows:\n\n• We have proposed a novel graph-based model, GLaRE (Graph-based Landmark Region Embedding Network), which effectively captures both local and global geometric relationships among facial landmarks for emotion recognition. • We have introduced a hierarchical quotient graph coarsening mechanism that has significantly reduced computational complexity while preserving essential facial structural information, thereby rendering the proposed model lightweight and well-suited for real-time facial emotion recognition tasks.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Problem Statement",
      "text": "A facial image can be represented as a landmark graph G = (V, E), where\n\nis the set of undirected edges connecting anatomically adjacent landmarks, and each node v i ∈ V is associated with a spatial coordinate feature vector x i ∈ R d , derived using a landmark detector Φ. Thus, the graph G = (V, E, X) captures the facial structure, where X ∈ R d is the matrix of node features. To incorporate anatomical hierarchy, the facial graph is partitioned into R predefined facial regions. r) , where V (r) ⊂ V (1) Each facial graph G i = (V i , E i , X i ) in the dataset is associated with a ground-truth emotion label y i ∈ Y where Y = {1, 2, • • • , C} denotes the set of C discrete expression classes. The objective is to learn the following mapping that can accurately predict the corresponding expression label from the structural and spatial properties of the facial graph.\n\nf :\n\nThis defines a graph classification problem, where each input graph encodes a face and its configuration, and the output is a categorical expression.\n\n3 Proposed Method",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph Construction From Facial Images",
      "text": "To prepare the input image for graph-based learning, a three-stage preprocessing pipeline has been adopted. First, 3D facial landmarks have been extracted from images using the Facial Alignment Network (FAN)  [3] . Each image has been processed to identify a fixed set of landmark points, thereby capturing the geometric structure of key facial regions such as the eyes, nose, mouth, and jawline. These landmark coordinates {p i ∈ R 3 } N i=1 have served as the geometric basis for node initialization.\n\nSecond, appearance features have been extracted using a pretrained ResNet-18 model  [11] , where the global average pooling and fully connected layers have been excluded to retain spatial feature maps for bilinear interpolation at landmark positions. The sampled descriptors have then been reduced to f dimensions using principal component analysis (PCA). These appearance features have been concatenated with the normalized 3D landmark coordinates, yielding node features.\n\nwhere pi ∈ R 3 denotes normalized coordinates and a i ∈ R f denotes the reduced appearance descriptor.\n\nFinally, a fine-level graph G f = (V f , E f ) has been constructed, where each node v i ∈ V f corresponds to a landmark with feature vector x i . Edges have been established using a k-nearest neighbor (kNN) criterion in Euclidean space:\n\n) This procedure has ensured that each landmark is connected to its local neighborhood, thereby encoding fine-grained spatial relationships among facial components. Together, the node features and adjacency structure have provided a rich representation of facial topology.\n\nAs illustrated in Fig.  2 , the proposed GLaRE model has followed a two-level graph-based architecture that incorporates equivariant message passing. The architecture has been organized into three key stages: (a) fine-grained node embeddings have been computed from landmark graphs using equivariant GNN layers, (b) a quotient graph has been constructed to compress landmark-level information into region-level nodes, and (c) region embeddings have been further processed through equivariant GNN layers to produce the final global embedding. The subsequent subsections present each stage of the architecture in detail.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fine-Level Node Embedding Using Edgeconv",
      "text": "Given the fine-grained landmark graph, where each node corresponds to a facial landmark and edges encode spatial proximity, node embeddings are computed using an EdgeConv-based message passing scheme  [27] . In this scheme, the embedding of each node i is updated by aggregating messages from its neighbors j ∈ N (i) using max aggregation, as defined in (5):\n\nHere, h (l) i and h (l) j represent the feature vectors of the target and neighboring nodes at layer l, respectively. The function ϕ, implemented as a two-layer multi-layer perceptron (MLP) with ReLU activation, transforms the concatenated source and relative features ([h\n\ni ]) into informative edge-level messages. This formulation captures local differences in landmark configurations and appearance. Moreover, it enhances the representation of facial expression patterns for subsequent processing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Region-Level Embedding Via Quotient Graph",
      "text": "To capture higher-level facial structures critical for FER, a quotient graph is introduced to model regional patterns by aggregating landmark nodes into coarse-grained regions. This approach reduces computational complexity, enhances robustness to local landmark variations, and enables hierarchical processing of local and global facial features, improving the model's ability to distinguish expressions.  Let G f = (V f , E f ) be a graph with node set V f and edge set E f , and let P = {V 1 , V 2 , . . . , V k } be a partition set of V f . The quotient graph  [10]  G q = G f /P = (V q , E q ) has node set V q = {V 1 , V 2 , . . . , V k }, where each node V i represents a cluster of nodes from V f . Each node v ∈ V f belongs to exactly one subset V i in the partition, where i ∈ {1, 2, . . . , k}. This one-to-one assignment naturally induces an equivalence relation ∼ on V f , defined as following.\n\nFig.  3  presents an example of how a quotient graph can be formed.\n\nLet the landmark-level graph be defined as G = (V, E), where V is the set of landmark nodes and E is the set of landmark edges. The landmark nodes are partitioned into k disjoint regions using k-Means clustering based on their coordinates.\n\nThe quotient graph is defined as G q = (V q , E q ), with |V q | = k, where each node u k ∈ V q corresponds to a region R k .\n\nThe embedding of a region node is computed by mean pooling over the embeddings of its constituent landmarks:\n\nSimilarly, the coordinates of the region node are aggregated:\n\nEdges in the quotient graph are defined using k-nearest neighbors (k-NN), based on the Euclidean distances between region coordinates p R k , ensuring connectivity reflects the spatial proximity of facial regions. In practice, this process is applied independently to each graph, producing a quotient graph with k nodes per input graph.\n\nThe quotient graph is processed with two EdgeConv layers, as described in  (5) . These layers enrich the region embeddings by capturing higher-order dependencies, such as correlations between the eyes and mouth or between the eyebrows and cheeks. The refined embeddings improve the effectiveness of the downstream classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Graph-Level Prediction From Region Embeddings",
      "text": "To produce a unified representation of the facial expression for classification, the region embeddings of the quotient graph are processed to capture inter-region interactions and aggregated into a permutation-invariant graph-level embedding. This phase ensures that the model captures unique facial patterns, such as coordinated movements of the eyes and mouth, critical for distinguishing expressions like happiness or sadness in FER.\n\nThe refined region embeddings are aggregated via global additive pooling  [16]  to obtain a permutation-invariant graph-level representation:\n\nAdditive pooling ensures robust aggregation across regions, accommodating variations in region contributions across samples.\n\nA linear layer maps h G to a probability distribution over 8 expression classes (neutral, happy, sad, surprise, fear, disgust, anger, contempt), enabling the model to classify facial expressions effectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Theoretical Achievements",
      "text": "The hierarchical architecture of the proposed GLaRE model achieves both computational efficiency and improved generalization for facial expression recognition by incorporating a quotient graph to represent regional facial structures. This approach reduces the computational complexity of message passing while maintaining the essential expressive information contained in facial landmarks.\n\nBy coarsening the fine-level facial landmark graph into a higher-level quotient graph, the number of nodes and edges involved in computation is significantly reduced. Given a fine-level graph with N nodes and E edges, GNN-based message passing typically incurs O(E) time complexity per layer. After coarsening, the coarse graph with N ′ ≪ N nodes and E ′ ≪ E edges reduces the message passing complexity to O(E ′ ), which leads to significant speedup.\n\nBeyond efficiency, the quotient graph groups landmarks into semantically meaningful facial regions (such as eyes, mouth, or eyebrows), thereby capturing higher-order interactions that are crucial for distinguishing expressions. This regional abstraction improves robustness to variations in landmark positions and local noise, enhancing the generalization ability of the model across diverse facial configurations.\n\nFinally, a permutation-invariant pooling mechanism aggregates region-level embeddings into a compact graph-level representation, ensuring consistent predictions irrespective of the input node ordering. This hierarchical approach allows the model to jointly exploit fine-grained landmark relations and coarse regional structures, resulting in a more efficient and reliable solution for facial expression recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Result & Discussion",
      "text": "This section presents the experimental evaluation of the proposed model GLaRE, which employs a hierarchical quotient graph to achieve computational efficiency and robustness to variations in facial landmark configurations. We provide a detailed analysis and discussion of classification accuracy, loss function, and comparisons with state-of-the-art models, including CNN-based approaches and graph-based methods, in this section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Description",
      "text": "We have conducted experiments on two benchmark facial expression datasets, AffectNet and FERG-DB, covering both real-world and stylized facial data.\n\nAffectNet  [24]  has emerged as the largest facial expression dataset to date, containing over one million facial images collected from the internet using emotion-related keywords in six different languages. Out of these, approximately 450,000 images have been manually annotated. The dataset defines eleven categories, including six basic emotions (anger, disgust, fear, happiness, sadness, surprise), as well as neutral, contempt, none, uncertain, and non-face. For our experiments, we have randomly created a subset consisting of the six basic expressions along with neutral, yielding a total of 83,901 training images and 1500 validation images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ferg-Db [2] (Facial Expression",
      "text": "Research Group Database) consists of 555,767 images of six stylized characters exhibiting seven types of facial expressions: the six basic emotions and neutral. Since the facial figures in this dataset are cartoon-style and lack realistic facial texture and contour details, extracting accurate facial landmarks has been particularly challenging. This difficulty arises because many landmark detection models are trained on real human faces and often fail to generalize to synthetic domains.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Methods",
      "text": "The gACNN  [18]  proposes an occlusion-aware attention mechanism that combines local and global facial features to improve facial expression recognition under occluded conditions. LDL-ALSG  [5]  introduces soft label supervision and models the correlations between samples to enhance the learning of facial expression features. OADN  [6]  incorporates a landmark-guided attention branch that directs the model to focus on non-occluded facial regions, thereby improving recognition performance under occlusions. FERGCN  [19]  employs a deep graph-based architecture that includes a feature extraction module, a graph convolution module, and a graph matching mechanism to recognize facial expressions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Additional Details",
      "text": "The model has been trained using the PyTorch-Geometric  [8]  framework for 200 epochs. The Adam optimizer  [15]  has been employed with a learning rate of 0.0001 and default betas of (0.9, 0.999). Cross-entropy loss has been used as the objective function, which is well-suited for multi-class classification tasks. It measures the discrepancy between the predicted class probabilities and the true labels, encouraging the network to assign higher confidence to the correct class. The loss for a single sample is defined as  (11) .\n\nwhere C is the total number of classes, y c is the ground truth indicator, and ŷc is the predicted probability for class c.  Table  1  summarizes the number of samples per class and their corresponding accuracies, while Fig.  4  provides a visual representation of this performance comparison with baseline models. These results indicate that GLaRE is highly effective on real-world facial data, especially when landmark features are accurate and consistent.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Evaluation And Analysis",
      "text": "As seen in Table  2 , the proposed GLaRE model achieves the best accuracy of 64.89% on this reduced AffectNet subset, outperforming all other baselines, including gACNN  [18] , OADN  [6] , LDL-ALSG  [5] , and FERGCN  [19] . To  assess stability, ten independent runs have been conducted, and a low standard deviation of ±0.27% has been observed, confirming the consistency of the model's performance. This demonstrates the model's ability to generalize well, even with limited training data. Table  1  further confirms that the performance remains consistent across different categories, with only minor variations. Performance comparisons have been conducted across all models on the same dataset, and statistical significance has been established through paired t-tests, yielding p < 0.01.\n\nIn contrast, while evaluating on the FERG dataset, which consists of synthetic or artificially-rendered characters, GLaRE achieves 94.24% accuracy which is slightly lower than FERGCN's 96.19%. This reduction is primarily due to the difficulty in extracting consistent facial landmarks from stylized characters. Unlike real human faces, cartoon faces often lack anatomically grounded features, making landmark-based graph construction noisy and less reliable. Since GLaRE relies on high-quality 3D landmark features to build expressive graphs, its performance is naturally impacted under such conditions.\n\nCNN-based models for FER often require millions of parameters, ranging from 2 to 18 million, which results in high computational demand. In contrast, graph-based approaches can achieve competitive results with far fewer parameters. The proposed GLaRE model contains only 44,424 parameters, highlights that GLaRE operates with significantly fewer parameters than established CNN-based methods while remaining more expressive than earlier graph-based baselines such as FERGCN  [19] . Such a lightweight design facilitates faster training, reduced memory consumption, and greater suitability for deployment in real-world scenarios.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "We have conducted ablation studies to evaluate the contribution of the quotient graph and the choice of region granularity in our model. In the first setting, we have removed the quotient graph and directly processed landmark-level graphs. This has resulted in a significant increase in computational time and a noticeable drop in recognition accuracy, highlighting the efficiency and effectiveness of the quotient graph representation. Furthermore, we have varied the number of facial regions when constructing the quotient graph. The results presented in the Table  3  and Fig.  6  have shown that both decreasing and increasing the number of regions beyond a certain point leads to performance degradation. The highest accuracy has been achieved when the number of regions is set to eight or nine, suggesting that this range provides an optimal balance between structural compression and information preservation. Additional ablation studies have been conducted to evaluate the contribution of different feature types. Two restricted variants of the input have been examined: (i) using only the 3D position vector of facial landmarks, and (ii) using only the appearance feature vector of size 16 extracted from local patches around the landmarks. As shown in Table  4 , both variants have resulted in significantly lower performance compared to the joint featurization, demonstrating that the positional and appearance cues are complementary in nature. The position-only model has achieved 33% accuracy, but it lacks the texture details necessary for reliable expression discrimination. Conversely, the appearance-only model has yielded only 56% accuracy, highlighting that geometric information plays a crucial role in stabilizing the representation. In contrast, the joint featurization has achieved the best performance, confirming the necessity of integrating both modalities.\n\nTable  4 : Ablation study on different feature choices for facial expression recognition. The results indicate that joint featurization of position and appearance vectors has provided a substantial improvement over using either feature alone.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Feature Type Accuracy (%)",
      "text": "Position vector only (3D) 56 Appearance vector only (16D) 33 Joint (Position + Appearance) 64.89",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion & Future Work",
      "text": "To conclude, GLaRE has been introduced as a hierarchical graph-based model that integrates fine-grained landmark information with high-level regional interactions for emotion recognition. The model has effectively captured both local geometry and global facial structure, while maintaining computational efficiency through graph coarsening.\n\nThe approach has achieved performance surpassing existing baselines on a balanced subset of AffectNet and has demonstrated strong generalization on FERG-DB, despite the challenges posed by stylized facial textures.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visual comparison of facial expression images and their corresponding 3D facial landmarks.",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates two example of faces",
      "page": 2
    },
    {
      "caption": "Figure 2: , the proposed GLaRE model has followed a two-level graph-based architecture that incorporates",
      "page": 3
    },
    {
      "caption": "Figure 2: Schematic diagram of GLaRE model for facial expression recognition.",
      "page": 4
    },
    {
      "caption": "Figure 3: The right hand graph is the quotient graph under the equivalence relation specified by the partition set",
      "page": 5
    },
    {
      "caption": "Figure 3: presents an example of how a quotient graph can be formed.",
      "page": 5
    },
    {
      "caption": "Figure 4: Model performance comparison on AffectNet and FERG datasets.",
      "page": 7
    },
    {
      "caption": "Figure 4: provides a visual",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion matrix of Emotion-wise accuracy on AffectNet Dataset.",
      "page": 8
    },
    {
      "caption": "Figure 6: have shown that both",
      "page": 9
    },
    {
      "caption": "Figure 6: Ablation study on the number of quotient graph regions. The accuracy improves as the number of regions",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotion-wise accuracy and precision (with standard deviation) of the GLaRE model on AffectNet Dataset",
      "page": 7
    },
    {
      "caption": "Table 1: summarizes the number of samples per class and their corresponding accuracies, while Fig. 4 provides a visual",
      "page": 7
    },
    {
      "caption": "Table 2: , the proposed GLaRE model achieves the best accuracy of 64.89% on this reduced AffectNet",
      "page": 7
    },
    {
      "caption": "Table 2: Accuracy (%) comparison of various models on AffectNet and FERG datasets. GLaRE outperforms baselines",
      "page": 8
    },
    {
      "caption": "Table 1: further confirms that the performance remains consistent across different categories,",
      "page": 8
    },
    {
      "caption": "Table 3: and Fig. 6 have shown that both",
      "page": 9
    },
    {
      "caption": "Table 3: Ablation study on the number of regions in the quotient graph. Accuracy (%) and inference time per batch",
      "page": 9
    },
    {
      "caption": "Table 4: Ablation study on different feature choices for facial expression recognition. The results indicate that joint",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "Modeling stylized character expressions via deep learning",
      "authors": [
        "D Aneja",
        "A Colburn",
        "G Faigin",
        "L Shapiro",
        "B Mones"
      ],
      "year": "2016",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Residual multi-task learning for facial landmark localization and expression recognition",
      "authors": [
        "B Chen",
        "W Guan",
        "P Li",
        "N Ikeda",
        "K Hirasawa",
        "H Lu"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Label distribution learning on auxiliary label space graphs for facial expression recognition",
      "authors": [
        "S Chen",
        "J Wang",
        "Y Chen",
        "Z Shi",
        "X Geng",
        "Y Rui"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Occlusion-adaptive deep network for robust facial expression recognition",
      "authors": [
        "H Ding",
        "P Zhou",
        "R Chellappa"
      ],
      "year": "2020",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "7",
      "title": "Computer vision for behaviour-based safety in construction: A review and future directions",
      "authors": [
        "W Fang",
        "P Love",
        "H Luo",
        "L Ding"
      ],
      "year": "2020",
      "venue": "Advanced Engineering Informatics"
    },
    {
      "citation_id": "8",
      "title": "Fast graph representation learning with pytorch geometric",
      "authors": [
        "M Fey",
        "J Lenssen"
      ],
      "year": "2019",
      "venue": "Fast graph representation learning with pytorch geometric",
      "arxiv": "arXiv:1903.02428"
    },
    {
      "citation_id": "9",
      "title": "Detecting emotional stress from facial expressions for driving safety",
      "authors": [
        "H Gao",
        "A Yüce",
        "J Thiran"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "10",
      "title": "Funqg: Molecular representation learning via quotient graphs",
      "authors": [
        "H Hajiabolhassan",
        "Z Taheri",
        "A Hojatnia",
        "Y Yeganeh"
      ],
      "year": "2023",
      "venue": "Journal of chemical information and modeling"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition",
      "authors": [
        "C Huang",
        "F Jiang",
        "Z Han",
        "X Huang",
        "S Wang",
        "Y Zhu",
        "Y Jiang",
        "B Hu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Deep learning for driver vigilance and road safety",
      "authors": [
        "P Kaushik",
        "K Yadav",
        "B Gopika",
        "N Kumar",
        "M Kaushik"
      ],
      "year": "2024",
      "venue": "2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "16",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "Semi-supervised classification with graph convolutional networks"
    },
    {
      "citation_id": "17",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C Kuo",
        "S Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "18",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "19",
      "title": "Fergcn: facial expression recognition based on graph convolution network",
      "authors": [
        "L Liao",
        "Y Zhu",
        "B Zheng",
        "X Jiang",
        "J Lin"
      ],
      "year": "2022",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "20",
      "title": "Deeply learning deformable facial action parts model for dynamic expression analysis",
      "authors": [
        "M Liu",
        "S Li",
        "S Shan",
        "R Wang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "21",
      "title": "A descriptive human visual cognitive strategy using graph neural network for facial expression recognition",
      "authors": [
        "S Liu",
        "S Huang",
        "W Fu",
        "J Lin"
      ],
      "year": "2024",
      "venue": "International Journal of Machine Learning and Cybernetics"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition in immersive virtual reality: From statistics to affective computing",
      "authors": [
        "J Marín-Morales",
        "C Llinares",
        "J Guixeres",
        "M Alcañiz"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Identity-aware convolutional neural network for facial expression recognition",
      "authors": [
        "Z Meng",
        "P Liu",
        "J Cai",
        "S Han",
        "Y Tong"
      ],
      "year": "2017",
      "venue": "12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "24",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Automated facial video-based recognition of depression and anxiety symptom severity: cross-corpus validation",
      "authors": [
        "A Pampouchidou",
        "M Pediaditis",
        "E Kazantzaki",
        "S Sfakianakis",
        "I Apostolaki",
        "K Argyraki",
        "D Manousos",
        "F Meriaudeau",
        "K Marias",
        "F Yang"
      ],
      "year": "2020",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "26",
      "title": "Iot-enabled facial recognition for smart hospitality for contactless guest services and identity verification",
      "authors": [
        "S Srinivasan",
        "R Raja",
        "C Jehan",
        "S Murugan",
        "C Srinivasan",
        "M Muthulekshmi"
      ],
      "year": "2024",
      "venue": "2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions"
    },
    {
      "citation_id": "27",
      "title": "Dynamic graph cnn for learning on point clouds",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Z Liu",
        "S Sarma",
        "M Bronstein",
        "J Solomon"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Graphics (tog)"
    },
    {
      "citation_id": "28",
      "title": "Cross-modal facial expression recognition with global channelspatial attention: Modal enhancement and proportional criterion fusion",
      "authors": [
        "J Yu",
        "Y Zheng",
        "L Wang",
        "Y Wang",
        "S Xu"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "29",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    }
  ]
}