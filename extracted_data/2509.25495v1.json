{
  "paper_id": "2509.25495v1",
  "title": "Emo-Tta: Improving Test-Time Adaptation Of Audio-Language Models For Speech Emotion Recognition",
  "published": "2025-09-29T20:52:01Z",
  "authors": [
    "Jiacheng Shi",
    "Hongfei Du",
    "Y. Alicia Hong",
    "Ye Gao"
  ],
  "keywords": [
    "speech emotion recognition",
    "audio-language models",
    "test-time adaptation",
    "out-of-distribution"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) with audio-language models (ALMs) remains vulnerable to distribution shifts at test time, leading to performance degradation in out-of-domain scenarios. Test-time adaptation (TTA) provides a promising solution but often relies on gradient-based updates or prompt tuning, limiting flexibility and practicality. We propose Emo-TTA, a lightweight, training-free adaptation framework that incrementally updates class-conditional statistics via an Expectation-Maximization procedure for explicit test-time distribution estimation, using ALM predictions as priors. Emo-TTA operates on individual test samples without modifying model weights. Experiments on six out-of-domain SER benchmarks show consistent accuracy improvements over prior TTA baselines, demonstrating the effectiveness of statistical adaptation in aligning model predictions with evolving test distributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) serves as a foundational component in applications such as conversational agents, contact center analytics, healthcare, and education. However, real-world deployment often faces significant distributional shifts due to variations in speakers, languages, devices, ambient noise, or even inconsistent label taxonomies. As a result, models that perform well in-domain often degrade substantially on out-of-domain (OOD) test scenarios  [1] .\n\nTest-time adaptation (TTA)  [2]  adapts models during inference using only unlabeled target data, without source access. Early methods tune class-specific prompts with a few target labels for alignment  [3, 4] . Later approaches remove labels but rely on gradientbased updates to learn prompts or adapters from unlabeled test batches  [5, 6] . Recent training-free methods eliminate all optimization by using heuristics or retrieval to improve robustness  [7, 8] . However, these methods still assume access to a batch of samples and introduce computational overhead during inference. While effective, most require batch-mode access and tuning at inference, limiting their use in practical applications.\n\nWhile test-time adaptation has recently gained traction in speech, particularly for ASR  [9, 10] , via unsupervised objectives, pseudo-labeling, and low-rank parameter tuning, its extension to SER remains relatively limited. Compared to ASR, SER exhibits greater speaker variability, emotional ambiguity, and weaker linguistic structure, posing unique adaptation challenges. Compounding this, many existing TTA methods rely on test batches  [9] , model updates  [10] , or prompt tuning  [3] , making them unsuitable when access to training sources or buffered inputs is severely restricted. This highlights the growing need for lightweight, training-free adaptation approaches that avoid parameter updates.\n\nTo address these challenges, we propose Emo-TTA, a trainingfree test-time adaptation method that integrates audio language models (ALMs)  [11, 12]  with a test time Expectation-Maximization (EM)  [13]  procedure. Emo-TTA is designed to meet three key requirements: (i) Test-time distribution estimation, by maintaining class-conditional statistics over the incoming test samples to inform predictions; (ii) Lightweight adaptation, by avoiding access to source data, model updates, or multi-sample buffering; and (iii) Training-free inference, without prompt tuning or retraining. Specifically, we model each class-conditional density p(x | y = i) âˆ¼ N (Âµi, Î£i) with prior Ï€i, and predict using Bayes rule: Å· = arg maxi p(x | y = i) p(y = i). Initialization sets Âµi from ALM-derived text prompts and uses simple priors Ï€i. At each time step t, the E-step computes soft assignments Î³t,i âˆ Ï€i N (xt | Âµi, Î£i), and the M-step incrementally updates {Âµi, Î£i, Ï€i} from {Î³t,i, xt}. This yields continuous, single-sample adaptation without modifying the model or storing past test data.\n\nIn summary, our main contributions are: (i) To the best of our knowledge, we are the first to simultaneously satisfy the three core requirements of TTA in ALMs, namely Test-time distribution estimation, lightweight adaptation, and training-free inference, thereby enabling efficient and practical adaptation for SER. (ii) We introduce Emo-TTA, a EM-based method that incrementally updates class-conditional statistics based on ALM-derived textual prototypes, enabling continuous adaptation without accessing source data, modifying model weights, or storing test samples. (iii) Extensive experiments on six out-of-domain SER benchmarks show that Emo-TTA consistently improves performance, highlighting its robustness across acoustic domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminaries",
      "text": "Audio-Language Models (ALMs) such as CLAP achieve strong zero-shot performance by aligning acoustic and textual modalities. In SER, CLAP classifies utterances by computing the similarity between audio features and text prompts describing each emotion:\n\nwhere f (â€¢) and g(",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Frozen Params",
      "text": "Fig.  1 . Overview of Emo-TTA for test-time adaptation in SER. Given a test audio, frozen CLAP encoders extract audio and class prototypes, which initialize a EM-based that continuously updates Gaussian parameters via entropy-weighted confidence. Final prediction fuses CLAP's zero-shot logits with generative scores for stable, training-free adaptation under distribution shift.\n\ndomain shifts (e.g., speaker, channel, or noise). While prior methods address this via fine-tuning on labeled target-domain data, they incur high adaptation costs. In contrast, our training-free test-time approach enables lightweight adaptation, improving practical feasibility under real-world constraints.\n\n2.2. Test-Time Adaptation (TTA) for ALMs.\n\nIn the TTA setting, an audio-language model pre-trained on a source domain is adapted to a target speech emotion recognition (SER) domain using only unlabeled test audio Dtest = {at}, with predictions made independently at inference. To improve CLAP's performance under distribution shift, prior TTA methods aggregate predictions over multiple audio augmentations via entropy-based filtering:\n\nwhere Am(at) is the m-th augmented view of utterance at, Ï is the selected proportion of low-entropy views, and H(â€¢) denotes entropy over K emotion classes. Despite promising results, existing approaches face limitations: (1) they neglect modeling the underlying emotion distribution, overlooking the structure among test utterances; (2) they rely on access to multiple samples or memory caches, incompatible with privacysensitive settings; and (3) they require test-time optimization, which introduces computational overhead and limits their applicability in settings without model updates or labeled data in SER task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Gaussian Discriminant Analysis",
      "text": "Assume CLAP audio embeddings for each emotion class y follow a multivariate normal distribution with class-specific means and shared covariance. Let Ft = f (at) âˆˆ R d be the CLAP audio embedding for the t-th utterance, with Âµy denoting the mean embedding of class y, and Î£ the shared covariance matrix. The class-conditional likelihood is\n\nwhere d is the embedding dimension. This likelihood captures the Mahalanobis distance between Ft and Âµy, adjusted for feature correlation and scale. Applying Bayes' rule yields the posterior:\n\nand substituting Eq. (  3 ) leads to the softmax form:\n\nFor the t-th test utterance sample, the final prediction corresponds to the class with highest posterior probability:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Em-Based Algorithm For Tta",
      "text": "Although GDA  [14]  captures the test time distribution, it typically requires labeled data for reliable parameter estimation. In TTA for SER, however, test instances are unlabeled and arrive sequentially, making direct use of GDA impractical. To address this, we introduce a EM-based algorithm leveraging CLAP's zero-shot predictions as soft priors to iteratively estimate posteriors and update parameters on-the-fly.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Parameter Initialization",
      "text": "At initialization, the CLAP text encoder g(â€¢) generates a semantic prototype g(ty) for each emotion class y, which we adopt as the initial class mean, i.e., Âµy = g(ty). Assuming the embeddings are i.i.d. with unit variance, we simply set the shared covariance matrix to the identity, i.e., Î£ = I.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E-Step: Posterior Inference",
      "text": "Given the CLAP audio embedding Ft = f (at) for the t-th test sample, we compute its posterior responsibility Î³y,t = P (zy = 1 | Ft) under the Gaussian assumption. Specifically,\n\nwhere zy indicates class membership and Ï€y is the class prior.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "M-Step: Parameter Updates",
      "text": "In the M-step, we update the class prior Ï€y, class mean Âµy, and shared covariance Î£ in the CLAP embedding space based on the posterior responsibility Î³y,t of test instance t. Let Ft = f (at) denote the CLAP audio embedding of the t-th test sample. Denote nt as the cumulative number of test samples observed so far, and initialize the effective class count Ny to 1/K. All parameter updates are computed jointly as follows:\n\nNy, Âµy + Î³y,t, Ft Ny + Î³y,t ,\n\nWe update the effective count as N â€² y = Ny + Î³y,t. This EM-based procedure adapts class statistics incrementally without supervision or gradient descent, enabling efficient TTA under distribution shift.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Incorporating Alm Priors",
      "text": "To stabilize early-stage TTA for SER, we incorporate CLAP priors by integrating zero-shot predictions and their confidence into the update process. Early predictions may deviate from emotion semantics due to distribution shift. To mitigate this, we estimate each utterance's uncertainty via the entropy of CLAP's predicted distribution and modulate its influence accordingly. Specifically, for the t-th test utterance at, we compute its self-entropy from CLAP's predicted probabilities , PCLAP(zy = 1 | at) | y = 1, . . . , K,. The entropy is\n\nWe define the confidence-based weight w(h) = e -Î²h (with Î² > 0) and revise the EM updates as:\n\nwhere Ft = f (at) is the CLAP audio embedding and Î³y,t is the posterior responsibility. After incorporating at, the counts are updated as N â€² y = Ny + w H(at) â€¢ Î³y,t and n â€² t = n â€² t-1 + w H(at) . This entropy-aware mechanism reduces the impact of uncertain utterances and mitigates noise-induced drift. Finally, we combine CLAP's zero-shot logits with those from the generative model:\n\nwhere F = f (at) is the audio feature, Ty = g(ty) is the text prototype, Î± is a tunable coefficient, and wy = Î£ -1 Âµy, by = log P (y) -1 2 Âµ âŠ¤ y Î£ -1 Âµy from the generative model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets And Implementation Details",
      "text": "Datasets. We study speech emotion recognition (SER) in a crosscorpus out-of-domain (OOD) setting. IEMOCAP  [19] , a dyadic scripted/improvised dialogue dataset with 10 speakers, follows a 5-fold leave-one-session-out cross-validation setup, merging excited into happy where applicable with a unified 4-class label space (angry, happy, sad, neutral) following  [20] . MELD  [21] , derived from multi-party dialogues in Friends, is evaluated using its official train/dev/test split, with emotions mapped to the unified 4-class scheme. RAVDESS  [22]  contains 2.5k studio-quality utterances averaging 5s, and we retain all original 8 emotion classes using a 5fold stratified cross-validation protocol. TESS  [23]  and SAVEE  [24] , both acted emotion datasets with consistent recording conditions, are evaluated using random 10-fold cross-validation with 4-class mapping. CREMA-D  [25] , containing 7k utterances averaging 4.5s, uses a 5-fold CV setup and retains its original 6 emotion classes, following  [1] . This diverse selection enables a rigorous evaluation of model generalization across varying speaker identities, acoustic conditions, and emotional taxonomies. Implementation Details.We resample all audio to 16 kHz and standardize input length to 5 seconds via truncation or zero-padding. Each clip is paired with a text label in the form \"This is a [EMO-TION] sound\" to ensure cross-modal alignment. We use CLAP with PANN-14 or HTS-AT as the audio encoder and RoBERTa as the text encoder. CoOp, CoCoOp, and Treff Adapter are implemented following prior work  [6, 26] , while TPT  [5] , MTA  [7] , and ZERO  [8]  are adapted by replacing original encoders with CLAP encoders following by  [12] . For Emo-TTA, we set Î±=0.2, Î²=4.5, and follow a test time emotional TTA setup with batch size 1 and no backpropagation. We report the mean top-1 classification accuracy, averaged over three runs with random seeds.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Main Results",
      "text": "Comparison with prior TTA methods. Table  1  reports out-ofdomain SER performance across six datasets under two CLAP backbones: PANN-14  [27]  and HTS-AT  [28] . CoOp  [3]  and CoCoOp  [4]  are few-shot prompt-learning methods requiring labeled target data. Treff-Adapter  [6]  and TPT  [5]  perform gradient-based test-time tuning, while MTA  [7]  and ZERO  [8]  are training-free, source-free baselines that do not explicitly model the test distribution. In contrast, Emo-TTA conducts sample-wise, optimization-free adaptation through a EM-based procedure that explicitly estimates the test-time distribution. Under CLAP-PANN- Update of Covariance. To assess the impact of covariance adaptation, we conduct an ablation study where the covariance matrix Î£ is fixed as an identity matrix during evaluation (Tab. 3). This simplification reduces the scoring function to Euclidean distance, constraining the model's capacity to capture intra-class variability. As a result, the performance deteriorates under distribution shift. In contrast, updating Î£ dynamically allows the model to more accurately represent the dispersion and correlation patterns of class features, leading to improved test-time recognition accuracy. ALM Priors. To evaluate the influence of audio-language priors and confidence-aware updates, we initialize class prototypes with CLAP-derived text embeddings and modulate their updates using prediction confidence measured via self-entropy (Tab. 3). Without prior-based initialization, the model suffers from poor alignment between prototypes and emotion classes, leading to performance degradation. In addition, disabling confidence weighting causes uncertain predictions to skew class statistics, reducing stability. Incorporating both components enables the model to adapt more robustly under distribution shifts by preserving semantic grounding and filtering noisy updates.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce Emo-TTA, a training-free adaptation method that improves the robustness of ALMs for speech emotion recognition under distribution shifts. By modeling test-time distributions through lightweight EM updates and using ALM-based predictions as uncertainty-aware priors, Emo-TTA incrementally refines class statistics without accessing source data or updating model weights. Unlike prior TTA approaches, it operates on individual samples without prompt tuning or batched inputs. Experiments across six out-of-domain benchmarks confirm consistent performance gains, validating its effectiveness for practical emotion understanding.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of Emo-TTA for test-time adaptation in SER. Given a test audio, frozen CLAP encoders extract audio and class prototypes,",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "hdu02,\n{jshi12,\nygao18}@wm.edu,"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ABSTRACT"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "Speech\nemotion\nrecognition\n(SER) with\naudio-language models"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "(ALMs) remains vulnerable to distribution shifts at test time, leading"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "to performance degradation in out-of-domain scenarios.\nTest-time"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "adaptation (TTA) provides a promising solution but often relies on"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "gradient-based updates or prompt\ntuning,\nlimiting flexibility and"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "practicality.\nWe propose Emo-TTA,\na\nlightweight,\ntraining-free"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "adaptation framework that\nincrementally updates class-conditional"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "statistics via\nan Expectation-Maximization procedure\nfor\nexplicit"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "test-time distribution estimation, using ALM predictions as priors."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "Emo-TTA operates on individual\ntest\nsamples without modifying"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "model weights. Experiments on six out-of-domain SER benchmarks"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "show consistent accuracy improvements over prior TTA baselines,"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "demonstrating the effectiveness of statistical adaptation in aligning"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "model predictions with evolving test distributions."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "Index Termsâ€” speech emotion recognition,\naudio-language"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "models, test-time adaptation, out-of-distribution."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "1.\nINTRODUCTION"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "Speech emotion recognition (SER) serves as a foundational com-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ponent\nin applications such as conversational agents, contact center"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "analytics, healthcare, and education. However,\nreal-world deploy-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ment often faces significant distributional shifts due to variations in"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "speakers, languages, devices, ambient noise, or even inconsistent la-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "bel taxonomies. As a result, models that perform well in-domain of-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ten degrade substantially on out-of-domain (OOD) test scenarios [1]."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "Test-time adaptation (TTA) [2] adapts models during inference"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "using only unlabeled target data, without source access. Early meth-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ods tune class-specific prompts with a few target\nlabels for align-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "ment\n[3, 4].\nLater approaches remove labels but\nrely on gradient-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "based updates\nto learn prompts or\nadapters\nfrom unlabeled test"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "batches [5, 6]. Recent\ntraining-free methods eliminate all optimiza-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "tion by using heuristics or\nretrieval\nto improve robustness\n[7, 8]."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "However,\nthese methods still assume access to a batch of samples"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "and introduce computational overhead during inference. While ef-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "fective, most\nrequire batch-mode access and tuning at\ninference,"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "limiting their use in practical applications."
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "While\ntest-time\nadaptation\nhas\nrecently\ngained\ntraction\nin"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "speech, particularly for ASR [9, 10], via unsupervised objectives,"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "pseudo-labeling,\nand low-rank parameter\ntuning,\nits extension to"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": ""
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "SER remains relatively limited. Compared to ASR, SER exhibits"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "greater speaker variability, emotional ambiguity, and weaker linguis-"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "tic structure, posing unique adaptation challenges.\nCompounding"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "this, many existing TTA methods\nrely on test batches\n[9], model"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "updates [10], or prompt\ntuning [3], making them unsuitable when"
        },
        {
          "â‹† College of William & Mary, â€¡ George Mason University": "access to training sources or buffered inputs is severely restricted."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ð’©happy": "ð’©neutral"
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "ð’©neutral"
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "where d is the embedding dimension. This likelihood captures the"
        },
        {
          "ð’©happy": "Mahalanobis distance between Ft and Âµy, adjusted for feature cor-"
        },
        {
          "ð’©happy": "relation and scale. Applying Bayesâ€™ rule yields the posterior:"
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "| y)\nP (y) p(Ft"
        },
        {
          "ð’©happy": ",\n(4)\nP (y | Ft) ="
        },
        {
          "ð’©happy": "(cid:80)"
        },
        {
          "ð’©happy": "yâ€² P (yâ€²) p(Ft"
        },
        {
          "ð’©happy": "and substituting Eq. (3) leads to the softmax form:"
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "exp(cid:0)âˆ’ 1\n2 (Ft âˆ’ Âµy)âŠ¤Î£âˆ’1(Ft âˆ’ Âµy)(cid:1)"
        },
        {
          "ð’©happy": "(5)\nP (y | Ft) ="
        },
        {
          "ð’©happy": "(cid:80)\nyâ€² exp(cid:0)âˆ’ 1\n2 (Ft âˆ’ Âµyâ€² )âŠ¤Î£âˆ’1(Ft âˆ’ Âµyâ€² )(cid:1) ."
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "For the t-th test utterance sample, the final prediction corresponds to"
        },
        {
          "ð’©happy": "the class with highest posterior probability:"
        },
        {
          "ð’©happy": ""
        },
        {
          "ð’©happy": "(cid:16)\n(cid:17)"
        },
        {
          "ð’©happy": "log P (y) âˆ’ 1\n2 (Ft âˆ’ Âµy)âŠ¤Î£âˆ’1(Ft âˆ’ Âµy)"
        },
        {
          "ð’©happy": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: reports out-of-",
      "data": [
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "3.1. Datasets and Implementation Details"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "2.4.3. M-Step: Parameter Updates",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Datasets. We study speech emotion recognition (SER) in a cross-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "In the M-step, we update the class prior Ï€y, class mean Âµy, and",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "corpus out-of-domain (OOD)\nsetting.\nIEMOCAP [19],\na dyadic"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "shared covariance Î£ in the CLAP embedding space based on the",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "scripted/improvised dialogue dataset with 10 speakers,\nfollows a"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "posterior responsibility Î³y,t of test\ninstance t. Let Ft = f (at) de-",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "5-fold\nleave-one-session-out\ncross-validation\nsetup, merging\nex-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "note the CLAP audio embedding of the t-th test sample. Denote nt",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "cited into happy where applicable with a unified 4-class label space"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "as the cumulative number of test samples observed so far, and ini-",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "(angry, happy,\nsad, neutral)\nfollowing [20]. MELD [21], derived"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "tialize the effective class count Ny to 1/K. All parameter updates",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "from multi-party dialogues in Friends,\nis evaluated using its official"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "are computed jointly as follows:",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "train/dev/test\nsplit, with emotions mapped to the unified 4-class"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny + Î³y,t",
          "3. EXPERIMENTS": "scheme.\nRAVDESS [22]\ncontains 2.5k studio-quality utterances"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ï€â€²\n,\ny =",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "nt",
          "3. EXPERIMENTS": "averaging 5s, and we retain all original 8 emotion classes using a 5-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny, Âµy + Î³y,t, Ft",
          "3. EXPERIMENTS": "fold stratified cross-validation protocol. TESS [23] and SAVEE [24],"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Âµâ€²\n,\ny =",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "(9)",
          "3. EXPERIMENTS": "both acted emotion datasets with consistent\nrecording conditions,"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny + Î³y,t",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "are evaluated using random 10-fold cross-validation with 4-class"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "(ntâˆ’1)Î£ + (cid:80)\ny)(Ftâˆ’Âµâ€²\ny)âŠ¤\ny Î³y,t(Ftâˆ’Âµâ€²",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "mapping. CREMA-D [25], containing 7k utterances averaging 4.5s,"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Î£â€² =",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "ntâˆ’1",
          "3. EXPERIMENTS": "uses a 5-fold CV setup and retains its original 6 emotion classes,"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "following [1]. This diverse selection enables a rigorous evaluation"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "We update the effective count as N â€²\ny = Ny + Î³y,t. This EM-based",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "of model generalization across varying speaker\nidentities, acoustic"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "procedure adapts class statistics incrementally without supervision",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "conditions, and emotional taxonomies."
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "or gradient descent, enabling efficient TTA under distribution shift.",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Implementation Details.We resample all audio to 16 kHz and stan-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "dardize input\nlength to 5 seconds via truncation or zero-padding."
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "2.5.\nIncorporating ALM Priors",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Each clip is paired with a text\nlabel\nin the form â€œThis is a [EMO-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "TION] soundâ€ to ensure cross-modal alignment. We use CLAP with"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "To stabilize early-stage TTA for SER, we incorporate CLAP priors",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "PANN-14 or HTS-AT as the audio encoder and RoBERTa as the text"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "by integrating zero-shot predictions and their confidence into the up-",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "encoder. CoOp, CoCoOp, and Treff Adapter are implemented fol-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "date process. Early predictions may deviate from emotion semantics",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "lowing prior work [6, 26], while TPT [5], MTA [7], and ZERO [8]"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "due to distribution shift.\nTo mitigate this, we estimate each utter-",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "are adapted by replacing original encoders with CLAP encoders fol-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "anceâ€™s uncertainty via the entropy of CLAPâ€™s predicted distribution",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "lowing by [12]. For Emo-TTA, we set Î±=0.2, Î²=4.5, and follow a"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "and modulate its influence accordingly. Specifically, for the t-th test",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "test\ntime emotional TTA setup with batch size 1 and no backpropa-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "utterance at, we compute its self-entropy from CLAPâ€™s predicted",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "gation. We report\nthe mean top-1 classification accuracy, averaged"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "probabilities , PCLAP(zy = 1 | at) | y = 1, . . . , K,. The entropy is",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "over three runs with random seeds."
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "K(cid:88) y\nH(at) = âˆ’\nPCLAP(zy = 1 | at) log PCLAP(zy = 1 | at).",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "3.2. Main Results"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "=1",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "(10)",
          "3. EXPERIMENTS": "Comparison with prior TTA methods.\nTable 1 reports out-of-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "We define the confidence-based weight w(h) = eâˆ’Î²h (with Î² > 0)",
          "3. EXPERIMENTS": "domain SER performance across six datasets under two CLAP back-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "and revise the EM updates as:",
          "3. EXPERIMENTS": "bones: PANN-14 [27] and HTS-AT [28]. CoOp [3] and CoCoOp [4]"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "are few-shot prompt-learning methods requiring labeled target data."
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny + w(cid:0)H(at)(cid:1) Â· Î³y,t",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Treff-Adapter\n[6]\nand TPT [5] perform gradient-based test-time"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": ",\nÏ€â€²\ny =",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "nâ€²\nt + w(cid:0)H(at)(cid:1)",
          "3. EXPERIMENTS": "tuning, while MTA [7] and ZERO [8] are training-free, source-free"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "baselines that do not explicitly model\nthe test distribution.\nIn con-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny Â· Âµy + w(cid:0)H(at)(cid:1) Â· Î³y,t Â· Ft",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "trast, Emo-TTA conducts sample-wise, optimization-free adaptation"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Âµâ€²\n,\ny =",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Ny + w(cid:0)H(at)(cid:1) Â· Î³y,t",
          "3. EXPERIMENTS": "through a EM-based procedure that explicitly estimates the test-time"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "distribution. Under CLAP-PANN-14, Emo-TTA achieves the best"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "average accuracy of 38.02%, outperforming Treff-Adapter\n(36.11)"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "K(cid:88) y\n(nâ€²\nÎ³y,t\n(cid:0)Ft âˆ’ Âµâ€²\n(cid:1)(cid:0)Ft âˆ’ Âµâ€²\nt âˆ’ 1)Î£ + w(cid:0)H(at)(cid:1)\ny\ny",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "by +1.91 and the zero-shot CLAP baseline (31.37) by +6.65, and"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "=1",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "Î£â€² =\n,",
          "3. EXPERIMENTS": "ranks first on 5 of 6 datasets\n(except CREMA-D). Under CLAP-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "nâ€²\nt âˆ’ 1",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "HTSAT, Emo-TTA again\nleads with\n40.47%,\nsurpassing Treff-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "(11)",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Adapter (37.10), MTA (36.04), and ZERO (35.69), and achieves the"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "is the\nwhere Ft = f (at) is the CLAP audio embedding and Î³y,t",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "top score on 5 of 6 datasets (TESS led by MTA). Overall, Emo-TTA"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "posterior\nresponsibility. After\nthe counts are up-\nincorporating at,",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "delivers the best results on 10 out of 12 dataset/backbone combina-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "dated as N â€²\ny = Ny + w(cid:0)H(at)(cid:1) Â· Î³y,t and nâ€²\nt = nâ€²\ntâˆ’1 + w(cid:0)H(at)(cid:1).",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "tions, consistently outperforming both prompt-learning and test-time"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "This\nentropy-aware mechanism reduces\nthe\nimpact\nof\nuncertain",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "adaptation baselines in the out-of-domain setting. These gains stem"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "utterances and mitigates noise-induced drift.\nFinally, we combine",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "from its\ncontinuous EM procedure, which incrementally updates"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "CLAPâ€™s zero-shot logits with those from the generative model:",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "class statistics and calibrates predictions without training or prompt"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "(cid:1),\n(12)\ny F + Î±(cid:0)wâŠ¤\ny F + by\nlogitsy = T âŠ¤",
          "3. EXPERIMENTS": "tuning, enabling robust test-time adaptation under distribution shifts."
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "",
          "3. EXPERIMENTS": "Comparison with Foundation Audio Models. We evaluate Emo-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "is\nis\nthe text\nwhere F = f (at)\nthe audio feature, Ty = g(ty)",
          "3. EXPERIMENTS": "TTA under\na\nstrict\nout-of-domain\nsetting\nagainst\nrecent\naudio-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "prototype, Î± is a tunable coefficient,\nand wy = Î£âˆ’1Âµy,\nby =",
          "3. EXPERIMENTS": "language models. Pengi formulates audio understanding as prompt-"
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "log P (y) âˆ’ 1",
          "3. EXPERIMENTS": ""
        },
        {
          "where zy indicates class membership and Ï€y is the class prior.": "2 ÂµâŠ¤",
          "3. EXPERIMENTS": "based text generation using transfer\nlearning. Whisper\nis a mul-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Out-of-domain performance of different models across",
      "data": [
        {
          "Method": "CLAP-PANN-14 [11]",
          "T.F.": "-",
          "L.W.": "-",
          "Est.": "-",
          "IEMOCAP": "34.52",
          "MELD": "17.11",
          "RAVDESS": "18.91",
          "TESS": "49.76",
          "SAVEE": "38.38",
          "CREMA-D": "29.54",
          "AVG": "31.37"
        },
        {
          "Method": "CoOp [3]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "33.83",
          "MELD": "16.47",
          "RAVDESS": "19.03",
          "TESS": "49.73",
          "SAVEE": "37.66",
          "CREMA-D": "33.55",
          "AVG": "31.71"
        },
        {
          "Method": "CoCoOp [4]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "33.72",
          "MELD": "17.28",
          "RAVDESS": "22.57",
          "TESS": "50.71",
          "SAVEE": "38.93",
          "CREMA-D": "36.36",
          "AVG": "33.26"
        },
        {
          "Method": "Treff-Adapter [6]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "35.86",
          "MELD": "18.85",
          "RAVDESS": "26.45",
          "TESS": "52.84",
          "SAVEE": "42.03",
          "CREMA-D": "40.59",
          "AVG": "36.11"
        },
        {
          "Method": "TPT [5]",
          "T.F.": "âœ—",
          "L.W.": "âœ“",
          "Est.": "âœ—",
          "IEMOCAP": "35.58",
          "MELD": "17.77",
          "RAVDESS": "25.53",
          "TESS": "50.34",
          "SAVEE": "39.95",
          "CREMA-D": "35.03",
          "AVG": "34.03"
        },
        {
          "Method": "Ours",
          "T.F.": "âœ“",
          "L.W.": "âœ“",
          "Est.": "âœ“",
          "IEMOCAP": "39.92",
          "MELD": "19.91",
          "RAVDESS": "29.54",
          "TESS": "54.54",
          "SAVEE": "44.76",
          "CREMA-D": "39.44",
          "AVG": "38.02"
        },
        {
          "Method": "CLAP-HTSAT [11]",
          "T.F.": "-",
          "L.W.": "-",
          "Est.": "-",
          "IEMOCAP": "36.35",
          "MELD": "18.62",
          "RAVDESS": "19.86",
          "TESS": "50.31",
          "SAVEE": "39.25",
          "CREMA-D": "31.07",
          "AVG": "32.57"
        },
        {
          "Method": "CoOp [3]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "34.77",
          "MELD": "17.73",
          "RAVDESS": "19.14",
          "TESS": "50.63",
          "SAVEE": "37.67",
          "CREMA-D": "33.12",
          "AVG": "32.17"
        },
        {
          "Method": "CoCoOp [4]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "35.42",
          "MELD": "18.57",
          "RAVDESS": "23.93",
          "TESS": "50.92",
          "SAVEE": "39.13",
          "CREMA-D": "36.74",
          "AVG": "34.18"
        },
        {
          "Method": "Treff-Adapter [6]",
          "T.F.": "âœ—",
          "L.W.": "âœ—",
          "Est.": "âœ—",
          "IEMOCAP": "38.13",
          "MELD": "19.61",
          "RAVDESS": "27.04",
          "TESS": "53.91",
          "SAVEE": "42.32",
          "CREMA-D": "41.61",
          "AVG": "37.10"
        },
        {
          "Method": "TPT [5]",
          "T.F.": "âœ—",
          "L.W.": "âœ“",
          "Est.": "âœ—",
          "IEMOCAP": "36.80",
          "MELD": "18.89",
          "RAVDESS": "26.63",
          "TESS": "50.58",
          "SAVEE": "40.76",
          "CREMA-D": "36.05",
          "AVG": "34.96"
        },
        {
          "Method": "MTA [7]",
          "T.F.": "âœ“",
          "L.W.": "âœ“",
          "Est.": "âœ—",
          "IEMOCAP": "38.92",
          "MELD": "18.93",
          "RAVDESS": "25.74",
          "TESS": "56.75",
          "SAVEE": "40.96",
          "CREMA-D": "34.94",
          "AVG": "36.04"
        },
        {
          "Method": "ZERO [8]",
          "T.F.": "âœ“",
          "L.W.": "âœ“",
          "Est.": "âœ—",
          "IEMOCAP": "37.85",
          "MELD": "18.50",
          "RAVDESS": "26.02",
          "TESS": "55.47",
          "SAVEE": "41.44",
          "CREMA-D": "34.88",
          "AVG": "35.69"
        },
        {
          "Method": "Ours",
          "T.F.": "âœ“",
          "L.W.": "âœ“",
          "Est.": "âœ“",
          "IEMOCAP": "43.65",
          "MELD": "20.17",
          "RAVDESS": "31.72",
          "TESS": "56.09",
          "SAVEE": "46.39",
          "CREMA-D": "44.78",
          "AVG": "40.47"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Out-of-domain performance of different models across",
      "data": [
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "three datasets, where the target dataset\nis fully excluded from both"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "training and any form of unsupervised test-time adaptation."
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "Model\nIEMOC\nCREMA\nRAVD\nAVG"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "Emo-TTA\n43.65\n44.78\n31.72\n40.05"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "Zero-shot CLAP [11]\n36.35\n31.07\n19.86\n29.09"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "w/out Mean Vectors Update\n42.33\n43.37\n29.51\n38.40"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "w/out Covariance Matrix Update\n37.94\n37.39\n25.58\n33.64"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": "w/out ALM priors\n38.69\n38.15\n26.67\n34.51"
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        },
        {
          "Table 2.\nOut-of-domain performance of different models across": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Out-of-domain performance of different models across",
      "data": [
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "(L.W.), and test-stream distribution estimation (Est.). Bold indicates the highest accuracy per dataset."
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Model\nIEMOC"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "CLAP [11]\n36.35"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Pengi [15]\n35.63"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "MMS large [16]\n36.89"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Whisper medium.en [16]\n37.90"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Whisper medium [16]\n36.81"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Whisper large-v2 [16]\n38.10"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "AudioFlamingo [17]\n-"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "EmoCLAP [18]\n-"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "SELM [1]\n40.02"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Emo-TTA\n43.65"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Table 2."
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "three datasets, where the target dataset"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "training and any form of unsupervised test-time adaptation."
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Model\nIEMOC"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Emo-TTA\n43.65"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Zero-shot CLAP [11]\n36.35"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "w/out Mean Vectors Update\n42.33"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "w/out Covariance Matrix Update\n37.94"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "w/out ALM priors\n38.69"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "Table 3. Ablation study demonstrates the impact of each Emo-TTA"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "component on SER accuracy (%)."
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "tilingual\nencoder-decoder\ntrained\non"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "AudioFlamingo integrates audio into LLMs with in-context"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "ing capabilities.\nSELM aligns audio and text"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "language model for emotion prediction. Following prior work [1],"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "we\nadopt\na\n4-class\nand\n6-class\nemotion"
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": ""
        },
        {
          "Table 1. Comparison of out-of-domain SER accuracy (%) across datasets. Methods are grouped by training-free (T.F.), lightweight adaptation": "IEMOCAP and CREMA-D, and an 8-class"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "ing Wang, â€œPengi: An audio language model for audio tasks,â€"
        },
        {
          "6. REFERENCES": "[1] Hazim Bukhari,\nSoham Deshmukh, Hira Dhamyal, Bhik-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Advances in Neural Information Processing Systems, vol. 36,"
        },
        {
          "6. REFERENCES": "sha Raj,\nand Rita Singh,\nâ€œSelm:\nEnhancing speech emo-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "pp. 18090â€“18108, 2023."
        },
        {
          "6. REFERENCES": "arXiv preprint\ntion recognition for out-of-domain scenarios,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[16] Rao Ma, Adian Liusie, Mark JF Gales,\nand Kate M Knill,"
        },
        {
          "6. REFERENCES": "arXiv:2407.15300, 2024.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "â€œInvestigating the emergent audio classification ability of asr"
        },
        {
          "6. REFERENCES": "[2]\nJameel Abdul Samadh, Mohammad Hanan Gani, Noor Hus-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "foundation models,â€ arXiv preprint arXiv:2311.09363, 2023."
        },
        {
          "6. REFERENCES": "sein, Muhammad Uzair Khattak, Muhammad Muzammal",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[17] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael"
        },
        {
          "6. REFERENCES": "Naseer, Fahad Shahbaz Khan, and Salman H Khan,\nâ€œAlign",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Valle, and Bryan Catanzaro,\nâ€œAudio flamingo: A novel audio"
        },
        {
          "6. REFERENCES": "your prompts: Test-time prompting with distribution alignment",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "language model with few-shot learning and dialogue abilities,â€"
        },
        {
          "6. REFERENCES": "for zero-shot generalization,â€ Advances in Neural Information",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "arXiv preprint arXiv:2402.01831, 2024."
        },
        {
          "6. REFERENCES": "Processing Systems, vol. 36, pp. 80396â€“80413, 2023.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[18] Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huam-"
        },
        {
          "6. REFERENCES": "[3] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "ing Wang, Bhiksha Raj, and Rita Singh, â€œDescribing emotions"
        },
        {
          "6. REFERENCES": "In-\nLiu,\nâ€œLearning to prompt\nfor vision-language models,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "with acoustic property prompts\nfor\nspeech emotion recogni-"
        },
        {
          "6. REFERENCES": "ternational Journal of Computer Vision, vol. 130, no. 9, pp.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "tion,â€ arXiv preprint arXiv:2211.07737, 2022."
        },
        {
          "6. REFERENCES": "2337â€“2348, 2022.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[19] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "6. REFERENCES": "[4] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "6. REFERENCES": "Liu,\nâ€œConditional prompt\nlearning for vision-language mod-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Chang, Sungbok Lee,\nand Shrikanth S Narayanan,\nâ€œIemo-"
        },
        {
          "6. REFERENCES": "els,â€ in Proceedings of the IEEE/CVF conference on computer",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "cap:\nInteractive emotional dyadic motion capture database,â€"
        },
        {
          "6. REFERENCES": "vision and pattern recognition, 2022, pp. 16816â€“16825.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Language resources and evaluation, pp. 335â€“359, 2008."
        },
        {
          "6. REFERENCES": "[5] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Gold-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[20]\nJiacheng Shi, Yanfu Zhang, and Ye Gao, â€œClep-dg: Contrastive"
        },
        {
          "6. REFERENCES": "stein, Anima Anandkumar,\nand Chaowei Xiao,\nâ€œTest-time",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "learning for\nspeech emotion domain generalization via\nsoft"
        },
        {
          "6. REFERENCES": "prompt\ntuning for zero-shot generalization in vision-language",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "prompt\ntuning,â€\nin Proc.\nInterspeech 2025, 2025, pp. 4498â€“"
        },
        {
          "6. REFERENCES": "models,â€ Neural Information Processing Systems, 2022.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "4502."
        },
        {
          "6. REFERENCES": "[6]\nJinhua Liang, Xubo Liu, Haohe Liu, Huy Phan, Emmanouil",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[21]\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder,"
        },
        {
          "6. REFERENCES": "Benetos, Mark D Plumbley,\nand Wenwu Wang,\nâ€œAdapting",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Gautam Naik, Erik Cambria, and Rada Mihalcea,\nâ€œMeld: A"
        },
        {
          "6. REFERENCES": "arXiv\nlanguage-audio models\nas\nfew-shot\naudio learners,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "multimodal multi-party dataset for emotion recognition in con-"
        },
        {
          "6. REFERENCES": "preprint arXiv:2305.17719, 2023.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "versations,â€ arXiv preprint arXiv:1810.02508, 2018."
        },
        {
          "6. REFERENCES": "[7] Maxime Zanella and Ismail Ben Ayed, â€œOn the test-time zero-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[22]\nSteven R Livingstone and Frank A Russo, â€œThe ryerson audio-"
        },
        {
          "6. REFERENCES": "shot generalization of vision-language models: Do we really",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "visual database of emotional speech and song (ravdess): A dy-"
        },
        {
          "6. REFERENCES": "need prompt learning?,â€ in Proceedings of the IEEE/CVF Con-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "namic, multimodal set of facial and vocal expressions in north"
        },
        {
          "6. REFERENCES": "ference on Computer Vision and Pattern Recognition, 2024.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "american english,â€\nPloS one, vol. 13, no. 5, pp. e0196391,"
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "2018."
        },
        {
          "6. REFERENCES": "[8] Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "Mancini, and Elisa Ricci, â€œFrustratingly easy test-time adapta-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[23] Kate Dupuis and M Kathleen Pichora-Fuller,\nâ€œToronto emo-"
        },
        {
          "6. REFERENCES": "tion of vision-language models,â€ Advances in Neural Informa-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "tional speech set (tess)-younger talker happy,â€ 2010."
        },
        {
          "6. REFERENCES": "tion Processing Systems, vol. 37, pp. 129062â€“129093, 2024.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[24]\nPhilip Jackson and Sameer Haq,\nâ€œSAVEE: Surrey Audio-"
        },
        {
          "6. REFERENCES": "[9]\nSoham Deshmukh, Rita Singh, and Bhiksha Raj,\nâ€œDomain",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "the Interna-\nVisual Expressed Emotion,â€\nin Proceedings of"
        },
        {
          "6. REFERENCES": "arXiv\nadaptation\nfor\ncontrastive\naudio-language models,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "tional Conference on Affective Computing and Intelligent\nIn-"
        },
        {
          "6. REFERENCES": "preprint arXiv:2402.09585, 2024.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "teraction (ACII), 2014."
        },
        {
          "6. REFERENCES": "[10] Guan-Ting Lin,\nShang-Wen Li,\nand Hung-yi Lee,\nâ€œLis-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[25] Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C"
        },
        {
          "6. REFERENCES": "ten, adapt, better wer: Source-free single-utterance test-time",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Gur, Ani Nenkova,\nand Ragini Verma,\nâ€œCrema-d: Crowd-"
        },
        {
          "6. REFERENCES": "arXiv preprint\nadaptation for automatic speech recognition,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "IEEE transac-\nsourced emotional multimodal actors dataset,â€"
        },
        {
          "6. REFERENCES": "arXiv:2203.14222, 2022.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "tions on affective computing, vol. 5, no. 4, pp. 377â€“390, 2014."
        },
        {
          "6. REFERENCES": "[11] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[26] Yiming Li, Xiangdong Wang,\nand Hong Liu,\nâ€œAudio-free"
        },
        {
          "6. REFERENCES": "Berg-Kirkpatrick,\nand Shlomo Dubnov,\nâ€œLarge-scale\ncon-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "prompt\ntuning for language-audio models,â€\nin ICASSP 2024-"
        },
        {
          "6. REFERENCES": "trastive\nlanguage-audio\npretraining with\nfeature\nfusion\nand",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "2024 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "6. REFERENCES": "keyword-to-caption augmentation,â€\nin International Confer-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Signal Processing (ICASSP). IEEE, 2024, pp. 491â€“495."
        },
        {
          "6. REFERENCES": "ence on Acoustics, Speech and Signal Processing (ICASSP).",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[27] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu"
        },
        {
          "6. REFERENCES": "IEEE, 2023, pp. 1â€“5.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Wang,\nand Mark D Plumbley,\nâ€œPanns:\nLarge-scale\npre-"
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "trained audio neural networks for audio pattern recognition,â€"
        },
        {
          "6. REFERENCES": "[12] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al\nIsmail,",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "IEEE/ACM Transactions\non Audio,\nSpeech,\nand Language"
        },
        {
          "6. REFERENCES": "and Huaming Wang,\nâ€œClap learning audio concepts from nat-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Processing, vol. 28, pp. 2880â€“2894, 2020."
        },
        {
          "6. REFERENCES": "ural language supervision,â€ in IEEE International Conference",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "[28] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-"
        },
        {
          "6. REFERENCES": "2023.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "Kirkpatrick,\nand Shlomo Dubnov,\nâ€œHts-at: A hierarchical"
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "token-semantic audio transformer for sound classification and"
        },
        {
          "6. REFERENCES": "[13] Todd K Moon,\nâ€œThe expectation-maximization algorithm,â€",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "detection,â€ in ICASSP 2022-2022 IEEE International Confer-"
        },
        {
          "6. REFERENCES": "IEEE Signal processing magazine, pp. 47â€“60, 1996.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "ence on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "6. REFERENCES": "[14] Christopher M Bishop and Nasser M Nasrabadi, Pattern recog-",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        },
        {
          "6. REFERENCES": "",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": "IEEE, 2022, pp. 646â€“650."
        },
        {
          "6. REFERENCES": "nition and machine learning, Springer, 2006.",
          "[15]\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huam-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Selm: Enhancing speech emotion recognition for out-of-domain scenarios",
      "authors": [
        "Hazim Bukhari",
        "Soham Deshmukh",
        "Hira Dhamyal",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2024",
      "venue": "Selm: Enhancing speech emotion recognition for out-of-domain scenarios",
      "arxiv": "arXiv:2407.15300"
    },
    {
      "citation_id": "3",
      "title": "Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization",
      "authors": [
        "Abdul Jameel",
        "Mohammad Samadh",
        "Hanan Gani",
        "Noor Hussein",
        "Muhammad Khattak",
        "Muhammad Muzammal Naseer",
        "Fahad Shahbaz Khan",
        "Salman Khan"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Learning to prompt for vision-language models",
      "authors": [
        "Kaiyang Zhou",
        "Jingkang Yang",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2022",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Conditional prompt learning for vision-language models",
      "authors": [
        "Kaiyang Zhou",
        "Jingkang Yang",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Test-time prompt tuning for zero-shot generalization in vision-language models",
      "authors": [
        "Manli Shu",
        "Weili Nie",
        "De-An Huang",
        "Zhiding Yu",
        "Tom Goldstein",
        "Anima Anandkumar",
        "Chaowei Xiao"
      ],
      "year": "2022",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Adapting language-audio models as few-shot audio learners",
      "authors": [
        "Jinhua Liang",
        "Xubo Liu",
        "Haohe Liu",
        "Huy Phan",
        "Emmanouil Benetos",
        "Mark Plumbley",
        "Wenwu Wang"
      ],
      "year": "2023",
      "venue": "Adapting language-audio models as few-shot audio learners",
      "arxiv": "arXiv:2305.17719"
    },
    {
      "citation_id": "8",
      "title": "On the test-time zeroshot generalization of vision-language models: Do we really need prompt learning?",
      "authors": [
        "Maxime Zanella",
        "Ismail Ben"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Frustratingly easy test-time adaptation of vision-language models",
      "authors": [
        "Matteo Farina",
        "Gianni Franchi",
        "Giovanni Iacca",
        "Massimiliano Mancini",
        "Elisa Ricci"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Domain adaptation for contrastive audio-language models",
      "authors": [
        "Soham Deshmukh",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "year": "2024",
      "venue": "Domain adaptation for contrastive audio-language models",
      "arxiv": "arXiv:2402.09585"
    },
    {
      "citation_id": "11",
      "title": "Listen, adapt, better wer: Source-free single-utterance test-time adaptation for automatic speech recognition",
      "authors": [
        "Guan-Ting Lin",
        "Shang-Wen Li",
        "Hung-Yi Lee"
      ],
      "year": "2022",
      "venue": "Listen, adapt, better wer: Source-free single-utterance test-time adaptation for automatic speech recognition",
      "arxiv": "arXiv:2203.14222"
    },
    {
      "citation_id": "12",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Mahmoud Ismail",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "The expectation-maximization algorithm",
      "authors": [
        "Todd K Moon"
      ],
      "year": "1996",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "15",
      "title": "Pattern recognition and machine learning",
      "authors": [
        "M Christopher",
        "Bishop",
        "M Nasser",
        "Nasrabadi"
      ],
      "year": "2006",
      "venue": "Pattern recognition and machine learning"
    },
    {
      "citation_id": "16",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Investigating the emergent audio classification ability of asr foundation models",
      "authors": [
        "Rao Ma",
        "Adian Liusie",
        "J Mark",
        "Kate Gales",
        "Knill"
      ],
      "year": "2023",
      "venue": "Investigating the emergent audio classification ability of asr foundation models",
      "arxiv": "arXiv:2311.09363"
    },
    {
      "citation_id": "18",
      "title": "Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities",
      "authors": [
        "Zhifeng Kong",
        "Arushi Goel",
        "Rohan Badlani",
        "Wei Ping",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "year": "2024",
      "venue": "Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities",
      "arxiv": "arXiv:2402.01831"
    },
    {
      "citation_id": "19",
      "title": "Describing emotions with acoustic property prompts for speech emotion recognition",
      "authors": [
        "Hira Dhamyal",
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Huaming Wang",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "year": "2022",
      "venue": "Describing emotions with acoustic property prompts for speech emotion recognition",
      "arxiv": "arXiv:2211.07737"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Clep-dg: Contrastive learning for speech emotion domain generalization via soft prompt tuning",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Ye Gao"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "23",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "24",
      "title": "Toronto emotional speech set (tess)-younger talker happy",
      "authors": [
        "Kate Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (tess)-younger talker happy"
    },
    {
      "citation_id": "25",
      "title": "SAVEE: Surrey Audio-Visual Expressed Emotion",
      "authors": [
        "Philip Jackson",
        "Sameer Haq"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "26",
      "title": "Crema-d: Crowdsourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Audio-free prompt tuning for language-audio models",
      "authors": [
        "Yiming Li",
        "Xiangdong Wang",
        "Hong Liu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Qiuqiang Kong",
        "Yin Cao",
        "Turab Iqbal",
        "Yuxuan Wang",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection",
      "authors": [
        "Ke Chen",
        "Xingjian Du",
        "Bilei Zhu",
        "Zejun Ma",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}