{
  "paper_id": "2403.05916v2",
  "title": "Gpt As Psychologist? Preliminary Evaluations For Gpt-4V On Visual Affective Computing",
  "published": "2024-03-09T13:56:25Z",
  "authors": [
    "Hao Lu",
    "Xuesong Niu",
    "Jiyao Wang",
    "Yin Wang",
    "Qingyong Hu",
    "Jiaqi Tang",
    "Yuting Zhang",
    "Kaishen Yuan",
    "Bin Huang",
    "Zitong Yu",
    "Dengbo He",
    "Shuiguang Deng",
    "Hao Chen",
    "Yingcong Chen",
    "Shiguang Shan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal large language models (MLLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better humancentric applications. This paper assesses the application of MLLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that GPT-4V has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT-4V for handling advanced tasks in emotion recognition and related fields by integrating with taskrelated agents for more complex tasks, such as heart rate estimation through signal processing. In conclusion, this paper provides valuable insights into the potential applications and challenges of MLLMs in human-centric computing. Our interesting example is in https://github. com/EnVision-Research/GPT4Affectivity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The development of multimodal large language models (MLLMs) has been a topic of growing interest in recent years  [12, [22] [23] [24] 44] . MLLMs are designed to process and integrate information from multiple modalities, such as text, speech, images, and videos. The development of these models has been driven by the need to improve the accuracy and efficiency of various tasks, such as affective computing, sentiment analysis, and natural language understanding.\n\nMLLMs have shown great promise in improving the accuracy and robustness of affective computing systems  [28, 38] . These models can process and integrate information from multiple modalities, such as facial expressions, speech patterns, and physiological signals, to infer emotional states accurately  [12, 22, 24, 44]  with significant implications for various applications, such as healthcare, education, and human-computer interaction.\n\nDespite the rapid development of MLLMs, the need for standardized evaluation metrics is highlighted for accurate assessment. Different from the general language understanding evaluation benchmark that has been widely used to evaluate the performance of language models in NLP tasks  [40] [41] [42] , similar benchmarks to evaluate the performance of MLLMs for affective computing tasks are lacked, which is of great benefit to advance this field.\n\nGPT-4V is the state-of-the-art MLLM that has shown remarkable success in various natural language processing tasks  [1] . Its ability to process and integrate information from multiple modalities makes it an ideal candidate for evaluating the performance of MLLMs in tasks for affective computing. Furthermore, it can invoke a variety of tools that benefit affective computing tasks, such as related program generation with self-correction. For example, GPT-4V can call DALL•E 2 to generate high-quality visual affective images shown in Fig.  1 .\n\nIn this paper, we evaluate GPT-4V with 5 typical humancentric tasks, spanning from visual affective tasks and reasoning tasks. We summarize our findings as follows:\n\n(1) GPT-4V is highly accurate in recognizing facial action units. This accuracy can be attributed to its advanced understanding of facial movements and their corresponding emotions, which allows it to effectively identify and analyze facial action units.\n\n(2) GPT-4V is also precise in detecting microexpressions. Its ability to process subtle and transient facial expressions enables it to accurately capture these fleeting emotional cues, which are often difficult for humans to perceive.\n\n(3) GPT-4V's performance in general facial expression recognition is not as accurate. This limitation may be due to the complexity and variety of facial expressions, resulting in the challenges in capturing and analyzing them. Nevertheless, when GPT-4V is used to process thought chains, its accuracy in facial expression recognition improves significantly. This improvement suggests that incorporating additional contextual information is of great importance to recognize facial expressions.\n\n(4) Achieving high accuracy in micro-expression recognition remains a challenging task. This difficulty arises from the transient nature of micro-expressions and the need to detect and classify them within a very short time period. These challenges call for continuing research and development in this area for improving affective computing (5) GPT-4V can also integrate with task-related agents to handle more complex tasks, such as detecting subtle facial changes and estimating heart rate with signal processing. By leveraging Python's powerful libraries and tools, GPT-4V can effectively process and analyze intricate facial data to derive valuable insights, such as heart rate estimation, which can further enhance its applications in mental health monitoring and virtual human companion systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Affective Evaluation",
      "text": "Affective computing emerges as an interdisciplinary domain, leveraging computational technologies to discern, comprehend, and emulate human emotions. Its objective is to augment human-computer interaction, enhance user experiences, and facilitate improved communication and selfexpression. Within the scope of computer vision, the analysis of human facial units  [26] , expressions  [30] , microexpressions  [48] , micro-gestures  [16] , and deception detection  [8] , alongside physiological measurements  [45] , are pivotal to advancing emotional computing. Notably, largescale pre-trained models, such as GPT-4V, have demonstrated substantial advancements in natural language processing, suggesting their considerable promise for application in affective computing. This study proposes to scrutinize the efficacy of GPT-4V across a variety of tasks, employing methodologies that include iterative conversations, open-ended inquiries, as well as multiple-choice and true/false questions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Action Unit Detection",
      "text": "The Facial Action Coding System (FACS)  [6]  offers an explainable and reliable framework for the analysis of human facial expressions. It systematically deconstructs facial expressions into discrete components, known as Action Units (AUs), which correspond to the activation of specific facial muscles or groups thereof. Through the identification and quantification of these AUs, researchers can conduct a methodical examination of facial expressions and the emotional states they signify. Our assessment of GPT-4V's performance on the DISFA dataset  [25] , utilizing a gamut of question types, underscores its proficiency in accurately identifying AUs, thereby enabling precise emotion recognition from minimal interaction.\n\nRemarkably, GPT-4V exhibits exceptional accuracy in AU identification, facilitating nearly flawless judgment across all AUs examined as shown in Tab. 1. Although our presentation includes a limited number of examples, our comprehensive evaluation reveals GPT-4V's surprising efficacy in this domain. To quantitatively appraise this performance, we adopted a quantitative analysis approach, benchmarking against the F1 metrics as reported in related studies.\n\nFollowing  [10] , we report F1 metrics on DISFA. Specifically, we judge whether the recognition is successful by searching whether there is AUX (such as AU1) keyword in the reply question. The results show that the performance of GPT-4V is stronger than that of later professional models. This shows that GPT-4V has learned the microcharacteristics of emotion in a large number of network data and achieved significant recognition accuracy. Our findings indicate that GPT-4V's performance surpasses that of subsequent specialized models, underscoring its adeptness at learning the nuanced characteristics of emotion through extensive analysis of online data, thus achieving remarkable accuracy in emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Expression Recognition",
      "text": "The facial expression recognition  [30]  task involves identifying and analyzing human facial expressions to determine emotions. This task plays a crucial role in understanding human emotions, enhancing communication, and improving mental health monitoring and virtual human companion Figure  2 . Action Unit detection on DISFA  [25]  dataset. We use the single round for the action unit. GPT-4V can accurately identify each AU.\n\nAU DRML  [49]  DSIN  [3]  LP  [26]  SRERL  [11]  EAC  [13]  JAA  [31]  ARL  [32]  FAUDT  [10]  PIAP  [33]   Table  1 . Comparison with state-of-the-art methods for AU detection on DISFA  [25]  dataset using the F1-score metric (in %).\n\nsystems. It can be challenging due to the complexity and variety of facial expressions, as well as the need to detect and classify subtle and transient expressions accurately. For this reason, we qualitatively analyze the performance of GPT-4V for emotion recognition on RAF-DB  [30]  dataset. Our methodology encompassed a multifaceted approach, employing iterative dialogues, open-ended questions, multiplechoice queries, and true/false assessments, specifically utilizing the CASME2 dataset as a basis for evaluation. Contrary to expectations, preliminary results indicate that GPT-4V exhibits limitations in accurately responding to even basic true/false questions related to emotion recognition, as depicted in the referenced figure shown in Fig.  3 .\n\nAs shown in Fig.  3 , natural emotions are thought to have no obvious characteristics, as soon as we pass a form of judgment question. For the emotion of Fear, GPT-4V thinks that the emotion is natural and cannot give the decision of fear. This is because emotions are inherently difficult to recognize without context, which is not considered an objective task. Therefore, GPT-4V cannot achieve good performance on these subjective tasks. This finding highlights a significant limitation in the application of advanced lan-guage models like GPT-4V for the nuanced task of emotion recognition. It suggests that while such models possess remarkable capabilities in various domains of natural language processing, their effectiveness in interpreting human emotions through facial expressions, especially in the absence of contextual information, remains constrained. The subjective nature of emotional expression, coupled with the subtleties and variations inherent in human facial expressions, necessitates a more sophisticated approach that incorporates contextual understanding and perhaps multimodal inputs that extend beyond textual analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Compound Emotion Recognition",
      "text": "The task of compound emotion recognition  [5]  extends beyond the scope of simple emotion recognition by necessitating the identification and analysis of multiple emotions simultaneously exhibited through human facial expressions. The complexity of this task is amplified by the requirement to accurately detect and classify a spectrum of emotions, which may often be overlapping or present ambiguous signals. It can be more challenging than simple emotion recognition due to the need to detect and classify multiple emo-Figure  3 . Expression recognition on RAF-DB  [30]  dataset. GPT-4V cannot achieve good performance on the subjective task of emotion recognition.\n\ntions accurately, as well as the potential for conflicting or ambiguous expressions. In our continued exploration of GPT-4V 's capabilities, we extend our assessment to include the recognition of compound emotions.\n\nAs shown in Fig.  4 , we qualitatively analyze the performance of GPT-4V for compound emotion recognition on RAF-DB  [30]  dataset and find that compound expressions can even be recognized. Even compound expressions are recognized more accurately than individual expressions. This does not mean GPT-4V is more accurate for compound than individual expressions. Instead, this is because the data of this compound expression is relatively more objective, and GPT-4V has an accurate judgment of this objective expression. This revelation underscores the importance of developing computational models that can navigate the intricacies of human emotions with a high degree of sensitivity and accuracy. For applications in mental health monitoring and virtual companionship, paving the way for innovations in emotional AI that can more closely mimic human empathetic and cognitive processes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Micro-Expression Recognition",
      "text": "The domain of micro-expression  [48]  research within emotion recognition is characterized by the endeavor to identify and interpret subtle, fleeting expressions that manifest on the human face. These micro-expressions, often resulting from rapid emotional shifts or attempts to conceal emotions, are particularly ephemeral, lasting only between 1/25 to 1/5 of a second. This attribute renders micro-expressions both a fascinating and formidable area of study  [14, 15, 37] . However, the transient and elusive nature of micro-expressions presents significant challenges, notably in their detection and accurate interpretation. In our investigation, we meticulously crafted cue words and deployed a series of experimental setups involving judgment questions, multiple-choice inquiries, and iterative dialogues, all facilitated based on the CASME2 dataset  [43]  through the GPT-4V platform. This approach aimed to explore the potential of GPT-4V in recognizing and interpreting micro-expressions within the constraints of textual communication.\n\nAs shown in Fig.  5 , GPT-4V did not answer the provided micro-expression test samples satisfactorily. GPT-4V cannot understand the difference between frames, and the difference is not visible to the human eye. We tried to amplify this difference, but GPT-4V thought the enlarged image was blurry, so GPT-4V was very weak on the microexpression task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Micro-Gesture Recognition",
      "text": "Micro-gesture recognition  [16]  tasks focus on recognizing and analyzing small, imperceptible body movements and facial expressions produced by people in specific scenarios, which usually represent an individual's inner emotions, attitudes, or cognitive responses. Micro-gesture recognition techniques are valuable for many applications, such as emotion recognition, negotiation, police interrogation, and mental health assessment. The core challenge of this technology is to capture brief and subtle changes in movements that are difficult for individuals to control due to their association with the autonomic nervous system. Micro-gesture recognition improves social interactions and communication by helping people better understand others' emotions and motivations. We carefully designed the cue words and tested several different micro-gesture sequences on judgment questions, multiple-choice questions, and multi-round conversations using GPT-4V.\n\nWe qualitatively analyze the performance of GPT-4V for micro-gesture recognition on iMiGUE  [16]  dataset. As shown in Fig.  6 , GPT-4V can give satisfactory answers to the micro-gesture test samples provided. It can give similar answers to even the most difficult questions (open-ended questions), such as rubbing the face (rubbing the eyes). As shown in Fig.  7 , GPT-4V doesn't recognize the shoulder flutter. GPT-4V can't recognize tiny movements. While GPT-4V marks a significant step forward in the application of AI in the field of emotion and behavior recognition, its current limitations in recognizing certain micro-gestures suggest that further refinement and development is needed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Deception Detection",
      "text": "Deception detection is an important task for determining the authenticity of video content, which is very important for security. To verify the performance of GPT-4V for deception detection, we evaluated on Real-Life Trial dataset  [29] .\n\nAs shown in 8, GPT-4V can't tell if a person in a video is lying. In fact, such subjective tasks are difficult for even real people to accurately judge. In addition, we try to input some multimodal information such as the sound spectrum to guide the GPT-4V to produce the correct result. But such operations do not allow GPT to reason the correct result. This shows that GPT-4V is still challenging for subjective tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Advanced Capability Of Reasoning",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Chain Of Thought",
      "text": "The concept of Chain-of-Thought (CoT)  [7, 34, 39]  was first introduced in the seminal work by researchers at Google, titled Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. This innovative approach represents a significant advancement in cue strategies designed to enhance the performance of Large Language Models (LLMs) in executing complex reasoning tasks, encompassing arithmetic, common sense, and symbolic reasoning domains. Contrary to the Implicit Context Learning (ICL) approach, which primarily relies on input-output pairings, CoT incorporates a series of intermediate inference steps that scaffold toward the final solution, thereby enriching the model's reasoning pathway. In essence, CoT facilitates discrete prompt learning by appending an example to the beginning of a given input, enabling the model to process these concatenated texts simultaneously and produce the desired output. This method, under including additional intermediate prompts for inference, represents a substantial improvement over traditional context learning approaches.GPT-4V has a hard time recognizing specific expressions without context. However, we asked GPT-4V to first recognize the specific AU representation and then deduce the emotion based on the relationship between AU and the expression, which allowed GPT-4V to give some possible outcomes of the expression.\n\nFurthermore, the application of CoT in emotion recognition tasks reveals its potential to circumvent some of the limitations faced by models such as GPT-4V in interpreting ambiguous or neutral expressions. Despite GPT-4V's pro-Figure  5 . Micro-expression recognition on the CASME2  [43]  dataset. GPT-4V has difficulty understanding the small differences in the image directly, so it is difficult to understand the micro facial expressions accurately. ficiency in Action Unit (AU) recognition, its performance in emotion recognition from expressions remains suboptimal. By leveraging the correlation between CoT, AU, and facial expressions, we aim to enhance GPT-4V's accuracy in this area. As evidenced in Fig.  9 , the incorporation of CoT significantly improves GPT-4V's capability to discern emotions, particularly in instances where expressions are ambiguous or lack clear contextual cues. This methodology enables GPT-4V to first accurately identify AUs, and subsequently infer the probable emotion based on the established relationship between AUs and facial expressions. The integration of CoT, as illustrated by the blue segments in the figure, thus facilitates a more nuanced understanding and recognition of emotional states by the model. Thus, the application of CoT in affective computing holds the potential to significantly improve the capability of visual  Micro-gesture (rubbing eyes) recognition on iMiGUE  [16]  dataset. GPT-4V can recognize obvious movements. language models in interpreting and predicting emotional states with greater accuracy, leveraging contextual information to bridge the gap between task-related cues and the corresponding emotional expressions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Tool Call And Processing",
      "text": "GPT-4V is one of the state-of-the-art multimodal language models that has achieved remarkable success in various natural language processing tasks. However, it is not directly applicable to some complex tasks, such as remote photoplethysmography (rPPG)  [2, 9, 18-20, 27, 35, 36, 45-47] . rPPG is a non-invasive technique used to measure heart rate and respiratory rate from facial videos. It has a wide range of applications in healthcare, entertainment, and humancomputer interaction.\n\nUnfortunately, GPT-4V cannot read in long time-series videos and cannot discern subtle chromatic variations. To address this issue, a solution is for professional researchers to collaborate with GPT-4V. In this regard, we found that GPT-4V can call Python tools to run code and debug it. To demonstrate this process, we extracted facial video chromatic changes and used GPT-4V to process the signal. As shown in the figure, GPT-4V called Python to process and visualize the signal. During this process, there were several bugs, but GPT-4V was able to self-correct based on the bug information and ultimately provided an accurate heart rate result.\n\nThis process has provided us with an insight that this can be turned into a framework for a human-large language multimodal model that can self-correct. In this process, any large model can self-correct. This framework has immense potential in enhancing the accuracy and efficiency of various tasks, including rPPG.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Further Discussion",
      "text": "GPT-4V is a powerful language model that has shown remarkable success in various natural language processing tasks. However, it faces several challenges in other domains, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection.\n\nEmotion recognition is the process of identifying and classifying emotions based on physiological signals, facial expressions, and speech patterns. GPT-4V has shown promising results in this task; however, it requires a large amount of training data and may not generalize well to new datasets. To overcome this limitation, future research can focus on developing transfer learning techniques that enable GPT-4V to learn from smaller datasets and generalize to new datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Detecting Deception",
      "text": "User Please describe the image sequence, and determine if the person is telling a lie？\n\nThe images you uploaded appear to show a sequence of a person during what seems to be an interview or an interrogation, given the timestamp and the background. The person's body language changes slightly across the images -in some, they are sitting with their arms crossed, in others, they are leaning slightly forward, and in one they are blurry, possibly indicating movement.\n\nDetermining whether someone is lying based on a sequence of images alone is not reliable. Lie detection typically involves a combination of physiological measurements, analysis of verbal statements, and nonverbal cues interpreted in context. Even in professional settings, the accuracy of lie detection is a subject of debate and is not always reliable without corroborating evidence. Facial expressions, body language, and inconsistencies in speech patterns may sometimes provide hints, but they are not definitive indicators of deception. Furthermore, without knowing the context of the conversation, the baseline behavior of the person, and the content of what is being said, it is impossible to make a judgment on truthfulness from images alone. Non-contact physiological measurement involves measuring physiological signals, such as heart rate, respiratory rate, and blood pressure, without direct contact with the body  [2, 9, 19, 20, 35, 36] . GPT-4V faces difficulty in this task due to its limited ability to process and interpret physiological signals accurately. To overcome this limitation, future research can focus on developing new technologies that can capture physiological signals accurately and integrate them with GPT-4V.\n\nDeception detection involves identifying and verifying the authenticity of a person or an object. GPT-4V faces difficulty in this task due to its limited ability to process and interpret visual and audio information accurately. To overcome this limitation, future research can explore ways to integrate GPT-4V with computer vision and audio processing techniques to improve authenticity detection accuracy.\n\nIn conclusion, GPT-4V faces several challenges in nonlanguage tasks, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and deception detection. Future research can focus on developing new techniques to enhance GPT-4V's ability to process and integrate multimodal data and improve its accuracy and efficiency in these tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Expression Recognition Without Cot",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have discussed the challenges that GPT-4V faces in non-language tasks, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection. While GPT-4V has shown remarkable success in various natural language processing tasks, it faces limitations in these domains due to its limited ability to pro-Figure  10 . Tool call and processing for rPPG task. GPT-4V can directly write programs for signal processing on request and try to run them. If there is an error, GPT-4V can be further corrected according to the error prompt, and finally give the heart rate prediction and visualization. cess and interpret visual and audio information accurately. To overcome these limitations, future research can focus on developing new techniques to enhance GPT-4V's ability to process and integrate multimodal data and improve its accuracy and efficiency in these tasks. This may involve integrating GPT-4V with computer vision and audio processing techniques, developing transfer learning techniques, exploring new sensor technologies, and improving the quality and quantity of training data. By addressing these challenges, GPT-4V has the potential to significantly advance the fields of facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection, and open up new avenues for research and application in these domains.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The propaganda image was generated by DALL·E 2.",
      "page": 1
    },
    {
      "caption": "Figure 1: In this paper, we evaluate GPT-4V with 5 typical human-",
      "page": 2
    },
    {
      "caption": "Figure 2: Action Unit detection on DISFA [25] dataset. We use the single round for the action unit. GPT-4V can accurately identify each",
      "page": 3
    },
    {
      "caption": "Figure 3: As shown in Fig. 3, natural emotions are thought to have",
      "page": 3
    },
    {
      "caption": "Figure 3: Expression recognition on RAF-DB [30] dataset. GPT-4V cannot achieve good performance on the subjective task of emotion",
      "page": 4
    },
    {
      "caption": "Figure 4: , we qualitatively analyze the per-",
      "page": 4
    },
    {
      "caption": "Figure 5: , GPT-4V did not answer the provided",
      "page": 4
    },
    {
      "caption": "Figure 4: Compound emotion recognition on RAF-DB [30] dataset. GPT-4V can deduce objective compound expressions based on",
      "page": 5
    },
    {
      "caption": "Figure 6: , GPT-4V can give satisfactory answers to",
      "page": 5
    },
    {
      "caption": "Figure 7: , GPT-4V doesn’t recognize the shoulder",
      "page": 5
    },
    {
      "caption": "Figure 5: Micro-expression recognition on the CASME2 [43]",
      "page": 6
    },
    {
      "caption": "Figure 9: , the incorporation of",
      "page": 6
    },
    {
      "caption": "Figure 6: Micro-gesture",
      "page": 6
    },
    {
      "caption": "Figure 7: Micro-gesture (shaking double shoulders) recognition",
      "page": 7
    },
    {
      "caption": "Figure 8: Deception Detection on Real-Life Trial dataset [29].",
      "page": 7
    },
    {
      "caption": "Figure 9: Improve the accuracy of expression recognition through",
      "page": 8
    },
    {
      "caption": "Figure 10: Tool call and processing for rPPG task. GPT-4V can",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Expression Recognition without COT\nUser\nPlease describe the picture first. Describe the man's expression\naccording to the picture. (Open-Ended Question)\nBased on the visible part of the face, there are no strong indicators\nof any specific emotion. The person's mouth is open wide as if they\nare yelling or singing loudly. The facial expression suggests a\nstrong emotional outburst that could be associated with excitement,\njoy, surprise, or possibly distress. However, when analyzing\nemotions through facial expressions, it is crucial to consider the\ncontext and other nonverbal cues, which are not available in this\nstatic image. Without more context, it is difficult to determine the\nexact nature of the emotion being expressed.\nExpression Recognition with COT\nUser\nPlease describe the picture first. Determine the facial action unit\nof the image, and based on the facial action unit, determine its\nexpression. For example, Action units (AU) 6 and AU 12 might\nmean happiness.. (Open-Ended Question)\nThe image shows an individual who appears to be a young adult with\nshort, light-colored hair. They are wearing a sleeveless top and have\ntheir eyes closed. The person's mouth is open wide as if they are\nyelling or singing loudly.\nIn terms of Facial Action Coding System (FACS), several Action Units\n(AUs) might be inferred from this expression. For example, an open\nmouth can be associated with AU25 (lips part) and AU26 (jaw drop).\nThe closed eyes and wrinkled nose might suggest AU6 (cheek raiser) Figure10. ToolcallandprocessingforrPPGtask. GPT-4Vcan\nand AU7 (lid tightener), which are typically associated with\ndirectly write programs for signal processing on request and try\nexpressions of intense joy or possibly fear when combined with\nother AUs. torunthem. Ifthereisanerror,GPT-4Vcanbefurthercorrected\nThe facial expression could be indicative of a positive emotion such accordingtotheerrorprompt,andfinallygivetheheartratepre-\nas happiness (often characterized by AU6 and AU12, which involve dictionandvisualization.\ncheek raising and lip corner pulling respectively) if it were in a\ndifferent context. However, the intensity of the expression might also\nimply a high-arousal emotion like surprise or fear if it were\nassociated with different circumstances.",
          "nition without COT\nfirst. Describe the man's expression\npen-Ended Question)\nhere are no strong indicators\nmouth is open wide as if they\nal expression suggests a\nbe associated with excitement,\nwever, when analyzing\nit is crucial to consider the\nich are not available in this\nis difficult to determine the\nressed.": "nition without COT\nfirst. Describe the man's expression\npen-Ended Question)\nhere are no strong indicators\nmouth is open wide as if they\nal expression suggests a\nbe associated with excitement,\nwever, when analyzing\nit is crucial to consider the\nich are not available in this\nis difficult to determine the\nressed."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Lama Ahmad",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal ; Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Deepphys: Videobased physiological measurement using convolutional attention networks",
      "authors": [
        "Weixuan Chen",
        "Daniel Mcduff"
      ],
      "year": "2018",
      "venue": "Proceedings of the european conference on computer vision (ECCV)"
    },
    {
      "citation_id": "3",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "Ciprian Corneanu",
        "Meysam Madadi",
        "Sergio Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the european conference on computer vision (ECCV)"
    },
    {
      "citation_id": "4",
      "title": "Biomechanics-guided facial action unit detection through force modeling",
      "authors": [
        "Zijun Cui",
        "Chenyi Kuang",
        "Tian Gao",
        "Kartik Talamadupula",
        "Qiang Ji"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "7",
      "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective",
      "authors": [
        "Guhao Feng",
        "Bohang Zhang",
        "Yuntian Gu",
        "Haotian Ye",
        "Di He",
        "Liwei Wang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Audio-visual deception detection: Dolos dataset and parameter-efficient crossmodal learning",
      "authors": [
        "Xiaobao Guo",
        "Muthuchamy Nithish",
        "Zitong Selvaraj",
        "Adams Yu",
        "Wai-Kin",
        "Bingquan Kong",
        "Alex Shen",
        "Kot"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Challenges and prospects of visual contactless physiological monitoring in clinical study",
      "authors": [
        "Bin Huang",
        "Shen Hu",
        "Zimeng Liu",
        "Chun-Liang Lin",
        "Junfeng Su",
        "Changchen Zhao",
        "Li Wang",
        "Wenjin Wang"
      ],
      "year": "2023",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "10",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Bjorn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "Guanbin Li",
        "Xin Zhu",
        "Yirui Zeng",
        "Qing Wang",
        "Liang Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "13",
      "title": "Eacnet: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "Wei Li",
        "Farnaz Abtahi",
        "Zhigang Zhu",
        "Lijun Yin"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "14",
      "title": "Microexpression action unit detection with spatial and channel attention",
      "authors": [
        "Yante Li",
        "Xiaohua Huang",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "Micro-expression action unit detection with dual-view attentive similaritypreserving knowledge distillation",
      "authors": [
        "Yante Li",
        "Wei Peng",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "16",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "Xin Liu",
        "Henglin Shi",
        "Haoyu Chen",
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Multi-scale promoted self-adjusting correlation learning for facial action unit detection",
      "authors": [
        "Xin Liu",
        "Kaishen Yuan",
        "Xuesong Niu",
        "Jingang Shi",
        "Zitong Yu",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "year": "2023",
      "venue": "Multi-scale promoted self-adjusting correlation learning for facial action unit detection",
      "arxiv": "arXiv:2308.07770"
    },
    {
      "citation_id": "18",
      "title": "rppg-mae: Self-supervised pretraining with masked autoencoders for remote physiological measurements",
      "authors": [
        "Xin Liu",
        "Yuting Zhang",
        "Zitong Yu",
        "Hao Lu",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "Hao Lu",
        "Hu Han",
        "Kevin Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Neuron structure modeling for generalizable remote physiological measurement",
      "authors": [
        "Hao Lu",
        "Zitong Yu",
        "Xuesong Niu",
        "Ying-Cong Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Learning multi-dimensional edge featurebased au relation graph for facial action unit recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "venue": "Learning multi-dimensional edge featurebased au relation graph for facial action unit recognition"
    },
    {
      "citation_id": "22",
      "title": "Video assistant with large language model enhanced ability",
      "authors": [
        "Ruipu Luo",
        "Ziwang Zhao",
        "Min Yang",
        "Junwei Dong",
        "Minghui Qiu",
        "Pengcheng Lu",
        "Tao Wang",
        "Zhongyu Wei",
        "Valley"
      ],
      "year": "2023",
      "venue": "Video assistant with large language model enhanced ability",
      "arxiv": "arXiv:2306.07207"
    },
    {
      "citation_id": "23",
      "title": "Video anomaly detection and explanation via large language models",
      "authors": [
        "Hui Lv",
        "Qianru Sun"
      ],
      "year": "2024",
      "venue": "Video anomaly detection and explanation via large language models",
      "arxiv": "arXiv:2401.05702"
    },
    {
      "citation_id": "24",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "25",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Local relationship learning with personspecific shape regularization for facial action unit detection",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Songfan Yang",
        "Yan Huang",
        "Shiguang Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation",
      "authors": [
        "Xuesong Niu",
        "Shiguang Shan",
        "Hu Han",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Rajiv Bajpai, and Amir Hussain. A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "29",
      "title": "Multimodal deception detection using real-life trial data",
      "authors": [
        "Veronica Umut Şen",
        "Berrin Perez-Rosas",
        "Mohamed Yanikoglu",
        "Mihai Abouelenien",
        "Rada Burzo",
        "Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Li Shan",
        "Weihong Deng"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "31",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "32",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Yunsheng Wu",
        "Lizhuang Ma"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "Piap-df: Pixel-interested and anti person-specific facial action unit detection net with discrete feedback learning",
      "authors": [
        "Yang Tang",
        "Wangding Zeng",
        "Dafei Zhao",
        "Honggang Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Self-supervised representation learning framework for remote physiological measurement using spatiotemporal augmentation loss",
      "authors": [
        "Hao Wang",
        "Euijoon Ahn",
        "Jinman Kim"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "A novel algorithm for remote photoplethysmography: Spatial subspace rotation",
      "authors": [
        "Wenjin Wang",
        "Sander Stuijk",
        "Gerard De Haan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "36",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "Wenjin Wang",
        "Albertus C Den",
        "Sander Brinker",
        "Gerard Stuijk",
        "De Haan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "37",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Yandan Wang",
        "John See",
        "C-W Raphael",
        "Yee-Hui Phan",
        "Oh"
      ],
      "year": "2014",
      "venue": "Computer Vision-ACCV 2014: 12th Asian Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao",
        "Antonio Liotta",
        "Dawei Yang",
        "Xinlei Li",
        "Shuyong Gao",
        "Yixuan Sun",
        "Weifeng Ge",
        "Wei Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "39",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving",
      "authors": [
        "Licheng Wen",
        "Xuemeng Yang",
        "Daocheng Fu",
        "Xiaofeng Wang",
        "Pinlong Cai",
        "Xin Li",
        "Tao Ma",
        "Yingxuan Li",
        "Linran Xu",
        "Dengke Shang"
      ],
      "year": "2023",
      "venue": "On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving",
      "arxiv": "arXiv:2311.05332"
    },
    {
      "citation_id": "41",
      "title": "Indonlu: Benchmark and resources for evaluating indonesian natural language understanding",
      "authors": [
        "Bryan Wilie",
        "Karissa Vincentio",
        "Genta Indra Winata",
        "Samuel Cahyawijaya",
        "Xiaohong Li",
        "Zhi Yuan Lim",
        "Sidik Soleman",
        "Rahmad Mahendra",
        "Pascale Fung",
        "Syafri Bahar"
      ],
      "year": "2020",
      "venue": "Indonlu: Benchmark and resources for evaluating indonesian natural language understanding",
      "arxiv": "arXiv:2009.05387"
    },
    {
      "citation_id": "42",
      "title": "Clue: A chinese language understanding evaluation benchmark",
      "authors": [
        "Liang Xu",
        "Hai Hu",
        "Xuanwei Zhang",
        "Lu Li",
        "Chenjie Cao",
        "Yudong Li",
        "Yechen Xu",
        "Kai Sun",
        "Dian Yu",
        "Cong Yu"
      ],
      "year": "2020",
      "venue": "Clue: A chinese language understanding evaluation benchmark",
      "arxiv": "arXiv:2004.05986"
    },
    {
      "citation_id": "43",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "Wen-Jing Yan",
        "Xiaobai Li",
        "Su-Jing Wang",
        "Guoying Zhao",
        "Yong-Jin Liu",
        "Yu-Hsin Chen",
        "Xiaolan Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "44",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "45",
      "title": "Facial-videobased physiological signal measurement: Recent advances and affective applications",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "46",
      "title": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "authors": [
        "Zitong Yu",
        "Yuming Shen",
        "Jingang Shi",
        "Hengshuang Zhao",
        "Philip Hs Torr",
        "Guoying Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Physformer++: Facial video-based physiological measurement with slowfast temporal difference transformer",
      "authors": [
        "Zitong Yu",
        "Yuming Shen",
        "Jingang Shi",
        "Hengshuang Zhao",
        "Yawen Cui",
        "Jiehua Zhang",
        "Philip Torr",
        "Guoying Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "Facial micro-expressions: An overview",
      "authors": [
        "Guoying Zhao",
        "Xiaobai Li",
        "Yante Li",
        "Matti Pietikäinen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "49",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "Kaili Zhao",
        "Wen-Sheng Chu",
        "Honggang Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}