{
  "paper_id": "2107.12096v2",
  "title": "Towards Unbiased Visual Emotion Recognition Via Causal Intervention",
  "published": "2021-07-26T10:40:59Z",
  "authors": [
    "Yuedong Chen",
    "Xu Yang",
    "Tat-Jen Cham",
    "Jianfei Cai"
  ],
  "keywords": [
    "Causal intervention",
    "backdoor adjustment",
    "facial expression recognition",
    "image emotion recognition",
    "dataset bias *"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although much progress has been made in visual emotion recognition, researchers have realized that modern deep networks tend to exploit dataset characteristics to learn spurious statistical associations between the input and the target. Such dataset characteristics are usually treated as dataset bias, which damages the robustness and generalization performance of these recognition systems. In this work, we scrutinize this problem from the perspective of causal inference, where such dataset characteristic is termed as a confounder which misleads the system to learn the spurious correlation. To alleviate the negative effects brought by the dataset bias, we propose a novel Interventional Emotion Recognition Network (IERN) to achieve the backdoor adjustment, which is one fundamental deconfounding technique in causal inference. Specifically, IERN starts by disentangling the dataset-related context feature from the actual emotion feature, where the former forms the confounder. The emotion feature will then be forced to see each confounder stratum equally before being fed into the classifier. A series of designed tests validate the efficacy of IERN, and experiments on three emotion benchmarks demonstrate that IERN outperforms state-ofthe-art approaches for unbiased visual emotion recognition. Code is available at https://github.com/donydchen/causal_emotion. \n CCS CONCEPTS â€¢ Information systems â†’ Multimedia information systems; â€¢ Humancentered computing â†’ Human computer interaction (HCI).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Visual emotion recognition, including facial expression recognition (FER)  [7, 20, 23, 50]  and image emotion recognition (IER)  [29, 52, 53] , has attracted great attention for decades, playing a vital role in lots of daily scenes such as mental health care and driver drowsiness detection. While deep neural networks (DNNs) show promising performances on several existing benchmarks  [10, 21] , it has been recently observed that DNNs may \"cheat\" by relying on unintended cognitive level  [58]  dataset characteristics, e.g., scene contexts  [29]  and human attributes  [45] , instead of the actual affective level  [58]  causal relationships between the input image and the output emotion label, harming their effectiveness in practice. Intuitively, such dataset characteristics are leveraged by DNNs as the shortcut  [14]  to learn spurious statistical associations between variables.\n\nDNNs may not appear to be affected by dataset characteristics if the training and test set data are from the same distribution  [37] , under which most emotion benchmarks are collected and constructed. However, a robust practical emotion recognition system is expected to have consistent generalization even for data from different distributions  [37] . Here we design a toy experiment to showcase how DNNs behave on data from both the same distribution and different distributions, as depicted in Fig.  1 . From a facial expression dataset, we rendered additional images by separately applying blur and noise degradation. They were next partitioned into two datasets (blue dashed vs orange solid borders) by selecting subsets of expressions and degradation labels. A VGG-11  [38]  was then trained (randomly initialized and without data augmentation) on samples drawn from the blue dataset. Such a trained model performed well on samples drawn from the same blue dataset (data from the same distribution), shown in the blue confusion matrix of Fig.  1 , but is impaired dramatically when tested on samples from the orange dataset (data from a different distribution). Instead of recognizing expressions based on relevant expression features, the model has been biased towards recognizing the degradation effects, which are easier to detect.\n\nUnlike existing methods in emotion recognition that address the dataset bias issue via solutions such as re-sampling and building larger datasets, we choose to leverage causal inference  [32, 33, 37]  to tackle it. In causal inference, entities or attributes that mislead DNNs to learn spurious correlations between input and output are termed as confounders  [32, 33, 35] , which refers to the dataset characteristics in our emotion recognition context. For example, in the above toy experiment, the model has been misled to confound blur with disgust and sadness, and fails to recognize anger when blur co-occurs. An intuitive solution for the toy experiment is to add noise or blur to all input images (see the bottom left of Fig.  1 ), whose underlying principle is to deconfound the emotion recognition systems from the confounder / dataset characteristics. This solution is also known as causal intervention  [32] .\n\nTo deal with more sophisticated confounders, i.e., unknown dataset characteristics and image scenes, we analyze the problem with Pearl's structure causal model (SCM)  [32] , and propose a novel framework we call Interventional Emotion Recognition Network (IERN), to do the intervention by embedding the backdoor adjustment theorem  [32] . Although backdoor adjustment has been incorporated in some recent deep learning models for other vision tasks  [12, 40, 51] , all of these only approximate the intervention via memory-query operations, rather than applying real intervention as is done in our IERN.\n\nSpecifically, our IERN starts by disentangling the dataset-related context feature from the actual emotion feature, where the former represents the confounder. Following application of the backdoor adjustment theorem, the emotion feature will then be forced to see each confounder stratum equally before being fed into the classifier. We showcase how IERN realizes the backdoor adjustment theorem effectively on the facial expression recognition task via a mixed-dataset configuration, where emotion-independent dataset characteristics are treated as confounder, with dataset name being the label. On the image emotion recognition task, IERN outperforms the state-of-the-art approach by a significant margin under the challenging cross-datset setting, where image scenes are treated as confounders. The contributions are threefold,\n\nâ€¢ We are the first to tackle dataset bias in visual emotion recognition from a causality perspective.\n\nâ€¢ We propose a novel trainable framework, named Interventional Emotion Recognition Network (IERN), to realize the backdoor adjustment theorem.\n\nâ€¢ With rigorous experiments done for both the mixed-dataset and cross-dataset on existing benchmarks, we show how IERN effectively alleviates negative effects raised by dataset bias and outperforms state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Visual Emotion Recognition. The majority of emotion recognition works belong to Facial Expression Recognition (FER), where the input are human facial images with simple and limited backgrounds  [16, 43, 57] . Most methods focus on enhancing performance by making use of related auxiliary priors, e.g., facial landmarks  [19, 55, 56] , local regions  [9, 17, 48] , action units  [7, 8, 24] , optical flows  [39] , etc. The assumption of these methods is that the training data is fairly distributed, which may be wrong and could harm the generalization performance of trained models. Recently, the bias problem has received much attention, and most approaches  [13, 22, 28, 36]  mainly aim to intuitively disentangle the bias features, so as to build more robust emotion features.\n\nOthers belong to Image Emotion Recognition (IER), which aims to recognize the emotions invoked from images that contain a variety of objects rather than recognizing emotions of human faces. Approaches along this line mainly deal with social media photos  [44, 47] , manually generated images  [18] , artistic photographs  [1, 58] , web crawled natural images  [29, 52, 53] , etc. The most relevant work is  [29] , which identified the image scene as the bias factor, and managed to alleviate the bias issue by building a new larger and more balanced dataset using web data.\n\nDifferent from all approaches that handle dataset bias in visual emotion recognition via heuristic solutions, our approach is the first to offer a fundamental understanding of the bias issue via causal inference, and propose a new trainable unbiased framework based on causal intervention, by employing the feature disentanglement technique and the center loss function  [46] .\n\nCausal inference. Causality  [32, 35, 37]  can help pursue the causal effect between two observed variables, rather than depend only on their correlation. Many works have been proposed to explore how causality can be leveraged in machine learning  [4, 5, 27, 30] . Recently, causal inference has been introduced into the deep learning framework for several computer vision tasks, including image captioning  [51] , image classification  [6, 25, 40] , semantic segmentation  [12] , and few-shot learning  [54] .\n\nOur work differs from causality-based deep learning methods in two aspects. Firstly, ours is the first to address the biased visual emotion recognition problem. Secondly and more importantly, we propose a novel solution by learning confounder features and conducting real intervention, while previous methods had chosen to model confounders using a predefined dictionary and approximate intervention via memory-query operations by adopting the Normalized Weighted Geometric Mean (NWGM)  [3, 49] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Given input image ğ‘‹ , visual emotion recognition aims to solve the problem ğ‘ƒ (ğ‘Œ |ğ‘‹ ), where ğ‘Œ is the emotion label. In this section, we will detail how confounder ğ· undermines the objective of ğ‘ƒ (ğ‘Œ |ğ‘‹ ) (Section 3.1) to raise dataset bias, how such a bias effect can be removed (Section 3.2), and what our unbiased solution is (Section 3.3).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Scm For Emotion Recognition",
      "text": "Structural causal models (SCM)  [32, 37]  are designed to analyze the causal relationships. Fig.  2  shows a SCM in our context constructed among input ğ‘‹ , emotion ğ‘Œ and confounder ğ·. The direction of an edge denotes only the causal relationship, while information can flow bidirectionally, e.g., ğ· â†’ ğ‘‹ means that ğ· is the cause and ğ‘‹ is the effect, while information can still flow from ğ‘‹ to ğ·. ğ‘¿ â†’ ğ’€ . It is the intended causal relationship that the network is supposed to learn, which is to recognize emotion ğ‘Œ based on input image ğ‘‹ . For simplicity, we mix the use of symbol ğ‘‹ , denoting either input images or image features.\n\nğ‘« â†’ ğ‘¿, ğ‘« â†’ ğ’€ . Confounder ğ· is the undesirable context feature, e.g., unknown dataset characteristics, image scenes. Without disentanglement, confounder are normally embedded in the input features, and thus we have ğ· â†’ ğ‘‹ . Similarly, confounder also affects networks in predicting emotion, so we have ğ· â†’ ğ‘Œ .\n\nğ‘¿ â† ğ‘« â†’ ğ’€ (backdoor path). The existence of confounder ğ· enables a backdoor path between ğ‘‹ and ğ‘Œ , by which networks can be biased to build up spurious correlation between ğ‘‹ and ğ‘Œ , leading to degradation of its generalization performance. For example, as shown in Fig.  1 , instead of identifying the correct anger image features to recognize anger, i.e. ğ‘‹ â†’ ğ‘Œ , networks may learn that noise is anger, i.e. ğ· â†’ ğ‘‹ , and should be predicted as anger, i.e. ğ‘‹ â† ğ· â†’ ğ‘Œ .\n\nThe backdoor path can be explained with the law of total probability. Specifically, ğ‘ƒ (ğ‘Œ |ğ‘‹ ) can be decomposed into\n\nwhere",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Intervention With Backdoor Adjustment",
      "text": "Once the confounder is identified, a straightforward solution to alleviate the dataset bias is to build a larger dataset to ensure that emotion is balanced among all confounder strata, similar to what has been done in  [29] . However, such solutions might not be practical due to three main reasons. Firstly, it is time-consuming and costly to collect and label a large amount of data. Secondly, if the confounder is too complex, it is hard to construct a balanced new dataset. Thirdly, there may be ethical or practical issues in collecting certain types of data, e.g., forcing a child to cry in a park in order to get negative-emotional park images.\n\nWe therefore turn to a more elegant solution of applying the intervention  [32]  by blocking the backdoor path in the SCM (Fig.  2  Right). Specifically, instead of learning the correlation ğ‘ƒ (ğ‘Œ |ğ‘‹ ), we set our target as ğ‘ƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğ‘‹ )), where ğ‘‘ğ‘œ (â€¢) is the do-operator (a.k.a. do-calculus), denoting the experimental intervention. Since both ğ· and ğ‘‹ are accessible in our tasks, we can solve ğ‘ƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğ‘‹ )) by applying the backdoor adjustment theorem  [32] :\n\nIntuitively, the backdoor adjustment removes the risk of spurious correlation arising from ğ‘ƒ (ğ· |ğ‘‹ ) via a two-step strategy. It starts by estimating the causal effect in each stratum of the confounder, followed by summing the effects of all strata, weighted by their known prevalence in the population. By doing so, it ensures that the contribution of the causal effect of an unobserved stratum is no different from that of an observed one. In this way, the overall causal effect would not be biased towards the observed strata. For example, with the help of the backdoor adjustment, each stratum of the confounder, including noise, blur and normal, will contribute equally to anger, forcing the model to learn to recognize anger from the intended affective features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iern",
      "text": "To overcome dataset bias in visual emotion recognition, we propose our Interventional Emotion Recognition Network (IERN) by embedding the backdoor adjustment. Compared with heuristic debiasing methods like building larger datasets  [29] , IERN is designed based on well-studied deconfounding theory. Also, unlike NWGM approximation-based backdoor adjustment models  [12, 51]  that absorb the probability sum into the network, IERN applies the real intervention ğ‘ƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğ‘‹ )). Fig.  3  gives an overview of the IERN, which is composed of four parts: Backbone, Feature Disentanglement, Confounder Builder and Classifier. The backbone refers to an existing image feature extractor, e.g., VGG  [38]  or ResNet  [15] , the In training, Feature Disentanglement splits features into emotion and context features. Confounder Builder groups context features and gets their centers as confounder features. Each emotion feature is combined with all strata of the confounder individually, and forwarded to the Classifier. Only the top branch will be leveraged in testing, using the learned confounders.\n\ndisentanglement part separates emotion and context features, the confounder builder does the causal intervention, and the classifier predicts the emotion label. Formally, ğ‘“ ğ‘ , ğ‘” ğ‘’ , ğ‘‘ ğ‘’ , ğ‘” ğ‘ , ğ‘‘ ğ‘ , ğ‘” ğ‘Ÿ and ğ‘“ ğ‘ refer to backbone, emotion generator, emotion discriminator, context generator, context discriminator, reconstruction network and classifier, respectively. Feature Disentanglement. As mentioned in Section 3.1, without proper handling, context features are mixed with emotion features. Thus, IERN starts by applying feature disentanglement to separate emotion and context features. Specifically, to obtain the clean emotion feature, the disentanglement is conducted by a dualdiscriminator structure, consisting of an emotion discriminator ğ‘‘ ğ‘’ and a context discriminator ğ‘‘ ğ‘ , where ğ‘‘ ğ‘’ is to ensure the extracted feature contains emotional information while ğ‘‘ ğ‘ is to ensure that the extracted feature does not contain any context information.\n\nThis procedure is implemented by alternating the optimization between the feature generators and the discriminators with the following training objective:\n\nwhere ğ‘™ CE and ğ‘™ MSE denote the cross-entropy loss and the mean squared error loss, respectively; ğ‘‘ ğœƒ ğ‘’ and ğ‘” ğœƒ ğ‘’ are the trainable parameters of ğ‘‘ ğ‘’ and ğ‘” ğ‘’ , respectively; and ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)) is the intended clean emotion feature, obtained by feeding the input image ğ‘¥ first into the backbone ğ‘“ ğ‘ and then into the emotion feature generator ğ‘” ğ‘’ . Also, ğ‘¦ ğ‘’ is the ground-truth emotion label for ğ‘¥, and ğ‘ ğ‘ is the number of levels in the confounder. Intuitively, in Eq. (  3 ), ğ‘‘ ğ‘’ is optimized to predict the ground truth emotion label given the generated emotion feature ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)) as input, so as to ensure that ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)) contains the correct emotion information. Here, ğ‘” ğ‘’ is optimized to encourage the softmax output of ğ‘‘ ğ‘ be equally distributed among all confounder levels, so as to ensure that ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)) contains no context information.\n\nTo obtain clean context features, we adopt a similar optimization approach with the following objective function:\n\nwhere ğ‘‘ ğœƒ ğ‘ and ğ‘” ğœƒ ğ‘ are the trainable parameters of the context discriminator ğ‘‘ ğ‘ and the context generator ğ‘” ğ‘ , respectively; ğ‘¦ ğ‘ is the confounder label, and ğ‘ ğ‘’ is the number of emotion classes.\n\nTo ensure that the separated features fall within reasonable domains, IERN should be capable of reconstructing the base feature ğ‘“ ğ‘ (ğ‘¥), given the emotion features and the context features as input. Thus, a feature reconstruction loss function is added as\n\nwhere ğ‘” ğœƒ ğ‘Ÿ is the trainable parameters of the feature reconstruction network ğ‘” ğ‘Ÿ .\n\nConfounder Builder. The purpose of the confounder builder is to combine each emotion feature with different context features so as to avoid the bias towards the observed context strata. To limit the diversity of the context features, we propose to use the center of all context features within each specific stratum as a confounder feature. This is reasonable since context features are usually similar within the same stratum while different across different strata. Essentially, a confounder feature represents the general concept of a specific stratum. So we adopt the center loss  [46]  to learn confounder features with the objective function as\n\nwhere C ğ‘¦ ğ‘ denotes the learned confounder feature of the ğ‘¦ ğ‘ -th stratum of the confounder, and C = {C 1 , C 2 , ..., C ğ‘ ğ‘ }, with all being learnable parameters.\n\nClassifier. As mentioned in Section 3.2, the backdoor adjustment aims to weigh the causal effect of each stratum with its prevalence in the population, i.e., ğ‘ƒ (ğ· = ğ‘‘), instead of ğ‘ƒ (ğ· = ğ‘‘ |ğ‘‹ ). To ensure fairness, our deconfounded classifier is set such that for any emotion feature, it is present in all confounder strata equally, i.e., ğ‘ƒ (ğ· = ğ‘‘) = 1/ğ‘ ğ‘ , via the following training objective:\n\nwhere ğ‘“ ğœƒ ğ‘ denotes the trainable parameters of the classification network ğ‘“ ğ‘ . Given a separated emotion feature ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)), IERN reuses the reconstruction network ğ‘” ğ‘Ÿ to combine it with each learned confounder feature C ğ‘– , so as to ensure that for any emotion, the classifier can be challenged with that emotion existing in every stratum of the confounder equally, thus avoiding bias. In other words, ğ‘ƒ (ğ‘Œ |ğ‘‹, ğ· = ğ‘‘) is obtained by ğ‘“ ğ‘ (ğ‘” ğ‘Ÿ (ğ‘” ğ‘’ (ğ‘“ ğ‘ (ğ‘¥)), C ğ‘– )), while by forwarding ğ‘ ğ‘ times and taking the average, we have ğ‘ƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğ‘‹ )), where ğ‘ƒ (ğ· = ğ‘‘) = 1/ğ‘ ğ‘ .\n\nTraining Phase. In general, IERN is trained with the weighted combination of the above loss functions. The overall objective is\n\nwhere Freeze ğ‘‘ ğœƒ ğ‘’ and ğ‘‘ ğœƒ ğ‘ , backward ğ‘” ğ‘’ using 2nd term of L ğ‘’ and L ğ‘Ÿ ; backward ğ‘” ğ‘ using 2nd term of L ğ‘ , L ğ‘Ÿ and L CB ; backward ğ‘” ğ‘Ÿ using L ğ‘Ÿ ; backward C using L CB 5:\n\nBackward ğ‘“ ğ‘ , ğ‘” ğ‘’ and ğ‘“ ğ‘ using L Cls Testing Phase. Given a test image, IERN will predict its emotion by going through only the top branch (see Fig.  3 ), which is\n\nwhere ğœ denotes the softmax function, and C ğ‘– is the confounder feature learned in the training phase. Following the backdoor adjustment theorem, to ensure the prediction of emotion is not biased towards specific strata at test time, we need to combine the emotion feature with each confounder stratum feature and take an average.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We conducted experiments on both facial expression recognition (FER) (Section 4.1) and image emotion recognition (IER) (Section 4.2).\n\nFor the former, we designed a challenging yet practical out-ofdistribution (o.o.d. ) test configuration through mixing different benchmarks, motivated by the fact that in the real world, images provided to practical recognition systems are taken by different people with different preferences, cameras and geographical locations, and this distribution cannot be controlled  [37] . And we conduct ablation study and compare to related SOTA, including DACL  [13]  and NWGM-based methods  [12] , using the introduced configurations.\n\nAs for the latter, IERN is compared, under the challenging crossdataset setting, to Curriculum Learning  [29] , which is the SOTA for unbiased image emotion recognition.\n\nImplementation details. To maintain a fair comparison, ResNet-50  [15]  was chosen as the backbone ğ‘“ ğ‘ following the setting of  [29] . While ğ‘” ğ‘’ , ğ‘” ğ‘ and ğ‘” ğ‘Ÿ are constructed by using the residual block  [15]  as building blocks, ğ‘‘ ğ‘’ and ğ‘‘ ğ‘ are composed of convolution layers, and ğ‘“ ğ‘ is composed of fully connected layers (see Section A.1 for more details). Note that other backbones or building blocks can also be adopted, as long as they follow the framework depicted in Fig.  3 . The hyper-parameters ğœ† 1 and ğœ† 3 in Eq. (  8 ) were set as default to ğœ† 1 =ğœ† 3 =1, while ğœ† 2 =5Ã—10 -4 were set mainly to balance the loss value magnitudes (see Section B for more details). We initialized ğ‘“ ğ‘ with ImageNet  [11]  pretrained weights, while other components were randomly initialized. An Adam optimizer was used with a learning rate of 2Ã—10 -4 , decayed using the standard warm-up strategy. All models were trained till converged before testing. Specifically, in the mix-dataset experiments (Section 4.1), IERN were trained for 80 epochs, while in the cross-dataset experiments (Section 4.2), IERN were trained for 140 epochs. All test sets remain untouched during training. We implemented IERN with PyTorch  [31] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Mix-Dataset Experiments For Fer",
      "text": "Dataset settings. The facial expression experiments were conducted on three lab-controlled benchmarks, including CK+  [26] , MMI  [42] , and Oulu-CASIA  [57] . In all the datasets, we only selected those sequences that contain frontal faces and are labeled with six basic expression labels, i.e., anger, disgust, fear, happiness, sadness and surprise, resulting in CK+ with 327 sequences of 118 subjects, MMI with 208 sequences of 32 subjects, and Oulu-CASIA with 480 sequences of 80 objects. As a general approach  [21] , three peak frames were extracted from each sequence for further experiments.\n\nTo ascertain the effectiveness of IERN in dealing with dataset bias, practical o.o.d. tests were designed by mixing the three datasets in a three-fold cross-validation setting. With reference to Tab. 1: in  the first fold, image sets denoted '1', e.g., all anger images from CK+, were included in the test set, while the other images were used to construct the training set. Likewise, the second fold comprises a test set of the '2' image sets, and so on. Here we chose to apply a sequence-independent setting rather than a person-independent one, the reason is that using the latter will make the data very unbalanced across different folds, since the numbers of emotions vary across different people on CK+ and MMI. The performance was measured in terms of classification accuracy, averaged across all three folds (% is omitted in all result tables). We point out that under such a setting, the dataset label is the confounder label, i.e., ğ‘¦ ğ‘ âˆˆ {CK+, MMI, Oulu-CASIA}. Note that we did not specify which parts of the dataset features were biased, but treated all features that can identify each specific dataset jointly as the confounder. This is reasonable since dataset bias in practice can be very complex and it is often hard to identify different bias factors.\n\nAblation study of IERN. We compared IERN to two variants: 1) Baseline, which is the backbone of IERN, i.e., ResNet-50. 2) Disentanglement, which directly feeds the output of ğ‘” ğ‘’ into ğ‘“ ğ‘ , bypassing the Confounder Builder. The results are shown in Tab. 2.\n\nCompared to Baseline, Disentanglement saw a 3.38% improvement in terms of average accuracy. This is because Disentanglement can distill the expected emotion feature whilst dispelling the undesirable confounding feature, so that the classifier can be better learned. This finding has also been explored and verified recently by an existing approach that dealt with biased facial attribute classification  [2] . IERN improved on Disentanglement by 6.55%. From a causality point of view, IERN invokes causal intervention, so that it can address the bias better as previously discussed. From a deep learning point of view, IERN can be seen as applying feature-level data augmentation. It combines emotion features with all kinds of confounder features to provide coverage even for combinations missing from training data. So it encounters greater data diversity and is thus more robust than pure disentanglement.\n\nWe also evaluated the related SOTA, DACL  [13] , on the designed setting by training and testing with the released codes. As shown in Tab. 2, IERN outperformed DACL with a healthy margin. Besides, the performance of DACL is close to Disentanglement, since they share similar underlying principles as mentioned in Section 2.\n\nConfounder features are correctly modeled. We plotted the extracted context features ğ‘” ğ‘ (ğ‘“ ğ‘ (ğ‘¥)) and confounder center features C using t-SNE. As seen in the left part of Fig.  4 , context features were grouped and separated, while confounder features were found near the center of each context feature distribution as expected. Therefore, by using the Confounder Builder, the feature of each stratum of the confounder is correctly modeled, allowing the following do intervention to be applied correctly.\n\nDataset is the confounder. The right part of Fig.  4  depicts the accuracy confusion matrix for ğ‘‘ ğ‘ , given the extracted context feature ğ‘” ğ‘ (ğ‘“ ğ‘ (ğ‘¥)) as input. We can see that the network achieved over 90% accuracy in predicting the confounder / dataset label, which is consistent with the finding of the Name That Dataset Game conducted in both  [41]  and  [29] . Compared with the emotion prediction results shown in Tab. 2, it is clear that the network is better at learning to recognize the cognitive level features (dataset characteristics) than to recognize the affective level features (image emotion). Without intervention, the network would thus be biased by the more easily learned dataset features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iern Vs Baseline In Detail.",
      "text": "To get a closer view of how IERN outperforms Baseline in dealing with dataset bias, the accuracy confusion matrices of both models on the third fold setting are shown in Fig.  5 . Compared to IERN, the results of Baseline were biased by the training data. For example, fear images in the test  set are from CK+ (see Tab. 1 number 3), Baseline predicted over 50% images as anger, disgust, happiness or sadness, which are all related to CK+ in the training set, while IERN alleviated the bias and improved the performance by near 20%.\n\nLimitation of using dataset as the confounder. We noticed that the accuracy results of anger for both Baseline and IERN are 0 as shown in Fig.  5 . The reason is that the manifestation of anger in MMI is inconsistent with that in CK+ and Oulu-CASIA, as depicted in Fig.  6 , where the mouth region in MMI (consistently opened within MMI) has a very different shape compared to the other two datasets. Since in the third fold, IERN was trained with anger images from only CK+ and Oulu-CASIA, it failed to recognize anger images in MMI. IERN was rather intended to solve biases caused by dataset characteristics that are consistently shared across the whole dataset, e.g., images in MMI have a blue background, CK+ contains mainly gray images, images in Oulu-CASIA are blurry.\n\nThe manifestation differences also make the Disentanglement perform much worse than Baseline and IERN in the anger expression as shown in Tab. 2. On the first and second folds, anger faces in the training set are mixed with images from MMI and another dataset. Under such settings, Feature Disentanglement failed to disentangle emotion and context features due to the inconsistent manifestation, resulting in wrong emotion features. Unlike Disentanglement that predicted emotion labels purely based on the extracted emotion features, our IERN still combined emotion with context features in the following steps before applying classification, allowing it to overcome the flaws to some extend.\n\nRe-sampling is essentially intervention. The most widely adopted method in handling dataset bias is to balance the training data through re-sampling. However, in our o.o.d. test setting, the training data does not contain any image that shares the same attribute as those in the test set, which makes it impossible to directly use resampling.\n\nIn fact, re-sampling can be considered as a vanilla form of intervention, since it also aims to solve the bias through balancing the data across all strata of the confounder. For better comparison between IERN and re-sampling, we next modified the setting by moving 10% of images (sequence-independent) from the test set into the training set for each attribute in each fold, so that re-sampling can be applied. The performances are reported in Tab. 3.\n\nCompared to Baseline, re-sampling achieved a higher performance with 3.04% improvement. The reason is that re-sampling is approximately intervention, so it can help alleviate the bias and improve the robustness of the model. However, IERN significantly outperformed re-sampling by 10.12%. This is because by modeling unobserved data using confounder features, IERN can improve not only the balance but also the diversity of training data, while re-sampling does not introduce new data.\n\nComparison to NWGM-based method. We also compared IERN to the NWGM-based method  [12] , given that both methods are based on backdoor adjustment. The confounder features in NWGM are built as a predefined dictionary, by grouping images in the training set according to confounder labels and averaging the extracted features of each group  [12, 51] .\n\nUsing the same modified setting as the re-sampling experiment, the results of the NWGM method are shown in Tab. 3. Although both re-sampling and NWGM are approximate interventions, NWGM outperformed re-sampling by 2.04%, suggesting that it is better to model confounders explicitly in the network. Besides, IERN outperformed NWGM by 8.08%. This is because IERN conducts real intervention by modeling confounder features as trainable parameters, while the pre-computed confounders in NWGM might not be sufficiently representative (see Section C for more details).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Cross-Dataset Experiments For Ier",
      "text": "Dataset settings. Experiments related to image emotion were conducted on three in-the-wild benchmarks, including Deep Sentiment  [52] , Emotion-6  [29]  and WEBEmo  [29] . All datasets are based on images collected from the internet, e.g., Flickr, Google. Deep Sentiment contains 1269 images and Emotion-6 has 8350 images, both of which were manually labeled by several human subjects. WEBEmo has around 268,000 images, which were automatically labelled by query keywords. Following  [29] , we report results based on the binary emotion labels, i.e., positive and negative.\n\nWe followed the same configurations as  [29] , which mainly used 80% of images for training and the rest for testing. According to the analysis given in  [29] , the image scene is the cause of the bias, and is thus the confounder. Places365-CNNs  [59]  was used to detect scene categories, which contains over 300 classes. To improve error tolerance and reduce computation, we further developed a two-step Table  4 : Cross-dataset validation for image emotion recognition. Values in cells are the results of Curriculum Learning  [29] , and values inside brackets are the improvements of IERN. \"Self Test\" means the self-validation (data from the same distribution) experiments, and \"Mean Others\" means the average of cross-datasets (data from different distributions) validations  approach to build confounder labels based on the predicted scene labels. Specifically, scene classes were first grouped into around 30 clusters using k-means, based on their semantic vectors given by GloVe  [34] .Secondly, within each dataset, the clusters were sorted by their importance, which is defined as the product of the class prior and the conditional entropy of emotion, i.e., ğ¼ (ğ‘) = ğ‘ƒ (ğ‘) â€¢ ğ‘’ âˆˆ {ğ‘,ğ‘› } ğ‘ƒ (ğ‘’ |ğ‘) log ğ‘ƒ (ğ‘’ |ğ‘), where ğ‘ refers to the cluster label, and ğ‘’ denotes the binary emotion label, with ğ‘ being positive and ğ‘› being negative. The underlying assumption is that if a cluster has more images and more biases, it should be considered more important. The 8 most important clusters were selected as the confounder strata, while other clusters were grouped into indoor or outdoor, resulting in 10 strata of the confounder for each dataset.\n\nComparison with state-of-the-art on cross-datasets. In order to showcase how dataset bias is alleviated,  [29]  designed a crossdatasets validation setting. Following their setting, we also trained IERN on the training set of one dataset to convergence, and directly tested it on test sets of all three datasets. Tab. 4 shows the results on all datasets, where the numbers in the cells are the accuracy results of the Curriculum Learning method  [29] , while the numbers inside the brackets are the improvements achieved by IERN.\n\nCompared with Curriculum Learning, IERN saw improvements in all settings. The reason is this: while Curriculum Learning aims to solve dataset bias through multi-stage fine tuning, IERN explicitly identifies and models the confounders that caused the bias. This allows intervention to be directly applied to balance emotion data across the confounders, which leads to better performance.\n\nWe noticed that IERN had a lower improvement when trained on WEBEmo compared to training on the other two datasets, which is mainly because WEBEmo is already well-balanced. WEBEmo was proposed in  [29]  as a solution to address the bias problem, with emotion data fairly distributed across scenes. To verify, we plot the emotion distribution within each scene stratum for both training and test sets of all datasets. As shown in Fig.  7 , compared with Deep Sentiment and Emotion-6, WEBEmo has better-balanced emotion distributions in many scenes.\n\nWe also found that IERN gained less improvement on the selfvalidation experiments than on cross-dataset ones, especially on Deep Sentiment and Emotion-6. The reason is that self-validation (data from the same distribution) might not be affected by bias  [37] . As shown in Fig.  7 , within the same dataset, the training and test set have similar bias in the distribution. Thus, although Curriculum Learning might learn biased features during training, it may still achieve good performance on a test set that has the same biases.\n\nConversely, in the cases of cross-datasets (data from different distributions) validation, this is no longer true. In particular for many scenes, the biases in Deep Sentiment and Emotion-6 are in opposite directions, e.g., in beauty_salon, the emotion of the former tends to be positive, while for the latter it tends to be negative. So without proper handling, Curriculum Learning may suffer from opposing biases when trained on one dataset and tested on another, while IERN can remove the bias in training via re-balancing, thus achieving higher performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We have studied the dataset bias problem in visual emotion recognition from a causality perspective. We have proposed a novel end-toend trainable framework, IERN, to model the backdoor adjustment theorem of causal intervention in order to achieve unbiased emotion recognition. Extensive experiments on two visual emotion recognition tasks have shown the effectiveness of IERN and its individual components. Future works include applying our solution to other vision tasks and considering more complex confounders, e.g., use gender / ethnicity as confounders, automatically learn the number of confounders by embedding clustering algorithms.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A Technical Details Of Iern A.1 Network Architecture",
      "text": "Table  5  provides the detailed structure of our implementation of the Interventional Emotion Recognition Network (IERN). Function name from the PyTorch 'torch.nn' package is used to denote the layer, e.g., 'Conv2d' refers to 'torch.nn.Conv2d', the 2D convolution layer. Note that although emotion generator ğ‘” ğ‘’ (emotion discriminator ğ‘‘ ğ‘’ ) and context generator ğ‘” ğ‘ (context discriminator ğ‘‘ ğ‘ ) have the same structure, they are declared separately, and their trainable parameters are not shared.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A.2 Implementation",
      "text": "To make clear how different components are optimized sequentially, Algorithm 1 is detailed with PyTorch-like pseudo codes, shown as below.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Implementation",
      "text": "To make clear how different components are optimized sequentially, Algorithm 1 is detailed with PyTorch-like pseudo codes, shown as below.\n\n1 # -----------------------DECLARE OPTIMIZERS -----------------------    ------------------------------END ------------------------------ B HYPER-PARAMETERS SEARCHING\n\nIn our experiments, we leveraged an off-the-shelf implementation of center loss 1 for L CB , and found that the magnitude of L CB is much larger than other loss functions. Therefore, we conducted several experiments on IERN to find the suitable ğœ† 2 (see Eq. (  8 )), by setting ğœ† 1 = ğœ† 3 = 1 and fixing all other training configurations. Average Accuracy (%)\n\nFigure  8 : Hyper-parameters searching on ğœ† 2 (Eq. (  8 ) of the main manuscript). We set ğœ† 2 = 5 Ã— 10 -4 in our final model.\n\nAs shown in Fig.  8 , IERN achieved the best performances when ğœ† 2 = 5 Ã— 10 -4 , therefore we used this setting in all following experiments. When ğœ† 2 was set to a smaller value, e.g., 10 -6 , 10 -4 , there is a small drop in the performances. Although confounders were not well learnt in such settings, the final classifier can still be trained sufficiently by L Cls , and the performances decrease were mainly caused by the noisy confounders. However, if ğœ† 2 was set to a larger value, e.g., 10 -3 , 10 -2 , 1, performances of IERN decreased dramatically. This is because that IERN failed to optimize Feature Disentanglement and Classifier when L CB was way too large, resulting in an unexpected collapse of the full recognition framework.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C Advantages Of Iern Over Nwgm",
      "text": "We give a brief introduction of how existing methods  [12, 51]  use the Normalized Weighted Geometric Mean (NWGM)  [49]  to approximate the causal intervention. As detailed in Section 3.\n\nwhere ğ‘‹ and ğ· are the intended feature and confounder feature, respectively. Here ğ‘ƒ (ğ‘Œ |ğ‘‹, ğ‘‘) is normally obtained from the predicted logits of a classification network, i.e., ğœ (ğ‘“ (ğ‘‹, ğ‘‘)), where ğœ denotes the softmax function. Thus, Eq. (  10 ) can be written as\n\nNWGM is then applied to move the outer expectation into the softmax function, that is\n\nSupposed that the operation of combining ğ‘‹ and ğ· is linear, then Eq. (  12 ) can be further deduced as\n\nwhere ğ‘Š 1 and ğ‘Š 2 both refer to trainable parameters of the fully connected layers. And as a general approach  [12, 51] , E ğ‘‘ [ğ‘”(ğ‘‘)] is modelled as a attention layer, where the query is set to the base feature and the key is set to a predefined confounder dictionary. Although existing works have demonstrated the effectiveness of the NWGM-based method, at least three shortcomings can be found.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Hyper-Parameters Searching",
      "text": "In our experiments, we leveraged an off-the-shelf implementation of center loss 1 for L CB , and found that the magnitude of L CB is much larger than other loss functions. Therefore, we conducted several experiments on IERN to find the suitable ğœ† 2 (see Eq. (  8 )), by setting ğœ† 1 = ğœ† 3 = 1 and fixing all other training configurations. Average Accuracy (%)\n\nFigure  8 : Hyper-parameters searching on ğœ† 2 (Eq. (  8 ) of the main manuscript). We set ğœ† 2 = 5 Ã— 10 -4 in our final model.\n\nAs shown in Fig.  8 , IERN achieved the best performances when ğœ† 2 = 5 Ã— 10 -4 , therefore we used this setting in all following experiments. When ğœ† 2 was set to a smaller value, e.g., 10 -6 , 10 -4 , there is a small drop in the performances. Although confounders were not well learnt in such settings, the final classifier can still be trained sufficiently by L Cls , and the performances decrease were mainly caused by the noisy confounders. However, if ğœ† 2 was set to a larger value, e.g., 10 -3 , 10 -2 , 1, performances of IERN decreased dramatically. This is because that IERN failed to optimize Feature Disentanglement and Classifier when L CB was way too large, resulting in an unexpected collapse of the full recognition framework.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C Advantages Of Iern Over Nwgm",
      "text": "We give a brief introduction of how existing methods  [12, 51]  use the Normalized Weighted Geometric Mean (NWGM)  [49]  to approximate the causal intervention. As detailed in Section 3.\n\nwhere ğ‘‹ and ğ· are the intended feature and confounder feature, respectively. Here ğ‘ƒ (ğ‘Œ |ğ‘‹, ğ‘‘) is normally obtained from the predicted logits of a classification network, i.e., ğœ (ğ‘“ (ğ‘‹, ğ‘‘)), where ğœ denotes the softmax function. Thus, Eq. (  10 ) can be written as\n\nNWGM is then applied to move the outer expectation into the softmax function, that is\n\nSupposed that the operation of combining ğ‘‹ and ğ· is linear, then Eq. (  12 ) can be further deduced as\n\nwhere ğ‘Š 1 and ğ‘Š 2 both refer to trainable parameters of the fully connected layers. And as a general approach  [12, 51] , E ğ‘‘ [ğ‘”(ğ‘‘)] is modelled as a attention layer, where the query is set to the base feature and the key is set to a predefined confounder dictionary. Although existing works have demonstrated the effectiveness of the NWGM-based method, at least three shortcomings can be found.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A toy experiment to illustrate the dataset bias prob-",
      "page": 1
    },
    {
      "caption": "Figure 1: From a facial expression dataset,",
      "page": 2
    },
    {
      "caption": "Figure 1: ), whose underlying principle is to deconfound the emotion",
      "page": 2
    },
    {
      "caption": "Figure 2: shows a SCM in our context constructed",
      "page": 3
    },
    {
      "caption": "Figure 1: , instead of identifying the correct anger image",
      "page": 3
    },
    {
      "caption": "Figure 2: Left: structural casual model (SCM) for visual emo-",
      "page": 3
    },
    {
      "caption": "Figure 2: Right). Specifically, instead of learning the correlation ğ‘ƒ(ğ‘Œ|ğ‘‹), we",
      "page": 3
    },
    {
      "caption": "Figure 3: gives an overview of the IERN,",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of Interventional Emotion Recognition Network (IERN). ğ‘“ğ‘,ğ‘”ğ‘’,ğ‘‘ğ‘’,ğ‘”ğ‘,ğ‘‘ğ‘,ğ‘”ğ‘Ÿand ğ‘“ğ‘refer to backbone, emotion",
      "page": 4
    },
    {
      "caption": "Figure 3: ), which is",
      "page": 5
    },
    {
      "caption": "Figure 3: The hyper-parameters ğœ†1 and ğœ†3 in Eq. (8) were set as default to",
      "page": 5
    },
    {
      "caption": "Figure 4: Left: t-SNE maps of context and confounder fea-",
      "page": 6
    },
    {
      "caption": "Figure 5: Confusion matrices of accuracy for both Baseline",
      "page": 6
    },
    {
      "caption": "Figure 4: , context features",
      "page": 6
    },
    {
      "caption": "Figure 4: depicts the ac-",
      "page": 6
    },
    {
      "caption": "Figure 5: Compared to IERN, the results of Baseline were",
      "page": 6
    },
    {
      "caption": "Figure 6: Two samples of anger from each dataset. See the",
      "page": 7
    },
    {
      "caption": "Figure 5: The reason is that the manifestation of anger in",
      "page": 7
    },
    {
      "caption": "Figure 6: , where the mouth region in MMI (consistently opened",
      "page": 7
    },
    {
      "caption": "Figure 7: Emotion distribution within each scene stratum",
      "page": 8
    },
    {
      "caption": "Figure 7: , compared with Deep",
      "page": 8
    },
    {
      "caption": "Figure 7: , within the same dataset, the training and test",
      "page": 8
    },
    {
      "caption": "Figure 8: Hyper-parameters searching on ğœ†2 (Eq. (8) of the",
      "page": 11
    },
    {
      "caption": "Figure 8: , IERN achieved the best performances when",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Backbone"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Feature Disentanglement"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Confounder Builder"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Classifier"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Three-fold cross-validation setting. For each fold,",
      "data": [
        {
          "Column_1": "CK+",
          "AN": "1",
          "DI": "2",
          "FE": "3",
          "HA": "1",
          "SA": "2",
          "SU": "3"
        },
        {
          "Column_1": "MMI",
          "AN": "3",
          "DI": "1",
          "FE": "2",
          "HA": "3",
          "SA": "1",
          "SU": "2"
        },
        {
          "Column_1": "Oulu-CASIA",
          "AN": "2",
          "DI": "3",
          "FE": "1",
          "HA": "2",
          "SA": "3",
          "SU": "1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "45.0\n)%(ycaruccAegarevA\n45.0 42.5\n)%(\n42.5 40.0\nycaruccA\n40.0 37.5\n37.5 35.0\n35.0 egarevA 32.5\n32.5\n30.0\n30.0\n1e-6 1e-4 5e-4 1e-3 1e-2 1.\nValueofÎ»\n1e-6 1e-4 5e-4 1e-3 1e-2 1. 2": ")%(ycaruccAegarevA\n45.0 42.5\n)%(\n42.5 40.0\nycaruccA\n40.0 37.5\n37.5 35.0\n35.0 egarevA 32.5\n32.5\n30.0\n30.0\n1e-6 1e-4 5e-4 1e-3 1e-2 1\nValueofÎ»\n1e-6 1e-4 5e-4 1e-3 1e-2 1. 2",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": ""
        },
        {
          "45.0\n)%(ycaruccAegarevA\n45.0 42.5\n)%(\n42.5 40.0\nycaruccA\n40.0 37.5\n37.5 35.0\n35.0 egarevA 32.5\n32.5\n30.0\n30.0\n1e-6 1e-4 5e-4 1e-3 1e-2 1.\nValueofÎ»\n1e-6 1e-4 5e-4 1e-3 1e-2 1. 2": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "1e-6",
          "Column_5": "",
          "Column_6": "1e-4",
          "Column_7": "",
          "Column_8": "5e-4",
          "Column_9": "",
          "Column_10": "1e-3",
          "Column_11": "",
          "Column_12": "1e-2",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": ""
        },
        {
          "45.0\n)%(ycaruccAegarevA\n45.0 42.5\n)%(\n42.5 40.0\nycaruccA\n40.0 37.5\n37.5 35.0\n35.0 egarevA 32.5\n32.5\n30.0\n30.0\n1e-6 1e-4 5e-4 1e-3 1e-2 1.\nValueofÎ»\n1e-6 1e-4 5e-4 1e-3 1e-2 1. 2": "Value of Î»\n2",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recognizing emotions from abstract paintings using non-linear matrix completion",
      "authors": [
        "Xavier Alameda-Pineda",
        "Elisa Ricci",
        "Yan Yan",
        "Nicu Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "2",
      "title": "Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings",
      "authors": [
        "Mohsan Alvi",
        "Andrew Zisserman",
        "Christoffer NellÃ¥ker"
      ],
      "year": "2018",
      "venue": "Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "3",
      "title": "The dropout learning algorithm",
      "authors": [
        "Pierre Baldi",
        "Peter Sadowski"
      ],
      "year": "2014",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms",
      "authors": [
        "Yoshua Bengio",
        "Tristan Deleu",
        "Nasim Rahaman",
        "Rosemary Ke",
        "SÃ©bastien Lachapelle",
        "Olexa Bilaniuk",
        "Anirudh Goyal",
        "Christopher Pal"
      ],
      "year": "2020",
      "venue": "Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "5",
      "title": "Counterfactuals uncover the modular structure of deep generative models",
      "authors": [
        "Michel Besserve",
        "Arash Mehrjou",
        "RÃ©my Sun",
        "Bernhard SchÃ¶lkopf"
      ],
      "year": "2020",
      "venue": "Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "6",
      "title": "Visual causal feature learning",
      "authors": [
        "Krzysztof Chalupka",
        "Pietro Perona",
        "Frederick Eberhardt"
      ],
      "year": "2015",
      "venue": "Conf. Uncertainty Artificial Intell"
    },
    {
      "citation_id": "7",
      "title": "Label Distribution Learning on Auxiliary Label Space Graphs for Facial Expression Recognition",
      "authors": [
        "Shikai Chen",
        "Jianfeng Wang",
        "Yuedong Chen",
        "Zhongchao Shi",
        "Xin Geng",
        "Yong Rui"
      ],
      "year": "2020",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Computer Vision Foundation / IEEE"
    },
    {
      "citation_id": "8",
      "title": "GeoConv: Geodesic guided convolution for facial action unit recognition",
      "authors": [
        "Yuedong Chen",
        "Guoxian Song",
        "Zhiwen Shao",
        "Jianfei Cai",
        "Tat-Jen Cham",
        "Jianmin Zheng"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Facial motion prior networks for facial expression recognition",
      "authors": [
        "Yuedong Chen",
        "Jianfeng Wang",
        "Shikai Chen",
        "Zhongchao Shi",
        "Jianfei Cai"
      ],
      "year": "2019",
      "venue": "In IEEE Vis. Comput. Image Process. IEEE"
    },
    {
      "citation_id": "10",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu SimÃ³n",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "11",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "12",
      "title": "Causal Intervention for Weakly Supervised Semantic Segmentation",
      "authors": [
        "Zhang Dong",
        "Zhang Hanwang",
        "Tang Jinhui",
        "Hua Xiansheng",
        "Sun Qianru"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "Amir Hossein",
        "Xiaojun Qi"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Winter Conf. Applicat. Comput. Vis"
    },
    {
      "citation_id": "14",
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": [
        "Robert Geirhos",
        "JÃ¶rn-Henrik Jacobsen",
        "Claudio Michaelis",
        "Richard Zemel",
        "Wieland Brendel",
        "Matthias Bethge",
        "Felix Wichmann"
      ],
      "year": "2020",
      "venue": "Shortcut Learning in Deep Neural Networks",
      "arxiv": "arXiv:2004.07780"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "16",
      "title": "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model",
      "authors": [
        "Xinya Ji",
        "Hang Zhou",
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Wayne Wu",
        "Feng Xu",
        "Xun Cao"
      ],
      "year": "2022",
      "venue": "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model"
    },
    {
      "citation_id": "17",
      "title": "Facial emotion distribution learning by exploiting low-rank label correlations locally",
      "authors": [
        "Xiuyi Jia",
        "Xiang Zheng",
        "Weiwei Li",
        "Changqing Zhang",
        "Zechao Li"
      ],
      "year": "2019",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "18",
      "title": "Predicting viewer perceived emotions in animated GIFs",
      "authors": [
        "Brendan Jou",
        "Subhabrata Bhattacharya",
        "Shih-Fu Chang"
      ],
      "year": "2014",
      "venue": "ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "Heechul Jung",
        "Sihaeng Lee",
        "Junho Yim",
        "Sunjeong Park",
        "Junmo Kim"
      ],
      "year": "2015",
      "venue": "Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "20",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "21",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "22",
      "title": "JDMAN: Joint Discriminative and Mutual Adaptation Networks for Cross-Domain Facial Expression Recognition",
      "authors": [
        "Yingjian Li",
        "Yingnan Gao",
        "Bingzhi Chen",
        "Zheng Zhang",
        "Lei Zhu",
        "Guangming Lu"
      ],
      "year": "2021",
      "venue": "ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Orthogonalization-guided feature fusion network for multimodal 2D+ 3D facial expression recognition",
      "authors": [
        "Shisong Lin",
        "Mengchao Bai",
        "Feng Liu",
        "Linlin Shen",
        "Yicong Zhou"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Au-aware deep networks for facial expression recognition",
      "authors": [
        "Mengyi Liu",
        "Shaoxin Li",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2013",
      "venue": "IEEE Int. Conf. Worksh. Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "25",
      "title": "Discovering causal signals in images",
      "authors": [
        "David Lopez-Paz",
        "Robert Nishihara",
        "Soumith Chintala",
        "Bernhard Scholkopf",
        "LÃ©on Bottou"
      ],
      "year": "2017",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "26",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "27",
      "title": "Domain adaptation by using causal inference to predict invariant conditional distributions",
      "authors": [
        "Sara Magliacane",
        "Tom Thijs Van Ommen",
        "Stephan Claassen",
        "Philip Bongers",
        "Joris Versteeg",
        "Mooij"
      ],
      "year": "2018",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "28",
      "title": "D 3 Net: Dual-Branch Disturbance Disentangling Network for Facial Expression Recognition",
      "authors": [
        "Rongyun Mo",
        "Yan Yan",
        "Jing-Hao Xue",
        "Si Chen",
        "Hanzi Wang"
      ],
      "year": "2021",
      "venue": "ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "Rameswar Panda",
        "Jianming Zhang",
        "Haoxiang Li",
        "Joon-Young Lee",
        "Xin Lu"
      ],
      "year": "2018",
      "venue": "Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "30",
      "title": "Learning independent causal mechanisms",
      "authors": [
        "Giambattista Parascandolo",
        "Niki Kilbertus",
        "Mateo Rojas-Carulla",
        "Bernhard SchÃ¶lkopf"
      ],
      "year": "2018",
      "venue": "Int. Conf. Mach. Learn. PMLR"
    },
    {
      "citation_id": "31",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "32",
      "title": "Models, reasoning and inference",
      "authors": [
        "Judea Pearl"
      ],
      "year": "2000",
      "venue": "Models, reasoning and inference"
    },
    {
      "citation_id": "33",
      "title": "The Book of Why: The New Science of Cause and Effect",
      "authors": [
        "Judea Pearl",
        "Dana Mackenzie"
      ],
      "year": "2018",
      "venue": "The Book of Why: The New Science of Cause and Effect"
    },
    {
      "citation_id": "34",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Elements of causal inference",
      "authors": [
        "Jonas Peters",
        "Dominik Janzing",
        "Bernhard SchÃ¶lkopf"
      ],
      "year": "2017",
      "venue": "Elements of causal inference"
    },
    {
      "citation_id": "36",
      "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
      "authors": [
        "Delian Ruan",
        "Yan Yan",
        "Shenqi Lai",
        "Zhenhua Chai",
        "Chunhua Shen",
        "Hanzi Wang"
      ],
      "year": "2021",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "37",
      "title": "Toward Causal Representation Learning",
      "authors": [
        "Bernhard SchÃ¶lkopf",
        "Francesco Locatello",
        "Stefan Bauer",
        "Nan Ke",
        "Nal Kalchbrenner",
        "Anirudh Goyal",
        "Yoshua Bengio"
      ],
      "year": "2021",
      "venue": "Toward Causal Representation Learning",
      "arxiv": "arXiv:2102.11107"
    },
    {
      "citation_id": "38",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "39",
      "title": "Deep spatialtemporal feature fusion for facial expression recognition in static images",
      "authors": [
        "Ning Sun",
        "Qi Li",
        "Ruizhi Huan",
        "Jixin Liu",
        "Guang Han"
      ],
      "year": "2019",
      "venue": "Pattern Recog. Letters"
    },
    {
      "citation_id": "40",
      "title": "Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect",
      "authors": [
        "Kaihua Tang",
        "Jianqiang Huang",
        "Hanwang Zhang"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "41",
      "title": "Unbiased look at dataset bias",
      "authors": [
        "Antonio Torralba",
        "Alexei Efros"
      ],
      "year": "2011",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "42",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "Michel Valstar",
        "Maja Pantic"
      ],
      "year": "2010",
      "venue": "Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "43",
      "title": "Mead: A large-scale audiovisual dataset for emotional talking-face generation",
      "authors": [
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Linsen Song",
        "Zhuoqian Yang",
        "Wayne Wu",
        "Chen Qian",
        "Ran He",
        "Yu Qiao",
        "Chen Loy"
      ],
      "year": "2020",
      "venue": "Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "44",
      "title": "Modeling emotion influence in image social networks",
      "authors": [
        "Xiaohui Wang",
        "Jia Jia",
        "Jie Tang",
        "Boya Wu",
        "Lianhong Cai",
        "Lexing Xie"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "45",
      "title": "Towards fairness in visual recognition: Effective strategies for bias mitigation",
      "authors": [
        "Zeyu Wang",
        "Klint Qinami",
        "Christos Ioannis",
        "Kyle Karakozis",
        "Prem Genova",
        "Kenji Nair",
        "Olga Hata",
        "Russakovsky"
      ],
      "year": "2020",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "46",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Yandong Wen",
        "Kaipeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "47",
      "title": "Inferring emotional tags from social images with user demographics",
      "authors": [
        "Boya Wu",
        "Jia Jia",
        "Yang Yang",
        "Peijun Zhao",
        "Jie Tang",
        "Qi Tian"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Facial expression recognition using hierarchical features with deep comprehensive multipatches aggregation convolutional neural networks",
      "authors": [
        "Siyue Xie",
        "Haifeng Hu"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov",
        "Rich Zemel",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "50",
      "title": "Joint deep learning of facial expression synthesis and recognition",
      "authors": [
        "Yan Yan",
        "Ying Huang",
        "Si Chen",
        "Chunhua Shen",
        "Hanzi Wang"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Deconfounded image captioning: A causal retrospect",
      "authors": [
        "Xu Yang",
        "Hanwang Zhang",
        "Jianfei Cai"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "52",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2015",
      "venue": "AAAI"
    },
    {
      "citation_id": "53",
      "title": "Building a large scale dataset for image emotion recognition: the fine print and the benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "AAAI"
    },
    {
      "citation_id": "54",
      "title": "Interventional Few-Shot Learning",
      "authors": [
        "Zhongqi Yue",
        "Hanwang Zhang",
        "Qianru Sun",
        "Xian-Sheng Hua"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "55",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "Kaihao Zhang",
        "Yongzhen Huang",
        "Yong Du",
        "Liang Wang"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "56",
      "title": "A deep neural network-driven feature learning method for multi-view facial expression recognition",
      "authors": [
        "Tong Zhang",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Jingwei Yan",
        "Keyu Yan"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "57",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "Guoying Zhao",
        "Xiaohua Huang",
        "Matti Taini",
        "Stan Li",
        "Matti PietikÃ¤inen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "58",
      "title": "Exploring principles-of-art features for image emotion recognition",
      "authors": [
        "Sicheng Zhao",
        "Yue Gao",
        "Xiaolei Jiang",
        "Hongxun Yao",
        "Tat-Seng Chua",
        "Xiaoshuai Sun"
      ],
      "year": "2014",
      "venue": "ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "59",
      "title": "Places: A 10 million Image Database for Scene Recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "60",
      "title": "OC: the number of output channels, K: kernel size, S: stride size, P: padding size, H: height, W: width, ğ‘ ğ‘’ : the number of emotion classes, ğ‘ ğ‘ : the number of confounder strata. Note that ğ‘” ğ‘’ (or ğ‘‘ ğ‘’ ) and ğ‘” ğ‘ (or ğ‘‘ ğ‘ ) are declared separately Network Component Detailed Structure Backbone ğ‘“ ğ‘ ResNet-50",
      "venue": "Table 5: Detailed structure of all components of IERN. IC: the number of input channels"
    },
    {
      "citation_id": "61",
      "title": "Conv2d(IC2048, OC512, K1Ã—1",
      "venue": "Conv2d(IC2048, OC512, K1Ã—1"
    },
    {
      "citation_id": "62",
      "title": "Residual Block: Conv2d(IC512, OC512, K3Ã—3, S1, P1), BatchNorm2d, ReLU Residual Block: Conv2d(IC512, OC512, K3Ã—3, S1, P1), BatchNorm2d, ReLU Emotion Discriminator ğ‘‘ ğ‘’ or Context Discriminator ğ‘‘ ğ‘ Conv2d(IC512",
      "venue": "LeakyReLU Conv2d(IC256"
    },
    {
      "citation_id": "63",
      "title": "Residual Block: Conv2d(IC512, OC512, K3Ã—3, S1, P1), BatchNorm2d, ReLU Residual Block: Conv2d(IC512, OC512, K3Ã—3, S1, P1), BatchNorm2d, ReLU Residual Block: Conv2d(IC512",
      "venue": "ReLU Classifier ğ‘“ ğ‘ AdaptiveAvgPool"
    },
    {
      "citation_id": "64",
      "title": "confounder features are not sufficiently representative when constructed via a predefined / precomputed manner. Secondly, it may not be correct to assume that ğ‘‹ and ğ· are linearly combined, given that ğ· could be very complex in practice. Thirdly, NWGM is essentially an approximation and may introduce unintended noise. By efficiently embedding the backdoor adjustment theorem, our proposed IERN overcomes the aforementioned limitations. Firstly, IERN directly learns confounder features through disentanglement and leveraging the center loss function. Secondly, IERN reuses the reconstruction network to combine ğ‘‹ and ğ·, removing the restrictive assumption of the linear combination operation. Thirdly, IERN does not involve NWGM approximation, instead, it forwards the combined features ğ‘ ğ‘ times to do the real intervention",
      "authors": [
        "Firstly"
      ],
      "venue": "confounder features are not sufficiently representative when constructed via a predefined / precomputed manner. Secondly, it may not be correct to assume that ğ‘‹ and ğ· are linearly combined, given that ğ· could be very complex in practice. Thirdly, NWGM is essentially an approximation and may introduce unintended noise. By efficiently embedding the backdoor adjustment theorem, our proposed IERN overcomes the aforementioned limitations. Firstly, IERN directly learns confounder features through disentanglement and leveraging the center loss function. Secondly, IERN reuses the reconstruction network to combine ğ‘‹ and ğ·, removing the restrictive assumption of the linear combination operation. Thirdly, IERN does not involve NWGM approximation, instead, it forwards the combined features ğ‘ ğ‘ times to do the real intervention"
    }
  ]
}