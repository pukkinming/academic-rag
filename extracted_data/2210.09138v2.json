{
  "paper_id": "2210.09138v2",
  "title": "An Open-Source Benchmark Of Deep Learning Models For Audio-Visual Apparent And Self-Reported Personality Recognition",
  "published": "2022-10-17T14:40:04Z",
  "authors": [
    "Rongfan Liao",
    "Siyang Song",
    "Hatice Gunes"
  ],
  "keywords": [
    "Self-reported (true) personality recognition",
    "Apparent personality (impression) recognition",
    "Audio-visual personality computing benchmark",
    "Spatio-temporal modelling",
    "Deep Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personality determines a wide variety of human daily and working behaviours, and is crucial for understanding human internal and external states. In recent years, a large number of automatic personality computing approaches have been developed to predict either the apparent personality or self-reported personality of the subject based on non-verbal audio-visual behaviours. However, the majority of them suffer from complex and dataset-specific pre-processing steps and model training tricks. In the absence of a standardized benchmark with consistent experimental settings, it is not only impossible to fairly compare the real performances of these personality computing models but also makes them difficult to be reproduced. In this paper, we present the first reproducible audio-visual benchmarking framework to provide a fair and consistent evaluation of eight existing personality computing models (e.g., audio, visual and audio-visual) and seven standard deep learning models on both self-reported and apparent personality recognition tasks. Building upon a set of benchmarked models, we also investigate the impact of two previously-used long-term modelling strategies for summarising short-term/frame-level predictions on personality computing results. We conduct a comprehensive investigation into all the benchmarked models to demonstrate their capabilities in modelling personality traits on two publicly available datasets, audio-visual apparent personality (ChaLearn First Impression) and self-reported personality (UDIVA) datasets. The experimental results conclude: (i) apparent personality traits, inferred from facial behaviours by most benchmarked deep learning models, show more reliability than self-reported ones; (ii) visual models frequently achieved superior performances than audio models on personality recognition; (iii) non-verbal behaviours contribute differently in predicting different personality traits; and (iv) our reproduced personality computing models generally achieved worse performances than their original reported results. We make all the code and settings of this personality computing benchmark publicly available at https://github.com/liaorongfan/DeepPersonality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "P ERSONALITY defines a characteristic sets of human cog- nitive processes and emotional patterns that evolve from various biological and environmental factors  [1] , which are well associated with a wide range of human behaviours and status such as purchasing behaviours  [2] , health conditions  [3] ,  [4] , social relationships  [5] , and even criminal activities  [6] . Consequently, automatic personality computing systems have drawn a lot of attention in recent years, and have been frequently developed for real-world human behaviour understanding applications, including computer assisted tutoring systems  [7] , human resource management  [8] , job interviews  [9] , and recommendation systems  [10] . In these systems, the complex personality is usually described by trait-based models  [11] ,  [12] ,  [13] ,  [14]  which focus on modelling personality aspects that are stable over time for the target person but differ in others  [15] .\n\nExisting automatic personality computing approaches can be categorized into two types: (i) Self-reported personality recognition (SPR) that recognises the target subject's true\n\n• Rongfan Liao is with SONY China Software Center. E-mail: rongfan.liao@sony.com • Siyang Song and Hatice Gunes are with the AFAR Lab, Department of Computer Science and Technology, University of Cambridge, Cambridge, CB3 0FT, United Kingdom. E-mail: ss2796@cam.ac.uk, Hatice.Gunes@cl.cam.ac.uk ( * Corresponding Author: Siyang Song, E-mail: ss2796@cam.ac.uk)\n\nManuscript accepted for publication on January 22, 2024.\n\npersonality traits; and (ii) apparent personality recognition (APR) that predicts external human observers' impression on the target subject (also called first impression or apparent personality). The majority of these approaches were focused on APR and were evaluated on the ChaLearn First Impression dataset  [16] . They recognise apparent personality using either human visual behaviours (e.g., facial behaviours)  [17] ,  [18] ,  [19] ,  [20] ,  [21] ,  [22] ,  [23] ,  [24] ,  [24] ,  [25]  or audio-visual behaviours  [26] ,  [27] ,  [28] ,  [29] ,  [30] . More specifically, a large part of these approaches  [17] ,  [21] ,  [22] ,  [26] ,  [31] ,  [32]  attempt to infer personality traits from every single frame or short segment  [19] , and then make the clip-level personality prediction by combining all frame/segment-level predictions, where video-level personality labels were used as the frame/short segment-level labels to train models. While subjects with different personalities can behave similarly in a frame or a short video segment, models trained by such strategies would be problematic as they pair similar behaviour samples with different personality labels  [33] . Alternatively, recent studies frequently emphasized that longterm behaviours enable more reliable personality inference  [25] ,  [29] ,  [34] , as personality traits are stable over time  [15] . These approaches propose to encode a clip-level personality representation from long-term behaviours of the subject, and generally achieve superior recognition results than most frame/segment-level approaches. However existing approaches usually employ differ-ent pre-processing, post-processing and training strategies. Such differences lead to their reported results not being fairly reflected by their models' capabilities in recognizing personality traits. Meanwhile, very few deep learning-based approaches  [18] ,  [35] ,  [36] ,  [37]  have been proposed to recognise self-reported personality traits. They infer personality traits from audio  [38] , static image  [39] , video  [40] , or audiovisual clip  [41]  recorded from various scenarios, such as dyadic dialogue, self-evaluating surveys and self-interviews  [42] . In summary, there is no comprehensive study that fairly demonstrates and compares the performances of existing audio-visual personality computing models and widelyused deep learning models on both APR and SPR (Research gap 1). Moreover, only a small number of them made their codes publicly available, which are even built on different platforms, e.g., Keras  [19] ,  [20] , TensorFlow  [43] , caffe  [24] , Torch  [27] , Matlab  [21] ,  [28]  and Chainer  [44]  (listed in Table  1 ). Also, some of these codes are incomplete and unimplementable. As a result, it is difficult for other researchers to reproduce or extend most of them (Research gap 2).\n\nIn this paper, we bridge the aforementioned research gaps by introducing two main contributions: (i) we benchmark eight existing audio-visual or visual only automatic apparent personality recognition approaches as well as seven widely-used static or spatio-temporal deep learning models with standardized pre-processing, training and post-processing strategies, on both widely-used selfreported personality (i.e., UDIVA  [45] ) and apparent personality datasets (ChaLearn First Impression  [46] ). The reported results provide a fair evaluation and comparison of all these models' capabilities for both apparent personality and selfreported personality recognition tasks (addressing research gap 1); and (ii) we make the code of all benchmarked models as well as their standardized pre-processing, training and post-processing scripts publicly available to further the science of automatic personality recognition. This provides new researchers entering this field a set of strong personality computing baselines, which will facilitate their exploration of new models and will enable them to apply these to new datasets in personality computing. To the best of our knowledge, this is the first study that benchmarks and fairly evaluates the existing personality computing approaches and standard deep learning models on both SPR and APR tasks, with consistent and reproducible settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we systematically review and summarize recent audio-visual automatic apparent personality recognition (Sec. 2.1) and self-reported personality recognition (Sec. 2.2) approaches.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Automatic Apparent Personality Recognition",
      "text": "Existing approaches frequently attempt to recognize apparent personality from non-verbal facial behaviours, where a large part of them infer apparent personality only from a single facial display  [17] ,  [19] ,  [20] . Joo et al.  [48]  conduct studies on face images of 650 American politicians of white ethnicity, which extracts Histogram of oriented gradients (HOG) features as the low-level static facial representation to predict personality traits. Dhall et al.  [49]  utilize both hand-crafted and deep-learned features to describe Twitter profile facial images and infer personality traits from them, where background information was also considered. Moreno-Armendáriz et al.  [19]  first extract one portrait picture as the representation for each video, and then train a CNN to predict video-level perception. In addition, although facial videos are provided, some studies also directly infer apparent personality from static facial appearance without considering temporal dynamics. For example, Ventura et al.  [17]  use Classification Activation Map (CAM)  [50]  to investigate the relationship between facial expressions and apparent personality, which feeds each frame to a CNN to infer video-level perception traits.\n\nWhile very few studies  [43]  infer personality traits from only audio behaviours, the majority of the studies focus on audio-visual methods that frequently show enhanced performances than the corresponding single modality (e.g., visual-based or audio-based) systems. Besides some early studies  [51] ,  [52]  investigated the relationship between frame-level facial cues and apparent personality traits in the context of human-robot interactions, most existing approaches attempt to predict video-level personality traits. Zhang et al.  [26]  first choose 6 images from each video, and extend the Descriptor Aggregation Network (DAN) to deep learn facial perception features at the frame-level. Meanwhile, a linear regressor is used to process log filter bank-based clip-level audio representation. The late fusion scheme is employed to get the final prediction by averaging audio and visual predictions. Gucluturk et al.  [32]  propose a two-stream ResNet to deep learn both audio and visual personality cues at the frame-level, which are combined at fully connected (FC) layer to predict perceptions. Subramaniam et al.  [27]  divide each audio-visual clip into several short segments, where one facial frame is selected from each segment to produce a segment-level visual feature. Then, the segment-level audio-visual features are combined via a FC layer to make the apparent personality prediction. To recognize apparent personality from clip-level (long-term) audio-visual behaviours, G ürpinar et al.  [28]  employ a pretrained network to extract frame-level facial emotion features. The video-level visual representation is then obtained by computing statistics of all frame-level features while cliplevel audio feature is extracted via OpenSMILE  [53] . The final prediction is made by the weighted average of audio and visual predictions. As discussed in Sec. 1, these static or short-term behaviour-based approaches usually re-use video-level personality labels as the frame/segment-level labels during the training, which may result in problematic models.\n\nTo avoid the above problem, many recent approaches are proposed to recognize apparent personality from longterm behaviours (i.e., applying video-level behaviour representation to recognize video-level personality traits). For example, Beyan et al.  [25]  summarize a video into a set of dynamic images, and then select a small part of key dynamic images based on their spatio-temporal saliency. Then, the produced key dynamic image sequence is used as the videolevel representation for the perception prediction. Helm et al.  [20]  extract a face image sequence by down-sampling the target video, whose features is used as the video-level facial\n\nHayat et al.  [43]  TensorFlow github.com/HassanHayat08 Unimplementable (Bugs exist) G ücl üt ürk et al.  [44]  Chainer github.com/yagguc/deep impression Training code is missing / ResNet-based approach Zhang et al.  [47]  TensorFlow github.com/zishansami102/First-Impression Unimplementable (Bugs exist)\n\nZhang et al.  [24]  Caffe github.com/ZhangLeUestc/PersEmoN C++ based code Subraman et al.  [27]  Torch (Lua) github.com/InnovArul/first-impressions Torch (Lua) is out of date G ürpinar et al.  [28]  MatConv github.com/frkngrpnr/lapfi MATLAB based code Bekhouche et al.  [21]  MatConv github.com/Bekhouche/CVPR2017 MATLAB based code behaviour representation. Then, a 3D-CNN is employed to infer apparent personality from this video-level representation. G ürpınar et al.  [23]  jointly extract facial expressionrelated features from all aligned face images and the ambient information from the first frame of a down-sampled video, both of which are combined as the video-level visual representation for apparent personality prediction. Li et al.  [29]  propose to down-sample each video into 32 frames, and extract video-level visual features from the corresponding facial region sequence as well as the whole image sequence.\n\nIn addition, the long-term audio features and text features are also combined with the visual features for APR. Since the above long-term behaviour modelling approach depend on down-sample videos, Song et al.  [34]  propose a selfsupervised learning strategy to use all frames for video-level representation construction. This method trains a personspecific layer for each individual using facial behaviours of the whole video, whose weights are then used as the videolevel personality representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Automatic Self-Reported Personality Recognition",
      "text": "Compared to apparent personality, only a small number of studies attempt to infer self-reported personality traits. Qin et al.  [54]  extract five types of hand-crafted features (i.e., HOG, Local Binary Pattern (LBP), Gabor, Scaleinvariant feature transform (SIFT) and Generalized Search Trees (Gist)) from each face image, which are fed to standard regressors (e.g., decision tree) to estimate self-reported personality (16PF) and intelligence. Besides such framelevel SPR approaches, recently proposed audio-visual selfreported personality analysis datasets  [45] ,  [55]  lead some studies to develop video-level SPR approaches. Similar to APR solutions, Curto et al.  [35]  propose a Transformer-based model to extract individual and interpersonal behaviour features from each short dyadic interaction segment using variable time windows, which can jointly recognize selfreported personality traits for both individuals. In addtion, Celiktutan et al.  [56]  specifically investigated the SPR under the human-robot interaction scenarios. Since personality is well associated with human cognitive process, Song and Shao et al.  [18] ,  [36]  propose the first audio-visual approach that employs Neural Architecture Search (NAS) to explore a person-specific network for each subject using all available frames, whose architecture and parameters are encoded as the person-specific cognitive process graph representation for SPR. This NAS-based idea was then followed by  [41] , which also search a personalized deep learning architectures for each subject to recognize Big Five personality traits.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Proposed Benchmarking Framework",
      "text": "In this paper, we propose a benchmarking framework that aims to provide a rigorous and reproducible evaluation of the existing personality computing models and the widelyused deep learning models for both automatic self-reported and apparent personality recognition. This framework will equip the future researchers with a set of strong audiovisual personality computing baselines, with standardized data pre-processing, training and post-processing strategies. Specifically, this section introduces our benchmarking framework by presenting its coding infrastructure (Sec.\n\n3.1), benchmarked models and their settings (Sec. 3.2), and datasets used for evaluation (Sec. 3.3). The pipeline of the proposed personality benchmarking framework is illustrated in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Coding Infrastructure",
      "text": "The goal of our paper is to fairly compare deep learning models' capabilities for personality computing. None of the previous studies show a fair comparison between existing models mainly due to four reasons. First, previous approaches frequently adopted different kinds of pre-trained weights. Second, different data pre-processing strategies have been employed. Third, the training and validation strategies vary from different approaches. This would lead to different criteria for choosing the final model weights.\n\nThe aforementioned three variables may largely impact the personality computing model performance. Another potential reason for the performance boost after the release of the test dataset is that some model architectures and weights might be tuned according to the test dataset rather than the validation set. To facilitate a fair comparison, our benchmark framework emphasises a unified framework focusing on evaluating the deep learning models' capacities in predicting personality traits. Specifically, our benchmarking framework unifies the components of the data input, data preprocessing, data post-processing, model initialisation, training, validation, evaluation and coding platform/libraries for all deep learning models. The only difference in experiments resides in different models' architectures and their training hyper-parameter settings, as the optimal hyper-parameter settings are model-dependent.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Input And Pre-Processing",
      "text": "For all experiments, we consistently employ the same training, validation and test sets that are predefined in the corresponding self-reported personality and apparent personality datasets for training the models and reporting the results. Instead of using complex and dataset-specific data pre-processing pipelines, we employ a widely-used and unified data pre-processing pipeline for each compared model (e.g., static models and spatio-temporal models) as follows:\n\n• Static image-based models: We first evenly divide each video into K short segments as suggested in  [27] ,  [31]  and only select the one frame of each segment (To make experiments to be reproducible, we consistently use the first frame of each segment), where we follow the winner  [31]  of ChaLearn 2016 impression challenge to set K as 100 for experiments conducted on ChaLearn First Impression dataset, and empirically set K as 2000 for UDVIA dataset. We then apply MTCNN  [57]  to obtain an cropped and aligned face image from each selected frame. During training, validation or testing, we again follow  [31]  to resize the resolution of these images to 456 × 256, where only a 224 × 224 sub-region in the center of the image is used as the final full frame. Meanwhile, the face region is contained in a cropped image with the resolution of 112 × 112 using the Dlib library  [58] . Specifically, standard data augmentation including center crop, random horizontal flip and pixel normalization are employed to process all training images. We follow previous studies  [17] ,  [31] ,  [32]  that compute personality traits prediction of each test video by averaging all frame-level predictions.\n\n• Spatio-temporal models: For each spatio-temporal model, we benchmark two types of systems: (i) short segment-level system, where we divide each video into several segments, where each segment that contains n frames (i.e., we empirically employed n = 32 or n = 64 according to (1) the setting employed in the original publication; and (2) the experimental results) is used as the input. The personality trait prediction of each test video is computed by averaging all segment-level predictions; and (ii) videolevel system, where we again follow  [29]  that evenly divides each video into 32 segments, and select the first frame of each segment to construct a sequence of 32 frames to represent the entire video. The sequence is then used as the input for spatio-temporal models to produce the video-level personality prediction. For both systems, MTCNN and Dlib library are again employed to obtain each full frame and its corresponding face image based on the same setting as the static image-based models.\n\n• Audio models: As most audio-visual studies did not provide the details of their raw audio signal extraction, we follow previous studies  [31] ,  [59]  to use the FFmpeg tool 1 to extract the raw audio signal",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training, Validation And Testing Protocol",
      "text": "For all benchmarked static models and short segment-based spatio-temporal models, we follow the previous studies  [17] ,  [31] ,  [32]  to re-use the clip-level label as the corresponding frame-level or short segment-level labels to train them. We also use the clip-level label as the label to train models that take clip-level sequence as the input. For all experiments, we consistently employ the same training, validation and testing protocol. Each static image-based model or short segment-based spatio-temporal model is trained on the training dataset with the shuffled data samples and evaluated on the validation set, where early stopping is used to prevent overfitting. We choose the model that obtains the best performance on the validation set as the final model for each experiment, which generates the reported testing results. The detailed training hyper-parameter settings for all benchmarked models are listed in Table  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "To measure the performance of the trained models, we first employ two standard evaluation metrics (i.e., ACC and Mean Squared Error (MSE)) proposed in  [16]  and  [45] . The ACC is defined as:\n\nand the MSE can be computed as:\n\nwhere N indicates the number of test videos; y pi and y gi denote the prediction and ground-truth of the n th test video, respectively. We also follow recent studies  [18] ,  [22] ,  [34]  that employ Concordance Correlation Coefficient (CCC, Eq. 3) to measure the correlation between predictions and groundtruths, which is defined as:\n\nwhere µ Yp and µ Yg denote the mean values of predictions and ground-truths of all test videos, respectively; σ Yp and σ Yg are the corresponding standard deviations; and PCC Yp,Yg denotes the Pearson Correlation Coefficient (PCC) between Y p and Y g . In this paper, we follow previous studies that individually employ MSE and CCC to evaluate the SPR systems while we use ACC and CCC to evaluate the APR systems.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Benchmarked Personality Computing Models",
      "text": "In this section, we provide the details of all the benchmarked models including (i) seven visual models, six audio models and five audio-visual models that have been employed by existing personality computing approaches;\n\n(ii) seven typical spatial/spatio-temporal deep learning CNN/Transformer models that have been widely-used in image or video processing tasks but have not yet been employed for personality recognition; and (iii) two previously used clip-level encoding models that allow all frame/short segment-level personality predictions of a clip to be combined for clip-level personality recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Inclusion And Exclusion Criteria",
      "text": "To the best of our knowledge, there are more than one hundred visual or audio-visual personality computing models that have been proposed in the literature. In this paper, we propose the following inclusion and exclusion criteria to benchmark the most representative personality computing models:\n\n• Inclusion criteria: The main criteria of this benchmark is to choose representative approaches that have been evaluated and compared in the wellknown and widely-used publicly available audiovisual personality computing dataset: ChaLearn First Impression. As a result, we first choose the Top-3 ranked models  [26] ,  [27] ,  [32]  from the ChaLearn First Impression Recognition Challenge. Then, we choose the best end-to-end audio-visual deep learning models that have been published in recent three years, respectively, which (  [24]  (2019),  [29]  (2020), and  [59]  2022) claimed that they have achieved the state-of-the-art performances on ChaLearn 2016 dataset and provided all details for model settings and training (the best model of 2021  [34]  is not an end-to-end approach). In addition, we also reproduce two models  [17] ,  [43]  that can visualise the relationship between human audio/visual non-verbal behaviours and personality traits.\n\n• Exclusion criteria: In this paper, we exclude personality computing methods that (i) are not based on audio-visual deep learning models; (ii) whose personality recognition network cannot be directly trained in an end-to-end manner, i.e., the 'end-toend' mentioned here refers to the network that can be directly trained by pairing the pre-processed video signals or/and audio representations with personality traits labels. In contrast, methods that require separated training for different parts of the network  [34] ,  [44]  or manual feature engineering  [60]  are excluded; (iii) were not evaluated on the ChaLearn First Impression dataset  [18] ,  [41] ,  [43] ,  [44] ,  [59] ; (iv) did not employ the Big-Five personality traits as the target of the recognition task; and (v) did not treat personality recognition as a regression task.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Existing Personality Computing Models",
      "text": "We first present seven deep learning visual models, six audio models and five audio-visual models that have been proposed to recognize apparent personality traits, which are evaluated on the ChaLearn First Impression dataset  [16] . For each deep learning-based model, the last FC layer has five neurons to jointly predict five personality traits. The original low-level settings of these models for reproducing are provided in the Supplementary Material, where the original pre-processing strategies employed for these models may slightly different from our standardised pre-processing strategy (i.e., we strictly follow these settings to additionally reproduce these models).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Visual Models:",
      "text": "The seven existing personality computing visual models reproduced in this paper are explained as follows:\n\n• (i) DAN  [31] : DAN is a CNN model that infers personality from a static image (including both face and background). It consists of several convolution-ReLU blocks and an additional block that is equipped with both average-pooling and max-pooling, which is added between the last convolutional layer and the final FC layer.\n\n• (ii) CAM-DAN + [17]: CAM-DAN + has a similar architecture to the DAN network, which also infers personality from each static image. It applies a max pooling and an average pooling as two parallel branches in addition to convolution layers, and is equipped with a Class Activation Map (CAM) module in order to visualise the most salient facial regions for personality recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "• (Iii) Bi-Modal Cnn-Lstm [27]:",
      "text": "The visual part of this approach consists of three 3D convolutional layers and two FC layers. It randomly takes six face images from each video as the video-level representation, and concatenates them as the clip-level input.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "• (Iv) Resnet [32]:",
      "text": "The visual part of this model is a 17-layer deep residual network, where an FC layer is attached at the top of the model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "• (V) Crnet [29]:",
      "text": "The visual module of the CR-Net consists of two 34-layer streams to learn personality features from down-sampled video-level face sequences and full image sequences. Then, a CRblock is attached to obtain personality classification features, based on which the regression feature is extracted. The Extra Trees Regressor (ETR) is finally employed to predict personality trait intensities from the extracted regression features. In this system, each video is divided into 32 segments, where a single frame is randomly selected from each segment (i.e., each video is down-sampled to 32 frames).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "• (Vi) Persemon [24]:",
      "text": "The model first employs four convolution layers and a FC layer to extract features from each face image of the video. Then, three parallel FC layers are used to simultaneously predict valence, arousal and personality traits. Besides, a coherence module (an FC layer) is introduced to process both images from the personality dataset and emotion dataset, allowing the final learned representation to be dataset invariant, while a RAM module that consists of two FC layers is employed to produce the apparent personality attributes from arousal and valence inputs, aiming to enhance the learned personality representations.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "• (Vii) Amb-Fac [59]:",
      "text": "The visual part of this system is a two-stream ResNet-101 model that extracts two different visual features: ambient features (extracted from full frames) and facial features. The system consists of three steps: (i) Pre-processing: the system selects six equally spaced images from each video clip, and applies MTCNN to obtain their face areas;\n\n(ii) ambient and facial feature extraction: ResNet-101 pre-trained on ImageNet is re-trained to extract ambient features and facial features from each selected frame; and (iii) the ambient features and facial features of each selected frame are individually fed to a MLP to make frame-level personality prediction. The video-level personality trait prediction is obtained by averaging all frame-level predictions.\n\nFor frame-level personality recognition models, all frame-level predictions of a clip are fused to obtain the final clip-level personality prediction, and thus the videolevel human facial behaviours are partially considered. Such models are not theoretically optimal for SPR, as it is difficult to infer self-reported personality traits from a single frame. Since no previous study has comprehensively investigated their performances on SPR, this paper conducts the first comprehensive evaluation for them on both APR and SPR tasks. However, this frame-level SPR strategy needs to be questioned and deeply investigated before adopting it in future studies. Audio models: The six existing personality computing audio models reproduced in this paper are explained as follows:\n\n• (i) FFT  [61] : The clip-level raw audio signal is represented by a 61208 dimensional vector. Meanwhile, the obtained vector is normalised to have zero mean and unit variance, and converted to the frequency domain using FFT. The produced spectral signal is then fed to a network that consists of three temporal convolution layers followed by max pooling and dropout.\n\n• (ii) MFCC/Logfbank  [31] : The clip-level raw audio signal is obtained from each audio-visual clip at 44,100 Hz, which are divided into 3059 audio frames. A 13-D MFCC feature and a 26-D logfbank feature are extracted from each audio frame. Then, the cliplevel 39767-D MFCC feature and 79534-D logfbank feature are produced by concatenating all frame-level MFCC and logfbank features, respectively.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "• (Iii) Bi-Modal Cnn-Lstm [27]:",
      "text": "The audio signal of each clip is first divided into six non-overlapping segments, where the mean and standard deviation of several properties of each audio segment are extracted, and combined as a 68 dimension vector using pyAudioAnalysis  [62] . We then concatenate six vectors from six segments and fed it to a FC-LSTM-FC block to jointly predict five personality traits.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "• (Iv) Resnet [32]:",
      "text": "The entire audio signal of each clip is represented as a 244,832-D vector using the librosa library  [63] . For each training iteration, it randomly selects a continuous 50,176-D sub-vector from the clip-level vector as the clip-level audio representation. Then, a 17-layer ResNet is employed, where a FC layer is attached at the top of it.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "• (V) Crnet [29]:",
      "text": "The entire audio signal of each clip is converted to a 244,832-D vector using the librosa library  [63] , which is then used as the clip-level audio representation. The ResNet-34 model is employed to process this audio representation to extract the clip-level audio personality features, from which the CR-block and ETR are used to jointly predict five personality traits.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "• (Vi) Vggish [59]:",
      "text": "The raw audio clip is re-sampled and encoded as spectrogram features, which are framed into non-overlapping examples of 0.96s. A pre-trained VGGish CNN is then introduced to extract audio personality features from each spectrogram frame. The features of all frames are finally concatenated and fed to a MLP consisting of FC layers and a sigmoid layer to make a clip-level personality prediction.\n\nAudio-visual models: Based on the reproduced audio and visual systems, we also reproduce four audio-visual models that have been used for APR on the ChaLearn First Impression dataset as follows:  [31] : Following the same settings introduced in  [31] , we build an audio-visual model that is made up of the corresponding visual and the audio models (the visual model (i) and the audio model (ii)) described above. The final clip-level prediction is obtained by averaging clip-level audio prediction and clip-level video prediction.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "• (Ii) Bi-Modal Cnn-Lstm [27]:",
      "text": "The audio and visual clip of each subject is firstly divided into 6 segments, where each visual segment is processed by the visual model (iii) described above and each audio segment is processed by the audio model (iii) described above. Then, each pair of segment-level audio and visual latent features are concatenated as a 160-D vector. Finally, the clip-level personality prediction is obtained by feeding the audio-visual vectors of six segments to an LSTM model with 128 hidden units. Here, the LSTM contains a single hidden layer with 128 neurons.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "• (Iii) Resnet [32]:",
      "text": "A two-steam ResNet model is employed, which consists of the visual model (iv) and the audio model (iv) described above. The audio feature (256-D) and the latent frame-level visual feature (256-D) are concatenated as a 512-D feature to a fully-connected layer to jointly predict the five personality traits. Specifically, the model randomly takes an audio and visual sample at each training iteration.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "• (Iv) Crnet [29]:",
      "text": "A three-stream ResNet-34 model is employed, which consists of two visual streams (the visual model (vi) described above), and an audio stream (the audio model (v) described above). Specifically, the two-stream visual model takes both the face image sequence and full frame sequence as the visual inputs. The latent features produced by audio and visual streams are combined as a single 512-D vector by element-wise sum.\n\n• (v) Amb-Fac-VGGish [59]: A multi-modal system that contains a visual stream (the visual model (vii) described above) which takes both face and full frames as the input, and an VGGish-based audio stream (the audio model (vii) described above) which learns personality features from the audio spectrogram.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Model",
      "text": "Learning rate Weight decay Momentum Optimizer Audio FFT  [61]  0.01 0.0005 0.9 SGD MFCC/Logfbank  [31]  0.00083 6.5 0.9 SGD Bi-modal CNN-LSTM  [27]  0.05 0.0005 0.9 SGD ResNet  [32]  0.0002 0.0005 0.9 SGD CRNet  [29]  0.002 0.005 0.9 SGD/Adam VGGish  [59]  0.001 0.0005 0.9 SGD Video DAN  [31]  0.05 0.0005 0.8 SGD Bi-modal CNN-LSTM  [27]  0.05 0.0005 0.9 SGD ResNet  [32]  0.05 0.0005 0.9 SGD CRNet  [29]  0.002 0.005 0.9 SGD CAM-DAN +  [17]  0.05 0.0005 0.8 SGD/Adam PersEmoN  [24]  0.001 0.0005 0.9 SGD Amb-Fac  [59]  0.01 0.0005 0.9 SGD SENet  [64]  0.01 0.0005 0.8 SGD HRNet  [65]  0.01 0.0005 0.9 SGD VIT  [66]  0.001 0.0005 0.9 SGD Swin-Transformer  [67]  0.01 0.0005 0.9 SGD 3D-Resnet  [68]  0.01 0.005 0.9 SGD Slow-Fast  [69]  0.002 0.0005 0.9 SGD TPN  [70]  0.001 0.0005 0.9 SGD VAT  [71]  0.01 0.0005 0.9 SGD",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Audio-Visual",
      "text": "Bi-modal CNN-LSTM  [27]  0.05 0.0005 0.9 SGD ResNet  [32]  0.001 0.0005 0.9 SGD CRNet  [29]  0.002 0.005 0.9 SGD",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Table 2",
      "text": "Training hyper-parameter settings for all benchmarked models",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Widely-Used Static/Spatio-Temporal Visual Deep Learning Models",
      "text": "Since visual information is more informative for personality recognition (validated in Sec. 4.1), we additionally benchmark six standard visual deep learning models that have been widely used for static image or video analysis, including a VAT model  [71]  that has been previously applied to self-reported personality recognition  [45] . Specifically, these include three models that take the static face/full image as the input and four models that infer personality from spatiotemporal visual data. The brief description of these models' unique characteristics are explained as follows:\n\n• (i) SENet  [64] : SENet is a CNN model whose \"Squeeze-and-Excitation\" (SE) block adaptively calibrates channel-wise feature responses, i.e., it explicitly models inter-dependencies between features extracted from different convolution channels.\n\n• (ii) HRNet  [65] : HRNet is a hierarchical CNN model that has multi-level high-to-low resolution convolutions in parallel, which maintains high-resolution representations through the whole propagation process and produces strong high-resolution representations by repeatedly conducting fusion for representations extracted from parallel convolutions.\n\n• • (iii) VIT  [66] : VIT is a transformer-style model that computes relationships among pixels in various small patches of the input image based on the attention operation, where feature extraction for each patch is also controlled by several learnable embeddings. The VIT can effectively explore topological relationships between patches, making it able to capture global and wider range relations among pixels at the cost of a higher training complexity.\n\n• (iv) Swin-Transformer  [67] : Swin-Transformer is a hierarchical transformer model which applies a set of shifted windows to the input image, allowing the attention operations only to be conducted to nonoverlapping local image regions, resulting in greater efficiency.\n\n• (v) 3DResNet  [68] : 3DResNet is a spatio-temporal CNN model which contains shortcut (residual) connections that allow a features to bypass one layer and move to the next layer during the propagation and back-propagation, where the 2D convolution operations in the standard ResNet  [72]  are replaced with spatio-temporal convolution operations.\n\n• (vi) Slow-Fast  [69] : Slow-Fast is a spatio-temporal CNN model for video recognition. It contains a slow pathway to capture spatial semantics, which operates at a low frame rate in the temporal dimension of the input, as well as a fast pathway, operating at a high frame rate in the temporal dimension, aiming to capture dynamics.\n\n• (vii) TPN  [70] : TPN is a spatio-temporal CNN that has hierarchical Temporal Pyramid structure, which can flexibly integrate multiple 2D or 3D backbone networks in a plug-and-play manner. As a result, it learns hierarchical spatial and temporal features from the input video, i.e., it can capture action instances at various tempos.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "• (Viii) Vat [71]:",
      "text": "VAT is a spatio-temporal transformer that consists of 3D convolution operations to learn action features from each subject. Then, a set of head networks aggregating features from the spatiotemporal context to predict predict actions and regresses tighter bounding boxes.\n\nThe training settings (hyper-parameter settings) of all benchmarked models are also listed in Table  2 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Clip-Level Representation Generation Models",
      "text": "To combine frame/segment-level personality predictions of an audio-visual clip for a clip-level personality trait prediction, this section also introduces two standard methods that have been employed in existing personality computing publications. In particular, besides the widely-used strategy which simply averages all frame/segment-level predictions at the clip-level prediction (used by  [17] ,  [24] ,  [31] ,  [32] ), we also benchmark the spectral representation  [33] ,  [73]  to summarize frame/segment-level predictions at the cliplevel, which has been used by  [34] . This is because the spectral representations not only effectively encode the temporal dependencies among all frame-level predictions of a clip, but also summarise time-series signal of an arbitrary length into a representation of a fixed size without any distortion.\n\n•",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Averaging Frame/Segment-Level Predictions (Afp):",
      "text": "The clip-level personality prediction is obtained by averaging all frame/segment-level predictions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "•",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Spectral Representation Of Frame/Segment-Level Predictions (Sfp):",
      "text": "We first encode a pair of spectral heatmaps (please see  [33]  for details) from the five-channel (corresponding to five traits) frame/segment-level personality trait prediction time-series. Then, we train a 1D-CNN to predict cliplevel personality traits from the produced spectral heatmaps.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Datasets",
      "text": "There are several publicly available audio-visual personality computing datasets for apparent personality recognition  [16] ,  [74] ,  [75] , and self-reported personality recognition  [45] ,  [76] ,  [77]  (please see  [46]  for a survey of existing personality databases). Among them, the two most widely used publicly available audio-visual self-reported personality and apparent personality datasets: the UDIVA Dataset  [45]  and ChaLearn First Impression dataset  [16] , are employed to evaluate all models described in Sec. 3.2. Both datasets provide the Big-Five personality traits (i.e., Extraversion (Ext), Agreeableness (Agr), Openness (Ope), Conscientiousness (Con), and Neuroticism (Neu)) as the label for each audiovisual clip.\n\nUDIVA dataset was released in 2021. It records 188 dyadic interaction clips between 147 voluntary participants, with total 90.5h of recordings. Each clip contains two audiovisual files, where each records a single participant's behaviours. For each dyadic interaction session, participants were matched based on their availability, language and three variables: gender, age group and the relationship among interlocutors. In particular, all participants were matched to enforce a close-to-uniform distribution among all possible combinations between these variables. During the recordings, participants were asked to sit at 90 degrees to the conversational partner around a table, and under the dyadic interactions based on five tasks: Talk, 'Animal games', Lego building, \"Ghost blitz\" card game, and Gaze events, where each pair of participants were interacting using one of the three languages (i.e., English, Spanish or Catalan). The self-reported personality labels are obtained for each participant based on the following rules: (i) parents of children up to 8 years old completed the Children Behavior Questionnaire (CBQ)  [78] ; participants from 9 to 15 years old completed the Early Adolescent Temperament Questionnaire (EATQ-R)  [79] ; and the rest of the participants completed both the Big Five Inventory  [80]  and the Honesty-Humility axis of the HEXACO personality inventory  [81] . However, due to the limited materials, it is neither possible to investigate the reliability of these labels, nor understand the real internal states of these participants. Also, behaviours in such short audio-visual clips may not be reliable to represent participants' personality traits as personality traits are long-term status.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Chalearn First Impression Dataset Was Released In 2016.",
      "text": "It contains talking-to-the-camera 10, 000 audio-visual clips that come from 2, 764 YouTube users, where each video lasts for about 15 seconds with 30 fps. Each video is labelled with the Big-Five personality traits that are annotated by external human annotators using the Amazon Mechanical Turk. The intensity of each trait was normalised to the range of [0, 1]. The dataset provides official splits for training (6, 000 videos), validation (2, 000 videos) and test (2, 000 videos). It is the largest audio-visual dataset openly available for research purposes, which has been annotated with Big-Five apparent personality traits. However, these annotations may suffer from unintentionally bias caused by pre-conception of annotators on each individual (e.g., gender and ethnicity). As a result, the classifier might model these bias patterns.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we first present the results of all benchmarked models for both self-reported personality and apparent personality traits recognition in Sec. 4.1. Then, in Sec. 4.2 we discuss the influences of different settings on personality recognition performances. Finally, we provide a systematic discussion of the benchmarking results in Sec. 4.3",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Benchmarking Personality Computing Models",
      "text": "This section reports the self-reported and apparent personality recognition results achieved by all benchmarked models. According to Table  3  and Table 4 , all benchmarked models failed to accurately infer the self-reported personality traits from human audio/visual/audio-visual behaviours, with only three visual models (e.g., Interpretimg, SENet, and HRNet) achieving more than 0.15 CCC performances. Standard audio feature extraction methods can barely extract self-reported personality-related information from non-verbal audio signals, as most of the benchmarked models achieved near-zero CCC values, while only the predictions of the VGGish achieved CCC > 0.1 with the ground-truth. Some audio models even degraded the corresponding visual systems under the audio-visual setting. The low CCC and high MSE results of almost all models on self-reported personality recognition indicate that it may difficult and unreliable for standard deep learning models to be used to directly infer human self-reported personality traits from external non-verbal behaviours.\n\nIn contrast, the relatively higher CCC and ACC results shown in Table  5  and Table  6  suggest that apparent personality traits can be better predicted from human nonverbal behaviours when using deep learning models, i.e., five visual models and two audio-visual models achieved CCC > 0.5. In particular, the predictions produced by HRNet and VAT from only facial behaviours have more than 0.6 correlation (measured by CCC) with the ground-truth. In addition, we found that the CR-Net and VGGish models can also deep learn valuable apparent personality-related cues from non-verbal audio signals. The aforementioned results suggest that compared to self-reported personality, apparent personality is generally more feasible and easier to be directly predicted from human non-verbal behaviours. Since apparent personality is defined as the external human observer's perception of the target subject, these results can be explained by the fact that it is straight-forward for deep learning models to be used as an external observer and make judgments based on behaviours that can be directly observed. In contrast, self-reported personality represents the internal state/attribute of the target person, which is not straightforward to be directly inferred from external behaviours using standard deep learning models. The detailed discussions on applying ML models for inferring selfreported personality traits can be found in  [18] ,  [36] .\n\nWe also conducted a 10-fold cross-validation to further compare the performances of the six best models (i.e., two audio models: CRNet and VGGish; two visual models: HRNet and VAT; and two audio-visual models: CRNet and Amb-Fac-VGGish), as they achieved top or runner-up performances in audio, visual and audio-visual personality recognition, respectively. The detailed results provided in",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Traits",
      "text": "Open Consc Extrav Agree Neuro Avg.\n\nAudio FFT  [61]  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 MFCC/Logfbank  [31]  0.0002 -0.0002 0.0002 0.0001 0.0001 0.0001 Bi-modal CNN-LSTM  [27]  0.0000 -0.0001 0.0000 0.0000 0.0000 0.0000\n\nResNet  [32]  -0.0459 0.1045 -0.0416 0.0429 0.0015 0.0123 CRNet  [29]  0.0005 0.0290 0.0201 0.0335 0.0267 0.0220 VGGish  [59]  0.0688 0.1882 0.1310 0.1069 0.0412 0.1072\n\nVideo DAN  [31]  0.0009 0.0087 0.0061 0.0035 -0.0016 0.0036 Bi-modal CNN-LSTM  [27]  -0.0001 0.0000 0.0000 0.0001 0.0000 0.0000\n\nResNet  [32]  0.0648 0.2424 0.0349 0.0305 -0.0009 0.0743 CRNet  [29]  -0.0206 0.0879 0.012 0.1454 0.0645 0.0578 CAM-DAN +  [17]  0.0336 0.3359 0.0270 0.2014 0.2709 0.1738 PersEmoN  [24]  -0.0095 0.0133 0.0045 0.0085 0.0058 0.0045 Amb-Fac  [59]  0.0532 0.1941 0.0442 0.0453 0.0204 0.0714 SENet  [64]  0.1678 0.2776 0.0538 0.0299 0.3093 0.1510 HRNet  [65]  0.2175 0.2998 -0.0039 0.1680 0.1945 0.1752 VIT  [66]  -0.0184 0.1532 -0.0002 0.0645 0.0607 0.0520 Swin-Transformer [67] -0.0273 0.0470 0.0361 -0.0142 0.0860 0.0256 3D-Resnet  [68]  -0.0478 0.0102 0.0478 0.0499 -0.0240 0.0072 Slow-Fast  [69]  -0.0102 0.0076 0.0010 -0.0063 0.0161 0.0016 TPN  [70]  0.0448 0.0348 0.0287 -0.0177 -0.0281 0.0125 VAT  [71]  -0.0139 -0.0016 0.0016 -0.0013 -0.0006 -0.0031\n\nAud-vis DAN-MFCC  [31]  0.0008 0.1154 -0.0132 -0.0222 0.1219 0.0405 Bi-modal CNN-LSTM  [27]  0.0001 -0.0001 0.0001 -0.0001 0.0005 0.0001\n\nResNet  [32]  -0.0530 0.1290 0.0230 0.0310 0.0002 0.0260 CRNet  [29]  0.0998 0.1780 0.1158 0.2168 0.0449 0.1311 Amb-Fac-VGGish [59] -0.0348 0.0468 0.0302 0.0397 0.0041 0.0171",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Table 3",
      "text": "The CCC results achieved for the self-reported personality recognition on the UDIVA dataset. The reported results are obtained by averaging the results achieved for four sessions. For different modality groups, audio, video, audio-visual, the values in bold denote the highest ones in the columns and the values with underline denote the second highest ones.\n\nthe Supplementary Material show that the performance of all models on APR are still much better than their corresponding performances on SPR. However, we found that visual and audio-visual models-based cross-validation APR results are clearly worse than the results achieved for the pre-defined training/validation/test protocol. Since the 'data hungry' experiments provided in the Supplementary Material show that the APR performances keep increasing on the test set when more training samples are provided, we assume that the pre-defined test set may contain simpler samples by chance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we evaluate the influences of different preprocessing, post-processing and model settings on personality recognition performances. The statistical significance analyses for different settings are additionally provided in the Supplementary Material, where we found that the differences in most settings brought much more impact on APR performances over SPR performances. This can be explained by the fact that the benchmarked models can hardly infer self-reported personality traits from human non-verbal behaviours, and thus no matter what the settings are, they always achieved very unreliable predictions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Full Frames Vs. Face Regions",
      "text": "We compare the results achieved by feeding aligned face regions and full frames (containing both the face and backgrounds) to standard visual deep learning models in Fig.  2  and Fig.  3 , where the standard visual deep learning models generally provide more reliable self-reported and apparent personality predictions by using face regions, despite the  Audio FFT  [61]  0.0002 -0.0001 -0.0002 -0.0002 -0.0003 -0.0002 MFCC/logfbank  [31]  0.1968 0.1497 0.1738 0.1295 0.1780 0.1655 Bi-modal CNN-LSTM  [27]  -0.0004 -0.0005 0.0004 -0.0005 -0.0008 0.0004\n\nResNet  [32]  0.1293 0.0830 0.0458 0.1101 0.1548 0.1046 CRNet  [29]  0.4122 0.3406 0.3846 0.2857 0.4306 0.3707 VGGish  [59]  0.4516 0.4493 0.4429 0.3127 0.4500 0.4213\n\nVisual DAN  [31]  0.5693 0.6254 0.6070 0.4855 0.6025 0.5779 Bi-modal CNN-LSTM  [27]  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n\nResNet  [32]  0.1561 0.1902 0.1355 0.0838 0.1373 0.1406 CRNet  [29]  0.3748 0.3646 0.3987 0.2390 0.3226 0.3399 CAM-DAN +  [17]  0.5882 0.6550 0.6326 0.5003 0.6199 0.5992 PersEmoN  [24]  0.2067 0.2441 0.2675 0.1369 0.1768 0.2064 Amb-Fac  [59]  0.5858 0.6750 0.5997 0.4971 0.5765 0.5868 SENet  [64]  0.5300 0.5580 0.5815 0.4493 0.5708 0.5379 HRNet  [65]  0.5923 0.6912 0.6436 0.5195 0.6273 0.6148 VIT  [66]  0.0184 0.0817 0.0247 0.0175 0.0318 0.0348 Swin-transformer  [67]  0.2223 0.2426 0.2531 0.1224 0.1942 0.2069 3D-Resnet  [68]  0.3248 0.3601 0.3601 0.2120 0.3352 0.3185 Slow-Fast  [69]  0.0256 0.0320 0.0185 0.0105 0.0184 0.0210 TPN  [70]  0.4427 0.4767 0.4998 0.3230 0.4675 0.4420 VAT  [71]  0.6216 0.6753 0.6836 0.5228 0.6456 0.6298\n\nAud-vis DAN-MFCC  [31]  0.4341 0.4645 0.4553 0.3519 0.4588 0.4329 Bi-modal CNN-LSTM  [27]  0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n\nResNet  [32]  0.4150 0.3671 0.3889 0.2679 0.4181 0.3714 CRNet  [29]  0.5193 0.5106 0.5024 0.4026 0.5119 0.4894 Amb-Fac-VGGish  [59]  0.5618 0.6421 0.5921 0.4620 0.5734 0.5663",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Table 5",
      "text": "The CCC results achieved for the apparent personality recognition on the ChaLearn First Impression dataset.For different modality groups, audio, video, audio-visual, the values in bold denote the highest ones in the columns and the values with underline denote the second highest ones.\n\nfact that neither full frames nor face regions provide reliable self-reported personality predictions. In contrast, apparent personality predictions generated by all visual models using both face regions and full frames have CCC > 0.37 on average with the ground-truth, where face region-based Traits Open Consc Extrav Agree Neuro Avg.\n\nAudio FFT  [61]  0.8303 0.8174 0.8502 0.8309 0.8331 0.8324 MFCC/logfbank  [31]  0.8891 0.8790 0.8835 0.8967 0.8802 0.8857 Bi-modal CNN-LST  [27]  0.8835 0.8747 0.8785 0.8937 0.8774 0.8816\n\nResNet  [32]  0.8822 0.8780 0.8782 0.8958 0.8820 0.8832 CRNet  [29]  0.9001 0.8895 0.8951 0.9022 0.8964 0.8967 Vggish-feat  [59]  0.9010 0.8962 0.8959 0.9028 0.8953 0.8982\n\nVisual DAN  [31]  0.9098 0.9106 0.9096 0.9102 0.9061 0.9093 Bi-modal CNN-LSTM  [27]  0.8832 0.8742 0.8778 0.8933 0.8770 0.8811\n\nResNet  [32]  0.8896 0.8835 0.8837 0.8968 0.8830 0.8873 CRNet  [29]  0.8987 0.8932 0.8952 0.9018 0.8908 0.8960 CAM-DAN +  [17]  0.9115 0.9139 0.9126 0.9118 0.9089 0.9118 PersEmoN  [24]  0.8934 0.8893 0.8913 0.8994 0.8866 0.8920 Amb-Fac  [59]  0.9101 0.9141 0.9082 0.9095 0.9038 0.9091 SENet  [64]  0.9076 0.906 0.908 0.9097 0.9061 0.9075 HRNet  [65]  0.9101 0.9154 0.9111 0.9113 0.9084 0.9113 VIT  [66]  0.8832 0.8760 0.8778 0.8934 0.8778 0.8817 Swin-transformer  [67]  0.8937 0.8870 0.8893 0.8983 0.8860 0.8909 3D-Resnet  [68]  0.8964 0.8921 0.8933 0.9008 0.8915 0.8948 Slow-Fast  [69]  0.8780 0.8604 0.8443 0.8809 0.8613 0.8650 TPN  [70]  0.9025 0.8963 0.9019 0.9013 0.8992 0.9003 VAT  [71]  0.9115 0.9123 0.9153 0.9099 0.9098 0.9118\n\nAud-vis DAN-MFCC  [31]  0.9049 0.9020 0.9024 0.9081 0.9015 0.9038 Bi-modal CNN-LSTM  [27]  0.8833 0.8744 0.8779 0.8935 0.8773 0.8813\n\nResNet  [32]  0.8996 0.8918 0.8945 0.9015 0.8948 0.8964 CRNet  [29]  0.9075 0.9019 0.9017 0.9055 0.9034 0.9040 Amb-Fac-VGGish  [59]  0.9127 0.9169 0.9117 0.9133 0.9088 0.9127  TABLE 6  The ACC results achieved for the apparent personality recognition on the ChaLearn First Impression dataset.For different modality groups, audio, video, audio-visual, the values in bold denote the highest ones in the columns and the values with underline denote the second highest ones.\n\npredictions still have 1.62% CCC advantage over full framebased predictions. However, it can be seen that using face regions and full frames does not cause a large difference for most models, where only HRNet is sensitive to this variable on both tasks. These results suggest that although some previous studies claimed that backgrounds and spatial contextual information can provide informative cues for personality recognition, they may also contain personalityunrelated noise that can negatively impact the personality recognition. This indicates that personality-related cues contained in background and spatial contextual information can not always compensate their negative impact for personality recognition. Interestingly, some of the benchmarked models (e.g., 3DResNet) achieved better performances with the full frame setting. As shown in Fig.  2  and Fig.  3 , we hypothesise that the ability of 3DResNet to extract personality-related cues from facial behaviours may be limited, as evidenced by the relatively poor performance of its face-based system compared to other face-based systems (i.e., other benchmarked models). In contrast, the full frame-based system of 3DResNet yielded the second best results among all full frame-based systems, indicating its potential to capture valuable personality-related cues from head/body movements (as the head/body movements are contained in full frame sequences) or even contextual factors such as the background setting (e.g., office, bedroom) contained in the full frames.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Spatial Vs. Spatio-Temporal Visual Models",
      "text": "Fig.  4  compares the average results achieved by spatial visual models (i.e., frame-level systems) and spatio-temporal visual models (i.e., short segment-level systems and videolevel systems) on both datasets. It can be observed that the  average CCC performance achieved by the spatial models outperforms the spatio-temporal settings for both tasks, i.e., while no model is reliable for inferring self-reported personality from facial behaviours, spatial models generally provide better self-reported personality predictions for four of the five traits than spatio-temporal models. Nevertheless, we found that a spatio-temporal model VAT achieved the best CCC performance in predicting apparent personality traits, with CCC > 0.6 for predicting four of the five traits, which suggest that the VAT's temporal modelling strategy is suitable to extract apparent personality-related facial behaviour cues. Overall, spatial visual models consistently achieved better average performance than spatio-temporal models in predicting all apparent personality traits. These results indicate that the most standard spatio-temporal deep learning models (except VAT) can not effectively extract personality-related cues from human spatio-temporal facial behaviours, which may partially caused by the fact that they are not able to specifically model personality-related temporal contextual cues from the given video.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Short Segment-Level Vs. Video-Level Personality Modelling",
      "text": "Fig.  4  also compares short segment-level modelling and video-level modelling systems, where four widely-used spatio-temporal models introduced in Sec. 3.2.3 are benchmarked. Although both short segment-level modelling and video-level modelling systems generally failed to provide reliable self-reported personality predictions, the short segment-level systems still achieved slightly better average performances than the video-level systems for four traits (i.e., Ope, Con, Agr, and Neu traits). Moreover, the short segment-level systems have clear advantages over the video-level systems for predicting all apparent personality traits. These results suggest that the short-term non-verbal behaviours contain crucial apparent personality-related cues while the standard video-level models which ignored shortterm behaviours can not extract reliable apparent personality cues from the down-sampled long-term behaviours (i.e., down-sampling a long clip that contains around 400 frames Fig.  4 . Comparison between average results achieved by frame-level models and spatio-temporal models, where both short segment-level system and video-level system are evaluated for spatio-temporal models.\n\nto a segment containing 32 frames results in the loss of shortterm behavioural cues).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Audio Vs. Visual Vs. Audio-Visual Models",
      "text": "As shown in Fig.  5 , visual models and audio-visual models on average generated superior personality recognition performances than audio models for all traits on both datasets. We found that CAM-DAN + , HRNet and SENet visual models achieved relatively promising results for both SPR and APR tasks. However, the low CCC performance on SPR still shows that standard audio/visual machine learning models can hardly extract self-reported personality-related cues from audio-visual behaviours. As a result, the five audio-visual systems displayed in Table  5  do not show clear and consistent advantages over their corresponding visual counterparts. In contrast, the audio and facial behaviours sometimes are more reliable in reflecting apparent personality traits. According to Table  5 , adding the audio modality allows three out of the five audio-visual systems  [29] ,  [32] ,\n\n(a) CCC results achieved for the UDIVA dataset.\n\n(b) CCC results achieved for the ChaLearn First Impression dataset.\n\nFig.  5 . Comparison between the average results achieved by the audio models, the visual models and the audio-visual models.\n\n[59] achieved better performances over their corresponding visual methods in predicting apparent personality traits. However, when the extracted audio features are not well associated with apparent personality traits, they may even negatively impact the personality recognition ability of the visual system (e.g., he MFCC/logbank audio features degrade the DAN-based visual system  [31] ). In summary, these results suggest that (i) non-verbal facial behaviours contain more discriminative personality-related cues than nonverbal audio signals for both self-reported and apparent personality recognition tasks; and (ii) if the audio modality can individually provide reliable personality predictions, it is expected to further enhance the predictions of the visual system.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Summarising Frame/Segment-Level Personality Predictions",
      "text": "We then compare the results achieved by the different strategies for summarising frame/segment-level personality predictions for obtaining clip-level personality prediction by: (i) simply averaging frame-level personality predictions as the clip-level personality prediction; and (ii) encoding a spectral representation of frame/segment-level predictions. Specifically, we first encode a pair of spectral heatmaps from the five-channel frame/segment-level personality trait predictions. Then, we train a regressor that contains seven 1D convolution layers followed by two fully connected layers to predict clip-level personality traits from the produced spectral heatmaps. Table  7  demonstrates the superiority of applying SFP to summarise frame/segment-level personality predictions over the other three strategies, as the performances of all models are improved using SFP. This also indicates that the spectral representation can effectively capture reliable personality-related spatio-temporal cues from frame/segment-level personality predictions. In contrast, the advantages of applying SFP to self-reported personality recognition are less evident. We assume this is because the frame-level predictions of self-reported personality traits lack reliability, making it challenging to capture dependable personality-related spatio-temporal cues from them.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Other Factors",
      "text": "In addition, our analysis also revealed the following key findings: (i) jointly predicting all five traits yielded superior performance compared to models predicting each trait individually; (ii) models' performances are largely influenced by the temporal scale of the input, especially for APR; and (iii) incorporating metadata of the subjects did not lead to a clear improvement in the performance of nonverbal behaviour-based SPR, which remained poor; (iv) The results of self-reported personality recognition are highly influenced by the scenarios in which the data is recorded.\n\nResults achieved for the UDIVA show that the best performances of various models were achieved under the 'Ghost' scenarios. This scenario requires each participant to not only solve the problem but also interact with the other participant, both of which would foster rich personalityrelated behaviours expressed by them  [82] ,  [83] . However, combining behaviours expressed from all tasks does not show clear benefits for SPR models, suggesting that other tasks could not effectively trigger participants to express personality-related behaviours.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "In addition to reporting the results achieved by the benchmarked models and discussing the results achieved by the different settings, this section discusses other relevant aspects of the benchmarked models, including: (i) some reproduced models achieved lower results than the originally reported results; (ii) personality traits having different relationships with the non-verbal behaviours; (iii) the benchmarked models generally have poor performance in SPR; and (iv) challenges and research gaps in the current audio-visual personality computing models.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Some Reproduced Approaches Have Lower Performances Than Originally Reported",
      "text": "As compared in Table  8 , an important finding is that 4 out of 7 reproduced models failed to achieve similar or superior results compared to their originally reported results on the ChaLearn First Impression dataset, even when we strictly followed the same pre-processing and training settings as reported in the original papers. This could be caused by various reasons, including incomplete reporting of training, evaluation and pre-processing details in the original papers  [24] ,  [29] , excluding the text modality  [29] , or random factors involved in their training and evaluation processes (e.g., randomly selected images from each video/segment  [24] ,  [27] ,  [29] ,  [32] , randomly cropped image regions  [29] ,  [32]  as well as the randomly cropped 50176 dimensional audio features  [32] ).\n\n4.3.2 Relationships between the personality traits and nonverbal behaviours Fig.  4  show that self-reported Conscientiousness and Neuroticism traits are relatively reliable to be inferred by static facial displays than other traits. Self-reported Extraversion trait is less related to static facial behaviours but more associated with long-term spatio-temporal facial behaviours. In terms of apparent personality traits, it can be seen that they have similar relationship patterns with facial behaviours of three temporal scales (frame-level, short segment-level and video-level), i.e., the behaviours of all three temporal scales are more reliable to infer Conscientiousness, Extraversion, Neuroticism, and Openness traits, while all of their associations with the Agreeableness trait are clearly weaker. While facial behaviours are clearly more informative than audio behaviours in predicting all personality traits, Fig.  5  illustrates that the self-reported Conscientiousness and Neuroticism traits can be better reflected by facial behaviours while Extraversion and Openness traits are less associated with facial behaviours. Again, both audio and facial behaviours are more reliable in inferring apparent Conscientiousness, Extraversion, Neuroticism, and Openness traits but are less correlated with the Agreeableness trait.\n\nBased on these results, we can conclude that each true or apparent personality trait has a unique relationship with human non-verbal behaviours, including their temporal scales and modalities. In particular, we found that: (i) nonverbal audio-visual behaviours are more reliable in inferring apparent personalities; (ii) facial behaviours are more informative than audio behaviours in inferring personality traits; (iii) self-reported Conscientiousness and Neuroticism traits can be easier to be inferred from non-verbal facial behaviours than other self-reported traits; and (iv) apparent Agreeableness trait is more difficult to be predicted from facial behaviours than other apparent personality traits.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Poor Performances In Self-Reported Personality Recognition",
      "text": "To gain comprehensive insights, we followed  [33]  to compute the correlations between 17 automatically detected human facial action units (AUs) (using OpenFace 2.0  [84] ) as well as 988 acoustic features identified with OpenSmile  [53] , and each self-reported/apparent personality trait, respectively. Fig.  6  clearly demonstrates that all human nonverbal behaviours have much lower correlations with selfreported personality traits. This could be a potential reason that non-verbal behaviour-based deep learning models can more effectively predict apparent personality traits over selfreported personality traits. We attribute this disparity to Clip-level representation DAN  [31]  ResNet  [32]  CRNet  [65]  PersEmoN  [24]  CAM-DAN +  [17]  SENet  [64]  HRNet  [65]  Swin  [67]  (   7  Long-term modelling CCC (%) results of the benchmarked frame/short segment-level personality computing visual models, where (C) denotes results achieved on ChaLearn Impression dataset, and (U) denotes results achieved on UDIVA dataset. The AFP, LRP, MLP and SFP represent the clip-level prediction computed from all frame-level predictions using averaging, linear regression, MLP and spectral representation, respectively.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Model",
      "text": "Reproduced Standardised Reported DAN  [31]  0.9131 0.9093 0.9130 CAM-DAN +  [17]  0.9120 0.9118 0.9120 Amb-Fac-VGGish  [59]  0.9127 0.9102 0.9146 Bi-modal CNN-LSTM  [27]  0.8812 0.8813 0.9121 ResNet  [32]  0.8939 0.8964 0.9109 CRNet  [29]  0.9079 0.9040 0.9188 PersEmoN  [24]  0.8920 0.8920 0.9170",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Table 8",
      "text": "Average ACC results of five personality traits achieved for the reproduced models on the ChaLearn First Impression dataset. Reproduced denotes the data was processed based on the method described in original papers; Standardised indicates the results obtained from the system used our standardised data processing pipeline; and Reported denotes the performances provided by the original papers.\n\nthe fact that apparent personality traits are annotated by observing the subjects' behaviours, which is not taken into account during the annotation of self-reported personality traits.\n\nAnother possible reason for the lower performance achieved in self-reported personality recognition may be the limited number of training clips/subjects in the UDIVA dataset. Compared to the ChaLearn First Impression dataset (6000 training clips from 2624 subjects and 2000 validation clips from 1484 subjects), the UDIVA dataset comprises only 232 training clips from 99 candidates and 36 validation clips from 20 candidates. To mitigate the effects stemming from variations in the number of training and validation clips, we randomly select 232 training clips from 99 candidates and 36 validation clips from 20 candidates from the ChaLearn First Impression dataset (i.e., the list is provided in 2 for the reproducible purpose). We also extracted 15-second segments from each UDIVA clip, aligning with the duration of clips in the ChaLearn First Impression dataset. Based on these reduced datasets, we evaluated the performance of the six best deep learning models. Results (reported in the Supplementary Material) show that less training/validation samples and shorter clip length lead to the significant performance drop in APR and SPR performances, respectively, which suggest that the lower performance achieved for SPR 2. https://github.com/liaorongfan/DeepPersonality/tree/add new data loader/datasets/chalearn16 few candidates could be attributed to the limited number of clips/subjects in the UDIVA dataset. However, it can be observed that even with the limited number of short clips, the benchmarked models still achieved better results for APR.\n\nFinally, we compared three types of systems: (i) systems taking individual static facial images, relying on static facial expressions to make predictions; (ii) systems taking selected static images with neutral facial display, making predictions based on identity cues; and (iii) systems taking anonymized multi-channel AU time-series, predicting personality based on temporal facial behaviours without considering identity information. The detailed results are provided in the supplementary material, where we found that APR models primarily relied on behavioural and expression cues rather than identity cues. In contrast, SPR models relied on identity cues, and thus achieved very poor performances. This observation suggests that human non-verbal audiovisual behaviours might not directly reflect the self-reported personality, and taking into account other contextual aspects might be important, which in itself is an interesting finding that has not been investigated by prior works.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Challenges And Research Gaps",
      "text": "Although an increasing number of automatic personality computing approaches have been proposed in recent years, both their reported results and our benchmarking results show that existing approaches are not very reliable for inferring personality traits, especially for self-reported personality. We can summarise the research gaps of existing personality computing approaches as follows:\n\n• (i) Most existing studies  [17] ,  [21] ,  [22] ,  [26] ,  [31] ,  [32] ,  [85]  attempt to infer personality traits directly from static frames or short audio-visual segments without considering contextual information. Since subjects that have different personalities can show similar behaviours in a single frame or a short audio-visual segment (e.g., different subjects can show a happy expression in a single frame), a static frame or a short audio-visual segment is not reliable for inferring personality. Also pairing a static frame or a short audiovisual segment with clip-level personality labels to train models results in an ill-posed machine learning problem, making the trained model theoretically not have good generalization capability. Our results  show that the spectral representation-based longterm modelling of frame/short segment-level predictions produced by reliable frame/short segmentbased systems (e.g., most frame/short segment-level APR models) leads to enhanced personality recognition results (Table  7 ).\n\n• (ii) Existing clip-level personality models can partially address the aforementioned ill-posed machine learning problem during model training. However, our benchmark shows that their results are worse as compared to the static frames or short audio-visual segments-based approaches. This can be explained by the fact that most clip-level spatio-temporal personality computing models  [29]  (including the benchmarked standard spatio-temporal models) require down-sampling the original video, i.e., a large number of frames are discarded, i.e., crucial shortterm personality-related cues are ignored during training and inference stages. While some clip-level personality computing studies  [18] ,  [34] ,  [36]  retain almost all frames in a video, these methods are timeconsuming to implement and train as they need to individually train a person-specific network for each subject. In short, there lacks an efficient clip-level audio-visual personality computing framework that can extract personality-related cues from all frames of the input data.\n\n• (iii) Most existing deep learning-based personality computing approaches simply pair the given audiovisual data and labels to train models, i.e., they do not take the personality-related domain knowledge into consideration when designing and training their models. As we can see from the benchmarked results, such strategies can not produce models that are able to provide reliable personality predictions. In other words, it is necessary for future studies to systematically review the physiological and psychological findings to concretely model the relationship between true/apparent personality traits and human non-verbal behaviours (e.g., the behaviours/temporal patterns that are more frequently displayed by subjects that have a certain personality traits pattern), and integrate such knowledge to design more advanced personality computing models and training strategies.\n\nBesides the research gaps, we also discuss the challenges for both current and future researchers in developing new and more advanced personality models as follows:\n\n• (i) There is a need of open-source and easily implemented code that allow researcher to improve the existing approaches or develop more advanced models based on it.\n\n• (ii) Existing approaches usually have different and complex pre-processing strategies. A large number of the published approaches do not provide full details of their pre-processing steps, model settings and training strategies due to the limited page numbers. Subsequently, it is difficult for researchers to fully reproduce their methods and results, i.e., our bench-marking results are not as the same as the results reported in their original papers.\n\n• (iii) Due to the limited number of publicly available datasets, most existing personality computing models are only evaluated on a single dataset (e.g., ChaLearn First Impression dataset). Based on the results achieved by our benchmark, we found the same model can have completely different performances in predicting self-reported or apparent personality traits on different a dataset. Therefore, we assume that personality computing models should be evaluated on more than one dataset (i.e., multiple apparent personality datasets or self-reported personality datasets) which have different data recording settings and scenarios. Otherwise, if a model achieves excellent performance on a single dataset, we can only conclude that this model is well adapted to the dataset at hand, but can not be made about its reliability and capability in predicting personality in general.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the first audio-visual personality computing benchmark for both self-reported and apparent personality recognition tasks, which are evaluated on the two widely-used and publicly available personality computing datasets: the ChaLearn First Impression dataset and the ChaLearn UDIVA self-reported personality dataset. We first benchmarked seven visual models, six audio models and five audio-visual models that have been published and evaluated on the ChaLearn First Impression datasets. We also benchmarked seven visual deep learning models that are widely used for visual problems, which have not been applied to video-based personality computing before. Our open-source, easy-to-use and standardised framework can be utilized to not only develop new personality computing models but also conduct quick yet robust evaluation of new models on both self-reported and apparent personality traits recognition tasks.\n\nBuilding on our benchmarked models, we systematically evaluated various factors that impact personality computing. This led us to make the following conclusions: (i) using cropped and aligned face images as input generally led models to produce slightly better personality predictions than using images with background; (ii) the static models frequently achieved better performances than most spatiotemporal models for personality recognition; (iii) visual models frequently achieved superior results than audio models on personality recognition, indicating that subjects' facial behaviours contain more discriminative apparent personality-related cues than non-verbal audio behaviours; (iv) most models achieved better APR results than SPR even when different datasets were used, which suggests that subjects do not show their self-reported personality through non-verbal audio-visual behaviours; and (v) each personality trait has a unique relationship with human non-verbal behaviours (i.e., modalities and temporal scale), and thus even the same data contribute differently for personality recognition. We also discussed the current research gaps and challenges in audio-visual personality computing.\n\nOur work provides a standardized data loading, preprocessing and model training framework as well as a set of reproduced personality computing models, which partially address the discussed issues. We are aware that our benchmark cannot accommodate all machine learning strategies, (e.g., all previous personality computing models, pre/post-processing strategies). However, creating an extensive benchmark should be a joint community effort. Therefore, we have made our benchmark fully open-source, accompanied by detailed guidelines, and welcome contributors from the international research community to enhance various aspects of the pipeline. Our future endeavours will focus on annual improvements to the benchmark by incorporating newly published personality computing models, evaluating benchmarked models on novel personality datasets, and integrating additional modalities (e.g., language, interaction cues). Additionally, we aim to accommodate more machine learning strategies, including advanced self-supervised learning and data augmentation techniques, with our extensions and with the assistance of other researchers working on this research area. We hope that this paper serves as an open call to all interested researchers to join forces with us to integrate their personality computing models into our publicly available framework. Given the challenges of directly inferring self-reported personality from audio-visual behaviours, our future work will also aim to investigate the impact of other contextual cues (e.g., data collection settings/scenarios), as previous studies  [18] ,  [36]  suggested that the behaviours expressed during self-presentation  [86]  or selfies (e.g., photos and videos)  [87]  are more informative for displaying subjects' self-reported personality traits. We will further follow  [86] ,  [87]  to investigate more efficient and effective ways to model personality-related person-specific cognitive processes. In addition, more comprehensive and trustworthiness annotation, behaviour modalities and long-term datasets will also be essential for future self-reported personality recognition studies.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of our personality computing benchmark framework.",
      "page": 4
    },
    {
      "caption": "Figure 2: and Fig. 3, where the standard visual deep learning models",
      "page": 9
    },
    {
      "caption": "Figure 2: and Fig. 3, we hypothesise",
      "page": 10
    },
    {
      "caption": "Figure 4: compares the average results achieved by spatial vi-",
      "page": 10
    },
    {
      "caption": "Figure 2: Comparison between the self-reported personality recognition",
      "page": 11
    },
    {
      "caption": "Figure 3: Comparison between the apparent personality recognition CCC",
      "page": 11
    },
    {
      "caption": "Figure 4: also compares short segment-level modelling and",
      "page": 11
    },
    {
      "caption": "Figure 4: Comparison between average results achieved by frame-level",
      "page": 12
    },
    {
      "caption": "Figure 5: , visual models and audio-visual models",
      "page": 12
    },
    {
      "caption": "Figure 5: Comparison between the average results achieved by the audio",
      "page": 12
    },
    {
      "caption": "Figure 4: show that self-reported Conscientiousness and Neu-",
      "page": 13
    },
    {
      "caption": "Figure 5: illustrates that the self-reported Conscientiousness",
      "page": 13
    },
    {
      "caption": "Figure 6: clearly demonstrates that all human non-",
      "page": 13
    },
    {
      "caption": "Figure 6: Correlation among non-verbal behaviours and personality traits.",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "Video",
          "FFT [61]\nMFCC/Logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nCAM-DAN+ [17]\nPersEmoN [24]\nAmb-Fac [59]\nSENet [64]\nHRNet [65]\nVIT [66]\nSwin-Transformer [67]\n3D-Resnet [68]\nSlow-Fast [69]\nTPN [70]\nVAT [71]",
          "0.01\n0.00083\n0.05\n0.0002\n0.002\n0.001": "0.05\n0.05\n0.05\n0.002\n0.05\n0.001\n0.01\n0.01\n0.01\n0.001\n0.01\n0.01\n0.002\n0.001\n0.01",
          "0.0005\n6.5\n0.0005\n0.0005\n0.005\n0.0005": "0.0005\n0.0005\n0.0005\n0.005\n0.0005\n0.0005\n0.0005\n0.0005\n0.0005\n0.0005\n0.0005\n0.005\n0.0005\n0.0005\n0.0005",
          "0.9\n0.9\n0.9\n0.9\n0.9\n0.9": "0.8\n0.9\n0.9\n0.9\n0.8\n0.9\n0.9\n0.8\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9",
          "SGD\nSGD\nSGD\nSGD\nSGD/Adam\nSGD": "SGD\nSGD\nSGD\nSGD\nSGD/Adam\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD"
        },
        {
          "Audio": "Audio-visual",
          "FFT [61]\nMFCC/Logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "Bi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]",
          "0.01\n0.00083\n0.05\n0.0002\n0.002\n0.001": "0.05\n0.001\n0.002",
          "0.0005\n6.5\n0.0005\n0.0005\n0.005\n0.0005": "0.0005\n0.0005\n0.005",
          "0.9\n0.9\n0.9\n0.9\n0.9\n0.9": "0.9\n0.9\n0.9",
          "SGD\nSGD\nSGD\nSGD\nSGD/Adam\nSGD": "SGD\nSGD\nSGD"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: and Table 4, all bench- Swin-Transformer[67] -0.0273 0.0470 0.0361 -0.0142 0.0860 0.0256",
      "data": [
        {
          "Audio": "Video",
          "FFT [61]\nMFCC/Logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nCAM-DAN+ [17]\nPersEmoN [24]\nAmb-Fac [59]\nSENet [64]\nHRNet [65]\nVIT [66]\nSwin-Transformer [67]\n3D-Resnet [68]\nSlow-Fast [69]\nTPN [70]\nVAT [71]",
          "0.0000\n0.0002\n0.0000\n-0.0459\n0.0005\n0.0688": "0.0009\n-0.0001\n0.0648\n-0.0206\n0.0336\n-0.0095\n0.0532\n0.1678\n0.2175\n-0.0184\n-0.0273\n-0.0478\n-0.0102\n0.0448\n-0.0139",
          "0.0000\n-0.0002\n-0.0001\n0.1045\n0.0290\n0.1882": "0.0087\n0.0000\n0.2424\n0.0879\n0.3359\n0.0133\n0.1941\n0.2776\n0.2998\n0.1532\n0.0470\n0.0102\n0.0076\n0.0348\n-0.0016",
          "0.0000\n0.0002\n0.0000\n-0.0416\n0.0201\n0.1310": "0.0061\n0.0000\n0.0349\n0.012\n0.0270\n0.0045\n0.0442\n0.0538\n-0.0039\n-0.0002\n0.0361\n0.0478\n0.0010\n0.0287\n0.0016",
          "0.0000\n0.0001\n0.0000\n0.0429\n0.0335\n0.1069": "0.0035\n0.0001\n0.0305\n0.1454\n0.2014\n0.0085\n0.0453\n0.0299\n0.1680\n0.0645\n-0.0142\n0.0499\n-0.0063\n-0.0177\n-0.0013",
          "0.0000\n0.0001\n0.0000\n0.0015\n0.0267\n0.0412": "-0.0016\n0.0000\n-0.0009\n0.0645\n0.2709\n0.0058\n0.0204\n0.3093\n0.1945\n0.0607\n0.0860\n-0.0240\n0.0161\n-0.0281\n-0.0006",
          "0.0000\n0.0001\n0.0000\n0.0123\n0.0220\n0.1072": "0.0036\n0.0000\n0.0743\n0.0578\n0.1738\n0.0045\n0.0714\n0.1510\n0.1752\n0.0520\n0.0256\n0.0072\n0.0016\n0.0125\n-0.0031"
        },
        {
          "Audio": "Aud-vis",
          "FFT [61]\nMFCC/Logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN-MFCC [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nAmb-Fac-VGGish [59]",
          "0.0000\n0.0002\n0.0000\n-0.0459\n0.0005\n0.0688": "0.0008\n0.0001\n-0.0530\n0.0998\n-0.0348",
          "0.0000\n-0.0002\n-0.0001\n0.1045\n0.0290\n0.1882": "0.1154\n-0.0001\n0.1290\n0.1780\n0.0468",
          "0.0000\n0.0002\n0.0000\n-0.0416\n0.0201\n0.1310": "-0.0132\n0.0001\n0.0230\n0.1158\n0.0302",
          "0.0000\n0.0001\n0.0000\n0.0429\n0.0335\n0.1069": "-0.0222\n-0.0001\n0.0310\n0.2168\n0.0397",
          "0.0000\n0.0001\n0.0000\n0.0015\n0.0267\n0.0412": "0.1219\n0.0005\n0.0002\n0.0449\n0.0041",
          "0.0000\n0.0001\n0.0000\n0.0123\n0.0220\n0.1072": "0.0405\n0.0001\n0.0260\n0.1311\n0.0171"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "Visual",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LST [27]\nResNet [32]\nCRNet [29]\nVggish-feat [59]": "DAN [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nCAM-DAN+ [17]\nPersEmoN [24]\nAmb-Fac [59]\nSENet [64]\nHRNet [65]\nVIT [66]\nSwin-transformer [67]\n3D-Resnet [68]\nSlow-Fast [69]\nTPN [70]\nVAT [71]",
          "0.8303\n0.8891\n0.8835\n0.8822\n0.9001\n0.9010": "0.9098\n0.8832\n0.8896\n0.8987\n0.9115\n0.8934\n0.9101\n0.9076\n0.9101\n0.8832\n0.8937\n0.8964\n0.8780\n0.9025\n0.9115",
          "0.8174\n0.8790\n0.8747\n0.8780\n0.8895\n0.8962": "0.9106\n0.8742\n0.8835\n0.8932\n0.9139\n0.8893\n0.9141\n0.906\n0.9154\n0.8760\n0.8870\n0.8921\n0.8604\n0.8963\n0.9123",
          "0.8502\n0.8835\n0.8785\n0.8782\n0.8951\n0.8959": "0.9096\n0.8778\n0.8837\n0.8952\n0.9126\n0.8913\n0.9082\n0.908\n0.9111\n0.8778\n0.8893\n0.8933\n0.8443\n0.9019\n0.9153",
          "0.8309\n0.8967\n0.8937\n0.8958\n0.9022\n0.9028": "0.9102\n0.8933\n0.8968\n0.9018\n0.9118\n0.8994\n0.9095\n0.9097\n0.9113\n0.8934\n0.8983\n0.9008\n0.8809\n0.9013\n0.9099",
          "0.8331\n0.8802\n0.8774\n0.8820\n0.8964\n0.8953": "0.9061\n0.8770\n0.8830\n0.8908\n0.9089\n0.8866\n0.9038\n0.9061\n0.9084\n0.8778\n0.8860\n0.8915\n0.8613\n0.8992\n0.9098",
          "0.8324\n0.8857\n0.8816\n0.8832\n0.8967\n0.8982": "0.9093\n0.8811\n0.8873\n0.8960\n0.9118\n0.8920\n0.9091\n0.9075\n0.9113\n0.8817\n0.8909\n0.8948\n0.8650\n0.9003\n0.9118"
        },
        {
          "Audio": "Aud-vis",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LST [27]\nResNet [32]\nCRNet [29]\nVggish-feat [59]": "DAN-MFCC [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nAmb-Fac-VGGish [59]",
          "0.8303\n0.8891\n0.8835\n0.8822\n0.9001\n0.9010": "0.9049\n0.8833\n0.8996\n0.9075\n0.9127",
          "0.8174\n0.8790\n0.8747\n0.8780\n0.8895\n0.8962": "0.9020\n0.8744\n0.8918\n0.9019\n0.9169",
          "0.8502\n0.8835\n0.8785\n0.8782\n0.8951\n0.8959": "0.9024\n0.8779\n0.8945\n0.9017\n0.9117",
          "0.8309\n0.8967\n0.8937\n0.8958\n0.9022\n0.9028": "0.9081\n0.8935\n0.9015\n0.9055\n0.9133",
          "0.8331\n0.8802\n0.8774\n0.8820\n0.8964\n0.8953": "0.9015\n0.8773\n0.8948\n0.9034\n0.9088",
          "0.8324\n0.8857\n0.8816\n0.8832\n0.8967\n0.8982": "0.9038\n0.8813\n0.8964\n0.9040\n0.9127"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "Video",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nCAM-DAN+ [17]\nPersEmoN [24]\nAmb-Fac [59]\nSENet [64]\nHRNet [65]\nVIT [66]\nSwin-transformer [67]\n3D-Resnet [68]\nSlow-Fast [69]\nTPN [70]\nVAT [71]",
          "1.0122\n0.9489\n0.9544\n0.9540\n0.9145\n0.9312": "1.0841\n0.8947\n0.9637\n1.0985\n1.0223\n0.9034\n0.8529\n1.1398\n1.1224\n1.1460\n1.0398\n1.3016\n1.0652\n0.9259\n0.8554",
          "0.7114\n1.0256\n0.7717\n0.6985\n0.6978\n0.6576": "1.1174\n0.6690\n0.6709\n0.6877\n0.5471\n0.6985\n0.6394\n0.6571\n0.8163\n0.8931\n0.7119\n0.7051\n0.9862\n0.7596\n0.7076",
          "1.5056\n2.0083\n1.2788\n1.2325\n1.4821\n1.3345": "2.0979\n1.3459\n1.5770\n1.3079\n1.4800\n1.4097\n1.4212\n1.5526\n1.8893\n2.1999\n1.3394\n1.2360\n2.0394\n1.2524\n1.1398",
          "0.9342\n1.2377\n0.8676\n0.8611\n0.9623\n0.9002": "1.4846\n0.9269\n1.0364\n0.909\n0.8446\n0.9446\n0.9334\n1.1313\n1.0362\n1.2980\n1.0186\n0.8741\n1.2631\n0.9458\n0.8520",
          "1.2851\n1.3526\n1.1965\n1.0744\n1.3858\n1.2227": "1.2400\n1.1518\n1.1526\n1.4140\n1.1493\n1.1469\n1.1410\n1.2733\n1.5447\n1.2843\n1.2767\n1.3855\n1.2956\n1.4147\n1.2286",
          "1.0897\n1.3144\n1.0138\n0.9641\n1.0885\n1.0092": "1.4048\n0.9977\n1.0801\n1.0834\n1.0087\n1.0206\n0.9976\n1.1508\n1.2818\n1.3641\n1.0773\n1.1004\n1.3299\n1.0597\n0.9566"
        },
        {
          "Audio": "Aud-vis",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN-MFCC [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nAmb-Fac-VGGish [59]",
          "1.0122\n0.9489\n0.9544\n0.9540\n0.9145\n0.9312": "1.1657\n0.9281\n1.1417\n1.2834\n0.9391",
          "0.7114\n1.0256\n0.7717\n0.6985\n0.6978\n0.6576": "0.7358\n0.7315\n0.7787\n0.8644\n0.6675",
          "1.5056\n2.0083\n1.2788\n1.2325\n1.4821\n1.3345": "2.0523\n1.2944\n1.6838\n1.2876\n1.4956",
          "0.9342\n1.2377\n0.8676\n0.8611\n0.9623\n0.9002": "1.1006\n0.8625\n1.0732\n1.0442\n0.9588",
          "1.2851\n1.3526\n1.1965\n1.0744\n1.3858\n1.2227": "1.0745\n1.3152\n1.1512\n1.6987\n1.1590",
          "1.0897\n1.3144\n1.0138\n0.9641\n1.0885\n1.0092": "1.2258\n1.0263\n1.1657\n1.2357\n1.0440"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio": "Visual",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nCAM-DAN+ [17]\nPersEmoN [24]\nAmb-Fac [59]\nSENet [64]\nHRNet [65]\nVIT [66]\nSwin-transformer [67]\n3D-Resnet [68]\nSlow-Fast [69]\nTPN [70]\nVAT [71]",
          "0.0002\n0.1968\n-0.0004\n0.1293\n0.4122\n0.4516": "0.5693\n0.0000\n0.1561\n0.3748\n0.5882\n0.2067\n0.5858\n0.5300\n0.5923\n0.0184\n0.2223\n0.3248\n0.0256\n0.4427\n0.6216",
          "-0.0001\n0.1497\n-0.0005\n0.0830\n0.3406\n0.4493": "0.6254\n0.0000\n0.1902\n0.3646\n0.6550\n0.2441\n0.6750\n0.5580\n0.6912\n0.0817\n0.2426\n0.3601\n0.0320\n0.4767\n0.6753",
          "-0.0002\n0.1738\n0.0004\n0.0458\n0.3846\n0.4429": "0.6070\n0.0000\n0.1355\n0.3987\n0.6326\n0.2675\n0.5997\n0.5815\n0.6436\n0.0247\n0.2531\n0.3601\n0.0185\n0.4998\n0.6836",
          "-0.0002\n0.1295\n-0.0005\n0.1101\n0.2857\n0.3127": "0.4855\n0.0000\n0.0838\n0.2390\n0.5003\n0.1369\n0.4971\n0.4493\n0.5195\n0.0175\n0.1224\n0.2120\n0.0105\n0.3230\n0.5228",
          "-0.0003\n0.1780\n-0.0008\n0.1548\n0.4306\n0.4500": "0.6025\n0.0000\n0.1373\n0.3226\n0.6199\n0.1768\n0.5765\n0.5708\n0.6273\n0.0318\n0.1942\n0.3352\n0.0184\n0.4675\n0.6456",
          "-0.0002\n0.1655\n0.0004\n0.1046\n0.3707\n0.4213": "0.5779\n0.0000\n0.1406\n0.3399\n0.5992\n0.2064\n0.5868\n0.5379\n0.6148\n0.0348\n0.2069\n0.3185\n0.0210\n0.4420\n0.6298"
        },
        {
          "Audio": "Aud-vis",
          "FFT [61]\nMFCC/logfbank [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nVGGish [59]": "DAN-MFCC [31]\nBi-modal CNN-LSTM [27]\nResNet [32]\nCRNet [29]\nAmb-Fac-VGGish [59]",
          "0.0002\n0.1968\n-0.0004\n0.1293\n0.4122\n0.4516": "0.4341\n0.0000\n0.4150\n0.5193\n0.5618",
          "-0.0001\n0.1497\n-0.0005\n0.0830\n0.3406\n0.4493": "0.4645\n0.0000\n0.3671\n0.5106\n0.6421",
          "-0.0002\n0.1738\n0.0004\n0.0458\n0.3846\n0.4429": "0.4553\n0.0000\n0.3889\n0.5024\n0.5921",
          "-0.0002\n0.1295\n-0.0005\n0.1101\n0.2857\n0.3127": "0.3519\n0.0000\n0.2679\n0.4026\n0.4620",
          "-0.0003\n0.1780\n-0.0008\n0.1548\n0.4306\n0.4500": "0.4588\n0.0000\n0.4181\n0.5119\n0.5734",
          "-0.0002\n0.1655\n0.0004\n0.1046\n0.3707\n0.4213": "0.4329\n0.0000\n0.3714\n0.4894\n0.5663"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Cambridge handbook of personality psychology",
      "authors": [
        "P Corr",
        "G Matthews"
      ],
      "year": "2020",
      "venue": "The Cambridge handbook of personality psychology"
    },
    {
      "citation_id": "2",
      "title": "Impulse buying: its relation to personality traits and cues",
      "authors": [
        "S Youn",
        "R Faber"
      ],
      "year": "2000",
      "venue": "ACR North American Advances"
    },
    {
      "citation_id": "3",
      "title": "Recognizing academic performance, sleep quality, stress level, and mental health using personality traits, wearable sensors and mobile phones",
      "authors": [
        "A Sano",
        "A Phillips",
        "Z Amy",
        "A Mchill",
        "S Taylor",
        "N Jaques",
        "C Czeisler",
        "E Klerman",
        "R Picard"
      ],
      "year": "2015",
      "venue": "2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN)"
    },
    {
      "citation_id": "4",
      "title": "Automatic prediction of depression and anxiety from behaviour and personality attributes",
      "authors": [
        "S Jaiswal",
        "S Song",
        "M Valstar"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "Personality effects on social relationships",
      "authors": [
        "J Asendorpf",
        "S Wilpers"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Genetic and environmental continuity in personality development: a meta-analysis",
      "authors": [
        "D Briley",
        "E Tucker-Drob"
      ],
      "year": "2014",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "7",
      "title": "The personality of ai systems in education: experiences with the watson tutor, a oneon-one virtual tutoring system",
      "authors": [
        "S Afzal",
        "B Dempsey",
        "C D'helon",
        "N Mukhi",
        "M Pribic",
        "A Sickler",
        "P Strong",
        "M Vanchiswar",
        "L Wilde"
      ],
      "year": "2019",
      "venue": "Childhood Education"
    },
    {
      "citation_id": "8",
      "title": "Relating ocean (big five) to job satisfaction in aviation",
      "authors": [
        "D Mansour",
        "A Bhardwaj",
        "A Chopra"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)"
    },
    {
      "citation_id": "9",
      "title": "Multi-modal score fusion and decision trees for explainable automatic job candidate screening from video cvs",
      "authors": [
        "H Kaya",
        "F Gurpinar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "10",
      "title": "Socially aware conference participant recommendation with personality traits",
      "authors": [
        "F Xia",
        "N Asabere",
        "H Liu",
        "Z Chen",
        "W Wang"
      ],
      "year": "2014",
      "venue": "IEEE Systems Journal"
    },
    {
      "citation_id": "11",
      "title": "Dimensions of personality: 16, 5 or 3?-criteria for a taxonomic paradigm",
      "authors": [
        "H Eysenck"
      ],
      "year": "1991",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "12",
      "title": "An analysis of reinforcement sensitivity theory and the five-factor model",
      "authors": [
        "J Mitchell",
        "N Kimbrel",
        "N Hundt",
        "A Cobb",
        "R Nelson-Gray",
        "C Lootens"
      ],
      "year": "2007",
      "venue": "European Journal of Personality: Published for the European Association of Personality Psychology"
    },
    {
      "citation_id": "13",
      "title": "Cloninger's psychobiological model of temperament and character and the five-factor model of personality",
      "authors": [
        "F De Fruyt",
        "L Van De Wiele",
        "C Van Heeringen"
      ],
      "year": "2000",
      "venue": "Personality and individual differences"
    },
    {
      "citation_id": "14",
      "title": "The five-factor theory of personality",
      "authors": [
        "R Mccrae",
        "P Costa"
      ],
      "year": "2008",
      "venue": "The five-factor theory of personality"
    },
    {
      "citation_id": "15",
      "title": "",
      "authors": [
        "G Matthews",
        "I Deary",
        "M Whiteman"
      ],
      "year": "2003",
      "venue": ""
    },
    {
      "citation_id": "16",
      "title": "Chalearn lap 2016: First round challenge on first impressions-dataset and results",
      "authors": [
        "V Ponce-L Ópez",
        "B Chen",
        "M Oliu",
        "C Corneanu",
        "A Clapés",
        "I Guyon",
        "X Bar",
        "H Escalante",
        "S Escalera"
      ],
      "year": "2016",
      "venue": "Chalearn lap 2016: First round challenge on first impressions-dataset and results"
    },
    {
      "citation_id": "17",
      "title": "Interpreting cnn models for apparent personality trait regression",
      "authors": [
        "C Ventura",
        "D Masip",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "18",
      "title": "Learning person-specific cognition from facial reactions for automatic personality recognition",
      "authors": [
        "S Song",
        "Z Shao",
        "S Jaiswal",
        "L Shen",
        "M Valstar",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Estimation of personality traits from portrait pictures using the five-factor model",
      "authors": [
        "M Moreno-Armendáriz",
        "C Martínez",
        "H Calvo",
        "M Moreno-Sotelo"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Single-modal video analysis of personality traits using low-level visual features",
      "authors": [
        "D Helm",
        "M Kampel"
      ],
      "year": "2020",
      "venue": "2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)"
    },
    {
      "citation_id": "21",
      "title": "Personality traits and job candidate screening via analyzing facial videos",
      "authors": [
        "S Bekhouche",
        "F Dornaika",
        "A Ouafi",
        "A Taleb-Ahmed"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "22",
      "title": "Dimensional affect uncertainty modelling for apparent personality recognition",
      "authors": [
        "M Tellamekala",
        "T Giesbrecht",
        "M Valstar"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Combining deep facial and ambient features for first impression estimation",
      "authors": [
        "H Kaya",
        "A Salah"
      ],
      "year": "2016",
      "venue": "Combining deep facial and ambient features for first impression estimation"
    },
    {
      "citation_id": "24",
      "title": "Persemon: a deep network for joint analysis of apparent personality, emotion and their relationship",
      "authors": [
        "L Zhang",
        "S Peng",
        "S Winkler"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Personality traits classification using deep visual activity-based nonverbal features of key-dynamic images",
      "authors": [
        "C Beyan",
        "A Zunino",
        "M Shahid",
        "V Murino"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Deep bimodal regression for apparent personality analysis",
      "authors": [
        "C.-L Zhang",
        "H Zhang",
        "X.-S Wei",
        "J Wu"
      ],
      "year": "2016",
      "venue": "Deep bimodal regression for apparent personality analysis"
    },
    {
      "citation_id": "27",
      "title": "Bi-modal first impressions recognition using temporally ordered deep audio and stochastic visual features",
      "authors": [
        "A Subramaniam",
        "V Patel",
        "A Mishra",
        "P Balasubramanian",
        "A Mittal"
      ],
      "year": "2016",
      "venue": "Bi-modal first impressions recognition using temporally ordered deep audio and stochastic visual features"
    },
    {
      "citation_id": "28",
      "title": "Multimodal fusion of audio, scene, and face features for first impression estimation",
      "authors": [
        "H Kaya",
        "A Salah"
      ],
      "year": "2016",
      "venue": "2016 23rd International conference on pattern recognition (ICPR"
    },
    {
      "citation_id": "29",
      "title": "Cr-net: A deep classification-regression network for multimodal apparent personality analysis",
      "authors": [
        "Y Li",
        "J Wan",
        "Q Miao",
        "S Escalera",
        "H Fang",
        "H Chen",
        "X Qi",
        "G Guo"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks",
      "authors": [
        "S Aslan"
      ],
      "year": "2019",
      "venue": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks",
      "arxiv": "arXiv:1911.00381"
    },
    {
      "citation_id": "31",
      "title": "Deep bimodal regression of apparent personality traits from short video sequences",
      "authors": [
        "X.-S Wei",
        "C.-L Zhang",
        "H Zhang",
        "J Wu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition",
      "authors": [
        "M Van Gerven",
        "R Van Lier"
      ],
      "year": "2016",
      "venue": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition"
    },
    {
      "citation_id": "33",
      "title": "Spectral representation of behaviour primitives for depression analysis",
      "authors": [
        "S Song",
        "S Jaiswal",
        "L Shen",
        "M Valstar"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Self-supervised learning of person-specific facial dynamics for automatic personality recognition",
      "authors": [
        "S Song",
        "S Jaiswal",
        "E Sanchez",
        "G Tzimiropoulos",
        "L Shen",
        "M Valstar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Dyadformer: A multi-modal transformer for long-range modeling of dyadic interactions",
      "authors": [
        "D Curto",
        "A Clapés",
        "J Selva",
        "S Smeureanu",
        "J Junior",
        "C Jacques",
        "D Gallardo-Pujol",
        "G Guilera",
        "D Leiva",
        "T Moeslund"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "36",
      "title": "Personality recognition by modelling person-specific cognitive processes using graph representation",
      "authors": [
        "Z Shao",
        "S Song",
        "S Jaiswal",
        "L Shen",
        "M Valstar",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "37",
      "title": "A framework for automatic personality recognition in dyadic interactions",
      "authors": [
        "E Dodd",
        "S Song",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "A framework for automatic personality recognition in dyadic interactions"
    },
    {
      "citation_id": "38",
      "title": "Listeners' perceptions of speaker personality traits based on speech",
      "authors": [
        "B Welch",
        "M Van Mersbergen",
        "L Helou"
      ],
      "year": "2021",
      "venue": "Journal of Speech, Language, and Hearing Research"
    },
    {
      "citation_id": "39",
      "title": "Prediction of the big five personality traits using static facial images of college students with different academic backgrounds",
      "authors": [
        "J Xu",
        "W Tian",
        "G Lv",
        "S Liu",
        "Y Fan"
      ],
      "year": "2021",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "40",
      "title": "Automated video interview personality assessments: Reliability, validity, and generalizability investigations",
      "authors": [
        "L Hickman",
        "N Bosch",
        "V Ng",
        "R Saef",
        "L Tay",
        "S Woo"
      ],
      "year": "2022",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "41",
      "title": "Learning personalised models for automatic self-reported personality recognition",
      "authors": [
        "H Salam",
        "V Manoranjan",
        "J Jiang",
        "O Celiktutan"
      ],
      "year": "2022",
      "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions"
    },
    {
      "citation_id": "42",
      "title": "Vptd: Human face video dataset for personality traits detection",
      "authors": [
        "K Kassab",
        "A Kashevnik",
        "A Mayatin",
        "D Zubok"
      ],
      "year": "2023",
      "venue": "Data"
    },
    {
      "citation_id": "43",
      "title": "On the use of interpretable cnn for personality trait recognition from audio",
      "authors": [
        "H Hayat",
        "C Ventura",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "CCIA"
    },
    {
      "citation_id": "44",
      "title": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition",
      "authors": [
        "M Van Gerven",
        "R Van Lier"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops",
      "doi": "10.1007/978-3-319-49409-8_28"
    },
    {
      "citation_id": "45",
      "title": "Context-aware personality inference in dyadic scenarios: Introducing the udiva dataset",
      "authors": [
        "C Palmero",
        "J Selva",
        "S Smeureanu",
        "J Junior",
        "A Clapés",
        "A Moseguí",
        "Z Zhang",
        "D Gallardo",
        "G Guilera",
        "D Leiva"
      ],
      "year": "2021",
      "venue": "WACV"
    },
    {
      "citation_id": "46",
      "title": "First impressions: A survey on vision-based apparent personality trait analysis",
      "authors": [
        "J Junior",
        "M Pérez",
        "C Andujar",
        "X Bar",
        "H Escalante",
        "I Guyon",
        "M Van Gerven",
        "R Van Lier"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Deep bimodal regression for apparent personality analysis",
      "authors": [
        "C.-L Zhang",
        "H Zhang",
        "X.-S Wei",
        "J Wu"
      ],
      "year": "2016",
      "venue": "Deep bimodal regression for apparent personality analysis"
    },
    {
      "citation_id": "48",
      "title": "Automated facial trait judgment and election outcome prediction: Social dimensions of face",
      "authors": [
        "J Joo",
        "F Steen",
        "S.-C Zhu"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "49",
      "title": "First impressions-predicting user personality from twitter profile images",
      "authors": [
        "A Dhall",
        "J Hoey"
      ],
      "year": "2016",
      "venue": "International Workshop on Human Behavior Understanding"
    },
    {
      "citation_id": "50",
      "title": "Learning deep features for discriminative localization",
      "authors": [
        "B Zhou",
        "A Khosla",
        "A Lapedriza",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "51",
      "title": "Continuous prediction of perceived traits and social dimensions in space and time",
      "authors": [
        "O Celiktutan",
        "H Gunes"
      ],
      "year": "2014",
      "venue": "Image Processing (ICIP)"
    },
    {
      "citation_id": "52",
      "title": "Maptraits 2014-the first audio/visual mapping personality traits challenge-an introduction: Perceived personality and social dimensions",
      "authors": [
        "O Celiktutan",
        "F Eyben",
        "E Sariyanidi",
        "H Gunes",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "53",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Modern physiognomy: an investigation on predicting personality traits and intelligence from the human face",
      "authors": [
        "R Qin",
        "W Gao",
        "H Xu",
        "Z Hu"
      ],
      "year": "2018",
      "venue": "Science China Information Sciences"
    },
    {
      "citation_id": "55",
      "title": "The noxi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "A Cafaro",
        "J Wagner",
        "T Baur",
        "S Dermouche",
        "M Torres",
        "C Pelachaud",
        "E André",
        "M Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "56",
      "title": "Multimodal humanhuman-robot interactions (mhhri) dataset for studying personality and engagement",
      "authors": [
        "O Celiktutan",
        "E Skordos",
        "H Gunes"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "58",
      "title": "Performance evaluation and comparison of software for face recognition, based on dlib and opencv library",
      "authors": [
        "N Boyko",
        "O Basystiuk",
        "N Shakhovska"
      ],
      "year": "2018",
      "venue": "2018 IEEE Second International Conference on Data Stream Mining & Processing"
    },
    {
      "citation_id": "59",
      "title": "A multi-modal personality prediction system",
      "authors": [
        "C Suman",
        "S Saha",
        "A Gupta",
        "S Pandey",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "60",
      "title": "Persons' personality traits recognition using machine learning algorithms and image processing techniques",
      "authors": [
        "K Ilmini",
        "T Fernando"
      ],
      "year": "2016",
      "venue": "Advances in Computer Science: an International Journal"
    },
    {
      "citation_id": "61",
      "title": "On the use of interpretable cnn for personality trait recognition from audio",
      "authors": [
        "H Hayat",
        "C Ventura",
        "À Lapedriza"
      ],
      "year": "2019",
      "venue": "CCIA"
    },
    {
      "citation_id": "62",
      "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "63",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "64",
      "title": "Squeeze-andexcitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "S Albanie",
        "G Sun",
        "E Wu"
      ],
      "year": "2019",
      "venue": "Squeeze-andexcitation networks"
    },
    {
      "citation_id": "65",
      "title": "Deep high-resolution representation learning for visual recognition",
      "authors": [
        "J Wang",
        "K Sun",
        "T Cheng",
        "B Jiang",
        "C Deng",
        "Y Zhao",
        "D Liu",
        "Y Mu",
        "M Tan",
        "X Wang",
        "W Liu",
        "B Xiao"
      ],
      "year": "2020",
      "venue": "Deep high-resolution representation learning for visual recognition"
    },
    {
      "citation_id": "66",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "67",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Swin transformer: Hierarchical vision transformer using shifted windows"
    },
    {
      "citation_id": "68",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Slowfast networks for video recognition",
      "authors": [
        "C Feichtenhofer",
        "H Fan",
        "J Malik",
        "K He"
      ],
      "year": "2019",
      "venue": "Slowfast networks for video recognition"
    },
    {
      "citation_id": "70",
      "title": "Temporal pyramid network for action recognition",
      "authors": [
        "C Yang",
        "Y Xu",
        "J Shi",
        "B Dai",
        "B Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "71",
      "title": "Video action transformer network",
      "authors": [
        "R Girdhar",
        "J Carreira",
        "C Doersch",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "Video action transformer network"
    },
    {
      "citation_id": "72",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "73",
      "title": "Human behaviour-based automatic depression analysis using hand-crafted statistics and deep learned spectral features",
      "authors": [
        "S Song",
        "L Shen",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "74",
      "title": "Voices of vlogging",
      "authors": [
        "J.-I Biel",
        "D Gatica-Perez"
      ],
      "year": "2010",
      "venue": "Fourth International AAAI Conference on Weblogs and Social Media"
    },
    {
      "citation_id": "75",
      "title": "A nonverbal behavior approach to identify emergent leaders in small groups",
      "authors": [
        "D Sanchez-Cortes",
        "O Aran",
        "M Mast",
        "D Gatica-Perez"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "76",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "78",
      "title": "Investigations of temperament at three to seven years: The children's behavior questionnaire",
      "authors": [
        "M Rothbart",
        "S Ahadi",
        "K Hershey",
        "P Fisher"
      ],
      "year": "2001",
      "venue": "Child development"
    },
    {
      "citation_id": "79",
      "title": "Revision of the early adolescent temperament questionnaire",
      "authors": [
        "L Ellis",
        "M Rothbart"
      ],
      "year": "2001",
      "venue": "Poster presented at the 2001 biennial meeting of the society for research in child development"
    },
    {
      "citation_id": "80",
      "title": "The next big five inventory (bfi-2): Developing and assessing a hierarchical model with 15 facets to enhance bandwidth, fidelity, and predictive power",
      "authors": [
        "C Soto",
        "O John"
      ],
      "year": "2017",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "81",
      "title": "The hexaco-60: A short measure of the major dimensions of personality",
      "authors": [
        "M Ashton",
        "K Lee"
      ],
      "year": "2009",
      "venue": "Journal of personality assessment"
    },
    {
      "citation_id": "82",
      "title": "Investigating the association between social interactions and personality states dynamics",
      "authors": [
        "D Gundogdu",
        "A Finnerty",
        "J Staiano",
        "S Teso",
        "A Passerini",
        "F Pianesi",
        "B Lepri"
      ],
      "year": "2017",
      "venue": "Royal Society open science"
    },
    {
      "citation_id": "83",
      "title": "Thinking styles and the five-factor model of personality",
      "authors": [
        "L.-F Zhang",
        "J Huang"
      ],
      "year": "2001",
      "venue": "European Journal of Personality"
    },
    {
      "citation_id": "84",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "85",
      "title": "Apparent personality recognition from uncertainty-aware facial emotion predictions using conditional latent variable models",
      "authors": [
        "M Tellamekala",
        "T Giesbrecht",
        "M Valstar"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "86",
      "title": "Your best self helps reveal your true self: Positive self-presentation leads to more accurate personality impressions",
      "authors": [
        "L Human",
        "J Biesanz",
        "K Parisotto",
        "E Dunn"
      ],
      "year": "2012",
      "venue": "Social Psychological and Personality Science"
    },
    {
      "citation_id": "87",
      "title": "Selfies reflect actual personality-just like photos or short videos in standardized lab conditions",
      "authors": [
        "A Kaurin",
        "L Heil",
        "M Wessa",
        "B Egloff",
        "S Hirschm Üller"
      ],
      "year": "2018",
      "venue": "Journal of Research in Personality"
    }
  ]
}