{
  "paper_id": "2404.17858v2",
  "title": "Revisiting Multi-Modal Emotion Learning With Broad State Space Models And Probability-Guidance Fusion",
  "published": "2024-04-27T10:22:03Z",
  "authors": [
    "Yuntao Shou",
    "Tao Meng",
    "Fuchen Zhang",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Multi-modal Emotion Recognition",
    "State Space Model",
    "Broad Learning System",
    "Feature Fusion",
    "Probabilistic Guidance"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal Emotion Recognition in Conversation (MERC) has received considerable attention in various fields, e.g., human-computer interaction and recommendation systems. Most existing works perform feature disentanglement and fusion to extract emotional contextual information from multi-modal features and emotion classification. After revisiting the characteristic of MERC, we argue that long-range contextual semantic information should be extracted in the feature disentanglement stage and the inter-modal semantic information consistency should be maximized in the feature fusion stage. Inspired by recent State Space Models (SSMs), Mamba can efficiently model long-distance dependencies. Therefore, in this work, we fully consider the above insights to further improve the performance of MERC. Specifically, on the one hand, in the feature disentanglement stage, we propose a Broad Mamba, which does not rely on a self-attention mechanism for sequence modeling, but uses state space models to compress emotional representation, and utilizes broad learning systems to explore the potential data distribution in broad space. Different from previous SSMs, we design a bidirectional SSM convolution to extract global context information. On the other hand, we design a multi-modal fusion strategy based on probability guidance to maximize the consistency of information between modalities. Experimental results show that the proposed method can overcome the computational and memory limitations of Transformer when modeling long-distance contexts, and has great potential to become a next-generation general architecture in MERC. \n CCS CONCEPTS ‚Ä¢ Computing methodologies ‚Üí Discourse, dialogue and pragmatics; Non-negative matrix factorization; ‚Ä¢ Theory of computation ‚Üí Fixed parameter tractability.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation  [2, 10, 39, 48, 55]  has received considerable research attention and has been widely used in various fields, e.g., emotion analysis  [20]  and public opinion warning  [56] , etc. Recently, research on Multi-modal Emotion Recognition in Conversation (MERC) has mainly focused on multimodality, i.e., text, video and audio  [4, 8, 35, 40, 46, 50] . As shown in Fig.  1 , MERC aims to identify emotion labels in sentences with text, video, and audio information. Unlike previous work  [26]  that only uses text information for emotion recognition, MERC improves the model's emotion understanding capabilities by introducing audio and video information  [19, 49] . The introduction of audio and video alleviates the limitation of insufficient semantic information caused by relying solely on text features.\n\nMany existing works  [33, 52, 62]  improve the performance of MERC by effectively extracting contextual semantic information of different modalities and fusing inter-modal complementary semantic information. By revisiting the characteristics of MERC, we argue that the core idea of MERC includes a feature disentanglement step and a feature fusion step.\n\nSpecifically, the goal of feature disentanglement is to extract the contextual semantic information most relevant to emotional features in multi-modal features  [54, 60] . Recent work on Transformers  [9, 30, 32]  has achieved great success in modeling long-range contextual semantic information. Compared with traditional Recurrent Neural Networks (RNNs)  [29, 37] , the advantage of Transformer is that it can effectively provide global contextual semantic information through the attention mechanism in parallel. However, the quadratic complexity of the self-attention mechanism in Transformers poses challenges in terms of speed and memory when dealing with long-range context dependencies. Inspired by the state space models, Mamba with linear complexity is proposed to achieve efficient training and inference. Mamba's excellent scaling performance shows that it is a promising Transformer alternative for context modeling. Therefore, to efficiently extract long-distance contextual semantic information, we designed the broad Mamba, which incorporates the SSMs for data-dependent global emotional context modeling, and a broad learning system to explore the potential data distribution in the broad space. Different from previous SSMs, we design a bidirectional SSM convolution to extract global context information. In addition, we also introduce position encoding information to improve SSMs' ability to understand sequences at different positions.\n\nAfter completing feature disentanglement, the model needs to perform feature fusion to maximize the consistency of information between different modalities. The core idea of feature fusion is to assign different weights by determining the importance of different modal features to downstream tasks. Many cross-modal feature fusions have been proposed in existing MERC research, e.g., tensor fusion network  [61] , graph fusion network  [63] , attention fusion  [46] . However, the feature fusion process in previous works is relatively coarse-grained and cannot actually determine the contribution of each modal feature to downstream tasks. We argue that label information plays an important role in guiding multi-modal information fusion. Therefore, how to properly fuse multi-modality and determine the contribution of multi-modal features to downstream tasks in a fine-grained manner remains a challenge  [24, 41] .\n\nTo tackle the above problems, we propose an effective probabilityguided fusion mechanism to achieve multi-modal contextual feature fusion, which utilizes the predicted label probability of each modal feature as the weight vectors of the modal features. Compared with other feature fusion models for emotion recognition tasks, the proposed fusion method can utilize the predicted label probability information in a fine-grained manner to actually determine the contribution of different modal features to the emotion prediction task.\n\nTo evaluate the effectiveness and efficiency of our proposed method, we conduct extensive experiments on two widely used benchmark datasets, IEMOCAP and MELD. In fact, the proposed method achieves state-of-the-art performance with low computational consumption, and experimental results demonstrate its effectiveness and efficiency.\n\nOverall, our main contributions can be summarized as follows:\n\n‚Ä¢ We propose a Broad Mamba, which combines a broad learning system for searching abstract emotional features in a broad space and a SSM for data-dependent global emotional context information extraction. Different from previous SSMs, we design a bidirectional SSM convolution to extract global context information. ‚Ä¢ We propose an effective probability-guided fusion mechanism to achieve multi-modal contextual feature fusion, which utilizes the predicted label probability of each modal feature as the weight vectors of the modal features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work 2.1 Multi-Modal Emotion Recognition In Conversation",
      "text": "In the early eras, GRU  [7]  and LSTM  [18]  are the de-facto standard network designs for Natural Language Processing (NLP). Many recurrent neural network architectures  [15, 16, 22, 29, 37]  have been proposed for various Multi-modal Emotion Recognition in Conversation (MERC). The pioneering work, Transformer changed the landscape by enabling efficient parallel computing under the premise of long sequence modeling. Transformer treats text as a series of 1D sequence data and applies an attention architecture to achieve sequence modeling. Transformer's surprising results on long sequence modeling and its scalability have encouraged considerable follow-up work for MERC  [6, 31, 32, 47, 64] . One line of works focus on achieving intra-modal and inter-modal information fusion. For example, CTNet  [32]  proposes a single Transformer and cross Transformer. CKETF  [12]  constructs a Context and Knowledge Enriched Transformer. TL-ERC applies the Transformer with the transfer learning. Another pioneering work, Graph Neural Network (GNN)  [57] [58] [59]  further improved the performance of ERC. The core idea of GNN is to learn the representation of nodes or graphs through the feature information of nodes and the connection relationships in the graph structure  [5, 42, 63] . For instance, DialogueGCN  [11]  proposes to use context information to build dialogue graphs. DER-GCN  [1]  fuses event relationships into speaker relationship graphs. These dominant follow-up works have demonstrated excellent performance and higher efficiency on various multi-modal conversational emotion recognition data sets by introducing attention mechanisms or GNNs. In this work, we draw inspiration from Mamba and explore the ability to build a state space model (SSM) based model to improve multi-modal emotion representation learning without efficient parallel sequence modeling using attention, while retaining the sequence modeling advantages of Transformer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "State Space Models",
      "text": "The State Space Models (SSMS) is used to describe the dynamic change process consisting of observed values and unknown internal state variables. Gu et al.  [14]  proposes a Structured State Space Sequence (S4) model, an alternative to the Transformer architecture that models long-range dependencies without using attention. The property of linear complexity of state space sequence lengths has received considerable research attention. Smith et al.  [51]  improves S4 by introducing MIMO SSM and efficient parallel scanning into the S4 layer to achieve parallel initialization and state reset of the hidden layer. He et al.  [17]  proposes introducing dense connection layers into SSM to improve the feature representation ability of shallow hidden layer states. Mehta et al.  [38]  improves the memory ability of the hidden layer by introducing gated units on S4. Recently, Gu et al.  [13]  proposes the general language model Mamba, which has better sequence modeling capabilities than Transformers and is linearly complex. Zhu et al.  [65]  introduces bidirectional SSM based on Mamba to improve the context information representation of the hidden layer. In this work, we are inspired by Mamba to transfer SSM to emotion representation learning without attention computation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminary Information 3.1 Multi-Modal Feature Extraction",
      "text": "Word Embedding: Following previous work  [28, 36] , we use RoBerta in this paper to obtain context-embedded representations of text. Specifically, we first segment the input text and add the start symbol '[CLS]' and the end symbol '[SEP]'. The processed input data is then passed to the RoBERTa model to obtain contextual representations ùùÉ ùë° of the text.\n\nVisual and Audio Feature Extraction: For video and audio features, following previous work  [9, 28] , we utilize DenseNet and openSMILE for feature extraction and obtain video embedding features ùùÉ ùë£ and audio embedding features ùùÉ ùëé , respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "State Space Model",
      "text": "The State Space Model (SSMs) is an efficient sequence modeling model that can capture the dynamic changes of data over time. Owing to the efficient sequence modeling capabilities, SSM has received widespread attention in various fields, e.g., video understanding and image segmentation. A typical SSM consists of a state equation and an observation equation, where the state equation describes the dynamic changes within the system, and the observation equation describes the connection between the system state and observations. Given an input ùë• (ùë°) ‚àà R and a hidden state ‚Ñé(ùë°) ‚àà R, ùë¶ (ùë°) is obtained mathematically through a linear ordinary differential equations (ODE) as follows:\n\nwhere A ‚àà R ùëÅ √óùëÅ is the evolution parameter and B ‚àà R ùëÅ √ó1 , C ‚àà R 1√óùëÅ are the projection parameters, and ùëÅ is the latent state size.\n\nSSM is a continuous time series model, which is difficult to efficiently integrate into deep learning algorithms. Inspired by SSM, Mamba discretizes ODEs to achieve computational efficiency.\n\nThe overall architecture of Broad Learning System (BLS). Z ùëñ represents the feature nodes, H ùëñ represents the enhancement nodes, and Y represents the predicted labels.\n\nMamba discretizes the evolution parameter A and the projection parameter B by introducing a timescale parameter ùö´ to obtain A and B. The formula is defined as follows:\n\nIn practice, we use a first-order Taylor series to obtain an approximation of B as follows:\n\nAfter obtaining the discretized A and B, we rewrite Eq. 1 as follows:\n\nand then the output is computed via global convolutiona as follows:\n\nWe adopted Mamba as a sequence modeling method in this work since Mamba can efficiently process sequence data without significant performance degradation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Broad Learning System",
      "text": "Broad Learning System (BLS) is different from traditional deep learning methods that it mainly focuses on discovering the relationship between features in the input data, rather than extracting features through multi-level nonlinear transformations. The core idea of BLS is to jointly solve the optimization problem by integrating the semantic information of feature nodes and enhancement nodes. Notably, the feature nodes and enhancement nodes only contain one layer of learnable network parameters, so BLS has faster inference speed than other deep learning architectures. The overall process of the BLS algorithm is shown in the Fig.  2 .\n\nSpecifically, for a given input data X ‚àà R ùëÅ √óùëÄ , where ùëÅ represents the number of samples and ùëÄ represents the dimension of the feature. The generated feature nodes are defined as follows:\n\nwhere W ùëß ùëñ ‚àà R ùëÄ √óùëë ùëß and ùõΩ ùëß ‚àà R 1√óùëë ùëß are the learnable parameters. ùëë ùëß is the embedding dimensions of generated features and ùúô is the activation function. The set of generated feature nodes is represented as\n\nwhere ùëõ is the size of the set of generated feature nodes. Similarly, enhancement node features are defined as follows:\n\nwhere W ‚Ñé ùëñ ‚àà R ùëë ùëß √óùëë ‚Ñé and ùõΩ ùëß ‚àà R 1√óùëë ‚Ñé are the learnable parameters. ùëë ‚Ñé is the embedding dimensions of enhancement features. The set of enhancement feature nodes is represented as\n\nwhere ùëö is the size of the set of enhancement feature nodes. The final model output by concatenating feature nodes and enhancement nodes is as follows:\n\nwhere W is the learnable parameters.\n\nwhere Œæ ùë° ‚àà R ùëá ùë° √óùëë ùëö , Œæ ùëé ‚àà R ùëá ùëé √óùëë ùëö , and Œæ ùë£ ‚àà R ùëá ùë£ √óùëë ùëö , ùëá ùë° ,ùëá ùëé ,ùëá ùë£ represent the feature dimensions of text, audio, and video respectively, ùëë ùëö represents the output feature dimensions. Furthermore, to facilitate the model to capture the dependencies between long-distance positions in the sequence, we introduce sine and cosine position encoding embedding as follows:  (10)  where ùëùùëúùë† represents the position in the sequence. ùëñ represents the dimension index of position encoding, ùëñ = 0, 1, ..., ùê∑ -1. ùê∑ represents the embedded dimension. We input Œæ ùë° , Œæ ùëé , ùëéùëõùëë Œæ ùë£ ( Œæ ùë° , Œæ ùëé , Œæ ùë£ = ùê∂ùëúùëõùë£1ùê∑ ùë° /ùëé/ùë£ ùùÉ ùë° , ùùÉ ùëé , ùùÉ ùë£ + ùëÉùê∏) that encodes position information at each time step into Broad Mamba.\n\n4.1.2 Broad Mamba. The overall architecture of the proposed Broad Mamba is shown in Fig.  4 . In order to aggregate the contextual semantic information from the forward and backward directions, we build a bidirectional SSM convolution module. Specifically, the first kernel ‚Üê -ùúø performs a 1D convolution operator to obtain forward context information. The second kernel -‚Üí ùúø performs a 1D convolution operator to obtain the mutual information associated with emotional information, and we add the two convolved results. The overall operating process is formally defined as follows:\n\n) where ‚Üê -ùúø , and -‚Üí ùúø are obtained via Eq. 5.\n\nTo explore the potential data distribution of multi-modal data in the broad space and improve the performance of Mamba, we",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dropout",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Add &Norm",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ssm Forward Convolution",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ssm Backward Convolution",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Linear",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Add &Norm",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Linear",
      "text": "Bi-SSM",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality Features",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Braod Learning Systems",
      "text": "Figure  4 : The overall architecture of Broad Mamba. We use a bidirectional SSM to encode forward and reverse contextual semantic information.\n\nuse Broad Learning Sytems (BLS) to enhance the emotional representation ability of features. Specifically, we map the features output by BiSSM to a random broad space and obtain feature nodes and enhancement nodes, and concatenate the feature nodes and enhancement nodes as the input of the feature fusion layer. Specifically, feature nodes can be formally defined as follows:\n\nand the enhancement nodes can be computed as:\n\nFurthermore, we introduce l2 regularization into the loss function to avoid the overfitting phenomenon of BLS, which is formally defined as follows:\n\nwhere ùúÜ is the weight decay coefficient, W",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Fusion",
      "text": "4.2.1 Probability-guided Fusion Model. Many studies have proven that different modalities have different contributions to the prediction of emotional labels, so modal features with higher contributions need to be given greater weight in the multi-modal feature fusion process. Different from previous works that fuse modal features at a coarse-grained level without using label information for guidance, we design a probability-guided fusion model (PFM) that dynamically assigns weights to each modality by using the predicted emotion label of the modalities. Specifically, we build an emotion classifier for the feature representation of each modality to obtain the predicted probability of the label as the weight of the modal features in the fusion process. The fusion process is formally defined as follows:\n\nand then we can obtain the fused multi-modal feature representations as follows:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "Finally, we utilize the fused multi-modal emotional context features for emotion classification. The formula is defined as follows:\n\nwhere ≈∑ùëñ is the predicted emotion labels, MLP represents the multilayer perceptron with multiple layers of learnable parameters. We use cross-entropy loss as the classification loss of the model, and the formula is defined as follows:\n\nwhere ùë¶ ùëñ is the true emotion labels, ùëõ is the number of the samples. Therefore, during the optimization phase of the model, the overall training loss function is defined as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we firstly introduce the experimental data set and evaluation metrics. Then the baselines method in the experiments are explained. We then compare the proposed method with baseline work on two benchmark datasets. Secondly, we conduct ablation studies to analyze the effectiveness of the proposed module and the importance of multi-modal features. Finally, we use tsne to visualize the distribution of the data set. In comparative experiments, our experimental results are the average of 10 runs with",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "In the experiments, we use AdamW as the optimizer to update the parameters of the network. The model learning rate is set to 1e-4.\n\nFor the experiment, the number of feature nodes ùëõ and the number of enhancement node features ùëö are set to 10 and 30 respectively. Following previous work, we use the same split ratio of training, test, and validation sets for model training and inference.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "We conduct experiments using two popular MERC datasets, IEMO-CAP  [3]  and MELD  [44] , which include three modal data: text, audio, and video. IEMOCAP contains 12 hours of conversations, each containing six emotion labels. The MELD dataset contains conversation clips from the TV show Friends and contains seven different emotion labels. In addition, in the experiments we report the accuracy (Acc.), and F1 of the proposed method and other baseline methods on each emotion category and the overall weighted average accuracy (W-Acc.), and weighted average F1 (W-F1).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "We compare several baselines on the IEMOCAP and MELD datasets, including bc-LSTM  [43] , LFM  [34] , A-DMN  [55] , DialogueGCN  [11] , RGAT  [21] , CoMPM  [27] , EmoBERTa  [25] , COGMEN  [23] , CTNet  [32] , LR-GCN  [45] , DER-GCN  [1] , AdaGIN  [53] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Results",
      "text": "Tables  1  and 2  show the experimental results on the IEMOCAP and MELD data sets. Experimental results show that our method significantly improves the recognition performance of emotion recognition. Specifically, on the MELD dataset, our model improves by 1.40% and 0.80% compared to the best-performing baselines W-Acc and W-F1, respectively. Similarly, on the IEMOCAP data set, our model improves by 2.60% and 2.60% on W-Acc and W-F1 respectively. The performance improvement may be attributed to the effective extraction of contextual semantic information and efficient integration of underlying data distribution. Furthermore, our method is optimal compared with other multimodal fusion methods in experimental results. The results demonstrate the effectiveness of our model in achieving multi-modal semantic information fusion. We also give W-Acc and W-F1 for each emotion. Specifically, on the IEMOCAP data set, our model's W-Acc is optimal on neutral and frustrated, and W-F1 is optimal on happy, neutral, and frustrated. On the MELD data set, our model's W-Acc is optimal on neutral and frustrated, and W-F1 is optimal on happy, neutral, and frustrated.\n\nWe also report the model parameter quantities of the proposed method and the baseline method. The results show that the parameter amount of our model is 1.73M, which is far lower than other methods. The model complexity of other baseline methods is relatively high, but the emotion recognition effect is relatively poor. Experimental results show that the larger the number of parameters, the better the model performance is not necessarily. Experimental results demonstrate that the proposed method is an effective and efficient MERC model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Running Time",
      "text": "In this section, we report the inference time of different baselines and our proposed method on the IEMOCAP and MELD datasets. As shown in Table  5 , the inference time of our method is below 10s, which is much lower than some GCN-based methods and",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "Ablation studies for PE, BLS, PFM. As shown in Table  3 , we found that the performance of the model will decrease after removing PE, which indicates that positional encoding information is quite important for understanding contextual semantic information. Furthermore, without BLS, the performance of the model also degrades. The performance degradation is attributed to the underlying contextual data distribution which is also crucial for emotion prediction. Finally, when the PFM module is removed, the performance of the model drops sharply. Experimental results demonstrate the necessity of multi-modal feature fusion. Different modalities contribute differently to the model's understanding of emotional information, and using label probabilities can guide the model to adaptively learn the weights of different modal features to better integrate multi-modal features.\n\nAblation studies for multi-modal features. To show the impact of different modal features on experimental results, we conducted ablation experiments to verify the combination of different modal features. From the experimental results in Table  IV , it is found  that: (1) In the single-modal experimental results of the model, the accuracy of emotion recognition in text mode is far better than the other two modes, indicating that text features play a dominant role in emotion recognition. effect. The emotion recognition effect of video modality features is the worst (2) The emotion recognition effect using bimodal features is better than its own single-modality result. Furthermore, since text features play a dominant role in emotion recognition, this results in the bimodal feature combination with text modality performing better than the combination of acoustic and visual modalities. (3) The emotion recognition effect using three modal features is optimal. Experimental results prove the necessity of fusion of multi-modal features for emotion recognition.\n\nEffect of Different Fusion Strategies. To study the effectiveness of the probability-guided fusion method proposed in this paper, we compare it with some previous multi-modal fusion strategies:\n\n(1) Add: multi-modality is implemented by element-wise addition of multi-modal features. Information fusion. (2) Concatenate: directly concatenating multi-modal features. LFM: feature fusion is achieved by introducing a low-rank tensor fusion network.\n\nAs shown in Fig.  5 , compared with other fusion methods, the probability-guided fusion strategy we proposed has better emotion recognition effects on the two data sets. The results show that the emotion recognition effect of directly adding or concatenating multi-modal features to achieve multi-modal information fusion is relatively poor. The multi-modal information fusion effect of LMF is better than the adding method and the concatenating method. The probabilistic fusion strategy we propose introduces label information to guide the fusion of multi-modal information and further achieves parameter optimization of the model. Interestingly, the fusion effect of the concatenate method on the IEMOCAP data set is better than that of the add method, but the effect on the MELD data set is worse than that of the add method. This may be because the dimensionality of the multi-modal features of the MELD dataset is relatively high, so dimensionality disaster may occur by concatenating multi-modal information. In contrast, our proposed probabilistic guided model effectively uses label information to guide the consistency of multi-modal semantic information.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Multi-Modal Representation Visualization",
      "text": "In order to intuitively demonstrate the classification results of our proposed method on the two data sets, we use t-SNE to project the high-dimensional multi-modal feature representation into a two-dimensional space, as shown in Fig.  6 . The results show that the proposed method is able to effectively separate different emotion categories from each other. However, there are still a small number of samples that are inseparable. In the future, we will consider constructing more stringent target constraints to optimize the distribution of different emotion categories in the feature space, so as to further improve the emotion recognition performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Error Analysis",
      "text": "As shown in Fig.  7 , we test the emotion classification results of Dia-logueRNN, DialogueGCN and the proposed method on the MELD dataset. In the disgust emotion category, the classification results of DialogueRNN and DialogueGCN are very poor, and they are all misclassified as neutral emotions. When the proposed method only uses text features, the emotion classification effect on the disgust category is unstable, but when multi-modal features are used, it can better classify disgust category emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we introduce a novel MERC method that comprehensively considers both feature disentanglement and multi-modal feature fusion. Specifically, during the feature disentanglement, we designed the broad Mamba, which incorporates the SSMs for datadependent global emotional context modeling, and a broad learning system to explore the potential data distribution in the broad space. Thanks to the proposed bidirectional SSMs, our method can efficiently extract global long-distance contextual semantic information, while only having linear complexity. During the multi-modal feature fusion, we propose an effective probability-guided fusion mechanism to achieve multi-modal contextual feature fusion, which utilizes the predicted label probability of each modal feature as the weight vectors of the modal features. Extensive experiments conducted on two widely used benchmark datasets, IEMOCAP and MELD demonstrate the effectiveness and efficiency of our proposed method.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustrative example of multi-modal emotion",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall architecture of Broad Learning System",
      "page": 3
    },
    {
      "caption": "Figure 2: Specifically, for a given input data X ‚ààRùëÅ√óùëÄ, where ùëÅrepre-",
      "page": 3
    },
    {
      "caption": "Figure 3: The overall framework of the proposed model. Specifically, we first input the extracted multi-modal features into a",
      "page": 4
    },
    {
      "caption": "Figure 4: In order to aggregate the contextual",
      "page": 4
    },
    {
      "caption": "Figure 4: The overall architecture of Broad Mamba. We use a",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotion recognition effects of different fusion",
      "page": 7
    },
    {
      "caption": "Figure 6: Visualizing feature embeddings for the multi-modal emotion on the IEMOCAP and meld benchmark dataset. Each",
      "page": 8
    },
    {
      "caption": "Figure 5: , compared with other fusion methods, the",
      "page": 8
    },
    {
      "caption": "Figure 7: An illustrative example of multi-modal emotion",
      "page": 8
    },
    {
      "caption": "Figure 6: The results show that",
      "page": 8
    },
    {
      "caption": "Figure 7: , we test the emotion classification results of Dia-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "‚Ä¶‚Ä¶",
          "Column_2": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Forward SSM": "Backward SSM"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Forward SSM": "Backward SSM"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DER-GCN: Dialog and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialog Emotion Recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning",
      "authors": [
        "Wei Ai",
        "Fuchen Zhang",
        "Tao Meng",
        "Yuntao Shou",
        "Hongen Shao",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Learning what and when to drop: Adaptive multimodal and contextual dynamics for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Zhengxiao Sun",
        "Deqiang Ouyang",
        "Xueliang Liu",
        "Jie Shao"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
      "authors": [
        "Wei-Lin Chiang",
        "Xuanqing Liu",
        "Si Si",
        "Yang Li",
        "Samy Bengio",
        "Cho-Jui Hsieh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "6",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "NIPS 2014 Workshop on Deep Learning"
    },
    {
      "citation_id": "8",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Mgat: Multigranularity attention based transformers for multi-modal emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiaofen Xing",
        "Bolun Cai",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "Ankita Gandhi",
        "Kinjal Adhvaryu",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Asif Ekbal, and Pushpak Bhattacharyya. 2021. Context and knowledge enriched transformer framework for emotion recognition in conversations",
      "authors": [
        "Soumitra Ghosh",
        "Deeksha Varshney"
      ],
      "venue": "2021 International joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "13",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "14",
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": [
        "Albert Gu",
        "Karan Goel",
        "Christopher Re"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "17",
      "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
      "authors": [
        "Wei He",
        "Kai Han",
        "Yehui Tang",
        "Chengcheng Wang",
        "Yujie Yang",
        "Tianyu Guo",
        "Yunhe Wang"
      ],
      "year": "2024",
      "venue": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
      "arxiv": "arXiv:2403.00818"
    },
    {
      "citation_id": "18",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "19",
      "title": "Semantic alignment network for multi-modal emotion recognition",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Chang Liu",
        "Guangming Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "20",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Relationaware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "22",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "24",
      "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "authors": [
        "Wei Ju",
        "Siyu Yi",
        "Yifan Wang",
        "Zhiping Xiao",
        "Zhengyang Mao",
        "Hourun Li",
        "Yiyang Gu",
        "Yifang Qin",
        "Nan Yin",
        "Senzhang Wang"
      ],
      "year": "2024",
      "venue": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
      "arxiv": "arXiv:2403.04468"
    },
    {
      "citation_id": "25",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "26",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "27",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "28",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Lizi Liao",
        "Yu Zhao",
        "Chong Teng",
        "Tat-Seng Chua",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "BiERU: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "30",
      "title": "SKIER: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "Wei Li",
        "Luyao Zhu",
        "Rui Mao",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "32",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "Shuai Liu",
        "Peng Gao",
        "Yating Li",
        "Weina Fu",
        "Weiping Ding"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "34",
      "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational MM '24"
    },
    {
      "citation_id": "36",
      "title": "A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Long Range Language Modeling via Gated State Spaces",
      "authors": [
        "Harsh Mehta",
        "Ankit Gupta",
        "Ashok Cutkosky",
        "Behnam Neyshabur"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "39",
      "title": "A multimessage passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Haiyan Liu",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "40",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "41",
      "title": "Revisiting Multimodal Emotion Recognition in Conversation from the Perspective of Graph Spectrum",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting Multimodal Emotion Recognition in Conversation from the Perspective of Graph Spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "42",
      "title": "Scattering gcn: Overcoming oversmoothness in graph convolutional networks",
      "authors": [
        "Yimeng Min",
        "Frederik Wenkel",
        "Guy Wolf"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "44",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "LR-GCN: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Wenhui Li",
        "Dan Song",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Interactive multimodal attention network for emotion recognition in conversation",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Xiaoqi Shi",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "47",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "49",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Sihan Yang",
        "Keqin Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "50",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "51",
      "title": "Simplified State Space Layers for Sequence Modeling",
      "authors": [
        "Jimmy Th Smith",
        "Andrew Warrington",
        "Scott Linderman"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "52",
      "title": "Layer-wise fusion with modality independence modeling for multi-modal emotion recognition",
      "authors": [
        "Jun Sun",
        "Shoukang Han",
        "Yu-Ping Ruan",
        "Xiaoning Zhang",
        "Shu-Kai Zheng",
        "Yulong Liu",
        "Yuxin Huang",
        "Taihao Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "53",
      "title": "Adaptive Graph Learning for Multimodal Conversational Emotion Detection",
      "authors": [
        "Geng Tu",
        "Tian Xie",
        "Bin Liang",
        "Hongpeng Wang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Inter-Modality and Intra-Sample Alignment for Multi-Modal Emotion Recognition",
      "authors": [
        "Yusong Wang",
        "Dongyuan Li",
        "Jialun Shen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "Songlong Xing",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Research on public opinion sentiment classification based on attention parallel dual-channel deep learning hybrid model",
      "authors": [
        "Chun Yan",
        "Jiahui Liu",
        "Wei Liu",
        "Xinhong Liu"
      ],
      "year": "2022",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "57",
      "title": "Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Long Lan",
        "Zeyu Ma",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Xiao Luo"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "58",
      "title": "Omg: towards effective graph classification against label noise",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Mengzhu Wang",
        "Xiao Luo",
        "Zhigang Luo",
        "Dacheng Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "59",
      "title": "Messages are never propagated alone: Collaborative hypergraph neural network for time-series forecasting",
      "authors": [
        "Nan Yin",
        "Li Shen",
        "Huan Xiong",
        "Bin Gu",
        "Chong Chen",
        "Xian-Sheng Hua",
        "Siwei Liu",
        "Xiao Luo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "60",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "61",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Structure Aware Multi-Graph Network for Multi-Modal Emotion Recognition in Conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Jianlong Chang",
        "Xiuyi Chen",
        "Qi Tian"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "63",
      "title": "Dualgats: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "65",
      "title": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "authors": [
        "Lianghui Zhu",
        "Bencheng Liao",
        "Qian Zhang",
        "Xinlong Wang",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "year": "2024",
      "venue": "Vision mamba: Efficient visual representation learning with bidirectional state space model",
      "arxiv": "arXiv:2401.09417"
    }
  ]
}