{
  "paper_id": "2311.13678v2",
  "title": "End-To-End Transfer Learning For Speaker-Independent Cross-Language And Cross-Corpus Speech Emotion Recognition",
  "published": "2023-11-22T20:11:16Z",
  "authors": [
    "Duowei Tang",
    "Peter Kuppens",
    "Luc Geurts",
    "Toon van Waterschoot"
  ],
  "keywords": [
    "Cross-language",
    "Cross-corpus",
    "Speech Emotion Recognition",
    "Transfer learning",
    "Deep within-class covariance normalisation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Data-driven models achieve successful results in Speech Emotion Recognition (SER). However, these models, which are often based on general acoustic features or end-to-end approaches, show poor performance when the testing set has a different language than the training set (i.e. in a cross-language setting) or when these sets are taken from different datasets (i.e. in a cross-corpus setting). To alleviate these problems, this paper presents an end-to-end Deep Neural Network (DNN) model based on transfer learning for cross-language and cross-corpus SER. We use the wav2vec 2.0 pre-trained model to transform audio time-domain waveforms from different languages, different speakers and different recording conditions into a feature space shared by multiple languages, thereby reducing the language variabilities in the speech embeddings. Next, we propose a new Deep-Within-Class Covariance Normalisation (Deep-WCCN) layer that can be inserted into the DNN model and aims to reduce other variabilities including speaker variability, channel variability and so on. The entire model is fine-tuned in an end-to-end manner on a combined loss and is validated on datasets from three languages (i.e. English, German, Chinese). Experimental results show that our proposed method not only outperforms the baseline model that is based on common acoustic feature sets for SER in the within-language setting, but also significantly outperforms the baseline model for the cross-language setting. In",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotions we daily experience determine for a large part our mental flourishing and suffering. Happiness, or psychological well-being, relies greatly on how people experience positive and negative emotions in their lives  [1] . Modern Human Computer Interaction (HCI) systems use image/video, speech, and physiological signals to determine people's emotion  [2] . Vocal expression is a direct and affectionate way of expressing emotions that has the advantage of being more accessible than image/video and physiological signals, for which a careful camera positioning or a well-worn wearable device is needed. As a result, SER is widely used in many applications, such as, an in-car board system that can provide aids or resolve errors in the communication according to the driver's emotion  [3] , a diagnostic tool that uses the user's speech emotion to provide diagnostic information to the physiotherapist  [4] , and an assistant robot that can provide emotional communication  [5] .\n\nSER using data-driven models has been successful in recognising emotions  [6] [7] [8] [9] [10] [11] [12] . However, many data-driven models rely strongly on the mechanism underlying the generation of the data (i.e. the independent and identically distributed (i.i.d) assumption), and may hence fail when the testing data is taken from a different distribution than the training data. Alike other speech-related tasks, SER becomes highly challenging in cross-language, cross-corpus, and cross-speaker scenarios  [13, 14] . To facilitate these challenges, traditional SER approaches use low-level-descriptor features that have been selected and grouped in the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) feature set, including various acoustic features such as frequency-related (e.g. pitch, formant), energy-related (e.g. loudness) and spectral (e.g. spectral slope) features. In  [8] , the authors use low-level-descriptor features in cross-language SER, showing that people speaking different languages may express emotion in a similar way at the low-level signal level. To extract features that contain higher-level information, the i-vector proposed in  [15]  for speaker verification, has been further extended to SER where the \"emotion i-vector\" shows robustness in an English-German cross-language SER setting  [16] .\n\nIn recent work, the complicated feature design process is taken over by a unified DNN model that enables end-to-end learning from raw data or shallow features  [9] [10] [11] [12] . Robust features are then learned from data to minimise an overall loss towards emotion recognition. Many works achieve good SER performance on a single dataset  [9] [10] [11] [12] , but the model performance degrades dramatically in cross-language and crosscorpus scenarios  [14] . To alleviate this problem, several transfer learning techniques have been applied. In  [17, 18] , the authors pre-train a neural network with emotional datasets from one or two languages, then fine-tune the model with a small portion of the target language data, which shows a performance improvement compared to when the model is trained with one language and tested on another languages (i.e. in the cross-language setting). In  [19] , a similar idea to pre-train a DNN model using multiple emotional datasets is proposed, and the authors propose to fine-tune only a task-specific parallel residual adapter which achieves increased SER performance on each target task while keeping the amount of parameters to be updated low.\n\nOther than to adapt the pre-trained model to the target language, one can also learn language-invariant or corpus-invariant features. This can be done by transforming the input space to a common subspace among each corpus. In  [20] , Song firstly proposed to transform the source and target input space to a common subspace where the input source-target neighbourhood relations are preserved. Zhang and Song later updated this framework to include sparsity and add discriminative power to the learning of the subspace  [21] . To enable non-linear modelling with the DNN, the transformation to the common subspace is merged into an encoder neural network with a discriminator, and the learning is adversarial between the encoder and the discriminator where the encoder transforms source and target inputs to embeddings that will fool the discriminator, and then the discriminator needs to find the true related inputs (i.e. from the source dataset or the target dataset)  [22, 23] . However, these methods require the definition of the \"source\" and \"target\" language beforehand, and their application is limited to those two languages. In  [24] , instead of using a binary discriminator, a Wasserstein distance  [25]  is used to measure the distances between target and source embeddings, and the method minimises the distances between them.\n\nWe propose an end-to-end transfer learning framework to facilitate cross-language SER. This method lies in-between the transfer learning method and the common subspace method. First, the proposed model uses a wav2vec 2.0 feature extractor which has been trained in a self-supervised manner on about 56,000 hours of raw speech waveforms from 53 languages. Since the pre-trained wav2vec 2.0 feature extractor learns contextual structures across various speech utterances, it may capture common speech factors among different languages, hence transforming the speech waveform inputs to a common speech subspace which is shared across languages  [26] , and in which the corresponding speech embeddings are obtained. Next, a statistical pooling layer is applied that pools a sequence of the embeddings into one feature vector per utterance. The utterance-level feature vectors are then reduced in dimensionality, and a Deep-WCCN operation is applied to compensate for other variabilities (i.e. other than language variabilities). Finally, the compensated feature is used to predict emotion class with a simple linear classifier (constructed by a dense neural network layer). The model is fine-tuned on three emotional speech datasets with English, German, and Chinese languages using a summation of the supervised cross-entropy loss and the original wav2vec 2.0 loss. We evaluate its leave-one-language-out performance on unseen speakers, and the experimental results show that the proposed framework largely increases the cross-language SER performance compared to two well-known feature sets extracted using openSMILE for SER. It also largely outperforms many recent approaches to both within-language and cross-language SER. Finally, additional experiments on the effectiveness of the Deep-WCCN operation, and the effects of merging small amounts of target language speech data into the training set will be presented.\n\nThe paper is structured as follows. First, Section 2 provides a brief overview about the most recent studies related to our work. Then we introduce the proposed model structure in Section 3, and propose some modifications to a similar approach in Deep-WCCN  [27] . In Section 4, we describe the experiment datasets and the experimental settings. After that, we will present the simulation results and discuss these in Section 5. Finally, Section 6 presents the conclusions and suggestions for future work.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Related Works 2.1 Ser Features",
      "text": "Traditional speech information retrieval systems use hand-crafted features such as the Mel-Frequency Cepstral Coefficients (MFCC) features and the Linear Predictive Coding (LPC) features  [28] , and those features are designed to largely reduce the data dimensionality and to encode temporal and spectral structures per time frame in a speech recording. A group of low-level descriptor features such as pitch, formant frequencies and bandwidths, loudness, and spectral slope have been selected and compared in  [6]  for SER. Reynolds and Rose later proposed to use Gaussian Mixture Models (GMMs) to model the speakers' MFCC distribution  [29] . Each individual Gaussian component of a GMM encodes a general acoustic class (e.g., vowels, nasals, or fricatives), and the spectral shape of the acoustic class can be characterised by the GMM component mean vector and the GMM component covariance matrix. The concatenation of GMM component mean vectors is a representation that contains abstract information such as speaker identity. This high-dimensional representation (referred to as the supervector in  [15] ) is then used to extract the i-vector which encodes the speaker and channel information in a total variability subspace  [15] . However, these hand-crafted features are not designed for SER specifically, and may contain dominant misleading factors (e.g., speaker identities, language factors) which leads to poor SER performance. This problem is tackled in  [16]  where a two-step approach is proposed. The approach first compensates for the speaker variability in the supvervector space, then an \"emotion i-vector\" that encodes the variabilities of each emotion supervector centred around a maximum likelihood emotion supervector is extracted.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "End-To-End Learning",
      "text": "With the growing popularity of deep learning, the traditional learning pipeline (i.e. pre-processing, feature extraction, modelling, inferencing) is replaced by a single DNN which learns the features and performs the modelling in a data-driven manner. In this case, raw data or shallow features are directly fed into the learning process, and the DNN model predicts the target (e.g. class labels, parameter estimates) at its output. This is referred as end-to-end learning. Due to the data-driven nature of end-to-end learning, very little domain expertise is used for learning, and brute-force feature validation is avoided, thereby significantly reducing labour costs. In addition, compared to the hand-crafted features that unavoidably leads to information loss during the extraction process, the end-to-end framework fuses feature learning and selection into one process so that dedicated features for the target problem can be learned, thus contributing to the modelling performance.\n\nIn the context of SER, Trigeorgis et al. first proposed an end-to-end DNN that consists of Convolutional Neural Network (CNN) layers for feature extraction from raw audio waveforms and Long Short-term Memory (LSTM) layers to model the temporal information in the feature space  [30] . In  [31] , the authors replace the CNN with a Time-Delay Neural Network (TDNN) to create a larger receptive field. A similar idea has been proposed in  [12]  where a dilated CNN having a receptive field as large as the input sequence is used for both feature extraction and temporal modelling. However, generalisability remains a problem in supervised end-to-end learning since annotated SER datasets are general of small scale and recorded in specific environments, hence a model learned from these datasets may perform poorly when the test conditions are different from the training conditions, as well as in a cross-language setting.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Self-Supervised Learning And Transfer Learning",
      "text": "Self-supervised learning provides an end-to-end learning pathway for constructing speech features from large quantities of data without annotations. Self-supervised learning methods first transform time-domain raw audio waveforms to an embedding space (i.e. feature space), and then aim to predict randomly masked embeddings from past embeddings or adjacent embeddings  [26, [32] [33] [34] . This learning scheme forces the DNN to learn intrinsic contextual structure from the data rather than modelling noise or irrelevant factors, because on a large scale, noise will not contain useful information to predict neighbouring embeddings, thus will be suppressed.\n\nThe speech features obtained from self-supervised learning can either be used directly in relevant tasks, or can be fine-tuned in a transfer learning framework. In the latter case, a few extra layers, which map the features into predictions, are added to the trained self-supervised feature extractor. Then, the predictions are evaluated with a supervised loss, and finally the entire set of DNN model parameters is updated with a supervised training dataset. An example in SER is  [35]  where the authors use a concatenation of several layer outputs of the wav2vec 2.0 pre-trained model for monolingual SER. However, the authors fine-tune the model on the Automatic Speech Recognition (ASR) task using a dataset that only contains neutral speech which may not be as efficient as fine-tuning the model directly on an emotive speech dataset and on a SER task.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Proposed Transfer Learning Method With",
      "text": "Deep-WCCN for cross-language SER\n\nThe proposed method is based on the wav2vec 2.0 self-supervised learning model  [34] . This model has been pre-trained and used in cross-language speech recognition and has been shown to deliver speech representations that are shared across languages  [26] .\n\nWe first give the details of the wav2vec 2.0 model in Section 3.1, then we will present how to use the wav2vec 2.0 model in the front-stage of an end-to-end SER system, together with a modified Deep-WCCN layer inspired by similar work in  [27]  (in Section 3.2) and finally how to fine-tune the model under a supervised scheme in Section 3.3.\n\n3.1 Self-supervised pre-training of wav2vec 2.0\n\nThe wav2vec 2.0 model learns contextual representations from speech waveforms. The model consists of a convolutional feature extractor f , a transformer-based sequence model e and a vector quantiser module  [26] . The feature extractor f first transforms the raw speech waveform X into a sequence of d z -dimensional latent speech representations Z = [z 1 , z 2 , ..., z T ] for T time-stamps. Then the latent speech representations on one hand are fed into the sequence modeller to build contextual representations c 1 , c 2 , ..., c T , and on the other hand are fed to the vector quantiser that contains G codebooks, and for each codebook, there are V entries. The vector quantiser discretises the latent representations and linearly transforms them into a sequence q 1 , q 2 , ..., q T . The feature extractor f , the sequence model e and the vector quantiser module are trainable and their parameters are optimised through back-propagation with two loss functions:\n\n1. The contrastive loss: For a c t obtained from masked representations centered at time-stamp t, the sequence model needs to identify the true quantised representation q t from K + 1 candidate quantised representations q ∈ Q t which include K distractors uniformly sampled from other masked time-stamps for the same sequence. The loss is defined as:\n\nwhere sim(a, b) is the cosine similarity between a and b, and κ is the temperature parameter that controls the kurtosis of the distribution. 2. The diversity loss: To increase the use of the codebooks, the diversity loss encourages the equal usage among the entries in every codebook by maximising the entropy H( ) of the averaged softmax distribution over codebook entries for each codebook pg across a batch of utterances:\n\nThe total loss is a weighted summation of the two losses with a hyper-parameter α,\n\nTo accommodate for cross-language speech variations, we use a pre-trained wav2vec 2.0 model that has been trained on 56,000 hours of speech in 53 different languages. This model is denoted as XLSR-53 and the details can be found in  [26] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Deep-Wccn",
      "text": "The Within-Class Covariance Normalisation (WCCN) was introduced in  [36] , and used in speaker recognition and verification applications  [15, 36] . The WCCN approach aims to learn a feature map that minimises upper bounds on the false positive and false negative rates in a linear classifier. First, let W = [w 1 , w 2 , . . . , w N ] denote a set of N d-dimensional feature vectors belonging to C different classes c ∈ {1, . . . , C}. In  [15] , W is a set of i-vectors and C is the number of different speakers. Differently in this paper, we define W to represent a set of intermediate features of the training set extracted from the front-end DNN feature extractor, and C is the total number of emotion classes.\n\nNext, we define the expected within-class covariance matrix as:\n\nwhere wc = 1 Then the optimal feature map is given as:\n\nwhere A is the Cholesky factors of S -1 w = AA T , and w represents an arbitrary feature vector.\n\nThe conventional WCCN pools the necessary statistics from the feature vectors of the entire training set, which is not compatible with mini-batch training in DNNs. In  [27] , the authors propose to estimate S w with mini-batches, and to maintain a moving average Â of the corresponding mini-batch projection matrix. We believe that this moving average operation proposed in  [27] , which is equivalent to a weighted average of new and old batches, where the old batches are subject to exponentially decaying weights, is not the best design choice for random mini-batch Stochastic Gradient Descent (SGD) training in DNNs. Therefore, we first define the class covariance matrix Ŝw,c , which is estimated on mini-batch, and also Ŝw , which is the average of Ŝw,c across the classes. Then we propose to maintain a cumulative average Sw for Ŝw ,\n\nwhere N tot is an accumulator that counts the total number of mini-batches during training. Next, we add a spectral smoothing term to the within-class covariance estimation similar to  [36] :\n\nwhere the hyper-parameter β ∈[0, 1] controls the smoothness of the estimated withinclass covariance matrix. Finally, S ′ w is used to calculate A as mentioned before. This mini-batch based WCCN is considered as a special DNN layer that has no trainable parameter and only updates A during training, denoted as Deep-WCCN. The last update of A during training is stored and used in testing. The output of Deep-WCCN is the result of the linear transformation in  (5) , which can be fed into the subsequent layers of the DNN model. We implement the Deep-WCCN using Pytorch, which is an automatic differentiation toolbox for deep learning. It should be noted that in order to avoid Pytorch to automatically generate gradients of w when calculating A, a \"detached\" (from the computational graph) copy of w is used in the calculation during training.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Fine-Tuning The Wav2Vec 2.0 Model For Ser",
      "text": "One problem of the pre-trained XLSR-53 model is that the training sets mostly consist of neutral speech, and as a consequence the pre-trained model may capture inadequate emotion structures embedded in the speech data. Therefore, we propose to fine-tune the XLSR-53 model with emotional speech data. We combine the XLSR-53 pre-trained model and the Deep-WCCN with a few extra layers into a unified model that maps time-domain raw waveforms to emotion class predictions, and this unified model is fine-tuned in a supervised manner. The model overview is shown in Figure  1 . After converting the speech raw waveform to the sequence of latent speech representations Z, we calculate an utterance-level feature by pooling and concatenating the statistics of Z along the time dimension as done in  [37] ,\n\nThe resulting feature vector u is a 2d z -dimensional vector that proceeds to a fullyconnected layer which reduces its dimensionality to a pre-defined hidden dimension d h = 1 4 d z and which is then followed by a Rectified Linear Unit (ReLU) non-linear activation and a dropout layer. The output from the dropout layer is the intermediate feature vector that will be fed into the Deep-WCCN to reduce the within-class variances. After that, the Deep-WCCN output feature vectors are normalised to have unit norm, and linearly transformed into predictions for emotion classification.\n\nThe fine-tuning loss is a weighted summation of the wav2vec 2.0 loss L ssl and the emotion classification log-softmax cross-entropy loss. Specifically, given one prediction p = [p 1 , p 2 , . . . , p C ] T and its corresponding one-hot emotion class label y = [y 1 , y 2 , . . . , y C ] T (where only the true class label is 1, and other labels are 0) the total loss is:\n\nwhere the hyper-parameter γ controls the weight of L ssl .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experiment Set-Ups 4.1 Datasets",
      "text": "To carry out simulations for cross-language SER, we choose three publicly available datasets including English, German, and Chinese emotive speech recordings. These speech recordings are performed by professional actors have a duration of a few seconds each. The datasets are summarised in Table  1 , and described in detail below.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Berlin Database Of Emotional Speech (Emo-Db)",
      "text": "The Emo-DB  [38]  dataset is a German emotive speech dataset that has been widely used in SER research. This dataset consists of 535 utterances including 7 basic emotion catalogues (anger, boredom, disgust, anxiety/fear, happiness, sadness, and neutral).\n\nThe utterances are performed by 10 professional actors, with 10 pre-defined sentences.\n\nEach sentence is performed with all different emotions, and the sentence content should not deliver sentimental information.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ryerson Audio-Visual Database Of Emotional Speech And Song (Ravdess)",
      "text": "The RAVDESS  [39]  dataset contains recordings with 24 professional actors (12 female, 12 male), vocalising two lexically-matched statements (i.e. \"Kids are talking by the door.\", \"Dogs are sitting by the door.\"). The speech recordings include calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression, resulting in a total of 8 emotion catalogues.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotional Speech Dataset (Esd)",
      "text": "The ESD  [40]  dataset is a recent multilingual and multi-speaker emotional speech dataset designed for various speech synthesis and voice conversion tasks. The dataset consists of 350 parallel utterances spoken by 10 native English and 10 native Mandarin speakers. In this work, we only use the Mandarin utterances, which involve 5 male speakers and 5 female speakers, and are performed in 5 emotion catalogues (happy, sad, neutral, angry, and surprise).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dataset Preprocessing And Partitioning",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Baseline Features",
      "text": "For comparison with the proposed model, we choose the baseline systems using the most widely used feature sets designed for SER. The Extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [41]  and the Emobase  [42]  feature sets produce features per utterance, computed from a set of low-level-descriptors to which various statistical functions are applied. Both the eGeMAPS and the Emobase feature sets contain spectral features (e.g., pitch-related, formant-related features), energy/amplitude features (e.g., loudness), and filter-bank features (e.g., MFCC). In Emobase, statistical functions including min/max, arithmetic mean, standard deviation, skewness, and kurtosis, are applied to all the low-level descriptors and their delta coefficients. In contrast, in eGeMAPS, only selected statistical functions are applied to some of the  low-level descriptors, resulting a feature set containing a few extra temporal features such as the rate of loudness peaks features, and the mean length of voiced regions and unvoiced regions. The low-level descriptor selection criteria for eGeMAPS are based on the theoretical significance of the feature, the feature usage frequency in literature, and the potential of an acoustic parameter to indicate physiological changes in voice production during affective processes  [41] . In term of the feature vector dimensionality, the Emobase feature vector has length 988, and the eGeMAPS feature vector has length 88.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training On Multiple Languages",
      "text": "Similarly to  [18, 19] , we consider all combinations of merging two out of three language datasets for training and validation in order to meet the data requirements of the data-driven model and to allow the model to capture the variations across languages and corpora thus not to over-fit to one dataset. In combination of each two datasets, we repeat the smallest dataset a few times so that for each training set, there is a similar amount of utterances from each language. This results in the following three training/validation sets: 4.4 Within-language and Cross-language settings",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Within-Language Ser",
      "text": "In the within-language setting, we aim to evaluate the effectiveness of the baseline features and the wav2vec 2.0 features in SER. Therefore, for each training scheme, we test the performance of the methods on the testing set in one of the training languages, resulting in a situation where only the speaker identity is different among the training and testing sets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Cross-Language Ser",
      "text": "To evaluate the methods in a cross-language setting, we employ a leave-one-languageout scheme, that is for each training scheme, we test the methods on the testing set of the third language which means the language, recording condition, and the speaker identity in the testing set is unseen during training. We use \"->\" followed by the language abbreviations (EN, CH, DE) to indicate the dataset/language used in the testing.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimental Settings",
      "text": "The Adagrad optimiser  [43]  is used to train the models, with a fixed learning rate of 3 × 10 -4 and weight decay. The batch size is 14 due to memory constraints. The weighting parameter α in (  3 ) is equal to 0.1 as in  [26] . The weight decay parameter, the dropout rate, and the hyper-parameters β and γ in (  7 ) and (  9 ) are optimised using Hyperopt  [44] . The optimal results are shown in Table  3  and correspond to the values that are used in the experiments.\n\nWe evaluate the model using two metrics, the Unweighted Accuracy (UA) and the Weighted Accuracy (WA). Specifically, UA is the average accuracy for each emotion class, which marginalises out the effect of class imbalance, and WA is the overall accuracy of the entire testing data, which indicates the overall model performance across all classes. Model selection uses early-stopping on the validation performance, and the reported testing results are averaged across cross-validation folds.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Baseline Classifier",
      "text": "The baseline features are firstly tested with a neural network model that has the same structure as the classifier and the output network in the proposed model shown in Figure  1 . However, this model essentially only contains one non-linear transformation and the Deep-WCCN operation, hence it is incapable to learn a good mapping from the baseline features to the emotion classes. In particular, when conducting a hyperparameter search for the Deep-WCCN layer, the dropout rate, and the weight decay rate using Hyperopt for the neural network model, and then training it with the baseline features, the model SER performance is close to chance level (e.g., 25% WA for 4 emotion classes). Therefore, we instead use a Random Forest (RF) classifier, which is essentially an ensemble classifier that has good generalisation capability. This RF classifier has 100 trees with maximum tree-depth of 5. Table  3  Hyper-parameter configurations for the proposed method.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Hyper",
      "text": "5 Results and discussion",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Within-Language Performance",
      "text": "The results for within-language evaluation are illustrated in Figure  2 . These results show that the baseline eGeMAPS feature and the Emobase feature with an RF classifier perform similarly in terms of UA and WA, and for some testing cases (e.g. testing on the German \"->DE\" dataset) the eGeMAPS performs better than the Emobase, but it is the other way around for the other testing cases (e.g. testing on the Chinese \"->CH\", and on the English \"->EN\" datasets). The largest UA and WA differences between these two methods are 4.8% and 4.2% that are obtained in the \"DEEN->DE\" and \"ENCH->EN\" scenarios, respectively. From the same figure, comparing the proposed wav2vec 2.0 method with the baseline methods, the wav2vec 2.0 shows significant improvements in both UA and WA for within-language SER. Specifically, the wav2vec 2.0 improves UA with about 20.9% to 66.4% compared to the eGeMAPS method, and with about 28.7% to 64.5% compared to the Emobase method, and it improves WA with about 18.8% to 66.4% and 21.5% to 64.5% compared to the eGeMAPS and the Emobase, respectively. The increase in classification accuracy may indicate that the wav2vec 2.0 pre-trained feature extractor combined with the proposed Deep-WCCN can extract contextual features that contribute to mixed-language SER capability and are robust to varying speaker identities.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Cross-Language/Cross-Corpus Performance",
      "text": "For the cross-language/cross-corpus experiments, the emotion classification accuracy is plotted in Figure  3 . For comparison, the within-language results (with suffix \"-WL\") from Section 5.1 are averaged across the same testing language for every method and plotted along with the cross-language performance (with suffix \"-CL\").\n\nFirstly, the wav2vec 2.0-CL performs significantly better than eGeMAPS-CL and Emobase-CL in all three testing languages, and it is followed by Emobase-CL which performs slightly better than eGeMAPS-CL in almost all the testing cases (except on English testing where eGeMAPS-CL and Emobase-CL have similar WA performance). The best UA and WA with wav2vec 2.0-CL, tested in English, Chinese, and German, are 64%, 75.4%, 94% and 61.2%, 75.4%, 94.6%, respectively. This is about 43% to 77% and 50.7% to 77% performance gain on UA and WA compared to the eGeMAPS-CL, and about 41% to 72.9% and 36.3% to 72.9% performance gain on UA and WA compared to the Emobase-CL.\n\nSecondly, although all three methods show performance degradation when moving from within-language to cross-language/cross-corpus SER, the wav2vec 2.0 still maintains high performance. It is notable that in the \"ENCH->DE\" experiment, wav2vec 2.0-CL shows a very subtle degradation (i.e. about 0.6% and 0.3% degradation in UA and WA, respectively, compared to the wav2vec 2.0-WL), whereas the eGeMAPS-CL shows a 17.8% and 18.5% decrease in UA and WA, respectively, and the Emobase-CL shows a 8.5% and 8% decrease in UA and WA, respectively.\n\nIn summary, the results in cross-language/cross-corpus SER may indicate that the proposed wav2vec 2.0 with Deep-WCCN model can largely alleviate the performance degradation due to language mis-match, speaker identity mis-match, and channel mismatch that commonly occur in cross-language SER. As the proposed model shows equally satisfactory results in both UA and WA, we may also conclude that the wav2vec 2.0 with Deep-WCCN model is not over-fitting to one of the testing emotion classes.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Evaluation Of The Deep-Wccn",
      "text": "In order to verify the effectiveness of Deep-WCCN, we compare the change in crosslanguage SER performance of wav2vec 2.0 with and without Deep-WCCN. The results are compared with other baseline methods. For eGeMAPS and Emobase, we calculate the change in their cross-language SER performance with and without WCCN. The increase in cross-language SER performances for all methods using either Deep-WCCN or WCCN are shown in Figure  4 .\n\nFirst, it can be clearly observed that using Deep-WCCN can improve the performance of wav2vec 2.0 in cross-language SER. The highest increase is observed in DECH->EN, where UA and WA increase by 2.6% and 3.2% respectively by applying Deep-WCCN, which is followed by increments in DEEN->CH, where both UA and WA increase by 0.4% after applying Deep-WCCN. No performance gain is observed in ENCH->DE when applying Deep-WCCN to the wav2vec 2.0 model, which might be due to the fact that the results on the German testing sets are already very good and there is limited room for improvement.\n\nSecondly, WCCN did not significantly improve cross-language SER performance when using the eGeMAPS and the Emobase features. Specifically, by applying WCCN, the experiments on all three cross-language cases show only small performance gains, no gains, or even performance drops when applying WCCN to eGeMAPS and to Emobase. Only in the case of ENCH->DE, the Emobase method has increases of 1.6% and 1.4% in UA and WA, respectively, when applying WCCN. This is due to the fact that WCCN is designed for linear classifiers and is not significantly helpful for ensemble models like the RF. Conversely, the output layer of our proposed model can be seen as a linear classifier, which satisfies the design conditions of WCCN.\n\nWe also visualise the embeddings (which are the inputs to the output network) with and without Deep-WCCN. The embeddings are originally 192-dimensional vectors, but for visualisation purposes, we compute t-SNEs on the validation set and testing set embeddings from each cross-validation fold. The visualisations for the DECH setting are plotted in Figure  6 .\n\nFirst, distinct clusters corresponding to different emotion classes emerge, demonstrating the model's ability to learn meaningful representations that separate the emotion categories. This explains the proposed method's effectiveness in SER. Second, incorporating Deep-WCCN notably improves the purity of the clusters in the validation sets. This improvement in cluster purity indicates a better discrimination of emotion classes when using Deep-WCCN, suggesting that the embeddings retain a better separability than without Deep-WCCN, even when applied to unseen data. Third, the proposed method generalises well to testing data that comes from a different language. And by applying Deep-WCCN, the embeddings exhibit enhanced discrimination properties, which help maintain robust performance in cross-language SER.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Influence Of Training Set Size Of Target Language",
      "text": "As the proposed method is in the realm of transfer learning, it might exhibit the data efficiency property typically associated to transfer learning. That is, the transferred sub-network, which is trained on a large quantity of speech data, learns intrinsic speech structure and can easily generalise to similar tasks with a small amount of annotated target data. Therefore, in this experiment, we merge a small amount of target language data into the fine-tuning process, and evaluate the model performance when using 30, 80, and 150 two-second-target language inputs with their corresponding labels. The duration of extra target language data used in the training is hence equal to 60 s, 160 s, and 300 s in total duration, respectively. The extra target data are repeated to achieve a size that is similar to the original training data size to avoid data imbalance for different languages. The results for DECH->EN, DEEN->CH and ENCH->DE are plotted respectively in Figure  5 , together with their performance when no extra target language data is used (i.e. the case corresponding to 0 s).\n\nFirst, the three testing cases all show an increasing trend when more target language data is used in training, and their performance increases rapidly when even less than 160 s of target language data is used while it slows down when even more target language data is used. Second, the performance of DECH->EN increases the most, followed by DEEN->CH, and lastly by ENCH->DE. For the case of using 160 s of target language data, compared to no target language data being used in training, DECH->EN, DEEN->CH and ENCH->DE perform 13.8%, 3.8% and 0.4% better in UA, respectively, and 15.6%, 4.4% and 0.4% better in WA, respectively. The low performance increment in ENCH->DE might be due to the limited room for improvement. Third, except for ENCH->DE having limited improvement, DEEN->CH also has overall less improvement in UA compared to DECH->EN, when 300 s of target language data is used in training. This could be due to the difficulty of transferring Germanic languages to Sino-Tibetan languages, which probably can be alleviated by using more target language annotated data, however this hypothesis might need more investigation.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "References",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Comparison To Existing Works",
      "text": "We also compare our method with a few recent methods proposed for crosslanguage/cross-corpus SER. The comparison is in Table  4  where we only present the results reported on the same testing datasets (i.e. on the Emo-DB and RAVDESS) as our work. Since there is hardly any research using ESD for cross-language SER, we do not include the results for this dataset.\n\nThe comparison shows that our proposed method has significantly improved the SER performance in both within-language and cross-language scenarios with both Emo-DB and RAVDESS datasets. This may indicate that transfer learning with pretraining on large speech data and Deep-WCCN plays an important role in DNN generalisation for SER.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "To alleviate the performance degradation in cross-language/cross-corpus SER compared to within-language/within-corpus SER, we proposed a transfer learning method that firstly uses the wav2vec 2.0 pre-trained model to transfer a time-domain audio waveform into a contextual embedding space that is shared across different languages, thereby reducing the language variabilities in the speech features. Then, by applying a Deep-WCCN layer, which is adapted to cope with DNN training, this Deep-WCCN layer can further reduce within-class variance caused by other factors (e.g. speaker identity, channel variability). Experimental results first show that the proposed method largely increases both within-language and cross-language SER performance compared to the eGeMAPS and Emobase feature sets that have been designed for and widely used in SER. Furthermore, an ablation study shows that Deep-WCCN can reduce the within-class variances which further improves the performance for the proposed DNN model. In contrast, the conventional WCCN does not show improvements on the eGeMAPS and Emobase feature sets with a RF classifier. Next, we show that the proposed transfer learning method exhibits good data efficiency in merging target language data in the fine-tuning process. The model speaker-independent SER performance increases for all testing target languages, and a performance gain in WA up to 15.6% and in UA up to 13.8% is achieved when only 160 s of target language data is used in the training set for fine-tuning. Finally, a comparison with recent work in cross-language/cross-corpus SER demonstrates that the proposed method can significantly improve within-language and cross-language SER performance among multiple datasets.\n\nFuture work firstly include the evaluation of the wav2vec 2.0 model pre-trained on an even larger speech dataset and the assessment of its benefit to cross-language SER performance. A good candidate dataset is proposed in  [47]  and consist of half a million hours of publicly available speech audio in 128 languages. Secondly, we plan to evaluate the proposed method on more emotive speech datasets, and to fine-tune the model on emotive speech datasets from more languages to create a general data-driven model for SER. Thirdly, a fine-tuning incorporating rank decomposition matrices as proposed in  [48]  could be tested under the scope of this paper. The fine-tuning scheme in  [48]  is expecting to retain the knowledge captured by the trained models, therefore improving the efficiency when adapting the model to more emotive speech datasets. Lastly, it may be relevant to reduce the model size through model distillation  [49]  to make it feasible for mobile and low-power devices. Fig.  3  Cross-language/cross-corpus emotion classification accuracy measured by unweighted (UA) and weighted accuracy. The testing set language is not included in training, which means the testing speaker ID, the testing language, and the testing channel is unseen during training. Results are compared also with the within-language performance averaged across each testing language.",
      "page_start": 17,
      "page_end": 25
    },
    {
      "section_name": "Statistical Pooling Across Time",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Wa (%)",
      "text": "Fig.  4  The performance gain in terms of UA and WA after applying Deep-WCCN for the proposed method (which is based on wav2vec 2.0), and the performance gain after applying conventional WCCN for eGeMAPS and Emobase methods.",
      "page_start": 26,
      "page_end": 26
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: However, this model essentially only contains one non-linear transformation",
      "page": 12
    },
    {
      "caption": "Figure 2: These results",
      "page": 13
    },
    {
      "caption": "Figure 3: For comparison, the within-language results (with suffix “-WL”)",
      "page": 13
    },
    {
      "caption": "Figure 4: First, it can be clearly observed that using Deep-WCCN can improve the per-",
      "page": 14
    },
    {
      "caption": "Figure 6: First, distinct clusters corresponding to different emotion classes emerge, demon-",
      "page": 15
    },
    {
      "caption": "Figure 5: , together with their performance when no extra",
      "page": 15
    },
    {
      "caption": "Figure 1: The network structure of the proposed end-to-end transfer learning model for cross-",
      "page": 24
    },
    {
      "caption": "Figure 2: Within-language emotion classification accuracy measured by unweighted (UA) and weighted",
      "page": 25
    },
    {
      "caption": "Figure 3: Cross-language/cross-corpus emotion classification accuracy measured by unweighted (UA)",
      "page": 25
    },
    {
      "caption": "Figure 4: The performance gain in terms of UA and WA after applying Deep-WCCN for the proposed",
      "page": 25
    },
    {
      "caption": "Figure 5: The SER performance when adding additional target language data (i.e. 60 s, 160 s and 300 s",
      "page": 26
    },
    {
      "caption": "Figure 6: The t-distributed Stochastic Neighbor Embedding (t-SNE) visualisation of validation set and",
      "page": 27
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deep-WCCN": "Dropout"
        },
        {
          "Deep-WCCN": "ReLU"
        },
        {
          "Deep-WCCN": "Dense dim=192"
        }
      ],
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Progress in measuring subjective well-being",
      "authors": [
        "A Krueger",
        "A Stone"
      ],
      "year": "2014",
      "venue": "Science",
      "doi": "10.1126/science.1256392"
    },
    {
      "citation_id": "2",
      "title": "Emotion Recognition and Detection Methods: A Comprehensive Survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "J. Artif. Intell. Syst",
      "doi": "10.33969/ais.2020.21005"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine -Belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigol",
        "M Lang"
      ],
      "year": "2004",
      "venue": "Proc. of 2004 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '04)",
      "doi": "10.1109/icassp.2004.1326051"
    },
    {
      "citation_id": "4",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi"
      ],
      "year": "2000",
      "venue": "IEEE Trans. Biomed. Eng",
      "doi": "10.1109/10.846676"
    },
    {
      "citation_id": "5",
      "title": "Emotion detection and regulation from personal assistant robot in smart environment. Personal assistants: Emerging computational technologies",
      "authors": [
        "J Castillo",
        "Á Castro-González",
        "F Alonso-Martín",
        "A Fernández-Caballero",
        "M Salichs"
      ],
      "year": "2018",
      "venue": "Emotion detection and regulation from personal assistant robot in smart environment. Personal assistants: Emerging computational technologies"
    },
    {
      "citation_id": "6",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2009",
      "venue": "Proc. of 2009 IEEE Work. Autom. Speech Recognit. Understanding"
    },
    {
      "citation_id": "7",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition cross language families: Mandarin vs. Western Languages",
      "authors": [
        "Z Xiao",
        "D Wu",
        "X Zhang",
        "Z Tao"
      ],
      "year": "2017",
      "venue": "Proc. of 2016 IEEE Int. Conf. Prog. Informatics Comput. (PIC 2016)",
      "doi": "10.1109/PIC.2016.7949505"
    },
    {
      "citation_id": "9",
      "title": "Attentionenhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. of Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2019)"
    },
    {
      "citation_id": "10",
      "title": "Attention-augmented End-to-end Multi-task Learning for Emotion Prediction from Speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. of 2019 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '19)"
    },
    {
      "citation_id": "11",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. of 2018 IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP '18)"
    },
    {
      "citation_id": "12",
      "title": "End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network",
      "authors": [
        "D Tang",
        "P Kuppens",
        "L Geurts",
        "T Waterschoot"
      ],
      "year": "2021",
      "venue": "Eurasip J. Audio, Speech, Music Process",
      "doi": "10.1186/s13636-021-00208-5"
    },
    {
      "citation_id": "13",
      "title": "Speaker dependent, speaker independent and cross language emotion recognition from speech using GMM and HMM",
      "authors": [
        "M Bhaykar",
        "J Yadav",
        "K Rao"
      ],
      "year": "2013",
      "venue": "Natl. Conf. Commun",
      "doi": "10.1109/NCC.2013.6487998"
    },
    {
      "citation_id": "14",
      "title": "Context-independent multilingual emotion recognition from speech signals",
      "authors": [
        "V Hozjan",
        "Z Kačič"
      ],
      "year": "2003",
      "venue": "Int. J. Speech Technol",
      "doi": "10.1023/A:1023426522496"
    },
    {
      "citation_id": "15",
      "title": "Front-end factor analysis for speaker verification",
      "authors": [
        "N Dehak",
        "P Kenny",
        "R Dehak",
        "P Dumouchel",
        "P Ouellet"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Audio, Speech Lang. Process"
    },
    {
      "citation_id": "16",
      "title": "Cross-lingual speech emotion recognition through factor analysis",
      "authors": [
        "B Desplanques",
        "K Demuynck"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2018)"
    },
    {
      "citation_id": "17",
      "title": "CRoss-lingual and Multilingual Speech Emotion Recognition on English and French",
      "authors": [
        "M Neumann",
        "Thang",
        "N Vu"
      ],
      "year": "2018",
      "venue": "Proc. of 2018 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '18)",
      "doi": "10.1109/ICASSP.2018.8462162"
    },
    {
      "citation_id": "18",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2018)",
      "doi": "10.21437/Interspeech.2018-1625"
    },
    {
      "citation_id": "19",
      "title": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition",
      "authors": [
        "M Gerczuk",
        "S Amiriparian",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "20",
      "title": "Transfer linear subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "P Song"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2705696"
    },
    {
      "citation_id": "21",
      "title": "Transfer sparse discriminant subspace learning for crosscorpus speech emotion recognition",
      "authors": [
        "W Zhang",
        "P Song"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2019.2955252"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised Adversarial Domain Adaptation for Cross-Lingual Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "Proc. of 8th Int. Conf. Affect",
      "doi": "10.1109/ACII.2019.8925513"
    },
    {
      "citation_id": "23",
      "title": "Cross-culture Multimodal Emotion Recognition with Adversarial Learning",
      "authors": [
        "J Liang",
        "S Chen",
        "J Zhao",
        "Q Jin",
        "H Liu",
        "L Lu"
      ],
      "year": "2019",
      "venue": "Proc. of 2019 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '19)",
      "doi": "10.1109/ICASSP.2019.8683725"
    },
    {
      "citation_id": "24",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/taffc.2019.2916092"
    },
    {
      "citation_id": "25",
      "title": "Wasserstein GAN",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Wasserstein GAN",
      "doi": "10.48550/arxiv.1701.07875",
      "arxiv": "arXiv:1701.07875"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "27",
      "title": "Deep Within-Class Covariance Analysis for Robust Audio Representation Learning",
      "authors": [
        "H Eghbal-Zadeh",
        "M Dorfer",
        "G Widmer"
      ],
      "year": "2017",
      "venue": "Deep Within-Class Covariance Analysis for Robust Audio Representation Learning",
      "arxiv": "arXiv:1711.04022"
    },
    {
      "citation_id": "28",
      "title": "Speech information retrieval: A review",
      "authors": [
        "R Hafen",
        "M Henry"
      ],
      "year": "2012",
      "venue": "Multimed. Syst",
      "doi": "10.1007/s00530-012-0266-0"
    },
    {
      "citation_id": "29",
      "title": "Robust Text-Independent Speaker Identification Using Gaussian Mixture Speaker Models",
      "authors": [
        "D Reynolds",
        "R Rose"
      ],
      "year": "1995",
      "venue": "IEEE Trans. Speech Audio Process"
    },
    {
      "citation_id": "30",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. of 2016 IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP '16)"
    },
    {
      "citation_id": "31",
      "title": "Emotion identification from raw speech signals using DNNs",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. of Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2018)"
    },
    {
      "citation_id": "32",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "33",
      "title": "wav2vec: Unsupervised pretraining for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. of Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2019)",
      "doi": "10.21437/Interspeech.2019-1873"
    },
    {
      "citation_id": "34",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2017",
      "venue": "Proc. of IEEE Conf"
    },
    {
      "citation_id": "35",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. of Annu. Conf. Int. Speech Commun. Assoc. (INTER-SPEECH 2021)",
      "doi": "10.21437/interspeech.2021-703"
    },
    {
      "citation_id": "36",
      "title": "Generalized linear kernels for one-versus-all classification: Application to speaker recognition",
      "authors": [
        "A Hatch",
        "A Stolcke"
      ],
      "year": "2006",
      "venue": "Proc. of 2006 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '06)",
      "doi": "10.1109/icassp.2006.1661343"
    },
    {
      "citation_id": "37",
      "title": "Contrastive Unsupervised Learning for Speech Emotion Recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Proc. of 2021 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '21)",
      "doi": "10.1109/ICASSP39728.2021.9413910"
    },
    {
      "citation_id": "38",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2005)"
    },
    {
      "citation_id": "39",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "40",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. of 2021 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP '21)",
      "doi": "10.1109/ICASSP39728.2021.9413391"
    },
    {
      "citation_id": "41",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective comput",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "42",
      "title": "OpenEAR -Introducing the Munich opensource emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "Proc. of 3rd Int. Conf. Affect",
      "doi": "10.1109/ACII.2009.5349350"
    },
    {
      "citation_id": "43",
      "title": "Adaptive subgradient methods for online learning and stochastic optimization",
      "authors": [
        "J Duchi",
        "E Hazan",
        "Y Singer"
      ],
      "year": "2010",
      "venue": "23rd Conf. Learn. Theory (COLT 2010)"
    },
    {
      "citation_id": "44",
      "title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
      "authors": [
        "J Bergstra",
        "D Yamins",
        "D Cox"
      ],
      "year": "2013",
      "venue": "30th Int. Conf. Mach. Learn. ICML 2013"
    },
    {
      "citation_id": "45",
      "title": "Cross-Corpus Speech Emotion Recognition Based on Hybrid Neural Networks",
      "authors": [
        "A Rehman",
        "Z Liu",
        "D Li",
        "B Wu"
      ],
      "year": "2020",
      "venue": "Proc. of Chinese Control Conf. (CCC 2020)",
      "doi": "10.23919/CCC50068.2020.9189368"
    },
    {
      "citation_id": "46",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Proc. of Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH 2019)",
      "doi": "10.21437/Interspeech.2019-2753"
    },
    {
      "citation_id": "47",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "48",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Proc. of 10th Int. Conf. Learn"
    },
    {
      "citation_id": "49",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    }
  ]
}