{
  "paper_id": "2311.03606v1",
  "title": "Multimodal Stress Detection Using Facial Landmarks And Biometric Signals *",
  "published": "2023-11-06T23:20:30Z",
  "authors": [
    "Majid Hosseini",
    "Morteza Bodaghi",
    "Ravi Teja Bhupatiraju",
    "Anthony Maida",
    "Raju Gottumukkala"
  ],
  "keywords": [
    "Stress Detection",
    "Deep Neural Network",
    "Multimodal Model",
    "Convolutional Neural Network",
    "Decision Layer Fusion",
    "early-fusion",
    "late-fusion Schmidt",
    "Philip",
    "et al. [35] BVP",
    "ECG",
    "EDA",
    "EMG",
    "RR",
    "TEMP",
    "and ACC. Acc 80% (3-class) 93% (2-class) Koldijk",
    "et al. [36] Facial Expressions",
    "Posture",
    "Physiology. Acc 90% (2-class) Giannakakis",
    "et al. [38] Facial Cues and camera-based HR. Acc 91.68% (3 -class) Padmaja",
    "B.",
    "et al. [42] FITBIT wearable device data Acc 62% (3-class) Shu",
    "Lin",
    "et al. [45] HR Acc 84% (3-class) Hsu",
    "Yu Liang",
    "et al. [46] ECG Acc 82.78% (2-class) Sharma",
    "et al. [48] EEG signals Acc 82.01% (4-class) Zhu",
    "Lili",
    "et al. [49] EDA Acc 85.7% (2-class) Zhu",
    "Lili",
    "et al. [50] EDA",
    "ECG",
    "and PPG Acc 86.4% (2-class) Jeon",
    "Taejae",
    "et al. [51] Facial Landmarks Acc 64.63% (3-class) Zhang",
    "Huijun",
    "et al. [52] Facial Expressions Acc 85.42% (2-class) Cardone",
    "Daniela",
    "et al. [53]"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The development of various sensing technologies is improving measurements of stress and the wellbeing of individuals. Although progress has been made with single signal modalities like wearables and facial emotion recognition, integrating multiple modalities provides a more comprehensive understanding of stress, given that stress manifests differently across different people. Multi-modal learning aims to capitalize on the strength of each modality rather than relying on a single signal. Given the complexity of processing and integrating high-dimensional data from limited subjects, more research is needed. Numerous research efforts have been focused on fusing stress and emotion signals at an early stage, e.g., feature-level fusion using basic machine learning methods and 1D-CNN Methods. This paper proposes a multi-modal learning approach for stress detection that integrates facial landmarks and biometric signals. We test this multi-modal integration with various early-fusion and late-fusion techniques to integrate the 1D-CNN model from biometric signals and 2-D CNN using facial landmarks. We evaluate these architectures using a rigorous test of models' generalizability using the leave-one-subject-out mechanism, i.e., all samples related to a single subject are left out to train the model. Our findings show that late-fusion achieved 94.39% accuracy, and early-fusion surpassed it with a 98.38% accuracy rate. This research contributes valuable insights into enhancing stress detection through a multi-modal approach. The proposed research offers important knowledge in improving stress detection using a multi-modal approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Stress has gained importance in recent years as people are placing more emphasis on holistic well-being, i.e., physical health, mental health, emotional and social well-being. Moreover, prolonged exposure to stress can in turn, affect physical and mental health, leading to conditions such as heart disease, depression, and anxiety  [1] . Recent research has shown the potential of employing computer vision and biometric data for stress detection  [28] . Biometric indicators such as Heart Rate (HR), Electrodermal Activity (EDA), and Skin Temperature (ST/TEMP) have the capacity to reflect an individual's physiological reactions to stress, making them reliable objective measures of stress levels. Conversely, facial landmarks represent crucial points on the face that provide valuable insights into facial expressions, enabling inference of emotional states and stress. Conventional stress detection methods mostly focus on a single modality, such as physiological signals or facial expressions. It is well known that Stress is a complex interplay of psychological and physiological responses to external or internal stimuli, making it difficult to predict accurately. One of the key challenges of predicting stress is that it is transient, meaning it can change quickly and is influenced by a variety of factors. These factors include environmental stimuli, prior conditioning or past experience with stress, emotional states, and external factors such as work demands, social interactions, and life events. These factors can significantly impact stress levels and vary widely between individuals. Stress manifests differently in different people, leading to individual differences in stress responses  [68] . This makes it difficult to develop a single, accurate method for stress prediction. Researchers are investigating new approaches to stress prediction that use multiple data sources such as heart rate, temperature, and facial expressions  [14, 13, 11, 12] . The idea of multimodal stress prediction is inspired by the human senses, which work together to provide us with a comprehensive understanding of the world around us.\n\nIt is also important to differentiate between \"multivariate\" and \"multimodal\" learning for neural networks. Multivariate networks focus on processing multiple variables or predictors but often the same kind of data. For example, it might analyze time series data signals such as ECG, temperature, and heart rate, all intended for stress detection. In contrast, multimodal neural networks are designed to integrate information from different types of data sources or modalities. For example, a multimodal network is one that processes voice and image data simultaneously, determining how one might be related to one another. Multimodal stress prediction models combine data from multiple sensors to learn complex patterns that are associated with stress.\n\nThis paper presents a novel multimodal approach for stress detection, combining facial landmarks analysis with biometric signals comparing early and late fusion techniques. The overarching theme of stress detection is preestablished, and various fusion methodologies, both late and early, have been explored previously  [12, 11, 16, 17] . Earlier research typically treats both modalities as time series signals, modeling each with a 1D CNN. In our approach, we model facial landmarks using a 2D CNN. The integration of 2D CNN and 1D CNN models for stress detection has not seen studied before. We study the performance of this against a wide range of both late and early fusion techniques that employ facial landmarks and biometric feedbacl. We also analyze the effects of various signal components and model architectures, including 2079 video and biometric signal features, EDA tonic and phasic components, and the performance of six deep learning models with different feature selection methods, along with eleven multimodal models using these features.\n\nWe first discuss related work in the area of stress detection, focusing on physiological signals from wearables and computer vision, as well as multi-modal stress detection approaches. We provide a comprehensive summary of biomedical features documented in the literature. Next, we offer an in-depth overview of the dataset and elaborate on the methodologies employed to extract various features from physiological and computer vision datasets. Following this discussion, we present early and late-fusion models and their architectures. Finally, we evaluate and discuss the outcomes of the study, weighing various model architectures against both accuracy and computational cost.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Researchers have studied stress detection through the use of wearable devices and facial expressions. Sioni et al.  [19]  discuss how physiological signals such as HR and EDA can detect stress. Alberdi et al.  [28]  studied automatic, continuous, and unobtrusive early stress detection methods for office workers. They found that ECG, especially using HRV features, and EDA are the most accurate physiological signals for recognizing stress. It is desirable to have better accuracy when incorporating data from multiple modalities, such as behavioral responses. This approach would lead to the development of a less intrusive and widely applicable monitoring system, significantly enhancing practicality. Well-engineered multimodal methods should offer reliable models through diverse signal information and enhanced robustness against noise. We review previous research in the fields of physiological signals, computer vision, and multi-modal signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Physiological Signals",
      "text": "Schmidt et al.  [35]  introduced the WESAD dataset consisting of BVP, ECG, EDA, EMG, RR, ST, and three-axis acceleration (ACC) to detect neutral, stress, and amusement using self-reports validation surveys. The benchmark achieved classification accuracies up to 80% for three classes (baseline, stress, amusement) and up to 93% for binary (stress, non-stress) classification. Padmaja, B. et al.  [42]  propose a method for detecting stress levels using data from a physical activity tracker device (Fitbit), using Logistic Regression to evaluate the impact of individual stressors. They achieved 62% accuracy for three stress levels. Hsu, Yu Liang, et al.  [46]  introduced an automatic algorithm for emotion recognition based on ECG signals. The classification rates for positive/negative valence, high/low arousal, and four emotion classification tasks were 82.78%, 72.91%, and 61.52%, respectively. Shu, Lin, et al.  [45]  achieved 84% accuracy for emotion recognition based on HR data from a wearable smart bracelet using the Gradient Boosted Decision Trees (GBDT) model. Sharma et al.  [48]  achieved an accuracy of 82.01% with four-labeled emotion classes using 10-fold cross-validation on online recognition of human emotions using EEG signals on the DEAP dataset  [61] . Zhu, Lili, et al.  [49]  examined the potential of utilizing EDA from two publicly available datasets collected from research-grade wearable devices. The experimental results demonstrated that Random Forests(RF) achieve an 85.7% accuracy rate in classifying stress from non-stress states. Zhu, Lili, et al.  [50]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Imaging/Video Signals",
      "text": "Giannakakis, Giorgos, et al.  [38]  introduces a framework for detecting stress and anxiety emotional states using facial cues captured in video recordings. Features like eye-related events, mouth activity, head motion parameters, and camera-based HR estimation were investigated and classified into three classes: neutral, relaxed, and stressed/anxious. They got 91.68% accuracy using the AdaBoost classifier. Kopaczka, Marcin, et al.  [55]  developed a system consisting of face detection and facial landmark detection and employed Histogram of Oriented Gradients (HOG) features using a random forest classifier, achieving a classification accuracy of 65.75% for four basic emotions (namely, Neutral-Happy-Sad-Surprised).  [52]  addressed the issue of stress detection using video-based camera sensors. They used facial expressions and action motions from videos to identify stress with a performance of 85.42% accuracy for stress binary classification. Cardone, Daniela, et al.  [53]  used thermal infrared images for stress detection. In this study, they achieved 80% for the area under the curve (AUC), sensitivity of 77%, and specificity of 78% for binary classification based on a non-linear support vector regression (SVR). Jeon, Taejae, et al.  [51]  proposes a stress recognition algorithm using face images and facial landmarks to understand eye, mouth, and head movements. They achieved an accuracy of 64.63%, detecting three levels of no, weak, and strong stress.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Signals",
      "text": "The SWELL project  [36]  focuses on multimodal sensor data (computer logging, facial expressions, posture, and physiology). The study distinguishes neutral and stressful working conditions with 90% accuracy using a support vector machine (SVM) model, using posture and facial expressions to provide valuable information. Bobade, Pramod, et al.  [11]  used motion sensors alongside physiological signals (ECG, BVP, TEMP, RR, EMG, and EDA) to detect stress. The results showed that machine learning techniques achieve accuracies of up to 81.65% and 93.20% for emotion detection(amusement, baseline, and stress) and binary stress detection(stress vs. non-stress), respectively. Deep learning achieves accuracies of up to 84.32% and 95.21% for the two and three-class classifications. Seo, Wonju, et al.  [56) ] used ECG, RR, and facial landmarks extracted from video data and achieved an accuracy of 73.3% for binary classification and 54.4% for three-level stress classification using multivariate deep neural networks. A multimodal AI-based framework is proposed by Walambe, Rahee, et al.  [58] . The methodology involves fusing data of facial expressions, posture, HR, and computer interaction (Mouse activity, Left click, etc.) to detect workload stress and achieve an accuracy of 96.09%. Naegelin, Mara, et al.  [59]  utilize mouse, keyboard, and HRV features to detect three levels of perceived stress, valence, and arousal, and they achieved a maximum F1 score of 77.5%.\n\nSome of these recent findings show the promise of multimodal approaches with respect to improving the accuracy and reliability of real-time stress detection. However, these studies are evaluated in small datasets and evaluated in laboratory conditions. More datasets and methods are needed to improve the efficacy of these methods. Table  1  summarizes some reviewed papers, listing the signals they used and their performance in terms of accuracy (Acc), area under the curve (AUC), precision (Prec), and F1 score.\n\n3 Methods",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "The EmpathicSchool  [15]  dataset contains video-based facial expressions and corresponding physiological signals, HR, EDA, TEMP, and ACC from Empatica E4. 26 hours of data were collected from 20 participants from two universities, Tampere University, Finland, and the University of Louisiana at Lafayette, United States. The stress levels were determined via National Aeronautics and Space Administration Task Load Index questionnaires. The dataset includes seven different signal types, including both computer vision and physiological features. The video dataset provides facial landmarks. To the best of our knowledge, facial information of 68 landmarks has never been studied before. Additionally, 30 unique landmark features were incorporated to improve further stress prediction accuracy, such as the difference and distance of two or more landmark points on the face.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Biometric Data",
      "text": "The following biometric signals were measured via the E4 wristband and are provided in the dataset:\n\n• HR: Healthy resting heart rate (HR) ranges from 60 to 100 beats per minute, regardless of gender. However, HR is influenced by activity and emotional state. Wearable HR sensors enable the analysis of patterns and fluctuations, providing insights into stress, exertion, and cardiovascular health.\n\n• TEMP: Skin temperature is influenced by skin blood flow and can reveal stress, fever, and changes in blood flow.\n\nWearable ST sensors can capture data, enabling analysis of temperature patterns and correlation with other physiological parameters.\n\n• EDA: EDA (Electrodermal Activity), also known as GSR (Galvanic Skin Response), is a signal that quantifies sweat gland activity and offers insights into emotional and psychological arousal. It reflects reactions to stress, excitement, anxiety, and other emotional states. Wearable EDA sensors detect changes in skin conductance, providing valuable information about an individual's physiological and emotional responses.\n\n• Accelerometers (ACC): ACC data, measured by sensors, records object acceleration. These sensors have diverse applications, such as human action recognition and step counting. Wearable ACCs track movement patterns, offering information about activity levels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Landmarks",
      "text": "Facial landmarks are key points on a face that are used to identify features on a face, such as corners of eyes, eyebrows, tip of the nose, and lips. Image processing systems typically use these features to detect facial orientation, expression, or general facial patterns. 2D pixel values on the other hand are raw signals stored as pixel values of the entire image in a 2D array. Compared to a 2D image, facial landmarks provide a compact representation of facial features, capturing essential and relevant features. Extracting 68 facial landmarks is consisted of two steps face detection and identifying smaller features of the face, like eyes. We used Dlib 68-point face detection  [8]  with OpenCV  [7]  to extract facial landmarks. Figure  1  shows 68 specific points on the face, which we refer to as landmarks. Analysis of spatial and temporal changes in these landmarks enables machine learning models to detect and classify stress. The original signal data was collected with a frequency of 4 Hz in the EmphaticSchool  [15]  dataset. We down-sample the data to a frequency of 1 Hz to reduce training and testing times. To reduce the effect of noise and artifacts on the stress data, we separated the EDA signal into its phasic and tonic components, and several features of each component were derived. Table  2  shows the different features of EDA signals. The phasic component of the EDA signal represents the rapid and temporary changes in skin conductance. Phasic changes are characterized by short-duration and high-amplitude fluctuations in the EDA signal. The tonic component of the EDA signal represents the relatively slower and more sustained changes in skin conductance. Unlike phasic changes, the tonic component is not directly linked to specific events or stimuli but rather reflects the overall arousal level of an individual over a longer period. Figure  2  shows a part of the raw EDA signal with relative phasic and tonic components  [60] .\n\nThe statistical features of each signal are generated using a rolling window of 40 seconds with a sliding of 20 seconds due to the data characteristic, computational efficiency, and training data availability. The average stress level of each sliding window was obtained from the raw signal and binned into three different bins, namely no-stress 0(0 to 6), medium-stress 1(7 to 13), and stressful 2(14 to 19). The statistical features of each participant were extracted using the tsfresh  [20]  Python library and normalized independently. The statistical features were extracted, and the missing data were removed. We used the data of 10 participants at different sessions for 26 hours. The dlib facial expressions model  [8]  consumes the video signals and generates the facial landmarks. We also extract several facial features and investigate the correlation of mentioned features using different feature selection methods. To evaluate their performance, the resulting multimodal data of biometric signals and facial landmarks consisted of 1897 features and the raw signals consumed by three deep learning networks: multivariate, early-fusion, and late-fusion.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Early-Fusion Model",
      "text": "The multimodal data comprising 2079 features combining biometric and facial landmarks data was decreased to 100 features using lasso regularization  [65] . The added unique features were found to be crucial in this study.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Late-Fusion Model",
      "text": "The multimodal fusion model consumed the same data as our early-fusion networks, including 30 biometric and 100 landmark features with 1Hz frequency. The model showed promising results and was comparable to the early-fusion approach. In addition, the concatenation outperforms the early-fusion models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Analysis",
      "text": "We employed three different types of feature selection, namely filter methods, namely Spearman  [64] , Pearson  [63] , and Variance Threshold  [66] , wrapper method namely Recursive Feature Elimination (RFE)  [67]  and embedded methods of Random Forest (RF) Importance  [62] , and Lasso Regularization  [65] . The top 10 features are presented in Table  3 . The Spearman and Pearson analyses yielded similar results, emphasizing facial landmark features. Conversely, the RF Importance and RFE analysis identified a combination of facial landmarks, ST, and EDA (Tonic and Phasic) features are the top contributors. The tonic component of EDA appears to have the highest variance using the variance threshold. Comparing the six feature selection methods, we observed that lasso regularization provided the most favorable outcome for running the models in this study, selecting features related to EDA and facial landmarks. The second and third best outcomes were from RF importance and RFE, respectively. We provided SHAP values and feature contributions using the Lasso regularization model. Figures  3, 4  show the explainable information of the model using SHAP library  [69] . Among all features, the Max_EDA had the maximum effect on the prediction results of all three stress levels, while some features, e.g., min_EDA and quantile_EDA, only discriminate between two stress levels. In Figure  4 , red signals represent the signals pushing higher stress levels positively associated with stress (e.g., Max_EDA, EDA_Clean, and phasic Max_EDA). In contrast, the blue features represent lower stress levels negatively associated with stress (e.g., Max_EDA_clean, Energy_EDA, standard deviation of tonic EDA).  We fine-tuned our proposed stress detection network using the LOOCV approach to circumvent potential pitfalls. Moreover, stress detection models often encounter data from new individuals they were not trained on. LOOCV ensures the model's performance is evaluated reliably and meaningfully. Our investigation focused on three distinct network architectures: multivariate, early-fusion, and late-fusion.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Multivariate",
      "text": "We analyzed biometric signals and facial landmarks for feature extraction. Then, each model consumed the features separately. The one-dimensional Convolutional Neural Network (1D-CNN), two-dimensional convolutional neural network (2D-CNN), and pure fully connected, deep neural network (FCDNN) was trained and tested using the LOOCV paradigm. By employing this approach, we aimed to obtain separate results for biometric signals and facial landmarks, allowing us to independently evaluate the models' performance for each type of data.\n\nIn our study, when using the 1D-CNN model to analyze facial landmarks, We treat the 68 facial landmarks as time series data, similar to how we handle ACC data that tracks coordinate changes over time. We focused on understanding how these landmarks change sequentially rather than capturing complex spatial relationships. On the other hand, using the 2D-CNN model, We pay more attention to how facial landmarks are arranged in space, which helps us capture overall spatial features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Early-Fusion",
      "text": "Figure  5  shows the early-fusion stress detection model. The model consumes the concatenation of selected facial landmarks and biometric signal features to detect stress. We compared three early-fusion models, namely 1DCNN, 2D-CNN, and FCDNN. The 1DCNN and 2DCNN models use a one or two-dimensional convolutional layer to extract the spatiotemporal information of the representation of the signals for fully connected neural network layers, respectively. However, the FCDNN model directly consumes the concatenated features to detect stress. This integration enables the model to simultaneously analyze the biometric signals and visual information following the LOOCV paradigm.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Late-Fusion",
      "text": "The proposed network architecture Figure  6  consists of two components that incorporate biometric signals and 68 landmarks in parallel. The late-fusion models consume the same features extracted using feature extraction modules. The biometric signals are fed into a 1D-CNN model, allowing us to capture temporal patterns and dependencies. Meanwhile, the landmarks data is processed using a 2D-CNN model to extract spatial information. We employ the decision-level Fusion technique to merge the results and produce a final output using the Softmax classifier (Figure  6 ). In the second experiment, we implemented a concatenation-based approach, wherein we fused the outputs of the two mentioned models, effectively giving rise to a novel composite model. We also used a late-fusion concatenation model, combining one and two-dimension convolutional networks (Figure  7 ) before the decision layer (SoftMax). The model concatenates the representation of facial landmarks and biometric signals before the decision layer for stress  We used accuracy, precision, recall, F1 score, and computational cost to perform model comparisons.\n\n• Accuracy: The ratio of the number of correct predictions to the total number of predictions\n\n• Precision: Measures the proportion of correctly predicted positive instances out of all instances predicted as positive\n\nPrecision is a measure of how accurately the model can classify a specific class and in cases where the work is severely stressful, like military, firefighting, and nursing. • Recall: Measures the proportion of correctly predicted positive instances out of all actual positive instances  Recall is important when misclassifying the stress is not crucial (e.g., the wellness, meditation, and relaxation tasks). We prefer models with higher recall scores in stress treatment tasks. • F1_score: Provides a balanced measure of a classification model's performance by combining the precision and recall scores.\n\nTP, TN, FP, and FN are True Positive, True Negative, False Positive, and False Negative, respectively.\n\nF1_score is needed in mental health applications where both stress onset and relaxation are important. Furthermore, the models with higher f1_score are desired in physiological signal analysis than the precision and recall score.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "The performance of trained deep neural networks with regard to multi-variate, early, and late-fusion approaches was evaluated on facial landmarks, biometric signals, and a combination of their features. We compared the performance and computational costs of the networks using training and testing times. We trained and tested ten networks using six feature selection methods and compared the results of the networks in detail. Furthermore, Table  4  summarizes the performance of each network in terms of the above-mentioned metrics.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multivariate Results",
      "text": "In the first experiment, we use the standard multivariate signals for a single neural network architecture. Biometric signals are multi-variate, but each signal sequence is still 1-dimensional, and 1DCNN has been shown to capture patterns within the individual signals and cross-channel relationships  [6, 54] . The 1D-CNN model captures temporal patterns and biometric signals, while the 2D-CNN extracts spatial features of facial landmarks. We can observe in Table  4  that 1D-CNN provides good performance for biometric signals. This is on par with prior studies where we achieved 93.01% accuracy  [42, 45, 46, 48, 49, 50, 17, 35] . Landmark data (LND) on the other hand are vector data (like images), and we observe that 2D-CNN models provide better performance for facial landmarks. The 2D CNN results are close to what is observed in  [38, 52, 53, 55, 51] . Fully connected DNN models, on the other hand, are good at capturing non-linear relationships, which has good overall performance when you look at both signals. Fully connected DNN models treat data in their true form and do not capture temporal dependencies, making these models vulnerable to capturing false positives as true. In other words, the fully connected DNN model cannot distinguish between decreasing and increasing trends. For example, when stress has passed, the signs from the tonic EDA signal still remain. As expected, the 1D-CNN model works better with biometric signals, resulting in 93.01% accuracy, and 2D-CNN works best with facial landmarks, showing 95.02% accuracy compared to fully connected deep neural network model with around 92% for both biometric and facial landmarks signals.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Late-Fusion Results",
      "text": "In the case of late fusion, each modality is processed independently, and their outputs are combined at a later stage, usually closer to the final decision or prediction stage. The main advantage of this approach is its flexibility because each modality is processed independently, accommodating the unique characteristics of each data source. Also, the spatial and temporal scales need not perfectly align for late-fusion. We experiment with late-fusion models that combine one and two-dimensional convolutional neural networks. The first late-fusion approach involved combining the outcomes of the 1D-CNN and 2D-CNN models through decision-level fusion. We can observe in Table-4 that results from late-fusion are better than the multi-variate model but lower than early-fusion for both decision-based fusion and concatenation-based fusion.\n\nThe concatenation-based approach concatenates the input and output of every mentioned model into a single composite model. The fusion of two distinct models, each specialized in a different modality, contributed to the overall accuracy of 94.39% and 92.67% for stress detection using decision-level and concatenation fusion, respectively. We can observe that the late fusion model provides better performance compared to the multivariate model, but the model performance is slightly lower compared to the 1D CNN-based early fusion model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Early-Fusion Results",
      "text": "Early fusion, or feature-level fusion, combines features at the early processing stage in the end-to-end model architecture. This is most suitable when data from modalities correspond to the same instance or context  [10] . Stress signals may exhibit some form of temporal alignment, but not always. There is high inter-subject variability with respect to how individuals perceive and react to stress  [9] . Also, acute stress leads to immediate and observable physiological and behavioral changes  [3]  compared to chronic stress resulting from subtle and long-term changes. It is also observed by  [4, 14]  that physiological signals such as HRV, skin conductance, and facial expressions due to the stressful event also occur within the close temporal context. In our case, the dataset includes two individual signals that have been shown to have temporal proximity. We observe that Early fusion performs better with accuracy, recall, and F1-score in most cases for 1D-CNN, 2D-CNN, and FCDNN compared to multi-variate models, but 1D-CNN-based early fusion has the best performance overall. The overall accuracy is 98.38%, which I better compared to previous studies which are based on multi-variate signals  [56) ,  58, 59, 15, 36, 11] . In the context of this dataset, the model is able to focus on the most relevant and informative features from both modalities. The analysis of False Positive (FP) and False Negative (FN) rates for biometric data, landmark data, and their combination (fusion) for the 1D-CNN model is illustrated in Figure8. Using biometric signals and facial landmarks showed good accuracy in identifying non-stressful instances but did not perform well in distinguishing stressed instances. However, when both signals are combined, the model achieves better accuracy in identifying all three instances. Reducing the FN rate is essential to ensure no high-stress situations are overlooked and individuals receive appropriate warning signals. The combined biometric and landmark data fusion approach showed promising results in decreasing the FN rate and enhancing the reliability of the stress detection system for individuals experiencing high stress.\n\nWe investigated various feature selection methods, and the following points summarize the results with the 1D-CNN model performance on combining both data types. Some of the results are as follows:\n\n• Using RF importance resulted in an Accuracy of 96.94%, Precision of 95.72%, Recall of 91.20%, and F1-score of 92.93%.\n\n• Using RFE resulted in an Accuracy of 96.63%, Precision of 95.34%, Recall of 92.54%, and F1-score of 92.97%.  We investigated the importance of using the Tonic and Phasic components of the EDA signal. Our experiment showed that using these components in the data in the 1D-CNN model resulted in 3.57%, 9.23%, 10.47%, and 10.71% higher Accuracy, Precision, Recall, and F1-score, respectively.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Computational Cost",
      "text": "Table  5  presents the computational cost of these models with respect to the trainable parameters, training time, and prediction time. We used a two-28-core (56 CPUs) node with Intel(R) Xeon(R) CPU E5-2660 v4 @ 2.00GHz CPU and L1d, L1i, L2, and L3 cache of 32K, 32K, 256K, 35840K, respectively. The node used a P100 Tesla GPU for deep-learning computations. Early-fusion models show lower training and test times, while late-fusion models require more training time. The early fusion model needs lower training time due to the model only needs to be trained once on the combined feature set. However, in the late-fusion model, Each model needs to be trained separately, which can take more time, especially because there are many features. Moreover, The complexity of the overall system is higher because it includes two different models. The 2D-CNN model on biometric signals had the lowest number of parameters. Because all the signals were concatenated before the convolutional model, it resulted in lower training time and accuracies due to not capturing enough information. The FDCNN model using facial landmarks requires lesser training time than late fusion models; however, the overall performance of these models drops about 8%. However, the 1D-CNN model has the maximum number of trainable parameters but has a lower training time than the late-fusion level neural networks. The 1D-CNN model is the most accurate with combined data but has the most trainable parameters.\n\nThe balance between accuracy, complexity, and computational efficiency varies across models, showing the importance of modality fusion and pre-processing choices for effective stress detection.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "This paper studies stress detection through multimodal learning that encompasses biometric signals and facial landmarks. We used the EmpathicSchool dataset that contains video-based facial expressions and physiological signals when subjects were asked to perform stressful student tasks. We experimented with various early and late-fusion techniques based on 1-D CNN, 2D-CNN, and fully connected DNN models for multi-modal stress detection. We observe that early-fusion techniques with a 1D-CNN model provide the best performance with an accuracy of 98.38%. This model performs better than multi-variate unimodal or multi-modal signals with late fusion. The fully connected DNN demonstrates strong performance, exceeding the 1D-CNN model on landmarks data alone and enhancing overall precision. Conversely, the 2D-CNN model performed well with facial landmarks data due to its ability to capture both spatial relationships and temporal dynamics of facial landmarks. This study's results are essential in creating reliable stress detection methods, especially in healthcare, education, and the workplace. The study should be further expanded to include a more diverse population and tasks. The source code and datasets used in this study are for public use through the University of Louisiana CPHS-lab GitHub repository  [18] .",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows 68 specific points on the face, which we refer to as landmarks. Analysis of spatial and",
      "page": 4
    },
    {
      "caption": "Figure 1: Illustration of the 68 facial landmarks",
      "page": 5
    },
    {
      "caption": "Figure 2: shows a part of the raw EDA signal with relative phasic and tonic components",
      "page": 5
    },
    {
      "caption": "Figure 2: Phasic and Tonic components of raw EDA signal",
      "page": 6
    },
    {
      "caption": "Figure 4: , red signals",
      "page": 6
    },
    {
      "caption": "Figure 3: Explainable AI features and feature contribution for stress level discrimination",
      "page": 7
    },
    {
      "caption": "Figure 4: force plot SHAP values",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the early-fusion stress detection model. The model consumes the concatenation of selected facial",
      "page": 8
    },
    {
      "caption": "Figure 6: consists of two components that incorporate biometric signals and 68",
      "page": 8
    },
    {
      "caption": "Figure 6: ). In the second experiment, we implemented a concatenation-based approach, wherein we fused the outputs of the",
      "page": 8
    },
    {
      "caption": "Figure 7: ) before the decision layer (SoftMax). The",
      "page": 8
    },
    {
      "caption": "Figure 5: 1D-CNN Early-Fusion Network",
      "page": 9
    },
    {
      "caption": "Figure 6: decision-level Fusion Network",
      "page": 9
    },
    {
      "caption": "Figure 7: late-fusion Concatenation Network",
      "page": 10
    },
    {
      "caption": "Figure 8: FP, and FN rate for each class in 1D-CNN Early-Fusion Network",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author": "Schmidt, Philip, et al. [35]",
          "Signals": "BVP, ECG, EDA, EMG, RR, TEMP, and ACC.",
          "Classification\nPerformance": "Acc 80% (3-class)\n93% (2-class)"
        },
        {
          "Author": "Koldijk, et al. [36]",
          "Signals": "Facial Expressions, Posture, Physiology.",
          "Classification\nPerformance": "Acc 90% (2-class)"
        },
        {
          "Author": "Giannakakis, et al. [38]",
          "Signals": "Facial Cues and camera-based HR.",
          "Classification\nPerformance": "Acc 91.68% (3 -class)"
        },
        {
          "Author": "Padmaja, B., et al. [42]",
          "Signals": "FITBIT wearable device data",
          "Classification\nPerformance": "Acc 62% (3-class)"
        },
        {
          "Author": "Shu, Lin, et al. [45]",
          "Signals": "HR",
          "Classification\nPerformance": "Acc 84% (3-class)"
        },
        {
          "Author": "Hsu, Yu Liang, et al. [46]",
          "Signals": "ECG",
          "Classification\nPerformance": "Acc 82.78% (2-class)"
        },
        {
          "Author": "Sharma, et al. [48]",
          "Signals": "EEG signals",
          "Classification\nPerformance": "Acc 82.01% (4-class)"
        },
        {
          "Author": "Zhu, Lili, et al. [49]",
          "Signals": "EDA",
          "Classification\nPerformance": "Acc 85.7% (2-class)"
        },
        {
          "Author": "Zhu, Lili, et al. [50]",
          "Signals": "EDA, ECG, and PPG",
          "Classification\nPerformance": "Acc 86.4% (2-class)"
        },
        {
          "Author": "Jeon, Taejae, et al. [51]",
          "Signals": "Facial Landmarks",
          "Classification\nPerformance": "Acc 64.63% (3-class)"
        },
        {
          "Author": "Zhang, Huijun, et al. [52]",
          "Signals": "Facial Expressions",
          "Classification\nPerformance": "Acc 85.42% (2-class)"
        },
        {
          "Author": "Cardone,\nDaniela,\net\nal.\n[53]",
          "Signals": "Thermal Infrared Imaging",
          "Classification\nPerformance": "AUC 80% (2-class)"
        },
        {
          "Author": "Kopaczka, Marcin,\net\nal.\n[55]",
          "Signals": "Facial Expressions",
          "Classification\nPerformance": "Acc 65.75% (4-class)"
        },
        {
          "Author": "Seo, Wonju, et al. [56)]",
          "Signals": "ECG, RR, and Facial Landmarks",
          "Classification\nPerformance": "Acc 73.3% (2-class)"
        },
        {
          "Author": "Bobade, Pramod et al. [11]",
          "Signals": "ACC, ECG, BVP, TEMP, RR, EMG and EDA",
          "Classification\nPerformance": "Acc 84.32 % (3-class)\n95.21% (2-class)"
        },
        {
          "Author": "Walambe, Rahee, et al. [58]",
          "Signals": "Facial Expressions, Posture, HR ,Computer Inter-\naction",
          "Classification\nPerformance": "Acc 96.09% (2-class)"
        },
        {
          "Author": "Naegelin, Mara, et al. [59]",
          "Signals": "HR and Behavioral data",
          "Classification\nPerformance": "F1\nscore\n77.5% (3-\nclass)"
        },
        {
          "Author": "Kuttala, Radhika et al. [17]",
          "Signals": "EDA and ECG",
          "Classification\nPerformance": "Acc 97.6% (2-class)"
        },
        {
          "Author": "Hosseini, Majid, et al. [15]",
          "Signals": "HR, TEMP, EDA, ACC and Facial Expression",
          "Classification\nPerformance": "Prec 85.04 (3-class)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EDA Signals": "EDA_Clean",
          "Description": "Filtered version of the raw EDA signal"
        },
        {
          "EDA Signals": "EDA_Tonic",
          "Description": "Sustained changes in skin conductance"
        },
        {
          "EDA Signals": "EDA_Phasic",
          "Description": "Rapid changes in skin conductance"
        },
        {
          "EDA Signals": "SCR_Onsets",
          "Description": "Starting points of SCR"
        },
        {
          "EDA Signals": "SCR_Peaks",
          "Description": "Highest points of SCR"
        },
        {
          "EDA Signals": "SCR_Height",
          "Description": "Intensity or strength of the response"
        },
        {
          "EDA Signals": "SCR_Amplitude Magnitude of the response",
          "Description": ""
        },
        {
          "EDA Signals": "SCR_RiseTime",
          "Description": "Duration for a SCR"
        },
        {
          "EDA Signals": "SCR_Recovery",
          "Description": "Period after a SCR"
        },
        {
          "EDA Signals": "SCR_Rec.Time",
          "Description": "Recovery time after an arousing event"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rank": "1",
          "Spearmans": "energy_X2",
          "Pearson": "energy_X2",
          "Variance Threshold": "below_mean_EDA_Tonic"
        },
        {
          "Rank": "2",
          "Spearmans": "energy_X1",
          "Pearson": "energy_X3",
          "Variance Threshold": "above_mean_EDA_Tonic"
        },
        {
          "Rank": "3",
          "Spearmans": "energy_X3",
          "Pearson": "energy_X4",
          "Variance Threshold": "skew_EDA_Tonic"
        },
        {
          "Rank": "4",
          "Spearmans": "energy_X4",
          "Pearson": "energy_X1",
          "Variance Threshold": "kurtosis_EDA_Tonic"
        },
        {
          "Rank": "5",
          "Spearmans": "RMS_X1",
          "Pearson": "RMS_X2",
          "Variance Threshold": "energy_EDA_Tonic"
        },
        {
          "Rank": "6",
          "Spearmans": "energy_Y9",
          "Pearson": "RMS_X3",
          "Variance Threshold": "max_EDA_Tonic"
        },
        {
          "Rank": "7",
          "Spearmans": "RMS_X2",
          "Pearson": "avg_X2",
          "Variance Threshold": "EDA_Tonic"
        },
        {
          "Rank": "8",
          "Spearmans": "avg_X1",
          "Pearson": "avg_X3",
          "Variance Threshold": "avg_EDA_Tonic"
        },
        {
          "Rank": "9",
          "Spearmans": "Y9",
          "Pearson": "RMS_X4",
          "Variance Threshold": "RMS_EDA_Tonic"
        },
        {
          "Rank": "10",
          "Spearmans": "energy_X5",
          "Pearson": "RMS_X1",
          "Variance Threshold": "quantile_EDA_Tonic"
        },
        {
          "Rank": "Rank",
          "Spearmans": "RF Importance",
          "Pearson": "RFE",
          "Variance Threshold": "Lasso Regularization"
        },
        {
          "Rank": "1",
          "Spearmans": "energy_X2",
          "Pearson": "std_EDA_Clean",
          "Variance Threshold": "max_EDA"
        },
        {
          "Rank": "2",
          "Spearmans": "energy_EDA_Tonic",
          "Pearson": "max_EDA",
          "Variance Threshold": "max_EDA_Clean"
        },
        {
          "Rank": "3",
          "Spearmans": "max_TEMP",
          "Pearson": "energy_EDA",
          "Variance Threshold": "variance_X21"
        },
        {
          "Rank": "4",
          "Spearmans": "variation_EDA_Phasic",
          "Pearson": "variation_EDA",
          "Variance Threshold": "std_Y56"
        },
        {
          "Rank": "5",
          "Spearmans": "skew_EDA_Tonic",
          "Pearson": "energy_X5",
          "Variance Threshold": "energy_Y31"
        },
        {
          "Rank": "6",
          "Spearmans": "energy_X32",
          "Pearson": "energy_X4",
          "Variance Threshold": "energy_EDA"
        },
        {
          "Rank": "7",
          "Spearmans": "max_EDA_Clean",
          "Pearson": "avg_X27",
          "Variance Threshold": "std_X3"
        },
        {
          "Rank": "8",
          "Spearmans": "energy_EDA_Phasic",
          "Pearson": "avg_X25",
          "Variance Threshold": "variation_Y22"
        },
        {
          "Rank": "9",
          "Spearmans": "max_Y24",
          "Pearson": "avg_X61",
          "Variance Threshold": "variation_Y28"
        },
        {
          "Rank": "10",
          "Spearmans": "RMS_EDA_Phasic",
          "Pearson": "energy_X65",
          "Variance Threshold": "energy_Y30"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Network": "Model",
          "Multi-Variate": "1D-CNN",
          "Early-Fusion": "1D-CNN",
          "Late-Fusion": "Decision"
        },
        {
          "Network": "Data",
          "Multi-Variate": "BIO",
          "Early-Fusion": "",
          "Late-Fusion": ""
        },
        {
          "Network": "Acc",
          "Multi-Variate": "93.01",
          "Early-Fusion": "98.38",
          "Late-Fusion": "94.39"
        },
        {
          "Network": "Prc",
          "Multi-Variate": "90.00",
          "Early-Fusion": "96.73",
          "Late-Fusion": "94.94"
        },
        {
          "Network": "Rec",
          "Multi-Variate": "85.44",
          "Early-Fusion": "95.61",
          "Late-Fusion": "86.35"
        },
        {
          "Network": "F1",
          "Multi-Variate": "85.71",
          "Early-Fusion": "95.70",
          "Late-Fusion": "88.63"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Network": "Model",
          "Multi-Variate": "1D-CNN\n2D-CNN\nFCDNN",
          "Early-Fusion": "1D-CNN\n2D-CNN\nFCDNN",
          "Late-Fusion": "decision\nConcat"
        },
        {
          "Network": "Data",
          "Multi-Variate": "BIO\nLND\nBio\nLND\nBIO\nLND",
          "Early-Fusion": "",
          "Late-Fusion": ""
        },
        {
          "Network": "Trainable Parameters",
          "Multi-Variate": "23,931\n79,931\n4,923\n27,323\n32,703\n46,703",
          "Early-Fusion": "79,931\n27,323\n46,703",
          "Late-Fusion": "51,254\n51,176"
        },
        {
          "Network": "Train Time (s)",
          "Multi-Variate": "2,610\n2,590\n2,870\n2,930\n2,290\n2,220",
          "Early-Fusion": "2,820\n2,870\n2,310",
          "Late-Fusion": "5,810\n3,850"
        },
        {
          "Network": "Test Time (s)",
          "Multi-Variate": "4.19\n4.01\n3.4\n3.65\n3.27\n3.14",
          "Early-Fusion": "4.07\n3.64\n3.35",
          "Late-Fusion": "3.73\n5.86"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Psychological stress and disease",
      "authors": [
        "S Cohen",
        "D Janicki-Deverts",
        "G Miller"
      ],
      "year": "2007",
      "venue": "Jama"
    },
    {
      "citation_id": "2",
      "title": "IoT for smart cities: Machine learning approaches in smart healthcare-A review",
      "authors": [
        "T Ghazal",
        "M Hasan",
        "M Alshurideh",
        "H Alzoubi",
        "M Ahmad",
        "S Akbar",
        "B Al Kurdi",
        "I Akour"
      ],
      "venue": "Future Internet"
    },
    {
      "citation_id": "3",
      "title": "The impact of stress on cognition and motivation",
      "authors": [
        "P Morgado",
        "J Cerqueira"
      ],
      "year": "2018",
      "venue": "Frontiers in Behavioral Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "A review on physiological signals: Heart rate variability and skin conductance",
      "authors": [
        "A Soni",
        "K Rawal"
      ],
      "year": "2019",
      "venue": "Proceedings of First International Conference on Computing, Communications, and Cyber-Security"
    },
    {
      "citation_id": "5",
      "title": "A multimodal sensor dataset for continuous stress detection of nurses in a hospital",
      "authors": [
        "S Hosseini",
        "R Gottumukkala",
        "S Katragadda",
        "R Bhupatiraju",
        "Z Ashkar",
        "C Borst",
        "K Cochran"
      ],
      "venue": "Scientific Data"
    },
    {
      "citation_id": "6",
      "title": "1-D convolutional neural networks for signal processing applications",
      "authors": [
        "S Kiranyaz",
        "T Ince",
        "O Abdeljaber",
        "O Avci",
        "M Gabbouj"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "The OpenCV Library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal of Software Tools"
    },
    {
      "citation_id": "8",
      "title": "Dlib-ml: A Machine Learning Toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on intelligent transportation systems"
    },
    {
      "citation_id": "10",
      "title": "Performance evaluation of early and late fusion methods for generic semantics indexing",
      "authors": [
        "Y Dong",
        "S Gao",
        "K Tao",
        "J Liu",
        "H Wang"
      ],
      "year": "2014",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "11",
      "title": "Stress detection with machine learning and deep learning using multimodal physiological data",
      "authors": [
        "P Bobade",
        "M Vani"
      ],
      "venue": "2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA) 2020"
    },
    {
      "citation_id": "12",
      "title": "Real-time mental stress detection using multimodality expressions with a deep learning framework",
      "authors": [
        "J Zhang",
        "H Yin",
        "J Zhang",
        "G Yang",
        "J Qin",
        "L He"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "13",
      "title": "Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data",
      "authors": [
        "T Islam",
        "P Washington"
      ],
      "year": "2023",
      "venue": "Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data",
      "arxiv": "arXiv:2307.03337"
    },
    {
      "citation_id": "14",
      "title": "Development and psychometric evaluation of family caregivers' hardiness scale: a sequential-exploratory mixed-method study",
      "authors": [
        "L Hosseini",
        "H Sharif Nia",
        "M Ashghali Farahani"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "15",
      "title": "EmpathicSchool: A multimodal dataset for real-time facial expressions and physiological data analysis under different stress conditions",
      "authors": [
        "M Hosseini",
        "F Sohrab",
        "R Gottumukkala",
        "R Bhupatiraju",
        "S Katragadda",
        "J Raitoharju",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "EmpathicSchool: A multimodal dataset for real-time facial expressions and physiological data analysis under different stress conditions",
      "arxiv": "arXiv:2209.13542"
    },
    {
      "citation_id": "16",
      "title": "Multi-modal acute stress recognition using off-the-shelf wearable devices",
      "authors": [
        "V Montesinos",
        "F Dell'agnola",
        "A Arza",
        "A Aminifar",
        "D Atienza"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "17",
      "title": "Multimodal Hierarchical CNN Feature Fusion for Stress Detection",
      "authors": [
        "R Kuttala",
        "R Subramanian",
        "V Oruganti"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Stress-Detection-in-Nurse",
      "authors": [
        "Majid Satya"
      ],
      "year": "2021",
      "venue": "Stress-Detection-in-Nurse"
    },
    {
      "citation_id": "19",
      "title": "Stress detection using physiological sensors",
      "authors": [
        "R Sioni",
        "L Chittaro"
      ],
      "venue": "Computer"
    },
    {
      "citation_id": "20",
      "title": "Time series feature extraction on basis of scalable hypothesis tests (tsfresh-a python package)",
      "authors": [
        "M Christ",
        "N Braun",
        "J Neuffer",
        "A Kempa-Liehr"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "21",
      "title": "Random decision forests",
      "authors": [
        "T Ho"
      ],
      "year": "1995",
      "venue": "Proceedings of 3rd international conference on document analysis and recognition"
    },
    {
      "citation_id": "22",
      "title": "Logistic regression",
      "authors": [
        "D Kleinbaum",
        "K Dietz",
        "M Gail",
        "M Klein",
        "M Klein"
      ],
      "year": "2002",
      "venue": "Logistic regression"
    },
    {
      "citation_id": "23",
      "title": "Xgboost: extreme gradient boosting",
      "authors": [
        "T Chen",
        "T He",
        "M Benesty",
        "V Khotilovich",
        "Y Tang",
        "H Cho",
        "K Chen",
        "R Mitchell",
        "I Cano",
        "T Zhou"
      ],
      "year": "2015",
      "venue": "R package version 0"
    },
    {
      "citation_id": "24",
      "title": "What is a support vector machine?",
      "authors": [
        "W Noble"
      ],
      "year": "2006",
      "venue": "Nature biotechnology"
    },
    {
      "citation_id": "25",
      "title": "Accuracy/diversity and ensemble MLP classifier design",
      "authors": [
        "T Windeatt"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "26",
      "title": "The perceptron: a probabilistic model for information storage and organization in the brain",
      "authors": [
        "F Rosenblatt"
      ],
      "year": "1958",
      "venue": "Psychological review"
    },
    {
      "citation_id": "27",
      "title": "Methods for interpreting and understanding deep neural networks",
      "authors": [
        "G Montavon",
        "W Samek",
        "K Müller"
      ],
      "year": "2018",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "28",
      "title": "Towards an automatic early stress recognition system for office environments based on multimodal measurements: A review",
      "authors": [
        "A Alberdi",
        "A Aztiria",
        "A Basarab"
      ],
      "year": "2016",
      "venue": "Journal of biomedical informatics"
    },
    {
      "citation_id": "29",
      "title": "Smartphone-based mental state estimation: A survey from a machine learning perspective",
      "authors": [
        "Y Fukazawa",
        "N Yamamoto",
        "T Hamatani",
        "K Ochiai",
        "A Uchiyama",
        "K Ohta"
      ],
      "year": "2020",
      "venue": "Journal of Information Processing"
    },
    {
      "citation_id": "30",
      "title": "Stress detection in computer users based on digital signal processing of noninvasive physiological variables",
      "authors": [
        "J Zhai",
        "A Barreto"
      ],
      "year": "2006",
      "venue": "2006 international conference of the IEEE engineering in medicine and biology society"
    },
    {
      "citation_id": "31",
      "title": "Stress detection from speech and galvanic skin response signals",
      "authors": [
        "H Kurniawan",
        "A Maslov",
        "M Pechenizkiy"
      ],
      "year": "2013",
      "venue": "Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems"
    },
    {
      "citation_id": "32",
      "title": "Human acute stress detection via integration of physiological signals and thermal imaging",
      "authors": [
        "M Abouelenien",
        "M Burzo",
        "R Mihalcea"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th ACM international conference on pervasive technologies related to assistive environments"
    },
    {
      "citation_id": "33",
      "title": "Towards mental stress detection using wearable physiological sensors",
      "authors": [
        "J Wijsman",
        "B Grundlehner",
        "H Liu",
        "H Hermens",
        "J Penders"
      ],
      "year": "2011",
      "venue": "2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "34",
      "title": "Stress detection using wearable physiological sensors",
      "authors": [
        "V Sandulescu",
        "S Andrews",
        "D Ellis",
        "N Bellotto",
        "O Mozos"
      ],
      "year": "2015",
      "venue": "Artificial Computation in Biology and Medicine: International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2015"
    },
    {
      "citation_id": "35",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "36",
      "title": "Detecting work stress in offices by combining unobtrusive sensors",
      "authors": [
        "S Koldijk",
        "M Neerincx",
        "W Kraaij"
      ],
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "Design of a biosignal based stress detection system using machine learning techniques",
      "authors": [
        "M Rizwan",
        "R Farhad",
        "F Mashuk",
        "F Islam",
        "M Imam"
      ],
      "year": "2019",
      "venue": "2019 international conference on robotics, electrical and signal processing techniques (ICREST)"
    },
    {
      "citation_id": "38",
      "title": "Stress and anxiety detection using facial cues from videos",
      "authors": [
        "G Giannakakis",
        "M Pediaditis",
        "D Manousos",
        "E Kazantzaki",
        "F Chiarugi",
        "P Simos",
        "K Marias",
        "M Tsiknakis"
      ],
      "year": "2017",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "39",
      "title": "Automatic stress detection in working environments from smartphones' accelerometer data: a first step",
      "authors": [
        "E Garcia-Ceja",
        "V Osmani",
        "O Mayora"
      ],
      "year": "2015",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "40",
      "title": "Continuous stress detection using a wrist device: in laboratory and real life",
      "authors": [
        "M Gjoreski",
        "H Gjoreski",
        "M Luštrek",
        "M Gams"
      ],
      "year": "2016",
      "venue": "proceedings of the 2016 ACM international joint conference on pervasive and ubiquitous computing: Adjunct"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "42",
      "title": "A machine learning approach for stress detection using a wireless physical activity tracker",
      "authors": [
        "B Padmaja",
        "V Prasad",
        "K Sunitha"
      ],
      "year": "2018",
      "venue": "International Journal of Machine Learning and Computing"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition based on skin potential signals with a portable wireless device",
      "authors": [
        "S Chen",
        "K Jiang",
        "H Hu",
        "H Kuang",
        "J Yang",
        "J Luo",
        "X Chen",
        "Y Li"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "44",
      "title": "Investigating the Physiological Correlates of Daily Well-being: A PERMA Model-Based Study. The Open",
      "authors": [
        "X Feng",
        "X Lu",
        "Z Li",
        "M Zhang",
        "J Li",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "Psychology Journal"
    },
    {
      "citation_id": "45",
      "title": "Wearable emotion recognition using heart rate data from a smart bracelet",
      "authors": [
        "L Shu",
        "Y Yu",
        "W Chen",
        "H Hua",
        "Q Li",
        "J Jin",
        "X Xu"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "46",
      "title": "Automatic ECG-Based Emotion Recognition in Music Listening",
      "authors": [
        "Y Hsu",
        "J Wang",
        "W Chiang",
        "C Hung"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Continuous stress detection using the sensors of commercial smartwatch",
      "authors": [
        "P Siirtola"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "48",
      "title": "Automated emotion recognition based on higher order statistics and deep learning algorithm",
      "authors": [
        "R Sharma",
        "R Pachori",
        "P Sircar"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "49",
      "title": "Feasibility study of stress detection with machine learning through eda from wearable devices",
      "authors": [
        "L Zhu",
        "P Ng",
        "Y Yu",
        "Y Wang",
        "P Spachos",
        "D Hatzinakos",
        "K Plataniotis"
      ],
      "year": "2022",
      "venue": "ICC 2022-IEEE International Conference on Communications"
    },
    {
      "citation_id": "50",
      "title": "Multimodal physiological signals and machine learning for stress detection by wearable devices",
      "authors": [
        "L Zhu",
        "P Spachos",
        "S Gregori"
      ],
      "year": "2022",
      "venue": "IEEE International Symposium on Medical Measurements and Applications (MeMeA)"
    },
    {
      "citation_id": "51",
      "title": "Stress recognition using face images and facial landmarks",
      "authors": [
        "T Jeon",
        "H Bae",
        "Y Lee",
        "S Jang",
        "S Lee"
      ],
      "venue": "2020 International Conference on Electronics, Information, and Communication (ICEIC) 2020"
    },
    {
      "citation_id": "52",
      "title": "Video-based stress detection through deep learning",
      "authors": [
        "H Zhang",
        "L Feng",
        "N Li",
        "Z Jin",
        "L Cao"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "53",
      "title": "Driver stress state evaluation by means of thermal imaging: A supervised machine learning approach based on ECG signal",
      "authors": [
        "D Cardone",
        "D Perpetuini",
        "C Filippini",
        "E Spadolini",
        "L Mancini",
        "A Chiarelli",
        "A Merla"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "54",
      "title": "Context Dual Hyper-Prior Neural Image Compression",
      "authors": [
        "A Khoshkhahtinat",
        "A Zafari",
        "P Mehta",
        "M Akyash",
        "H Kashiani",
        "N Nasrabadi",
        "Multi"
      ],
      "year": "2023",
      "venue": "Context Dual Hyper-Prior Neural Image Compression",
      "arxiv": "arXiv:2309.10799"
    },
    {
      "citation_id": "55",
      "title": "A modular system for detection, tracking and analysis of human faces in thermal infrared recordings",
      "authors": [
        "M Kopaczka",
        "L Breuer",
        "J Schock",
        "D Merhof"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "56",
      "title": "Deep Learning Approach for Detecting Work-Related Stress Using Multimodal Signals",
      "authors": [
        "W Seo",
        "N Kim",
        "C Park",
        "S Park"
      ],
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "57",
      "title": "Driver stress detection via multimodal fusion using attention-based CNN-LSTM. Expert Systems with Applications 2021",
      "authors": [
        "L Mou",
        "C Zhou",
        "P Zhao",
        "B Nakisa",
        "M Rastgoo",
        "R Jain",
        "W Gao"
      ],
      "venue": "Driver stress detection via multimodal fusion using attention-based CNN-LSTM. Expert Systems with Applications 2021"
    },
    {
      "citation_id": "58",
      "title": "Employing multimodal machine learning for stress detection",
      "authors": [
        "R Walambe",
        "P Nayak",
        "A Bhardwaj",
        "K Kotecha"
      ],
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "59",
      "title": "An interpretable machine learning approach to multimodal stress detection in a simulated office environment",
      "authors": [
        "M Naegelin",
        "R Weibel",
        "J Kerr",
        "V Schinazi",
        "R La Marca",
        "F Von Wangenheim",
        "C Hoelscher",
        "A Ferrario"
      ],
      "year": "2023",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "60",
      "title": "Electrodermal activity (EDA)",
      "authors": [
        "R Bailey"
      ],
      "year": "2017",
      "venue": "The international encyclopedia of communication research methods"
    },
    {
      "citation_id": "61",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "Deap: A database for emotion analysis; using physiological signals"
    },
    {
      "citation_id": "62",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "63",
      "title": "Pearson correlation coefficient",
      "authors": [
        "I Cohen",
        "Y Huang",
        "J Chen",
        "J Benesty",
        "J Benesty",
        "J Chen",
        "Y Huang",
        "I Cohen"
      ],
      "venue": "Noise reduction in speech processing 2009"
    },
    {
      "citation_id": "64",
      "title": "Spearman correlation coefficients, differences between. Encyclopedia of statistical sciences",
      "authors": [
        "L Myers",
        "M Sirois"
      ],
      "year": "2004",
      "venue": "Spearman correlation coefficients, differences between. Encyclopedia of statistical sciences"
    },
    {
      "citation_id": "65",
      "title": "Regression shrinkage and selection via the lasso",
      "authors": [
        "R Tibshirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology"
    },
    {
      "citation_id": "66",
      "title": "An introduction to variable and feature selection",
      "authors": [
        "I Guyon",
        "A Elisseeff"
      ],
      "year": "2003",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "67",
      "title": "Gene selection for cancer classification using support vector machines",
      "authors": [
        "I Guyon",
        "J Weston",
        "S Barnhill",
        "V Vapnik"
      ],
      "year": "2002",
      "venue": "Machine learning"
    },
    {
      "citation_id": "68",
      "title": "Stress, trauma, and posttraumatic growth: Social context, environment, and identities",
      "authors": [
        "R Berger"
      ],
      "year": "2015",
      "venue": "Stress, trauma, and posttraumatic growth: Social context, environment, and identities"
    },
    {
      "citation_id": "69",
      "title": "From local explanations to global understanding with explainable AI for trees",
      "authors": [
        "S Lundberg",
        "G Erion",
        "H Chen",
        "A Degrave",
        "J Prutkin",
        "B Nair",
        "R Katz",
        "J Himmelfarb",
        "N Bansal",
        "S Lee"
      ],
      "venue": "Nature machine intelligence"
    }
  ]
}