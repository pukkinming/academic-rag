{
  "paper_id": "2507.18026v1",
  "title": "Emotion Recognition From Skeleton Data: A Comprehensive Survey",
  "published": "2025-07-24T01:58:57Z",
  "authors": [
    "Haifeng Lu",
    "Jiuyi Chen",
    "Zhen Zhang",
    "Ruida Liu",
    "Runhao Zeng",
    "Xiping Hu"
  ],
  "keywords": [
    "CCS Concepts:",
    "Do Not Use This Code → Generate the Correct Terms for Your Paper",
    "Generate the Correct Terms for Your Paper",
    "Generate the Correct Terms for Your Paper",
    "Generate the Correct Terms for Your Paper Emotion Recognition, Skeleton, Posture, Gait Emotion Recognition from Skeleton Data: A Comprehensive Survey 111:3 Emotion Recognition from Skeleton Data: A Comprehensive Survey Models of Emotion and Body Movement (Sec.2) Models of Emotion (Sec.2.1) Discrete Emotions Theory Multidimensional Emotions Theory Componential Emotions Theory Emotional Body Expressions (Sec.2.2) Posture-based Emotion Expression Gait-based Emotion Expression Data (Sec.3) Data Collection Paradigm (Sec.3.1) Body Movement Capture Methodology (Sec.3.2) Optical Motion Capture System Inertial Motion Capture System Depth-sensing Camera RGB Camera Posture-based Datasets (Sec.3.3) Gait-based Datasets (Sec.3.4) Dataset Comparison and Analysis (Sec.3.5) Body Movement Based Emotion Recognition (Sec.4) Posture-based Emotion Recognition (Sec.4.1) Traditional Approaches Feat2Net Approaches End2EndNet Approaches Pre-training Approaches Evaluation of Methods on Public Datasets Gait-based Emotion Recognition (Sec.4.2) Traditional Approaches Feat2Net Approaches FeatFusionNet Approaches End2EndNet Approaches Unsupervised Approaches Evaluation of Methods on Public Datasets Methods Comparison and Analysis (Sec.4.3) Task-Specific Applications (Sec.5) Depression Detection Using Skeleton Data (Sec.5.1) Autism Detection Using Skeleton Data (Sec.5.2) Abnormal Behavior Detection Using Skeleton Data (Sec.5.3) Challenges and Future Research Directions (Sec.6) Constructing Diversified Datasets (Sec.6.1) Improving Model Performance (Sec.6.2) Building End-to-End and Efficient Emotion Recognition Frameworks (Sec.6.3) Expanding to Multi-person Emotion Recognition (Sec.6.4) Expanding to Multimodal Emotion Recognition (Sec.6.5) Leveraging Large Models for Skeleton-Based Emotion Recognition (Sec.6.6)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition through body movements has emerged as a compelling and privacy-preserving alternative to traditional methods that rely on facial expressions or physiological signals. Recent advancements in 3D skeleton acquisition technologies and pose estimation algorithms have significantly enhanced the feasibility of emotion recognition based on full-body motion. This survey provides a comprehensive and systematic review of skeleton-based emotion recognition techniques. First, we introduce psychological models of emotion and examine the relationship between bodily movements and emotional expression. Next, we summarize publicly available datasets, highlighting the differences in data acquisition methods and emotion labeling strategies. We then categorize existing methods into posture-based and gait-based approaches, analyzing them from both datadriven and technical perspectives. In particular, we propose a unified taxonomy that encompasses four primary technical paradigms: Traditional approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works within each category are reviewed and compared, with benchmarking results across commonly used datasets. Finally, we explore the extended applications of emotion recognition in mental health assessment, such as detecting depression and autism, and discuss the open challenges and future research directions in this rapidly evolving field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Table 1. A Comparison Of Related Survey",
      "text": "Year Posture Gait Kleinsmith et al.  [9]  2013 ✓ × Noroozi et al.  [10]  2018 ✓ × Deligianni et al.  [13]  2019 × ✓ Xu et al.  [21]  2023 × ✓ Mahfoudi et al.  [22]  2023\n\nEmotion plays a vital role in daily communication and behavioral decision-making, shaping individual cognition, social interactions, and group dynamics. In the field of Artificial Intelligence (AI), enabling machines to recognize human emotions has become a central research objective, as it significantly enhances the interactivity, adaptability, and decision-making capabilities of intelligent systems. Furthermore, emotion recognition holds substantial value across a broad spectrum of applications, including Human-Computer Interaction (HCI), mental health monitoring, educational technology, safe driving, abnormal behavior detection, intelligent tutoring systems, market analysis, and security surveillance  [1] [2] [3] . As such, the development of efficient and accurate emotion recognition methods is critical not only for advancing AI research but also for improving the functionality and real-world applicability of intelligent systems across diverse domains. Currently, the most widely used approaches for emotion recognition rely on facial expression analysis  [4] , audio signal processing  [5] , text analysis  [6] , and physiological signal monitoring  [7, 8] . However, these methods often depend on contact-based sensors or wearable devices, which can be expensive, uncomfortable to use, and potentially intrusive, thereby raising privacy concerns and limiting their practical deployment. In summary, achieving accurate emotion recognition while preserving user privacy and ensuring a comfortable, non-invasive experience remains a significant challenge in the field.\n\nExtensive research has shown that full-body movements play an important role in conveying emotional states, with body gestures serving as a rich, nonverbal channel for affective communication  [9] [10] [11] [12] [13] . Compared to facial expressions or voice, body-based cues-particularly those from the torso and limbs-enable long-range emotion sensing  [14] . The rapid development of depth-sensing technologies  [15]  and advances in human pose estimation  [16, 17]  have made it increasingly feasible to extract accurate 2D/3D skeleton data, which is more robust to environmental variations and better suited for privacy-sensitive scenarios than traditional visual or audio modalities. These technical advances have fueled growing interest in skeleton-based emotion recognition, an emerging topic in affective computing.\n\nEmotion recognition based on 3D skeleton data can be broadly categorized into two main categories. The first category focuses on posture-based emotion recognition, where emotional states are inferred by analyzing motion features associated with specific actions, such as knocking, waving, or jumping. The second category centers on gait-based emotion recognition, which explores dynamic features during natural walking to model underlying emotional states. The data indicate a steady increase in the number of publications leveraging posture and gait for emotion recognition. This growth highlights the growing interest and practical potential of skeleton-based methods for emotion understanding and human-computer interaction  [18] [19] [20] . Despite increasing research interest in this field, a comprehensive and unified survey of skeletonbased emotion recognition is still lacking. Existing reviews tend to adopt fragmented perspectives, often overlooking a key insight: both posture and gait stem from the same underlying skeleton data and therefore share common representations, feature extraction methods, and modeling strategies. Their primary difference lies in temporal dynamics rather than data modality. As summarized in Table  1 , most existing surveys focus on a single modality-such as body expressions  [9, 10, 22]  or gait-based affective analysis  [13, 21] .\n\nGiven the methodological overlap and the recent surge of interest in skeleton-based emotion recognition, it is both timely and necessary to establish a unified review framework that integrates posture-and gait-based approaches. In this work, we present a comprehensive survey that unifies posture and gait analysis under a single framework, offering a structured comparison of existing methodologies, datasets, and applications, this paper is structured in Fig.  1 . Sec. 2 introduces the fundamental emotion models and examines the relationship between body movements and emotional expression. In Sec. 3, we review existing public datasets, highlight variations in data collection methods, and compare the characteristics of each dataset. Sec. 4 provides a detailed analysis of methodologies, categorizing them into posture-based and gait-based approaches, and examines the technical features of each. Sec. 5 explores extended applications of skeleton-based emotion recognition. Finally, Sec. 6 concludes the survey and outlines promising directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Models Of Emotion And Body Movement",
      "text": "This section provides a comprehensive overview of the major emotion models discussed in Sec. 2.1, including discrete, dimensional, and componential models. The relationship between emotional states and posture-based expressions is reviewed in Sec. 2.2.1, while gait-based emotional expressions are explored in Sec. 2.2.2.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models Of Emotion",
      "text": "From a psychological perspective, emotions are stimulus-driven responses characterized by distinct physiological changes  [23] , and are commonly classified as reactional, hormonal, or automatic processes  [24] . The modeling of affect has long been a subject of scholarly debate, with three prominent approaches emerging: discrete, dimensional, and componential models  [25] .\n\n2.1.1 Discrete Emotions Theory. The classification of emotions into distinct, easily recognizable categories has long been a foundational approach in emotion research. A widely accepted perspective, heavily influenced by the work of Paul Ekman  [26] , posits the existence of a set of universal primary emotions-typically happiness, sadness, fear, anger, disgust, and surprise-that are biologically hardwired and universally recognized across cultures. This discrete-state view has gained significant traction in affective computing due to its conceptual simplicity and its claim of cross-cultural applicability.\n\nHowever, the discrete emotions theory has faced increasing debate. While the discrete emotions theory emphasize emotional consistency across cultures, numerous studies have demonstrated that emotional perception and expression are significantly influenced by factors such as age, gender, and cultural or linguistic context  [9, 27, 28] . These factors are critical for experimental design and the accurate interpretation of findings across diverse populations, and have been extensively discussed in the literature.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multidimensional Emotions",
      "text": "Theory. Another widely adopted approach to emotion modeling is the dimensional model, which represents emotions as points within a continuous, multidimensional space  [29, 30] . Among the most commonly used dimensions are valence (the degree of pleasantness or unpleasantness) and arousal (the level of activation or intensity). This framework acknowledges the complexity of emotional experiences and enables more nuanced analysis by capturing subtle variations across emotional states.\n\nThe 2D VA model (Valence-Arousal), most notably represented by Russell's Circumplex Model  [31]  (see Fig.  2 .(a)), maps emotions onto the valence-arousal plane and has been widely applied across disciplines such as psychology, marketing, and education. Similarly, the 3D PAD model (Pleasure, Arousal, Dominance), proposed by Mehrabian  [32]  (see Fig.  2 .(b)), extends this framework by incorporating dominance as a third axis. These continuous models offer a rich descriptive space for representing emotions and have been shown to relate intuitively to physiological signals, particularly those associated with valence and arousal  [9, 28] . Notably, studies analyzing body movements in relation to affect suggest that the arousal dimension accounts for the greatest variance in movement-related emotional expression  [33] [34] [35] .\n\nHowever, despite their theoretical richness, dimensional models present challenges for automatic emotion recognition systems, as mapping subtle and continuous affective states to discrete, observable body expressions remains a non-trivial task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Componential Emotions Theory.",
      "text": "Componential models occupy a middle ground between categorical and dimensional approaches in terms of descriptive capacity. These models organize emotions hierarchically, positing that complex emotions arise from combinations of more basic ones.   [36] , which defines complex emotions as dyads-pairs of basic emotions-whose likelihood decreases as their complexity increases.\n\nAlthough less prevalent in affective computing, compound emotions such as \"happily surprised\" or \"angrily surprised\" have garnered growing interest for their greater expressive flexibility. Componential models offer a useful balance between interpretability and richness, making them a promising framework for developing more discriminative affective computing systems.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotional Body Expressions",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Posture-Based Emotion Expression.",
      "text": "Human emotions are closely intertwined with bodily movements, as emotional states often manifest through subtle yet consistent changes in physical behavior  [37] . Scientific studies have demonstrated that parameters such as joint speed, body sway, and gesture dynamics are modulated by affective states. These observable variations in movement serve as important non-verbal cues that can be analyzed to infer an individual's emotional condition.\n\nIn the investigation of posture and emotional expression, Dahl et al.  [38]  proposed that musicians convey core emotions through distinct movement patterns: happiness and anger are expressed via large, fast gestures (fluent versus jerky), sadness is characterized by small, slow, smooth motions, and fear is marked by minimal, jerky movements with low recognizability due to potential suppression. In  [39] , the author observed that emotion-specific movement patterns vary across affective states: anger involves large, forceful gestures and rapid striking; sadness is expressed through slower, constricted motions with reduced range; joy features fluent, frequent movements and an upward head tilt; and anxiety is characterized by tense, restricted actions and a hurried rhythm. Dael et al.  [40]  further identified that emotion-specific movement patterns are manifested through distinct bodily expressions: anger is associated with a forward-leaning posture, outstretched arms, and aggressive gestures; amusement includes intermittent behaviors such as object-touching and an upright, sideways-oriented head directed toward the interlocutor; and joy often involves an upward and off-centered head tilt accompanied by asymmetric arm movements.\n\nTo better illustrate the strong association between body movement patterns and discrete emotional states, this paper summarizes the characteristic motion features corresponding to four basic emotions, as presented in Table  2 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Gait-Based Emotion Expression.",
      "text": "Gait refers to the periodic locomotion pattern exhibited by humans during walking and serves as an important biometric trait. Studies have shown that emotional states can significantly influence the kinematic parameters of gait, including walking speed, acceleration, and other movement features associated with specific emotions  [13] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fear",
      "text": "Postures characterized by crossed limbs and visible muscular tension, such as clenched hands and inward-drawn elbows. Movements may be restrained, sudden, or bouncy. The head is typically lowered between the shoulders, or the body may adopt a crouched, motionless stance.\n\nIn the investigation of gait and emotional expression, Michalak et al.  [12]  found that individuals tend to display a more bouncy gait with increased arm swing when experiencing happiness. In contrast, sadness results in slower, heavier steps, reduced arm movement, and a more relaxed upper body posture. Anger is typically characterized by fast and forceful stomping, while fear is associated with a rapid but short-stepped gait. Kim et al.  [41]  reported that peak plantar pressure shifts toward the forefoot under happy emotions, whereas it concentrates on the outer rear heel during sadness. Cross et al.  [42]  conducted kinematic analyses on motion capture data from 16 individuals walking under five emotional conditions (happiness, satisfaction, fear, anger, and neutrality). Their results showed that gait speed was highest in happy and angry states and lowest during sadness. Additionally, sad participants frequently bent their necks and contracted their chests, whereas happy individuals tended to extend their torsos or lower their shoulders. Montepare et al.  [11]  examined gait patterns in ten female undergraduates across four emotional conditions. They observed that arm swing was significantly reduced during sadness, while happy emotions resulted in faster gait rhythms and more dynamic arm movements. Emotions such as anger and pride were associated with increased stride length. Furthermore, observers were able to accurately infer participants' emotional states based on their gait features.\n\nTo more intuitively illustrate the strong correlation between gait patterns and emotional states, this paper summarizes the typical gait characteristics associated with four basic emotions, as shown in Table  3 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Data",
      "text": "This section provides a comprehensive overview of common methods for capturing body movement data in Sec. 3.1 and Sec. 3.2. Emotion recognition datasets based on general body movements and gait patterns are reviewed in Sec. 3.3 and Sec. 3.4, respectively. Finally, Sec. 3.5 presents a detailed comparison of these datasets, highlighting their similarities and differences to inform and support future research efforts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Collection Paradigm",
      "text": "Since skeleton-based emotion recognition can be categorized into two types, there are correspondingly two data collection paradigms, as illustrated in Fig.  3 . Fig.  3 .(a) depicts the paradigm for collecting posture data, where actors typically perform various emotional expressions within a predefined area-usually less than one meter in size. In contrast, Fig.  3 .(b) illustrates the gait data collection paradigm, in which participants walk back and forth along a designated path, covering a larger activity area.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Body Movement Capture Methodology",
      "text": "The methods for capturing 3D human skeleton data have evolved significantly over time, transitioning from professional, lab-based motion capture systems to more accessible, home-based solutions, as illustrated in Fig.  4 . Initially, wearable-based systems such as optical motion capture systems (Fig.  4 .(a)) were widely used in controlled laboratory environments. Subsequently, inertial motion In the following sections, we provide a detailed overview of the characteristics and capabilities of each of these approaches.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Optical Motion",
      "text": "Capture System. In 2006, Ma et al.  [43]  employed an optical motion capture system to record three types of daily activities, annotating them with corresponding emotional labels. The DMCD dataset  [44]  was collected using a similar system to capture the movements of dancers. This system relies on reflective markers-typically spherical and coated with retroreflective material-affixed to key anatomical landmarks on the subject's body. Multiple infrared cameras positioned around the capture space emit light and detect the signals reflected from the markers. By triangulating the marker positions from different camera angles, the system can accurately reconstruct the 3D trajectories of joints. This method offers high spatial precision and is widely used in controlled laboratory environments for detailed motion analysis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Inertial Motion",
      "text": "Capture System. The Emilya dataset  [45] , KDAE  [46] , and MEBED dataset  [47]  utilize inertial motion capture systems for data collection. These systems employ wearable IMU sensors, which are typically attached to major body segments such as the limbs, torso, and head. Each IMU contains accelerometers, gyroscopes, and, in some cases, magnetometers, enabling the measurement of linear acceleration, angular velocity, and orientation. By integrating these signals, the system can reconstruct the 3D motion of the human skeleton without the need for external cameras. IMU-based systems are portable and well-suited for capturing movement in unconstrained or real-world environments.\n\nAdditionally, Cui et al.  [48]  utilized the built-in triaxial accelerometers in smartphones to monitor participants' daily activities and classify their emotional states. In this study, two smartphones and one tablet were used: the smartphones were worn on the participant's wrist and ankle to capture raw data from the accelerometer and gravity sensors, sampled at a rate of 5 Hz.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3.2.3",
      "text": "Depth-sensing Camera. The release of the Kinect depth camera in 2011  [15]  significantly simplified the process of skeleton sequence acquisition. Since then, many researchers have adopted the Kinect series for data collection  [49, 50] . Microsoft Kinect is a depth-sensing camera that integrates an RGB camera, an infrared (IR) emitter, and an IR depth sensor to capture both color and depth information. By projecting a structured infrared light pattern and analyzing its deformation, Kinect constructs a depth map of the scene. Using this depth data, built-in body tracking algorithms can identify human figures and estimate the 3D positions of joints in real time.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Rgb Camera.",
      "text": "In 2017, breakthroughs in human pose estimation-particularly the development of real-time multi-person pose estimation methods such as OpenPose  [51] , HRNet  [52] , and Vnect  [53] -further advanced emotion recognition based on body movements. These techniques enabled the extraction of 2D or 3D skeletal joint coordinates directly from standard RGB video frames, eliminating the need for wearable sensors or depth cameras. The underlying algorithms typically employ Convolutional Neural Networks (CNNs) to detect body parts and estimate joint positions, followed by part affinity fields to associate joints with specific individuals in multi-person scenes. By 2019, researchers had begun leveraging skeleton sequences extracted from RGB videos for emotion recognition tasks  [18, 54, 55] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Posture-Based Datasets",
      "text": "Table  4  summarizes datasets commonly used in skeleton-based emotion recognition research. Their distinctive features are discussed in detail in the following section.\n\nEmilya Dataset. The EMotional body expression In daILY Actions dataset (Emilya)  [45, 62]  comprises a total of 8,206 samples. Eleven actors (six females and five males) performed eight distinct emotions within the context of seven everyday actions. These actions were carefully selected to involve full-body movements, particularly emphasizing the upper body and arm motions, in order to ensure diverse emotional expressions. The seven daily activities include sitting down, walking, walking while carrying an object, moving books across a table with both hands, knocking on a door, lifting an object, and throwing an object.\n\nIn the walking tasks, actors were instructed to walk back and forth along the longer side of a room, with two distinct variations: normal walking and walking while carrying an object. Each sample was labeled according to one of eight emotional categories: Neutral, Joy, Anger, Panic, Fear, Anxiety, Sadness, and Shame.\n\nAll data were captured using the Xsens MVN motion capture system, which records 3D skeleton sequences at a frame rate of 120 Hz. Each skeleton sequence consists of 28 joints.\n\nBML Dataset. The BML Dataset  [43]  comprises 4,080 body movement sequences. Thirty actors (15 females and 15 males) performed four distinct actions-walking, knocking, lifting, and throwing-each interleaved with walking segments, under four different emotional states: angry, happy, neutral, and sad. All motion data were captured using the Falcon Analog optical motion capture system, which provides 3D positional data for 16 joints. MEBED Dataset. The MPI Emotional Body Expression Database (MEBED) consists of 1,477 samples. Eight actors (four males and four females), each around 25 years old, participated in the data collection. The participants were asked to perform emotional expressions across four scenario types, each containing ten distinct emotion categories. The four scenario types are: Solitary non-verbal emotional scenarios, Communicative non-verbal emotional scenarios, Short sentences without direct speech, and Short sentences with direct speech. The ten emotions include: amusement, joy, pride, relief, surprise, anger, disgust, fear, sadness, and shame, with an additional neutral emotion category included.\n\nAll motion data were recorded using the Xsens MVN motion capture system at a sampling rate of 120 Hz, capturing 3D skeletal sequences composed of 28 joints. During the recording sessions, all participants were seated on stools. Consequently, the ten lower-body joints are typically excluded during analysis.\n\nMEBED includes dual-label annotations: self-reported emotions from the performers and thirdparty annotations from external observers viewing each motion sequence. It is worth noting that the dataset exhibits class imbalance, with significant variation in the number of samples per emotion category.\n\nKDAE Dataset. The Kinematic Dataset of Actors Expressing Emotions (KDAE)  [46]  contains a total of 1,402 samples. Twenty-two semi-professional actors (11 females and 11 males) were asked to perform seven distinct emotions-happiness, sadness, neutral, anger, disgust, fear, and surprise-across 70 everyday event scenarios (10 scenarios per emotion). The actors performed the emotional expressions on a 1m × 1m square stage, positioned 0.5 meters away from a wall.\n\nAll motion data were recorded using a portable wireless motion capture system capable of tracking 72 body markers at a frame rate of 125 Hz. Notably, nearly half of these 72 markers correspond to the hands. However, as this work focuses specifically on full-body emotional expression, the hand markers were excluded from the analysis. Only 24 key body markers were retained.\n\nEGBM Dataset. The Emotional Gestures and Body Movements Corpus (EGBM)  [56]  consists of 560 samples. The dataset features performances by 16 professional Polish actors (8 females and 8 males), who expressed seven distinct emotions-happiness, sadness, neutral, anger, disgust, fear, and surprise-without any predefined instructions or cues.\n\nAll motion data were captured using a Kinect V2 camera at a frame rate of 30 Hz. Each emotion category contains 80 samples, and each pose sequence provides the 3D positions of 25 joints.\n\nUCLIC Dataset. The UCLIC dataset  [57]  comprises 108 samples. It features performances from 13 actors, including 11 Japanese, 1 Sri Lankan, and 1 American. The actors expressed four basic emotions-anger, fear, happiness, and sadness-freely and based on their own interpretations, without any imposed constraints.\n\nAll motion data were collected using a MoCap system, with each pose sequence providing the 3D positions of 32 joints.\n\nDMCD Dataset. The Dance Motion Capture Database (DMCD)  [44]  consists of 108 samples. The dataset features performances by six female dancers with diverse backgrounds in gymnastics, ballet, theatrical dance, and other styles. Each participant performed a dance sequence, with each sequence associated with a specific emotional state.\n\nIn total, 12 emotions were expressed: excited, happy, pleased, satisfied, relaxed, tired, bored, sad, miserable, annoyed, angry, and afraid. All motion sequences were captured using the PhaseSpace Impulse X2 MoCap System at a frame rate of 120 Hz. Each pose sequence contains the 3D positions of 38 joints.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Gait-Based Datasets",
      "text": "E-Gait Dataset. The Emotion-Gait (E-Gait) dataset  [58]  comprises 2,177 real-world gait sequences, each annotated with one of four emotion categories: happy, sad, neutral, or angry. The dataset is divided into two subsets:\n\n• Subset A: Contains 342 gait sequences sourced from datasets including BML  [43] , ICT  [63] ,\n\nCMU-MOCAP  [64] , and Human3.6M  [65] . • Subset B: Comprises 1,835 gait sequences extracted from the Edinburgh Locomotion Mocap Database (ELMD)  [59] .\n\nAll motion data were captured using a motion capture (MoCap) system at a frame rate of 60 Hz, recording 3D positions of 21 joints. To ensure consistency across both subsets, the authors standardized the skeletal data to include 16 joints. Each gait sequence in the E-Gait dataset was independently annotated by 10 individuals. The final emotion label for each sequence was determined through majority voting among the annotators.\n\nBME Dataset. The Body Motion-Emotion dataset (BME), dataset  [60]  dataset involves two experiments.\n\n• Emotional Gaits: 8 professional actors (4 males, 4 females) performed five emotions-neutral, joy, anger, sadness, and fear-while walking. Each emotion was recorded in at least 5 trials, resulting in 200 total samples. Actors were instructed to embody emotions (e.g., \"walking in a dark, dangerous room\" for fear) to standardize expressions. • Control Gaits: 5 naive male subjects walked at three speeds-slow, normal, and fast -with 5 repetitions per speed, yielding 75 total samples. This subset isolated speed-related kinematic variations for comparison.\n\nAll motion data were captured using a Vicon V8 optoelectronic motion capture system with 24 cameras (16 in Experiment 2) at a frame rate of 120 Hz. The system tracked 3D positions of markers on 12 body segments, including joints like shoulders, elbows, hips, and knees. EMOGAIT Dataset. The EMOGAIT dataset  [61]  consists of 1,440 real-world gait sequences collected from 60 subjects (33 females and 27 males). Each sequence is annotated with one of four emotion labels: happy, sad, neutral, or angry. All data were captured using standard RGB cameras, and 2D human skeletons were extracted from the videos using a pose estimation algorithm. Each skeleton sequence contains 16 joints.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Dataset Comparison And Analysis",
      "text": "Table  5  provides a comparative overview of emotion induction methods and performance instructions across several widely used datasets. Most datasets-such as Emilya, BML, MEBED, KDAE, and EGBM-utilize pre-selected emotionally charged scenarios to elicit specific emotional expressions from participants. While datasets like Emilya and BML offer explicit movement instructions to ensure consistency, others such as KDAE and EGBM adopt a freer performance style, allowing participants to express emotions more naturally. The level of performer expertise also varies across datasets, ranging from college students and amateur actors to trained professionals, potentially influencing both the expressiveness and reliability of the recorded data.\n\nThe EMOGAIT dataset is notable for its use of emotionally evocative film clips as an induction method, aiming to capture changes in gait following the emotional priming. In contrast, the UCLIC dataset is distinctive in its lack of both a defined emotion induction protocol and specific performance instructions, relying instead on participants' spontaneous emotional postures. These differences reflect the varied design philosophies and intended applications of each dataset, ultimately affecting the naturalness, consistency, and emotional richness of the captured body movement data.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Body Movement Based Emotion Recognition",
      "text": "This section provides a comprehensive overview of research advances in skeleton-based emotion recognition. From the perspectives of data characteristics and technical development, existing methods can be broadly categorized into two main groups: posture-based (in Sec.   6  summarizes a selection of representative studies from recent years that focus on automatic emotion recognition using human motion data. To facilitate a clearer understanding and comparison of posture-based emotion recognition approaches, this section is organized according to the data acquisition techniques employed to capture posture information. Specifically, we categorize existing works based on whether the posture data is collected using motion capture systems, depth sensors, or RGB cameras.\n\nMotion Capture Systems. Several studies have focused on extracting low-level kinematic features from skeleton or motion data to recognize affective states. Kapur et al.  [66]  laid the foundation by extracting the mean and standard deviation of position, velocity, and acceleration as input features for training a machine learning model to classify human emotions. Bernhardt et al.  [67]  advanced this line of work by segmenting complex movements into primitives, analyzing key dynamic features, and applying normalization techniques to reduce individual movement biases. Extending these efforts, Samadani et al.  [76]  proposed a hybrid generative-discriminative approach for affective movement recognition that accounts for kinematic variation, interpersonal differences, and stochastic noise. Their method employs Hidden Markov Models (HMMs) to capture the temporal dynamics of affective movements and generates Fisher Score representations to encode both kinematic and dynamic characteristics.\n\nA notable contribution to emotion recognition from body movement comes from the work of Fourati et al., who developed a multi-level framework for systematically describing and analyzing emotional body behaviors. In their 2014 study  [77] , the authors introduced the Multi-Level Body Notation System (MLBNS) along with a corresponding emotional body behavior dataset. This system employed a movement-quality-based coding scheme with three descriptive levels, enabling a structured characterization of body movement features. In addition to the coding framework, they recorded and validated a new dataset to support further analysis. To simplify the perceptual rating task, a follow-up study  [45]  selected a subset of key variables from MLBNS. Building on this foundation, Fourati et al.  [68]  expanded the framework by extracting 110 emotion-related features across three hierarchical levels: anatomical description, directional description, and posture/movement dynamics. These features included 38 body part descriptors, 30 inter-limb relationship measures, and 42 local limb movement descriptors. While both  [68]  and  [69]  pursued similar objectives, the latter provided a more detailed analysis of the relative importance of individual features for distinguishing emotional categories. Further extending the MLBNS framework, the authors later introduced four additional feature groups-temporal patterns, slope-based descriptors, peak dynamics, global motion statistics, and temporal regularity measures-which led to notable improvements in classification accuracy  [70] .\n\nAnother influential line of research has been led by Crenn et al., who systematically investigated emotion recognition through detailed analysis of skeletal motion features. In their early work, Crenn et al.  [71]  proposed a comprehensive feature extraction framework that integrates geometric relationships, motion correlations, and frequency-based periodicity. The extracted features included inter-joint distances, triangle areas formed by specific joints, joint angles, and the velocity and acceleration of various joints. Experimental results showed that these features were strongly correlated with emotional expression. Building on this foundation, Crenn et al.  [72]  introduced the concept of a neutral pose, suggesting that emotional states could be inferred by analyzing the residual differences between observed skeletal data and this neutral reference. This concept was further refined in a 2020 study  [73] , where a cost function optimization strategy was employed to more effectively synthesize neutral motion, thereby improving emotion recognition performance. In subsequent research, the motion-emotion feature framework proposed by Crenn has been widely adopted and extended by others, highlighting its lasting impact on the field.\n\nEmotion recognition based on full-body movement data in gaming scenarios has emerged as a significant and increasingly influential research direction. Kleinsmith et al.  [78]  investigated the recognition of non-basic emotional states within video game contexts. They collected posture data from players engaged in Nintendo Wii sports games, established ground-truth labels through human observer evaluations, and developed automatic recognition models based on the captured movements. Expanding on this idea, Savva et al.  [79]  explored whether players' full-body movements in gaming environments could reflect their aesthetic and emotional experiences. Using similar Nintendo sports games, their study demonstrated that the automatic system achieved an emotion recognition accuracy comparable to that of human raters. Further advancing this line of research, Garber et al.  [80]  collected motion capture data during gameplay and extracted features such as body symmetry, head displacement, and posture openness to infer players' emotional states.\n\nDepth Sensors. Saha et al.  [74]  conducted research on gesture-based emotion recognition using a Kinect sensor. They selected eleven joint coordinates from the upper body and hands to extract nine features related to joint distances, accelerations, and angles, which were used to recognize five basic emotions: anger, fear, happiness, sadness, and relaxation. The results showed that an ensemble decision tree achieved the highest average classification accuracy of 90.83%. Piana et al.  [75]  processed motion capture data to extract features such as joint energy, movement direction, posture, and body symmetry. They constructed adaptive representations through dictionary learning and employed a linear support vector machine for emotion classification. Ahmed et al.  [81]  computed a comprehensive set of body movement features categorized into ten groups. In the first stage, they applied analysis of variance (ANOVA) and multivariate analysis of variance (MANOVA) to remove irrelevant features and distribute the remaining ones across groups. In the second stage, a binary chromosome-based genetic algorithm was used to select an optimal feature subset for maximizing emotion recognition performance. Finally, score-and rank-level fusion techniques were applied to further enhance classification accuracy.\n\nRGB Camera. Glowinski et al.  [82]  proposed a method for automatic emotion recognition by analyzing upper-body gestures, specifically head and hand movements. Using 40 emotional clips-representing anger, joy, relief, and sadness-performed by professional actors from the GEMEP database  [83] , the researchers captured video recordings with a dual-camera setup and extracted dynamic features such as motion energy (sum of velocity magnitudes) and spatial extent (perimeter of the enclosing triangle). In a follow-up study, Glowinski et al.  [84]  extracted additional kinematic features from head and hand trajectories, including energy, spatial range, smoothness, and symmetry. They applied Principal Component Analysis (PCA) to reduce the feature dimensionality to a four-dimensional representation, which effectively grouped emotions along the axes of valence (positive vs. negative) and arousal (high vs. low).",
      "page_start": 4,
      "page_end": 16
    },
    {
      "section_name": "Feat2Net",
      "text": "Approaches. In  [85] , a multi-branch deep learning architecture is proposed, consisting of a local branch based on stacked Long Short-Term Memory (LSTM) units and a global branch based on a Multi-Layer Perceptron (MLP). The local branch processes temporal local features through stacked LSTM layers to achieve higher levels of abstraction and capture fine-grained action details. In parallel, the global branch utilizes a MLP to process temporal global features and extract salient patterns. The outputs from both branches are concatenated and passed through a fully connected layer followed by a softmax function for emotion classification.\n\nIn a more recent study, Oğuz et al.  [86]  investigated a wide range of time-domain, frequencydomain, and statistical features, applying feature selection techniques to identify four salient features per frame. These selected features were then aggregated into a feature matrix used for subsequent emotion recognition.\n\nWang et al.  [14]  proposed a multi-scale feature selection algorithm grounded in a pseudo-energy model and introduced a multi-scale spatiotemporal network to decode the complex relationship between emotional states and full-body movements. This approach effectively captures both long-term postural variations and short-term dynamics, thereby enhancing emotion recognition performance. Further extending this line of research, Wang et al.  [87]  explored joint energy features by constructing a body expression energy model and designing a multi-input symmetric positive definite matrix network. This framework facilitates the extraction of interpretable spatiotemporal features, contributing to both the robustness and interpretability of emotion classification.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "End2Endnet",
      "text": "Approaches. In recent years, with the remarkable success of neural networks in the broader field of artificial intelligence, deep learning models have become the dominant paradigm for emotion recognition from 3D skeletal motion sequences. In this section, we summarize existing methods based on differences in their network architectures.\n\nRNN-based models. Sapiński et al.  [88]  collected body movement data from professional actors using Kinect V2 and employed LSTM networks to classify emotions based on skeletal sequences. Zhang et al.  [19]  proposed an Attention-based Stacked LSTM (AS-LSTM) model for emotion recognition from whole-body movements in virtual reality (VR) environments. By integrating an attention mechanism into the traditional LSTM framework, the model assigns varying weights to joint point sequences across motion frames, enabling it to focus on key joints while suppressing redundant information. This enhancement improves both the learning capacity of the network and the overall recognition accuracy.\n\nCNN-based models. Karumuri et al.  [89]  were among the first to propose encoding skeletal information into image representations. They converted 3D joint coordinates into 8-bit RGB images using four encoding schemes: coarse position, fine position, logical position, and logical velocity. Two convolutional neural network (CNN) architectures were then designed for training-Single-Input Architecture (SIA-CNN) and Multi-Input Architecture (MIA-CNN). Cui et al.  [90]  drew on findings from body language literature to associate specific postures with emotions. They employed the Convolutional Pose Machine (CPM) algorithm to extract human keypoint coordinates, generated simplified line diagrams via clustering, and used a CNN combined with a Softmax layer for emotion classification. Beyan et al.  [20]  proposed a dual-branch CNN architecture that processes coarsegrained (e.g., 4s) and fine-grained (e.g., 1s) temporal features in parallel, leveraging logical position image representations and data augmentation techniques to improve classification performance.\n\nGCN-based models. Shen et al.  [91]  employed a Temporal Segment Network (TSN) to extract RGB features and used ST-GCN for skeletal features. These features were unified through a residual encoder and classified via a residual fully connected network. Ghaleb et al.  [18]  developed an emotion recognition framework based on Spatial-Temporal Graph Convolutional Networks (ST-GCN), enhanced with a spatial attention mechanism to highlight the relationship between joint spatial patterns and emotional states. Building on the ST-GCN architecture, Shi et al.  [92]  introduced a selfattention mechanism that dynamically adjusts the skeletal connectivity structure, demonstrating the effectiveness of adaptive topologies for capturing cues. Shirian et al.  [93]  proposed the Learnable Graph Inception Network (L-GrIN), which incorporates non-linear spectral graph convolution, a graph inception layer, learnable adjacency, and a learnable pooling function. The model jointly learns the graph structure and performs emotion classification by optimizing a composite loss function that combines classification and graph structure losses.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Pre-Training Approaches.",
      "text": "With the rapid advancement of large-scale models, researchers have increasingly recognized the value of pretrained models in providing transferable prior knowledge across diverse domains, thereby significantly enhancing task performance. Consequently, there has been growing interest in incorporating pretraining strategies into skeleton-based emotion recognition.\n\nPaiva et al.  [94]  adopted a two-stage approach: self-supervised pretraining on large-scale unlabeled skeleton datasets (MPOSE2021 and Panoptic Studio) using a Masked Autoencoder (MAE) to learn spatiotemporal dependencies, followed by fine-tuning on the BoLD dataset using an MLP classifier. In  [95] , the authors proposed the Emotion-Action Interpreter based on Large Language Models (EAI-LLM). This model first extracts skeleton features using a Graph Convolutional Network (GCN), and then maps these features into a semantic space through components such as a multi-granularity skeletal annotator and a unified skeletal labeling module. By leveraging the reasoning capabilities of large language models, EAI-LLM not only performs emotion recognition but also generates interpretable textual explanations.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Evaluation Of Methods On Public Datasets.",
      "text": "To facilitate a clear comparison of different approaches, we summarize the performance of representative methods on three widely used public datasets-EGBM, KDAE, and Emilya-as shown in Tables  7 8 9 .\n\nOn the EGBM dataset, early RNN-based methods achieved moderate accuracy (69% and 74%, respectively). In contrast, more recent approaches that incorporate handcrafted features with fully connected networks (FC), such as those proposed by Wang et al.  [14, 87] , report significantly higher performance, exceeding 95%. A similar pattern is observed on the KDAE dataset. While GCN-based models  [18]  yielded modest results (65%), methods combining handcrafted features with shallow classifiers or neural networks demonstrated strong performance. Notably, Oğuz et al.  [86]  achieved an impressive 99.99% accuracy under a hold-out protocol. On the Emilya dataset, both traditional and deep learning models performed well; CNN-based methods  [20]  and handcrafted feature-driven fully connected networks  [14]  both surpassed 94% accuracy.\n\nOverall, while end-to-end models generally yield superior performance, handcrafted feature-based approaches remain highly competitive-particularly in scenarios with limited data availability.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Gait-Based Emotion Recognition",
      "text": "",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Traditional",
      "text": "Approaches. Machine learning-based methods have consistently served as a cornerstone in the field of emotion recognition from gait. Table  10  summarizes a selection of representative studies in recent years that focus on automatic emotion recognition based on human gait. To facilitate a clearer understanding and comparison of gait-based emotion recognition approaches, this section is organized according to the data acquisition methods used for capturing gait information. Specifically, we categorize existing works based on whether gait data is obtained through motion capture systems, depth sensors (e.g., Kinect), wearable devices, or video-based pose estimation. Motion capture systems. In the early stages of skeleton-based gait emotion recognition research, motion capture systems-particularly the VICON system-were widely used to acquire high-fidelity gait data. Omlor et al.  [96]  were among the first to utilize the VICON system to collect gait data from participants. They introduced a nonlinear source separation technique to extract spatiotemporal primitives from joint-angle trajectories of complex full-body movements, with a particular focus on patterns associated with emotional gait. Venture et al.  [97, 98]  first conducted psychological experiments to investigate the factors influencing human perception of emotional gaits. They subsequently used feature vectors and Principal Component Analysis (PCA) to demonstrate the feasibility of numerical emotion recognition and proposed a similarity index-based algorithm for classification. Their experiments employed both 6-degree-of-freedom (DOF) and 12-DOF models.\n\nKarg et al.  [99] [100] [101]  extracted gait-related features such as walking speed, stride length, and the minimum, average, and maximum values of key joint angles (e.g., neck, shoulder, and chest). They applied dimensionality reduction techniques, including PCA, to manage feature complexity before employing machine learning models for emotion classification. In a related study,  Daoudi et  al.  [102]  transformed joint position and velocity data into covariance matrices, which were then mapped onto the non-linear Riemannian manifold of Symmetric Positive Definite (SPD) matrices. Emotion classification was performed by computing geodesic distances and geometric means on the manifold.\n\nDepth sensors. In 2010, the release of Microsoft's depth-sensing camera introduced a novel technological approach for gait data acquisition and analysis. Researchers from the Chinese Academy of Sciences designed an experiment in which participants viewed emotion-inducing video clips, followed by walking sessions recorded using the Kinect v2 sensor  [49, 103] . A Fourier transform was applied to extract 168 frequency-domain features to differentiate among happy, sad, and neutral emotional states  [49] . Building on this work, Li et al.  [103]  further incorporated time-domain features such as stride length and gait cycle duration, resulting in a significant improvement in classification performance.\n\nIn a related study  [104] , geometric and kinematic features were extracted from human gait data. These included body-related features (e.g., head inclination angle, joint flexion angle), effort-related features (e.g., kinetic energy, average joint velocity), shape-related metrics (e.g., density index), and spatial descriptors (e.g., body contraction index, symmetry). A total of 17 features were extracted, processed through vector quantization and normalization. A binary chromosome-based genetic algorithm was then used to select optimal feature subsets for four expert models, thereby enhancing the emotion recognition performance of each model.\n\nWearable devices. With the widespread adoption of smartphones and wearable devices, the built-in sensors and cameras of these technologies have provided more accessible and convenient means for gait data collection. Zhang et al.  [105]  and Cui et al.  [48]  employed a custom-designed smart wristband to capture 3D acceleration data from various joints, including the right wrist and ankle. They computed time-domain features such as skewness, kurtosis, and standard deviation, and further extracted both time-and frequency-domain features using power spectral density and Fast Fourier transform (FFT) to classify three emotional states.\n\nSimilarly, Quiroz et al.  [106, 107]  had participants wear smartwatches while walking along a 250meter S-shaped corridor. Their study focused on inferring individuals' emotional states from sensor data collected via smartwatches, examining the relationship between motion sensor-captured gait patterns and emotional expressions. They extracted common statistical features from accelerometer data-including mean, standard deviation, maximum, and minimum values-and applied machine learning algorithms such as Random Forest and Logistic Regression for emotion classification.\n\nRGB Camera. In addition to using smartphone sensors, Chiu et al.  [108]  leveraged smartphone cameras to record participants' gait videos and applied OpenPose  [51]  to extract skeletal data. By analyzing Euclidean distance features, angular features, and velocity-based features derived from the skeletons, they successfully performed emotion classification based on gait.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Feat2Net",
      "text": "Approaches. Since 2018, research on gait-based emotion recognition using skeletal data has entered a new phase characterized by the integration of handcrafted features with deep learning techniques.\n\nRandhavane et al.  [109]  employed LSTM networks to extract gait features, followed by emotion classification using traditional machine learning algorithms such as Support Vector Machines (SVM) and Random Forests (RF). Bhatia et al.  [110]  extracted handcrafted features, including joint angles and inter-joint distances, and used LSTM networks for emotion classification. Zhang et al.  [111]  proposed a hierarchical attention-based neural network (MAHANN) for gait emotion recognition. Their framework extracts motion features-such as the relative positions and velocities of 21 joints and walking speed-through a motion sentiment module, and action features-comprising SMO: Sequential Minimal Optimization; LDA: Linear Discriminant Analysis five selected joint angles-via an action sentiment module. These features are then fused using a three-layer fully connected network for final emotion classification.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Featfusionnet",
      "text": "Approaches. Bhattacharya et al.  [58]  further integrated handcrafted features with deep learning models by extracting 29 emotion-related features (e.g., stride length, joint angles) and concatenating them with the final layer of a Spatial-Temporal Graph Convolutional Network (ST-GCN) to enhance classification performance. In another study, they proposed a semi-supervised framework based on an autoencoder, which incorporates hierarchical attention pooling and latent embedding learning to perform emotion recognition  [112] . Sun et al.  [113]  extracted features such as joint angles and acceleration, fused visual information with raw skeleton data, and employed a bidirectional LSTM-based classifier to recognize four emotion categories. Hu et al.  [114]  encoded skeleton joints and affective features into image representations, applied a two-stream CNN to extract features, and used a Transformer-based Complementarity Module (TCM) to capture long-range dependencies by leveraging complementary information between the two streams.\n\nZhang et al.  [115, 116]  utilized a GCN to model skeletal dynamics while concurrently employing a CNN to process handcrafted features; the outputs of both streams were fused at the decision level to enhance classification accuracy. Zhai et al.  [117]  proposed a dual-stream framework combining posture flow and motion flow. The posture flow applied regression constraints based on handcrafted features to embed prior emotional knowledge into the deep model, while the motion flow constructed a high-order velocity-acceleration relational graph to capture emotional intensity. The final classification was achieved by fusing the outputs of both streams.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "End2Endnet",
      "text": "Approaches. With the advancement of deep learning techniques, there has been a gradual shift from traditional feature engineering toward end-to-end learning approaches in gait-based emotion recognition. Instead of relying on manually extracted temporal and frequencydomain features, recent studies focus on directly modeling the mapping between gait and emotional states using architectures such as Transformers and Graph Convolutional Networks (GCNs). Since 2020, end-to-end approaches have become the predominant paradigm for emotion recognition based on gait skeleton data.\n\nCNN-based models. Narayanan et al.  [118]  encoded multi-view skeleton data into pseudoimages by embedding 3D skeleton sequences into 244×244 RGB images. Specifically, the 𝑍 , 𝑌 , and 𝑋 coordinates of skeletal joints at each time step were mapped to the 𝑅, 𝐺, and 𝐵 channels, respectively. A convolutional neural network (CNN), combined with temporal Transformers, was then used to extract cross-modal emotional features.\n\nGCN-based models. Zhuang et al.  [119]  proposed an extended joint connectivity scheme that incorporates a root-node fully connected strategy along with a contraction-denoising module, resulting in a 12.6% improvement in model performance. Lu et al.  [120]  conducted an in-depth investigation into the impact of joint topology on emotion recognition and introduced an optimized joint connectivity design to enhance classification accuracy. Sheng et al.  [61]  developed an Attentionenhanced Spatiotemporal Graph Convolutional Network (AE-STGCN) with an encoder-decoder architecture, enabling simultaneous modeling of spatial dependencies and temporal dynamics. Their framework supports multi-task learning for joint identity recognition and emotion classification. Yin et al.  [121]  introduced the Multi-Scale Adaptive Graph Convolution Network (MSA-GCN) for gait emotion recognition. Their model integrates Adaptive Selective Spatio-Temporal Graph Convolution (ASST-GCN) to dynamically select convolution kernels based on emotional context and applies cross-scale mapping interaction to fuse multiscale information. Chen et al.  [122]  presented the Spatial-Temporal Adaptive Graph Convolutional Network (STA-GCN), which addresses the limitations of conventional models in capturing implicit joint relationships and rigid multi-scale temporal feature aggregation. This is achieved through dedicated spatial and temporal feature learning modules.\n\nTransformer-based models. Zeng et al.  [123]  introduced GaitCycFormer, a Transformer-based framework that incorporates cycle position encoding along with a bi-level architecture consisting of Intra-cycle and Inter-cycle Transformers. This design enables the model to effectively capture both local intra-cycle and global inter-cycle temporal features for gait-based emotion recognition. 4.2.5 Unsupervised Approaches. Due to the limited availability of gait emotion datasets, some researchers have begun exploring unsupervised emotion recognition methods. The primary strategy involves training an encoder to extract emotion-relevant gait features without labeled supervision, followed by evaluating the quality of the learned representations through a series of downstream tasks.\n\nLu et al.  [124]  proposed a self-supervised contrastive learning framework for gait-based emotion recognition, aiming to address the challenges of limited gait diversity and semantic consistency in existing methods. The framework incorporates two core components: (1) Ambiguity Contrastive Learning, which generates ambiguous samples by modifying gait speed and joint angles, and integrates them into the memory bank to enrich semantic diversity; (2) Cross-coordinate Contrastive Learning (𝐶 3 𝐿), which performs contrastive learning between Cartesian and Spherical coordinate systems to leverage complementary representations for improved semantic invariance.\n\nSimilarly, Song et al.  [125]  presented a self-supervised contrastive framework that introduces selective strong augmentation, including techniques such as upper-body jitter and random spatiotemporal masking, to generate diverse positive samples and promote robust feature learning. They further designed a Complementary Feature Fusion Network (CFFN) to integrate topological features from the graph domain (via ST-GCN) with global adaptive features from the image domain (via an adaptive frequency filter), thereby enhancing representational capacity. To ensure distributional consistency between general and strongly augmented samples, a distributional divergence minimization loss is applied.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Evaluation Of Methods On Public Datasets.",
      "text": "To provide a clear and systematic comparison of method performance, we summarize the results of representative models evaluated on two widely   11  and 12 .\n\nOn the E-Gait dataset, GCN-based approaches such as MSA-GCN  [121]  and BPM-GCN  [117]  demonstrated strong performance, with MSA-GCN achieving the highest reported accuracy of 93.51%. CNN-and Transformer-based models, including MAHANN  [111]  and GaitCycFormer  [123] , also performed competitively, indicating the effectiveness of deep learning models in capturing both local and global gait dynamics.\n\nOn the EMOGAIT dataset, recent GCN variants-such as T2A  [115]  and TT-GCN  [116] -achieved over 90% accuracy, further confirming the efficacy of graph-based temporal modeling for emotion recognition from gait.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Methods Comparison And Analysis",
      "text": "Despite differences in data sources and classification algorithms, most traditional methods share common strategies for extracting emotion-related features from skeletal data. Through a detailed investigation of numerous representative studies, we observed that frequently used features include joint velocities, inter-joint angles, and relative distances-parameters that effectively reflect subtle emotional variations in posture and movement. We summarized these commonly adopted features in Table  13 .\n\nWhile deep learning methods are widely regarded as end-to-end solutions, our analysis reveals that many still rely on handcrafted features-particularly in cases where data scarcity or interpretability is a concern. For example, several Feat2Net and FeatFusionNet models incorporate domain-specific features such as motion energy or joint angle dynamics as inputs to neural networks. This hybrid design reflects a transitional stage in the evolution of deep learning approaches toward fully automated representation learning.\n\nOverall, graph-based approaches are playing an increasingly dominant role among deep learning methods, particularly those augmented with temporal attention mechanisms or multi-scale modeling strategies. CNN-and Transformer-based models also demonstrate significant potential, especially when designed to capture fine-grained motion patterns or long-range dependencies. These trends reflect a broader shift toward end-to-end, data-driven frameworks capable of learning expressive and discriminative representations directly from skeletal gait sequences.\n\nMeanwhile, although the method proposed by Lu et al.  [95]  trails slightly in classification accuracy, it introduces a unique advantage: the ability to recognize emotions while simultaneously generating descriptive textual explanations. This LLM-based approach opens a promising direction for future research, enabling emotion recognition systems not only to detect emotional states but also to offer human-interpretable insights that enhance transparency and user trust.\n\n5 Task-Specific Applications",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Depression Detection Using Skeleton Data",
      "text": "Emotion is typically considered a short-term mental state, whereas depression is a long-term and multifaceted psychological condition. Despite these distinctions, the two are closely interconnected  [13] . Individuals experiencing depression often suffer from persistent negative emotions and extended periods of emotional decline. As a result, researchers have sought to extend the insights gained from gait-based emotion recognition to the domain of gait-based depression detection, aiming to explore the potential of gait features as indicators of depressive symptoms.\n\nA research team from the Institute of Psychology, Chinese Academy of Sciences  [126] [127] [128]  analyzed gait characteristics in the frequency domain using methods such as Fast Fourier Transform (FFT) and Hilbert-Huang Transform (HHT), establishing a mapping between gait features and levels of depression. Lu et al.  [129, 130]  investigated gait patterns in individuals with depression and proposed joint energy features that effectively differentiated between depressed patients and healthy controls. Fang et al.  [131] , from the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, extracted 12 time-domain gait features-such as walking speed, stride width, stride length, and vertical head motion-and analyzed their correlations with depressive symptoms.\n\nBuilding on these earlier works, Wang et al.  [132]  integrated time-domain, frequency-domain, and spatial-geometric features to develop a novel gait-based depression assessment algorithm. In addition, Yang et al.  [133]  applied various data augmentation strategies to skeleton sequences to examine how different forms of skeleton data influence depression recognition performance. Shao et al.  [134]  further enhanced model robustness by incorporating video data alongside skeleton sequences and introducing a fusion mechanism between skeletal and gait contour features.\n\nBased on skeletons extracted from video dataset  [135] , Li et al.  [136]  proposes a novel Spatio-Temporal Multi-granularity Network (STM-Net) for skeleton-based depression risk recognition, incorporating a Multi-Grain Temporal Focus (MTF) module and a Multi-Grain Spatial Focus (MSF) module to capture dynamic temporal information and spatial features in gait patterns.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Autism Detection Using Skeleton Data",
      "text": "Autism spectrum disorder (ASD) is a long-term neurodevelopmental condition characterized by difficulties in social communication and the presence of restricted, repetitive behaviors. Emerging studies have suggested that individuals with ASD often exhibit amplified emotional responses and impaired emotional regulation, which may be reflected in distinct physical behaviors compared to neurotypical individuals  [137] . As a result, researchers have begun to investigate the identification of ASD through behavioral cues-particularly gait patterns-as a non-invasive means of understanding and detecting autism-related traits. This line of research holds promise for contributing to early diagnosis and intervention strategies.\n\nStudy  [138]  was the first to introduce a framework for capturing and constructing a 3D gait and full-body motion dataset of children with ASD using the Kinect v2 depth camera, providing valuable resources for behavioral analysis and ASD-related research. Zhang et al.  [139]  employed the OpenPose algorithm to extract initial skeleton data from video recordings. A skeleton distance-matching algorithm was then used for multi-person tracking to associate the skeleton data of multiple ASD children within the same scene. Finally, an LSTM network was applied to classify the denoised skeleton sequences and automatically extract temporal features.\n\nZahan et al.  [140]  utilized a GCN with angle embedding to capture spatiotemporal features from skeletal data and incorporated skeleton image representations (Skepxels) alongside a Vision Transformer (ViT) for auxiliary training. Their findings indicate that children with autism exhibit atypical gait characteristics, such as greater joint angle variability and increased gait asymmetry. Yang et al.  [141]  used HRNet  [52]  to extract skeletal keypoint coordinates and applied the PoTion algorithm to generate joint motion trajectory maps. Their study revealed that children with ASD tend to exhibit broader movement ranges and more irregular motion patterns compared to neurotypical peers.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Abnormal Behavior Detection Using Skeleton Data",
      "text": "Abnormal behavior detection and emotion recognition are two interrelated yet distinct tasks. Emotion recognition aims to identify short-term affective states, whereas abnormal behavior detection focuses on recognizing unusual or unexpected patterns that may signal underlying psychological or behavioral conditions  [142] . Despite this distinction, the two domains are closely connected, as many abnormal behaviors are either driven by or accompanied by emotional changes-for example, heightened anxiety may manifest as restlessness or avoidance behavior.\n\nRNN-based models. In the early stages of research, recurrent neural networks (RNNs) were the predominant technique for addressing these tasks. In  [143] , a monocular fixed-camera depth estimation algorithm was used to convert 2D skeletons into 3D representations. A self-attention-based spatiotemporal convolutional neural network (ST-CNN) was employed to extract local spatiotemporal features, while an attention-enhanced LSTM (ATT-LSTM) focused on key frames to capture global temporal dynamics. Morais et al.  [144]  decomposed human skeletal movements into global body motion and local body posture components, which were modeled using a novel Message-Passing Encoder-Decoder Recurrent Network (MPED-RNN). This model consists of two interacting RNN branches-one for global and one for local features-that exchange information through cross-branch message passing at each time step. The network is trained using both reconstruction and prediction losses.\n\nGCN-based models. Subsequently, Graph Convolutional Networks (GCNs) gradually emerged as the dominant paradigm for skeleton-based abnormal behavior detection. Markovitz et al.  [145]  introduced the Graph Embedded Pose Clustering (GEPC) method for anomaly detection, representing human poses as spatio-temporal graphs. Their framework employs a Spatio-Temporal Graph Convolutional Autoencoder (STGCAE) to embed pose graphs, applies deep embedded clustering to produce soft-assignment vectors, and utilizes a Dirichlet Process Mixture Model (DPMM) to compute normality scores.\n\nLiu et al.  [146]  proposed a Spatial Self-Attention Augmented Graph Convolution (SAA-Graph) module that integrates improved spatiotemporal graph convolutions with Transformer-style selfattention to capture both local and global joint information. Their architecture uses a SAA-STGCAE for feature extraction, followed by deep embedded clustering and DPMM for anomaly scoring.\n\nFlaborea et al.  [147]  introduced COSKAD, a method that encodes skeletal motion using a Space-Time-Separable Graph Convolutional Network (STS-GCN). The model projects skeletal embeddings into multiple latent spaces-Euclidean, spherical, and hyperbolic-and detects anomalies by minimizing distances from a learned central point in each space.\n\nKarami et al.  [148]  presented GiCiSAD, a Graph-Jigsaw Conditioned Diffusion Model for skeletonbased video anomaly detection. The framework consists of three novel modules: (1) a Graph Attention-based Forecasting module to model spatio-temporal dependencies, (2) a Graph-level Jigsaw Puzzle Maker to highlight subtle region-level discrepancies, and (3) a Graph-based Conditional Diffusion model for generating diverse human motion patterns to aid anomaly detection.\n\nPretrained and prompt-guided models. More recently, an increasing number of studies have begun to incorporate pretrained weights and large-scale models to enhance recognition performance in skeleton-based abnormal behavior detection. Sato et al.  [149]  proposed a promptguided zero-shot framework for anomaly action recognition, addressing several limitations of traditional skeleton-based methods-namely, the reliance on target-domain training, sensitivity to skeleton errors, and the scarcity of normal sample annotations. Their approach employs a PointNet-based permutation-invariant extractor to enable sparse feature propagation and improve robustness against skeletal noise. The framework also integrates cosine similarity between skeleton features and text embeddings (derived from user-provided abnormal action prompts) into anomaly scores via contrastive learning, thereby indirectly incorporating knowledge of normal behavior.\n\nLiu et al.  [150]  introduced the Contrastive Language-Skeleton Pre-training framework (Skele-tonCLSP), which leverages large language models to enhance skeleton-based action recognition through three key mechanisms: semantic compensation, cross-modal feature integration, and anomaly correction. This framework bridges the semantic gap between textual and skeletal modalities, enabling more robust and generalizable recognition of abnormal actions.\n\n6 Challenges and Future Research Directions",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Constructing Diversified Datasets",
      "text": "Despite growing interest in recognizing emotional body expressions from 3D skeleton data, the availability of high-quality, large-scale datasets remains limited. Most existing datasets are relatively small in scale and are predominantly annotated using discrete emotion categories (e.g., happiness, anger, sadness), as summarized in Table  4 . Based on this limitation, several key aspects need to be addressed in dataset development:\n\n• Enriching Annotation Schemes: Most existing datasets are annotated with discrete emotion categories (e.g., happiness, anger, sadness), but these may not capture the full complexity of emotional body movements. There is a need to incorporate dimensional emotion models, such as valence-arousal or PAD, for more nuanced emotional representations, especially in applications like abnormal behavior detection and context-aware interventions  [151] . Moreover, dataset development should not only focus on scale and diversity but also adopt richer, multi-modal annotation schemes that integrate both categorical and dimensional labels. Including variables such as age, gender, and cultural background could enhance the generalizability and fairness of recognition systems across diverse populations  [57] . • Expanding Data Collection Scenarios: Most existing public datasets for emotional body expressions are collected in highly controlled laboratory environments, as shown in Table 5. While such settings ensure clean data and consistent annotations, they often lack the robustness in real-world applications. It is crucial to extend data collection efforts from highly controlled laboratory environments to more naturalistic, real-world settings. This includes acquiring 3D skeleton data in diverse environments such as workplaces, public spaces, classrooms, or healthcare settings, where emotional expressions emerge in response to complex social and environmental stimuli  [152] . • Exploring Generative Techniques: Future research should explore generative techniques for data augmentation, such as semi-supervised, unsupervised  [153] , or synthetic data generation  [133] . These approaches can help create richer, more diverse datasets and reduce reliance on large manually labeled datasets while improving model generalizability and scalability.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Improving Model Performance",
      "text": "Based on the analysis of existing skeleton-based emotion recognition methods, there remains significant room for improvement in several aspects of model design.\n\n• Accuracy: Achieving high recognition accuracy remains a core goal. Emotion expressions in body movements can be subtle, ambiguous, or vary across individuals and contexts, making it difficult for models to capture discriminative features reliably. Complex emotions or overlapping expressions further compound the difficulty. • Generalization: Models trained on a specific dataset or population often struggle to generalize to unseen users, diverse cultural backgrounds, or different recording environments. Domain shift-caused by variations in pose quality, motion style, or camera viewpoints-can significantly degrade performance in real-world applications. • Interpretability: Black-box models offer limited insight into why a particular emotion was predicted. Enhancing model interpretability is important for debugging, trust, and downstream applications such as emotional reasoning or ethical AI. Approaches such as attention mechanisms, saliency mapping, or symbolic reasoning may help uncover the decision logic behind model outputs.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Building End-To-End And Efficient Emotion Recognition Frameworks",
      "text": "Real-time emotion recognition is critical for applications such as human-robot interaction, public safety surveillance, and affective computing on edge devices. However, current skeleton-based emotion recognition pipelines often rely on multi-stage processes-either acquiring skeletal data from dedicated sensors or extracting skeletons from RGB videos using pose estimation algorithms  [90, 91] . These intermediate steps not only increase system complexity and latency but also introduce noise and error propagation, ultimately degrading recognition accuracy and computational efficiency.\n\nTo overcome these limitations, future research should explore the development of end-to-end frameworks that map raw input data (e.g., video or depth frames) directly to emotional states, without a heavy dependence on intermediate skeletal representations or extensive preprocessing. Such approaches could significantly reduce computational overhead and enable more streamlined deployment, particularly in resource-constrained environments  [94, 154] .\n\nIn addition, designing lightweight models with strong generalization capabilities and low memory or computational requirements is essential for real-time inference. This necessitates innovations in efficient model architectures, including transformer pruning, knowledge distillation, and edgeoptimized neural operators, to balance performance and deployment feasibility.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Expanding To Multi-Person Emotion Recognition",
      "text": "Most existing methods focus on single-person emotion recognition; however, many real-world applications-such as public safety, education, and social robotics-demand the ability to understand group-level emotions  [155] .\n\nRecognizing collective affect requires modeling not only individual emotional states but also interpersonal dynamics and group context. This introduces a range of challenges, including occlusions in crowded scenes, variations in individual expressiveness, and the lack of dedicated multi-person skeleton-based emotion datasets.\n\nFuture research should investigate relational modeling approaches, such as graph or hypergraph representations, to capture both individual cues and group-level interactions. Such methods could enable the development of more socially aware, context-sensitive, and robust emotion recognition systems suitable for real-world, multi-agent environments.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Expanding To Multimodal Emotion Recognition",
      "text": "While 3D skeleton data captures rich information about body movements, emotional expressions are inherently multimodal, encompassing facial expressions, vocal cues, physiological signals, and even brain activity. Relying solely on skeletal data may constrain recognition accuracy, particularly for subtle or ambiguous emotional states.\n\nIntegrating additional modalities-such as audio, video, or EEG-can provide complementary cues and enhance system robustness  [156, 157] . For instance, combining body gestures with speech prosody or facial expressions can substantially improve emotion recognition in naturalistic settings  [158] . EEG signals, which reflect internal affective processes, are especially valuable in scenarios where external emotional expressions are weak or intentionally suppressed  [159] .\n\nHowever, multimodal fusion introduces several key challenges:\n\n• Modality alignment and synchronization, particularly when input signals have different temporal or spatial resolutions. • Increased model complexity and computational demands, which may hinder real-time deployment. • Limited availability of large-scale, well-annotated multimodal emotion datasets, impeding training and evaluation.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Leveraging Large Models For Skeleton-Based Emotion Recognition",
      "text": "Recent advances in large-scale models-such as ChatGPT  [160] , LLaVa  [161] , and Gemini  [162] -have opened new avenues for skeleton-based emotion recognition. However, directly applying these models to 3D skeleton sequences remains challenging due to the modality's sparse, temporal, and structured nature. Unlike text or images, skeleton data requires careful preprocessing-such as transformation into token-like representations or pseudo-images-to align with the input formats expected by large-scale models  [95] .\n\nTo better harness the capabilities of these models, future research could explore unified architectures that support not only emotion recognition but also understanding and generation. In particular, such models could be designed to:\n\n• Interpret: the underlying causes of emotional expressions by incorporating contextual and situational information. • Generate: emotionally expressive body movements conditioned on target emotion labels or predefined scenarios. • Reason: about the temporal evolution of emotional states in interactive or multi-agent environments.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Conclusion",
      "text": "This survey presents a comprehensive review of recent advances in emotion recognition based on 3D skeletal data, encompassing both posture-based and gait-based approaches. By examining data acquisition methods, publicly available datasets, and a range of technical strategies-from traditional handcrafted feature extraction to deep learning architectures and large-scale pretrained models-we offer a unified perspective on the evolution of this rapidly developing field. Compared to facial expression or voice-based methods, skeleton-based emotion recognition offers distinct advantages, including robustness to environmental variations and improved privacy protection. These characteristics make it particularly well-suited for real-world applications such as healthcare monitoring, human-computer interaction, and public safety.\n\nDespite considerable progress, several key challenges remain. These include the need for more diverse and ecologically valid datasets, improved generalization across users and environments, and enhanced interpretability of deep learning models. In addition, integrating multimodal signals (e.g., voice, facial expression, and physiological data) and leveraging large-scale pretrained models present promising avenues for future research.\n\nWe anticipate that the next wave of research will focus on developing unified, end-to-end frameworks that are lightweight, explainable, and capable of adapting to dynamic, multimodal environments. Such systems will be critical for enabling robust, scalable, and human-centered affective computing in real-world scenarios.",
      "page_start": 28,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall structure of our survey",
      "page": 3
    },
    {
      "caption": "Figure 1: Sec. 2 introduces",
      "page": 3
    },
    {
      "caption": "Figure 2: (a)), maps emotions onto the valence–arousal plane and has been widely applied",
      "page": 4
    },
    {
      "caption": "Figure 2: (b)), extends this framework",
      "page": 4
    },
    {
      "caption": "Figure 2: Various emotion models",
      "page": 5
    },
    {
      "caption": "Figure 2: (c)) [36], which defines complex emotions as",
      "page": 5
    },
    {
      "caption": "Figure 3: Fig. 3.(a) depicts the paradigm for",
      "page": 7
    },
    {
      "caption": "Figure 3: (b) illustrates the gait data",
      "page": 7
    },
    {
      "caption": "Figure 4: Initially, wearable-based systems such as optical motion capture systems",
      "page": 7
    },
    {
      "caption": "Figure 4: (a)) were widely used in controlled laboratory environments. Subsequently, inertial motion",
      "page": 7
    },
    {
      "caption": "Figure 3: Comparison of different data collection scenarios",
      "page": 8
    },
    {
      "caption": "Figure 4: Common methods for capturing 3D human skeleton data",
      "page": 8
    },
    {
      "caption": "Figure 4: (b)) emerged, utilizing wearable sensors such as Inertial Measurement Units",
      "page": 8
    },
    {
      "caption": "Figure 4: (c)) enable real-time skeleton tracking by capturing the 3D structural information",
      "page": 8
    },
    {
      "caption": "Figure 4: (d)) support image-based skeleton estimation through deep",
      "page": 8
    },
    {
      "caption": "Figure 5: Comparison of different technical approaches",
      "page": 13
    },
    {
      "caption": "Figure 5: (a)), which involve extracting handcrafted affective",
      "page": 13
    },
    {
      "caption": "Figure 5: (b)), which extract handcrafted features from skeleton",
      "page": 13
    },
    {
      "caption": "Figure 5: (c)), which extract handcrafted features and inte-",
      "page": 13
    },
    {
      "caption": "Figure 5: (d)), which directly learn body movement representa-",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Survey"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "HAIFENG LU, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "JIUYI CHEN, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "ZHEN ZHANG, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "RUIDA LIU, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "RUNHAO ZENG∗, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "XIPING HU∗, Shenzhen MSU-BIT University, China"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Emotion recognition through body movements has emerged as a compelling and privacy-preserving alternative"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "to traditional methods that rely on facial expressions or physiological signals. Recent advancements in 3D"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "skeleton acquisition technologies and pose estimation algorithms have significantly enhanced the feasibility of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "emotion recognition based on full-body motion. This survey provides a comprehensive and systematic review"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "of skeleton-based emotion recognition techniques. First, we introduce psychological models of emotion and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "examine the relationship between bodily movements and emotional expression. Next, we summarize publicly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "available datasets, highlighting the differences in data acquisition methods and emotion labeling strategies. We"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "then categorize existing methods into posture-based and gait-based approaches, analyzing them from both data-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "driven and technical perspectives. In particular, we propose a unified taxonomy that encompasses four primary"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "technical paradigms: Traditional approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "within each category are reviewed and compared, with benchmarking results across commonly used datasets."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Finally, we explore the extended applications of emotion recognition in mental health assessment, such as"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "detecting depression and autism, and discuss the open challenges and future research directions in this rapidly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "evolving field."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "CCS Concepts: • Do Not Use This Code → Generate the Correct Terms for Your Paper; Generate the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Paper."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Additional Key Words and Phrases: Emotion Recognition, Skeleton, Posture, Gait"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "ACM Reference Format:"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Haifeng Lu, Jiuyi Chen, Zhen Zhang, Ruida Liu, Runhao Zeng, and Xiping Hu. 2025. Emotion Recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "from Skeleton Data: A Comprehensive Survey. J. ACM 37, 4, Article 111 (August 2025), 34 pages. https:"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "//doi.org/XXXXXXX.XXXXXXX"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "∗Runhao Zeng and Xiping Hu are corresponding authors."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Authors’ Contact Information: Haifeng Lu,\nluhf18@lzu.edu.cn, Shenzhen MSU-BIT University, Shenzhen, Guangdong,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "China; Jiuyi Chen,\nftchenjiuyi@mail.scut.edu.cn, Shenzhen MSU-BIT University, Shenzhen, Guangdong, China; Zhen"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Zhang, zhangzhen19@lzu.edu.cn, Shenzhen MSU-BIT University, Shenzhen, Guangdong, China; Ruida Liu, soduku645@"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "gmail.com, Shenzhen MSU-BIT University, Shenzhen, Guangdong, China; Runhao Zeng, zengrh@smbu.edu.cn, Shenzhen"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "MSU-BIT University, Shenzhen, Guangdong, China; Xiping Hu, huxp@bit.edu.cn, Shenzhen MSU-BIT University, Shenzhen,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Guangdong, China."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "prior specific permission and/or a fee. Request permissions from permissions@acm.org."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "ACM 1557-735X/2025/8-ART111"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive": "https://doi.org/XXXXXXX.XXXXXXX"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. A comparison of related survey": "Year"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "2013"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "2018"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "2019"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "2023"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "2023"
        },
        {
          "Table 1. A comparison of related survey": ""
        },
        {
          "Table 1. A comparison of related survey": "-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Models of Emotion (Sec.2.1)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Models of Emotion",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "and Body Movement (Sec.2)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Emotional Body Expressions (Sec.2.2)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Data Collection Paradigm (Sec.3.1)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Body Movement Capture Methodology (Sec.3.2)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Posture-based Datasets (Sec.3.3)\nData (Sec.3)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Gait-based Datasets (Sec.3.4)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Dataset Comparison and Analysis (Sec.3.5)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Posture-based Emotion Recognition (Sec.4.1)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Body Movement Based",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Emotion Recognition (Sec.4)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Gait-based Emotion Recognition (Sec.4.2)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Methods Comparison and Analysis (Sec.4.3)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Depression Detection Using Skeleton",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Data (Sec.5.1)",
          "111:3": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Autism Detection Using Skeleton\nTask-Specific Applications",
          "111:3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:4\nLu et al.": "2\nModels of Emotion and Body Movement"
        },
        {
          "111:4\nLu et al.": "This section provides a comprehensive overview of the major emotion models discussed in Sec."
        },
        {
          "111:4\nLu et al.": "2.1,\nincluding discrete, dimensional, and componential models. The relationship between emo-"
        },
        {
          "111:4\nLu et al.": "tional states and posture-based expressions is reviewed in Sec. 2.2.1, while gait-based emotional"
        },
        {
          "111:4\nLu et al.": "expressions are explored in Sec. 2.2.2."
        },
        {
          "111:4\nLu et al.": "2.1\nModels of Emotion"
        },
        {
          "111:4\nLu et al.": "From a psychological perspective, emotions are stimulus-driven responses characterized by distinct"
        },
        {
          "111:4\nLu et al.": "physiological changes [23], and are commonly classified as reactional, hormonal, or automatic"
        },
        {
          "111:4\nLu et al.": "processes [24]. The modeling of affect has long been a subject of scholarly debate, with three"
        },
        {
          "111:4\nLu et al.": "prominent approaches emerging: discrete, dimensional, and componential models [25]."
        },
        {
          "111:4\nLu et al.": "2.1.1\nDiscrete Emotions Theory. The classification of emotions into distinct, easily recognizable cat-"
        },
        {
          "111:4\nLu et al.": "egories has long been a foundational approach in emotion research. A widely accepted perspective,"
        },
        {
          "111:4\nLu et al.": "heavily influenced by the work of Paul Ekman [26], posits the existence of a set of universal primary"
        },
        {
          "111:4\nLu et al.": "emotions—typically happiness, sadness, fear, anger, disgust, and surprise—that are biologically"
        },
        {
          "111:4\nLu et al.": "hardwired and universally recognized across cultures. This discrete-state view has gained signifi-"
        },
        {
          "111:4\nLu et al.": "cant traction in affective computing due to its conceptual simplicity and its claim of cross-cultural"
        },
        {
          "111:4\nLu et al.": "applicability."
        },
        {
          "111:4\nLu et al.": "However, the discrete emotions theory has faced increasing debate. While the discrete emotions"
        },
        {
          "111:4\nLu et al.": "theory emphasize emotional consistency across cultures, numerous studies have demonstrated that"
        },
        {
          "111:4\nLu et al.": "emotional perception and expression are significantly influenced by factors such as age, gender, and"
        },
        {
          "111:4\nLu et al.": "cultural or linguistic context [9, 27, 28]. These factors are critical for experimental design and the"
        },
        {
          "111:4\nLu et al.": "accurate interpretation of findings across diverse populations, and have been extensively discussed"
        },
        {
          "111:4\nLu et al.": "in the literature."
        },
        {
          "111:4\nLu et al.": "2.1.2\nMultidimensional Emotions Theory. Another widely adopted approach to emotion modeling is"
        },
        {
          "111:4\nLu et al.": "the dimensional model, which represents emotions as points within a continuous, multidimensional"
        },
        {
          "111:4\nLu et al.": "space [29, 30]. Among the most commonly used dimensions are valence (the degree of pleasantness"
        },
        {
          "111:4\nLu et al.": "or unpleasantness) and arousal (the level of activation or intensity). This framework acknowledges"
        },
        {
          "111:4\nLu et al.": "the complexity of emotional experiences and enables more nuanced analysis by capturing subtle"
        },
        {
          "111:4\nLu et al.": "variations across emotional states."
        },
        {
          "111:4\nLu et al.": "The 2D VA model (Valence–Arousal), most notably represented by Russell’s Circumplex Model"
        },
        {
          "111:4\nLu et al.": "[31] (see Fig. 2.(a)), maps emotions onto the valence–arousal plane and has been widely applied"
        },
        {
          "111:4\nLu et al.": "across disciplines such as psychology, marketing, and education. Similarly, the 3D PAD model"
        },
        {
          "111:4\nLu et al.": "(Pleasure, Arousal, Dominance), proposed by Mehrabian [32] (see Fig. 2.(b)), extends this framework"
        },
        {
          "111:4\nLu et al.": "by incorporating dominance as a third axis. These continuous models offer a rich descriptive space"
        },
        {
          "111:4\nLu et al.": "for representing emotions and have been shown to relate intuitively to physiological signals,"
        },
        {
          "111:4\nLu et al.": "particularly those associated with valence and arousal [9, 28]. Notably, studies analyzing body"
        },
        {
          "111:4\nLu et al.": "movements in relation to affect suggest that the arousal dimension accounts for the greatest variance"
        },
        {
          "111:4\nLu et al.": "in movement-related emotional expression [33–35]."
        },
        {
          "111:4\nLu et al.": "However, despite their theoretical richness, dimensional models present challenges for auto-"
        },
        {
          "111:4\nLu et al.": "matic emotion recognition systems, as mapping subtle and continuous affective states to discrete,"
        },
        {
          "111:4\nLu et al.": "observable body expressions remains a non-trivial task."
        },
        {
          "111:4\nLu et al.": "2.1.3\nComponential Emotions Theory. Componential models occupy a middle ground between"
        },
        {
          "111:4\nLu et al.": "categorical and dimensional approaches in terms of descriptive capacity. These models organize"
        },
        {
          "111:4\nLu et al.": "emotions hierarchically, positing that complex emotions arise from combinations of more basic ones."
        },
        {
          "111:4\nLu et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2025."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "\u00009\u0000$\u0000/\u0000(\u00001\u0000&\u0000("
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "",
          "Posture Characteristics": "Open and dynamic posture with moving arms and legs, feet oriented toward"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "objects or people of interest. Often accompanied by spontaneous, non-goal-"
        },
        {
          "Emotion": "Happiness",
          "Posture Characteristics": "directed actions such as jumping, clapping, stamping, or rhythmic body"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "movements. The body is typically held upright and may shake during"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "intense laughter."
        },
        {
          "Emotion": "",
          "Posture Characteristics": "Expansive posture with hands on hips or waist, clenched fists, palm-down"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "or pointing gestures. Frequently includes trembling or shaking of hands or"
        },
        {
          "Emotion": "Anger",
          "Posture Characteristics": "body, squared shoulders, erect head, and expanded chest. The stance is firm,"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "with rigid or suspended arms. Aggressive actions such as pacing, striking"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "objects, or making frantic, purposeless gestures may occur."
        },
        {
          "Emotion": "",
          "Posture Characteristics": "Contracted posture with bowed shoulders, forward-leaning trunk, and a"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "lowered or motionless head. Hand movements are slow or minimal, often"
        },
        {
          "Emotion": "Sadness",
          "Posture Characteristics": ""
        },
        {
          "Emotion": "",
          "Posture Characteristics": "involving self-touching of the face, head, or neck. Overall body language is"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "passive and withdrawn, indicating emotional withdrawal."
        },
        {
          "Emotion": "",
          "Posture Characteristics": "Postures characterized by crossed limbs and visible muscular tension, such"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "as clenched hands and inward-drawn elbows. Movements may be restrained,"
        },
        {
          "Emotion": "Fear",
          "Posture Characteristics": ""
        },
        {
          "Emotion": "",
          "Posture Characteristics": "sudden, or bouncy. The head is typically lowered between the shoulders, or"
        },
        {
          "Emotion": "",
          "Posture Characteristics": "the body may adopt a crouched, motionless stance."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "",
          "Gait Characteristics": "Increased gait speed and cadence, greater step length, enhanced arm swing,"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "noticeable vertical head movement, elevated thigh angles, and overall"
        },
        {
          "Emotion": "Happiness",
          "Gait Characteristics": ""
        },
        {
          "Emotion": "",
          "Gait Characteristics": "smoother motion patterns. The torso often appears more expansive or"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "relaxed, frequently accompanied by a bouncy walking rhythm."
        },
        {
          "Emotion": "",
          "Gait Characteristics": "Faster gait with higher cadence, longer stride length, pronounced thigh"
        },
        {
          "Emotion": "Anger",
          "Gait Characteristics": "elevation, and vigorous arm swing. Inter-segment coordination is smoother"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "but footfalls are heavier, conveying forceful intent."
        },
        {
          "Emotion": "",
          "Gait Characteristics": "Reduced walking speed and cadence, shorter strides, diminished arm swing,"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "and limited upper-body involvement. Joint excursions—pelvic rotation,"
        },
        {
          "Emotion": "Sadness",
          "Gait Characteristics": ""
        },
        {
          "Emotion": "",
          "Gait Characteristics": "hip and shoulder flexion—are minimal. Posture tends to be slouched, with"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "head flexion and chest contraction."
        },
        {
          "Emotion": "",
          "Gait Characteristics": "Elevated cadence with shorter step length and reduced overall speed. Pos-"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "ture is tense and contracted, limbs show greater flexion, and motions can"
        },
        {
          "Emotion": "Fear",
          "Gait Characteristics": ""
        },
        {
          "Emotion": "",
          "Gait Characteristics": "be sharp or jittery. Walk often appears cautious, with increased thigh lift"
        },
        {
          "Emotion": "",
          "Gait Characteristics": "and guarded foot placement."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:8\nLu et al.": "(a) Scene Settings for posture data col-\n(b) Scene Settings for gait data collection"
        },
        {
          "111:8\nLu et al.": "lection"
        },
        {
          "111:8\nLu et al.": "Fig. 3. Comparison of different data collection scenarios"
        },
        {
          "111:8\nLu et al.": "Vision-based systems\nWearable-based systems"
        },
        {
          "111:8\nLu et al.": "(a). Optical motion \n(b). Inertial motion \n(c). Depth-sensing \n(d). RGB camera"
        },
        {
          "111:8\nLu et al.": "capture system\ncapture system\ncamera"
        },
        {
          "111:8\nLu et al.": "Home-based\nLab-based"
        },
        {
          "111:8\nLu et al.": "Fig. 4. Common methods for capturing 3D human skeleton data"
        },
        {
          "111:8\nLu et al.": "capture systems (Fig. 4.(b)) emerged, utilizing wearable sensors such as Inertial Measurement Units"
        },
        {
          "111:8\nLu et al.": "(IMUs) to record body movements.\nWearable-based systems"
        },
        {
          "111:8\nLu et al.": "With advancements in sensor technology and computer vision, vision-based systems have gained"
        },
        {
          "111:8\nLu et al.": "increasing popularity due to their convenience and non-invasiveness. For example, depth-sensing"
        },
        {
          "111:8\nLu et al.": "cameras (Fig. 4.(c)) enable real-time skeleton tracking by capturing the 3D structural information"
        },
        {
          "111:8\nLu et al.": "of the body, while RGB cameras (Fig. 4.(d)) support image-based skeleton estimation through deep"
        },
        {
          "111:8\nLu et al.": "learning algorithms."
        },
        {
          "111:8\nLu et al.": "In the following sections, we provide a detailed overview of the characteristics and capabilities"
        },
        {
          "111:8\nLu et al.": "(a)\n(b)\nof each of these approaches."
        },
        {
          "111:8\nLu et al.": "In 2006, Ma et al. [43] employed an optical motion capture\n3.2.1\nOptical Motion Capture System."
        },
        {
          "111:8\nLu et al.": "system to record three types of daily activities, annotating them with corresponding emotional"
        },
        {
          "111:8\nLu et al.": "labels. The DMCD dataset [44] was collected using a similar system to capture the movements"
        },
        {
          "111:8\nLu et al.": "of dancers. This system relies on reflective markers—typically spherical and coated with retro-"
        },
        {
          "111:8\nLu et al.": "reflective material—affixed to key anatomical landmarks on the subject’s body. Multiple infrared"
        },
        {
          "111:8\nLu et al.": "cameras positioned around the capture space emit light and detect the signals reflected from the"
        },
        {
          "111:8\nLu et al.": "markers. By triangulating the marker positions from different camera angles,\nthe system can"
        },
        {
          "111:8\nLu et al.": "accurately reconstruct the 3D trajectories of joints. This method offers high spatial precision and is"
        },
        {
          "111:8\nLu et al.": "widely used in controlled laboratory environments for detailed motion analysis."
        },
        {
          "111:8\nLu et al.": "3.2.2\nInertial Motion Capture System. The Emilya dataset [45], KDAE [46], and MEBED dataset"
        },
        {
          "111:8\nLu et al.": "[47] utilize inertial motion capture systems for data collection. These systems employ wearable"
        },
        {
          "111:8\nLu et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2025."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "IMU sensors, which are typically attached to major body segments such as the limbs, torso, and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "head. Each IMU contains accelerometers, gyroscopes, and, in some cases, magnetometers, enabling"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "the measurement of linear acceleration, angular velocity, and orientation. By integrating these"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "signals, the system can reconstruct the 3D motion of the human skeleton without the need for"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "external cameras.\nIMU-based systems are portable and well-suited for capturing movement in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "unconstrained or real-world environments."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Additionally, Cui et al. [48] utilized the built-in triaxial accelerometers in smartphones to monitor"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "participants’ daily activities and classify their emotional states. In this study, two smartphones and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "one tablet were used: the smartphones were worn on the participant’s wrist and ankle to capture"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "raw data from the accelerometer and gravity sensors, sampled at a rate of 5 Hz."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "3.2.3\nDepth-sensing Camera. The release of the Kinect depth camera in 2011 [15] significantly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "simplified the process of skeleton sequence acquisition. Since then, many researchers have adopted"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "the Kinect series for data collection [49, 50]. Microsoft Kinect\nis a depth-sensing camera that"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "integrates an RGB camera, an infrared (IR) emitter, and an IR depth sensor to capture both color and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "depth information. By projecting a structured infrared light pattern and analyzing its deformation,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Kinect constructs a depth map of the scene. Using this depth data, built-in body tracking algorithms"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "can identify human figures and estimate the 3D positions of joints in real time."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "In 2017, breakthroughs in human pose estimation—particularly the develop-\n3.2.4\nRGB Camera."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "ment of real-time multi-person pose estimation methods such as OpenPose [51], HRNet [52], and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Vnect [53]—further advanced emotion recognition based on body movements. These techniques"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "enabled the extraction of 2D or 3D skeletal joint coordinates directly from standard RGB video"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "frames, eliminating the need for wearable sensors or depth cameras. The underlying algorithms"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "typically employ Convolutional Neural Networks (CNNs) to detect body parts and estimate joint"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "positions, followed by part affinity fields to associate joints with specific individuals in multi-person"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "scenes. By 2019, researchers had begun leveraging skeleton sequences extracted from RGB videos"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "for emotion recognition tasks [18, 54, 55]."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "3.3\nPosture-based Datasets"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Table 4 summarizes datasets commonly used in skeleton-based emotion recognition research. Their"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "distinctive features are discussed in detail in the following section."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Emilya Dataset. The EMotional body expression In daILY Actions dataset (Emilya) [45, 62]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "comprises a total of 8,206 samples. Eleven actors (six females and five males) performed eight distinct"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "emotions within the context of seven everyday actions. These actions were carefully selected to"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "involve full-body movements, particularly emphasizing the upper body and arm motions, in order"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "to ensure diverse emotional expressions. The seven daily activities include sitting down, walking,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "walking while carrying an object, moving books across a table with both hands, knocking on a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "door, lifting an object, and throwing an object."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "In the walking tasks, actors were instructed to walk back and forth along the longer side of a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "room, with two distinct variations: normal walking and walking while carrying an object. Each"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "sample was labeled according to one of eight emotional categories: Neutral, Joy, Anger, Panic, Fear,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "Anxiety, Sadness, and Shame."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "All data were captured using the Xsens MVN motion capture system, which records 3D skeleton"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "sequences at a frame rate of 120 Hz. Each skeleton sequence consists of 28 joints."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "BML Dataset. The BML Dataset [43] comprises 4,080 body movement sequences. Thirty actors"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "(15 females and 15 males) performed four distinct actions—walking, knocking, lifting, and throw-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:9": "ing—each interleaved with walking segments, under four different emotional states: angry, happy,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "Dataset"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "Emilya [45]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "BML [43]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "MEBED [47]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "KDAE [46]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "EGBM [56]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "UCLIC [57]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "DMCD [44]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "E-Gait I [58, 59]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "E-Gait II [58, 59]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "BME I [60]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "BME II [60]"
        },
        {
          "Table 4. Overview of posture-based and gait-based datasets for skeleton-based emotion recognition": "EMOGAIT [61]"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "EGBM Dataset. The Emotional Gestures and Body Movements Corpus (EGBM) [56] consists of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "560 samples. The dataset features performances by 16 professional Polish actors (8 females and 8"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "males), who expressed seven distinct emotions—happiness, sadness, neutral, anger, disgust, fear,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "and surprise—without any predefined instructions or cues."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "All motion data were captured using a Kinect V2 camera at a frame rate of 30 Hz. Each emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "category contains 80 samples, and each pose sequence provides the 3D positions of 25 joints."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "UCLIC Dataset. The UCLIC dataset [57] comprises 108 samples. It features performances from"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "13 actors, including 11 Japanese, 1 Sri Lankan, and 1 American. The actors expressed four basic"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "emotions—anger,\nfear, happiness, and sadness—freely and based on their own interpretations,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "without any imposed constraints."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "All motion data were collected using a MoCap system, with each pose sequence providing the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "3D positions of 32 joints."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "DMCD Dataset. The Dance Motion Capture Database (DMCD) [44] consists of 108 samples."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "The dataset features performances by six female dancers with diverse backgrounds in gymnastics,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "ballet, theatrical dance, and other styles. Each participant performed a dance sequence, with each"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "sequence associated with a specific emotional state."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "In total, 12 emotions were expressed: excited, happy, pleased, satisfied, relaxed, tired, bored, sad,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "miserable, annoyed, angry, and afraid. All motion sequences were captured using the PhaseSpace"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "Impulse X2 MoCap System at a frame rate of 120 Hz. Each pose sequence contains the 3D positions"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "of 38 joints."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "3.4\nGait-based Datasets"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "E-Gait Dataset. The Emotion-Gait (E-Gait) dataset [58] comprises 2,177 real-world gait sequences,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "each annotated with one of four emotion categories: happy, sad, neutral, or angry. The dataset is"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "divided into two subsets:"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:11": "• Subset A: Contains 342 gait sequences sourced from datasets including BML [43], ICT [63],"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "Dataset"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "Emilya [45]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "BML [43]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "MEBED [47]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "KDAE [46]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "EGBM [56]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "UCLIC [57]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": "EMOGAIT [61]"
        },
        {
          "Table 5. Comparison of emotion induction methods and performance instructions across different datasets": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "categories based on their technical design:"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "• Traditional approaches (see Fig. 5.(a)), which involve extracting handcrafted affective"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "features (e.g., joint angles, body symmetry, velocity) from skeleton data, followed by machine"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "learning classifiers for emotion recognition;"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "• Feat2Net approaches (see Fig. 5.(b)), which extract handcrafted features from skeleton"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "sequences and employ neural networks solely for classification;"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "• FeatFusionNet approaches (see Fig. 5.(c)), which extract handcrafted features and inte-"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "grate them into the training process of deep learning models to enhance discriminative"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "performance;"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "• End2EndNet approaches (see Fig. 5.(d)), which directly learn body movement representa-"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "tions and perform emotion recognition from raw skeleton data without relying on manually"
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "designed features."
        },
        {
          "(in Sec. 4.2) approaches. Within each group, methods are further classified into four representative": "The following subsections are structured based on this taxonomy, offering a detailed analysis of"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Feature"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Mean/SD of position, velocity, acceleration"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Max hand"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "distance/speed/acceleration/jerk"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Power, Fluidity, Speed,"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Quantity/Regularity, Body openness,"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Leaning, Straightness"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "110 expressive body cues (multi-level"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "notation)"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "114 expressive body cues (multi-level"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "notation)"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "114 cues; 80 position; 80 kinetic-energy"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "feats"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "68 low-level geometric, motion, Fourier"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "feats"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Spectral amplitude differences (neutral vs"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "expressive)"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Posture, Temporal, Residue features"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Hand–spine dist., max acc., various joint"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "angles"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "Holistic (kinetic, contraction,"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": "symmetry. . . ) and local features"
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        },
        {
          "Table 6. Emotion recognition from posture using machine learning methods": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "the latter provided a more detailed analysis of the relative importance of individual features for"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "distinguishing emotional categories. Further extending the MLBNS framework, the authors later"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "introduced four additional feature groups—temporal patterns, slope-based descriptors, peak dynam-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "ics, global motion statistics, and temporal regularity measures—which led to notable improvements"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "in classification accuracy [70]."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Another influential line of research has been led by Crenn et al., who systematically investigated"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "emotion recognition through detailed analysis of skeletal motion features. In their early work,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Crenn et al. [71] proposed a comprehensive feature extraction framework that integrates geometric"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "relationships, motion correlations, and frequency-based periodicity. The extracted features included"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "inter-joint distances, triangle areas formed by specific joints,\njoint angles, and the velocity and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "acceleration of various joints. Experimental results showed that\nthese features were strongly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "correlated with emotional expression. Building on this foundation, Crenn et al. [72] introduced"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "the concept of a neutral pose, suggesting that emotional states could be inferred by analyzing the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "residual differences between observed skeletal data and this neutral reference. This concept was"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "further refined in a 2020 study [73], where a cost function optimization strategy was employed to"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "more effectively synthesize neutral motion, thereby improving emotion recognition performance."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "In subsequent research, the motion-emotion feature framework proposed by Crenn has been widely"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "adopted and extended by others, highlighting its lasting impact on the field."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Emotion recognition based on full-body movement data in gaming scenarios has emerged as"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "a significant and increasingly influential research direction. Kleinsmith et al. [78] investigated"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "the recognition of non-basic emotional states within video game contexts. They collected posture"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "data from players engaged in Nintendo Wii sports games, established ground-truth labels through"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "human observer evaluations, and developed automatic recognition models based on the captured"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "movements. Expanding on this idea, Savva et al. [79] explored whether players’ full-body movements"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "in gaming environments could reflect their aesthetic and emotional experiences. Using similar"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Nintendo sports games, their study demonstrated that the automatic system achieved an emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "recognition accuracy comparable to that of human raters. Further advancing this line of research,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Garber et al. [80] collected motion capture data during gameplay and extracted features such as"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "body symmetry, head displacement, and posture openness to infer players’ emotional states."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "Depth Sensors. Saha et al. [74] conducted research on gesture-based emotion recognition using"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "a Kinect sensor. They selected eleven joint coordinates from the upper body and hands to extract"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "nine features related to joint distances, accelerations, and angles, which were used to recognize five"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "basic emotions: anger, fear, happiness, sadness, and relaxation. The results showed that an ensemble"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "decision tree achieved the highest average classification accuracy of 90.83%. Piana et al. [75]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "processed motion capture data to extract features such as joint energy, movement direction, posture,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "and body symmetry. They constructed adaptive representations through dictionary learning and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "employed a linear support vector machine for emotion classification. Ahmed et al. [81] computed a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "comprehensive set of body movement features categorized into ten groups. In the first stage, they"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "applied analysis of variance (ANOVA) and multivariate analysis of variance (MANOVA) to remove"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "irrelevant features and distribute the remaining ones across groups. In the second stage, a binary"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "chromosome–based genetic algorithm was used to select an optimal feature subset for maximizing"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "emotion recognition performance. Finally, score- and rank-level fusion techniques were applied to"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "further enhance classification accuracy."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "RGB Camera. Glowinski et al. [82] proposed a method for automatic emotion recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "by analyzing upper-body gestures, specifically head and hand movements. Using 40 emotional"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "clips—representing anger,\njoy, relief, and sadness—performed by professional actors from the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "GEMEP database [83], the researchers captured video recordings with a dual-camera setup and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:15": "extracted dynamic features such as motion energy (sum of velocity magnitudes) and spatial extent"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:16\nLu et al.": "(perimeter of the enclosing triangle). In a follow-up study, Glowinski et al. [84] extracted additional"
        },
        {
          "111:16\nLu et al.": "kinematic features from head and hand trajectories, including energy, spatial range, smoothness, and"
        },
        {
          "111:16\nLu et al.": "symmetry. They applied Principal Component Analysis (PCA) to reduce the feature dimensionality"
        },
        {
          "111:16\nLu et al.": "to a four-dimensional representation, which effectively grouped emotions along the axes of valence"
        },
        {
          "111:16\nLu et al.": "(positive vs. negative) and arousal (high vs. low)."
        },
        {
          "111:16\nLu et al.": "In [85], a multi-branch deep learning architecture is proposed, consist-\n4.1.2\nFeat2Net Approaches."
        },
        {
          "111:16\nLu et al.": "ing of a local branch based on stacked Long Short-Term Memory (LSTM) units and a global branch"
        },
        {
          "111:16\nLu et al.": "based on a Multi-Layer Perceptron (MLP). The local branch processes temporal\nlocal\nfeatures"
        },
        {
          "111:16\nLu et al.": "through stacked LSTM layers to achieve higher levels of abstraction and capture fine-grained action"
        },
        {
          "111:16\nLu et al.": "details. In parallel, the global branch utilizes a MLP to process temporal global features and extract"
        },
        {
          "111:16\nLu et al.": "salient patterns. The outputs from both branches are concatenated and passed through a fully"
        },
        {
          "111:16\nLu et al.": "connected layer followed by a softmax function for emotion classification."
        },
        {
          "111:16\nLu et al.": "In a more recent study, Oğuz et al. [86] investigated a wide range of time-domain, frequency-"
        },
        {
          "111:16\nLu et al.": "domain, and statistical\nfeatures, applying feature selection techniques to identify four salient"
        },
        {
          "111:16\nLu et al.": "features per frame. These selected features were then aggregated into a feature matrix used for"
        },
        {
          "111:16\nLu et al.": "subsequent emotion recognition."
        },
        {
          "111:16\nLu et al.": "Wang et al.[14] proposed a multi-scale feature selection algorithm grounded in a pseudo-energy"
        },
        {
          "111:16\nLu et al.": "model and introduced a multi-scale spatiotemporal network to decode the complex relationship"
        },
        {
          "111:16\nLu et al.": "between emotional states and full-body movements. This approach effectively captures both"
        },
        {
          "111:16\nLu et al.": "long-term postural variations and short-term dynamics, thereby enhancing emotion recognition"
        },
        {
          "111:16\nLu et al.": "performance. Further extending this line of research, Wang et al.[87] explored joint energy features"
        },
        {
          "111:16\nLu et al.": "by constructing a body expression energy model and designing a multi-input symmetric positive"
        },
        {
          "111:16\nLu et al.": "definite matrix network. This framework facilitates the extraction of interpretable spatiotemporal"
        },
        {
          "111:16\nLu et al.": "features, contributing to both the robustness and interpretability of emotion classification."
        },
        {
          "111:16\nLu et al.": "In recent years, with the remarkable success of neural networks in\n4.1.3\nEnd2EndNet Approaches."
        },
        {
          "111:16\nLu et al.": "the broader field of artificial intelligence, deep learning models have become the dominant paradigm"
        },
        {
          "111:16\nLu et al.": "for emotion recognition from 3D skeletal motion sequences. In this section, we summarize existing"
        },
        {
          "111:16\nLu et al.": "methods based on differences in their network architectures."
        },
        {
          "111:16\nLu et al.": "RNN-based models. Sapiński et al.[88] collected body movement data from professional actors"
        },
        {
          "111:16\nLu et al.": "using Kinect V2 and employed LSTM networks to classify emotions based on skeletal sequences."
        },
        {
          "111:16\nLu et al.": "Zhang et al.[19] proposed an Attention-based Stacked LSTM (AS-LSTM) model for emotion recogni-"
        },
        {
          "111:16\nLu et al.": "tion from whole-body movements in virtual reality (VR) environments. By integrating an attention"
        },
        {
          "111:16\nLu et al.": "mechanism into the traditional LSTM framework, the model assigns varying weights to joint point"
        },
        {
          "111:16\nLu et al.": "sequences across motion frames, enabling it to focus on key joints while suppressing redundant"
        },
        {
          "111:16\nLu et al.": "information. This enhancement improves both the learning capacity of the network and the overall"
        },
        {
          "111:16\nLu et al.": "recognition accuracy."
        },
        {
          "111:16\nLu et al.": "CNN-based models. Karumuri et al.[89] were among the first to propose encoding skeletal"
        },
        {
          "111:16\nLu et al.": "information into image representations. They converted 3D joint coordinates into 8-bit RGB images"
        },
        {
          "111:16\nLu et al.": "using four encoding schemes: coarse position, fine position, logical position, and logical velocity."
        },
        {
          "111:16\nLu et al.": "Two convolutional neural network (CNN) architectures were then designed for training—Single-"
        },
        {
          "111:16\nLu et al.": "Input Architecture (SIA-CNN) and Multi-Input Architecture (MIA-CNN). Cui et al.[90] drew on"
        },
        {
          "111:16\nLu et al.": "findings from body language literature to associate specific postures with emotions. They employed"
        },
        {
          "111:16\nLu et al.": "the Convolutional Pose Machine (CPM) algorithm to extract human keypoint coordinates, generated"
        },
        {
          "111:16\nLu et al.": "simplified line diagrams via clustering, and used a CNN combined with a Softmax layer for emotion"
        },
        {
          "111:16\nLu et al.": "classification. Beyan et al. [20] proposed a dual-branch CNN architecture that processes coarse-"
        },
        {
          "111:16\nLu et al.": "grained (e.g., 4s) and fine-grained (e.g., 1s) temporal features in parallel, leveraging logical position"
        },
        {
          "111:16\nLu et al.": "image representations and data augmentation techniques to improve classification performance."
        },
        {
          "111:16\nLu et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2025."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 10: summarizes a selection of",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "GCN-based models. Shen et al.[91] employed a Temporal Segment Network (TSN) to extract"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "RGB features and used ST-GCN for skeletal features. These features were unified through a residual"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "encoder and classified via a residual fully connected network. Ghaleb et al.[18] developed an emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "recognition framework based on Spatial-Temporal Graph Convolutional Networks (ST-GCN),"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "enhanced with a spatial attention mechanism to highlight the relationship between joint spatial"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "patterns and emotional states. Building on the ST-GCN architecture, Shi et al.[92] introduced a self-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "attention mechanism that dynamically adjusts the skeletal connectivity structure, demonstrating"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "the effectiveness of adaptive topologies for capturing emotional cues. Shirian et al.[93] proposed"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "the Learnable Graph Inception Network (L-GrIN), which incorporates non-linear spectral graph"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "convolution, a graph inception layer, learnable adjacency, and a learnable pooling function. The"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "model\njointly learns the graph structure and performs emotion classification by optimizing a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "composite loss function that combines classification and graph structure losses."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "4.1.4\nPre-training Approaches. With the rapid advancement of large-scale models, researchers have"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "increasingly recognized the value of pretrained models in providing transferable prior knowledge"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "across diverse domains, thereby significantly enhancing task performance. Consequently, there"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "has been growing interest in incorporating pretraining strategies into skeleton-based emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "recognition."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "Paiva et al. [94] adopted a two-stage approach: self-supervised pretraining on large-scale unla-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "beled skeleton datasets (MPOSE2021 and Panoptic Studio) using a Masked Autoencoder (MAE) to"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "learn spatiotemporal dependencies, followed by fine-tuning on the BoLD dataset using an MLP"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "classifier. In [95], the authors proposed the Emotion-Action Interpreter based on Large Language"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "Models (EAI-LLM). This model first extracts skeleton features using a Graph Convolutional Net-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "work (GCN), and then maps these features into a semantic space through components such as"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "a multi-granularity skeletal annotator and a unified skeletal labeling module. By leveraging the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "reasoning capabilities of large language models, EAI-LLM not only performs emotion recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "but also generates interpretable textual explanations."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "4.1.5\nEvaluation of Methods on Public Datasets. To facilitate a clear comparison of different ap-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "proaches, we summarize the performance of representative methods on three widely used public"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "datasets—EGBM, KDAE, and Emilya—as shown in Tables 7–9."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "On the EGBM dataset, early RNN-based methods achieved moderate accuracy (69% and 74%,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "respectively). In contrast, more recent approaches that incorporate handcrafted features with fully"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "connected networks (FC), such as those proposed by Wang et al. [14, 87], report significantly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "higher performance, exceeding 95%. A similar pattern is observed on the KDAE dataset. While"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "GCN-based models [18] yielded modest results (65%), methods combining handcrafted features"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "with shallow classifiers or neural networks demonstrated strong performance. Notably, Oğuz et al."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "[86] achieved an impressive 99.99% accuracy under a hold-out protocol. On the Emilya dataset, both"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "traditional and deep learning models performed well; CNN-based methods [20] and handcrafted"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "feature–driven fully connected networks [14] both surpassed 94% accuracy."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "Overall, while end-to-end models generally yield superior performance, handcrafted feature–based"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "approaches remain highly competitive—particularly in scenarios with limited data availability."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "4.2\nGait-based Emotion Recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "4.2.1\nTraditional Approaches. Machine learning–based methods have consistently served as a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "cornerstone in the field of emotion recognition from gait. Table 10 summarizes a selection of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "representative studies in recent years that\nfocus on automatic emotion recognition based on"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "human gait. To facilitate a clearer understanding and comparison of gait-based emotion recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:17": "approaches, this section is organized according to the data acquisition methods used for capturing"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7. Method comparison on EGBM dataset": "Backbone"
        },
        {
          "Table 7. Method comparison on EGBM dataset": "RNN"
        },
        {
          "Table 7. Method comparison on EGBM dataset": "RNN"
        },
        {
          "Table 7. Method comparison on EGBM dataset": "Manual features+FC 10 - fold"
        },
        {
          "Table 7. Method comparison on EGBM dataset": "Manual features+FC 10 - fold"
        },
        {
          "Table 7. Method comparison on EGBM dataset": "LLMs"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 8. Method comparison on KDAE dataset": "Backbone"
        },
        {
          "Table 8. Method comparison on KDAE dataset": ""
        },
        {
          "Table 8. Method comparison on KDAE dataset": "Manual features+FC"
        },
        {
          "Table 8. Method comparison on KDAE dataset": "Manual features+FC"
        },
        {
          "Table 8. Method comparison on KDAE dataset": "Manual features+NN Hold-out"
        },
        {
          "Table 8. Method comparison on KDAE dataset": "LLMs"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "al. [102] transformed joint position and velocity data into covariance matrices, which were then"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "mapped onto the non-linear Riemannian manifold of Symmetric Positive Definite (SPD) matrices."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Emotion classification was performed by computing geodesic distances and geometric means on"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "the manifold."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Depth sensors. In 2010, the release of Microsoft’s depth-sensing camera introduced a novel tech-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "nological approach for gait data acquisition and analysis. Researchers from the Chinese Academy"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "of Sciences designed an experiment in which participants viewed emotion-inducing video clips,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "followed by walking sessions recorded using the Kinect v2 sensor [49, 103]. A Fourier transform was"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "applied to extract 168 frequency-domain features to differentiate among happy, sad, and neutral"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "emotional states [49]. Building on this work, Li et al. [103] further incorporated time-domain"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "features such as stride length and gait cycle duration, resulting in a significant improvement in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "classification performance."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "In a related study [104], geometric and kinematic features were extracted from human gait data."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "These included body-related features (e.g., head inclination angle, joint flexion angle), effort-related"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "features (e.g., kinetic energy, average joint velocity), shape-related metrics (e.g., density index), and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "spatial descriptors (e.g., body contraction index, symmetry). A total of 17 features were extracted,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "processed through vector quantization and normalization. A binary chromosome–based genetic"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "algorithm was then used to select optimal feature subsets for four expert models, thereby enhancing"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "the emotion recognition performance of each model."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Wearable devices. With the widespread adoption of smartphones and wearable devices, the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "built-in sensors and cameras of these technologies have provided more accessible and convenient"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "means for gait data collection. Zhang et al. [105] and Cui et al. [48] employed a custom-designed"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "smart wristband to capture 3D acceleration data from various joints, including the right wrist and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "ankle. They computed time-domain features such as skewness, kurtosis, and standard deviation,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "and further extracted both time- and frequency-domain features using power spectral density and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Fast Fourier transform (FFT) to classify three emotional states."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Similarly, Quiroz et al. [106, 107] had participants wear smartwatches while walking along a 250-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "meter S-shaped corridor. Their study focused on inferring individuals’ emotional states from sensor"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "data collected via smartwatches, examining the relationship between motion sensor–captured gait"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "patterns and emotional expressions. They extracted common statistical features from accelerometer"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "data—including mean, standard deviation, maximum, and minimum values—and applied machine"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "learning algorithms such as Random Forest and Logistic Regression for emotion classification."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "RGB Camera. In addition to using smartphone sensors, Chiu et al.[108] leveraged smartphone"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "cameras to record participants’ gait videos and applied OpenPose[51] to extract skeletal data. By"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "analyzing Euclidean distance features, angular features, and velocity-based features derived from"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "the skeletons, they successfully performed emotion classification based on gait."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "4.2.2\nFeat2Net Approaches. Since 2018, research on gait-based emotion recognition using skeletal"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "data has entered a new phase characterized by the integration of handcrafted features with deep"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "learning techniques."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Randhavane et al. [109] employed LSTM networks to extract gait features, followed by emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "classification using traditional machine learning algorithms such as Support Vector Machines (SVM)"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "and Random Forests (RF). Bhatia et al. [110] extracted handcrafted features, including joint angles"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "and inter-joint distances, and used LSTM networks for emotion classification. Zhang et al. [111]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "proposed a hierarchical attention-based neural network (MAHANN) for gait emotion recognition."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "Their framework extracts motion features—such as the relative positions and velocities of 21"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:19": "joints and walking speed—through a motion sentiment module, and action features—comprising"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": "1300"
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        },
        {
          "Table 10. Emotion recognition from gait using deep learning methods": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "CNN-based models. Narayanan et al. [118] encoded multi-view skeleton data into pseudo-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "images by embedding 3D skeleton sequences into 244×244 RGB images. Specifically, the 𝑍 , 𝑌 ,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "and 𝑋 coordinates of skeletal joints at each time step were mapped to the 𝑅, 𝐺, and 𝐵 channels,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "respectively. A convolutional neural network (CNN), combined with temporal Transformers, was"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "then used to extract cross-modal emotional features."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "GCN-based models. Zhuang et al. [119] proposed an extended joint connectivity scheme that"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "incorporates a root-node fully connected strategy along with a contraction-denoising module,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "resulting in a 12.6% improvement in model performance. Lu et al. [120] conducted an in-depth"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "investigation into the impact of joint topology on emotion recognition and introduced an optimized"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "joint connectivity design to enhance classification accuracy. Sheng et al. [61] developed an Attention-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "enhanced Spatiotemporal Graph Convolutional Network (AE-STGCN) with an encoder-decoder"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "architecture, enabling simultaneous modeling of spatial dependencies and temporal dynamics. Their"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "framework supports multi-task learning for joint identity recognition and emotion classification."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Yin et al. [121] introduced the Multi-Scale Adaptive Graph Convolution Network (MSA-GCN)"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "for gait emotion recognition. Their model integrates Adaptive Selective Spatio-Temporal Graph"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Convolution (ASST-GCN) to dynamically select convolution kernels based on emotional context and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "applies cross-scale mapping interaction to fuse multiscale information. Chen et al. [122] presented"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "the Spatial-Temporal Adaptive Graph Convolutional Network (STA-GCN), which addresses the"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "limitations of conventional models in capturing implicit joint relationships and rigid multi-scale"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "temporal feature aggregation. This is achieved through dedicated spatial and temporal feature"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "learning modules."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Transformer-based models. Zeng et al. [123] introduced GaitCycFormer, a Transformer-based"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "framework that incorporates cycle position encoding along with a bi-level architecture consisting"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "of Intra-cycle and Inter-cycle Transformers. This design enables the model to effectively capture"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "both local intra-cycle and global inter-cycle temporal features for gait-based emotion recognition."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "4.2.5\nUnsupervised Approaches. Due to the limited availability of gait emotion datasets, some"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "researchers have begun exploring unsupervised emotion recognition methods. The primary strategy"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "involves training an encoder to extract emotion-relevant gait features without labeled supervision,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "followed by evaluating the quality of the learned representations through a series of downstream"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "tasks."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Lu et al. [124] proposed a self-supervised contrastive learning framework for gait-based emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "recognition, aiming to address the challenges of limited gait diversity and semantic consistency in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "existing methods. The framework incorporates two core components: (1) Ambiguity Contrastive"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Learning, which generates ambiguous samples by modifying gait speed and joint angles, and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "integrates them into the memory bank to enrich semantic diversity; (2) Cross-coordinate Contrastive"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Learning (𝐶3𝐿), which performs contrastive learning between Cartesian and Spherical coordinate"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "systems to leverage complementary representations for improved semantic invariance."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "Similarly, Song et al. [125] presented a self-supervised contrastive framework that introduces"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "selective strong augmentation, including techniques such as upper-body jitter and random spa-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "tiotemporal masking, to generate diverse positive samples and promote robust feature learning."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "They further designed a Complementary Feature Fusion Network (CFFN) to integrate topological"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "features from the graph domain (via ST-GCN) with global adaptive features from the image domain"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "(via an adaptive frequency filter), thereby enhancing representational capacity. To ensure distribu-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "tional consistency between general and strongly augmented samples, a distributional divergence"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "minimization loss is applied."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "4.2.6\nEvaluation of Methods on Public Datasets. To provide a clear and systematic comparison of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:21": "method performance, we summarize the results of representative models evaluated on two widely"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 11. Method comparison on E-Gait dataset": "Backbone"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "RNN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "GCN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "CNN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "CNN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "CNN"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "Transfromer"
        },
        {
          "Table 11. Method comparison on E-Gait dataset": "Gaitcycformer [123] Graph-Transformer Hold-out"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 13. Common posture-emotion feature list": "Detailed Description"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left shoulder–neck–right shoulder, left shoulder–neck–left elbow,"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left hip–waist–left knee∗, left shoulder–left elbow–left hand∗"
        },
        {
          "Table 13. Common posture-emotion feature list": "left hip–left knee–left foot∗, Head–neck–torso"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left hand–torso∗, left hand–left shoulder∗, left hand–left hip∗"
        },
        {
          "Table 13. Common posture-emotion feature list": ""
        },
        {
          "Table 13. Common posture-emotion feature list": "Left hand–neck∗, left elbow–torso∗, left foot–right foot"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left hand–neck–right hand, left shoulder–neck–right shoulder"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left hand–hip–right hand, left elbow–neck–right elbow"
        },
        {
          "Table 13. Common posture-emotion feature list": "Left foot–waist–right foot, left knee–neck–right knee"
        },
        {
          "Table 13. Common posture-emotion feature list": "Head, hands, shoulders, knees, feet"
        },
        {
          "Table 13. Common posture-emotion feature list": ""
        },
        {
          "Table 13. Common posture-emotion feature list": "Acceleration Features Head, hands, shoulders, knees, feet"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "Temporal Multi-granularity Network (STM-Net) for skeleton-based depression risk recognition,",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "5.2",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "diagnosis and intervention strategies.",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "5.3",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "heightened anxiety may manifest as restlessness or avoidance behavior.",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "",
          "Lu et al.": ""
        },
        {
          "111:24": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2025.",
          "Lu et al.": ""
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "spatiotemporal convolutional neural network (ST-CNN) was employed to extract local spatiotem-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "poral features, while an attention-enhanced LSTM (ATT-LSTM) focused on key frames to capture"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "global temporal dynamics. Morais et al. [144] decomposed human skeletal movements into global"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "body motion and local body posture components, which were modeled using a novel Message-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Passing Encoder-Decoder Recurrent Network (MPED-RNN). This model consists of two interacting"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "RNN branches—one for global and one for local\nfeatures—that exchange information through"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "cross-branch message passing at each time step. The network is trained using both reconstruction"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "and prediction losses."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "GCN-based models. Subsequently, Graph Convolutional Networks (GCNs) gradually emerged"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "as the dominant paradigm for skeleton-based abnormal behavior detection. Markovitz et al. [145]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "introduced the Graph Embedded Pose Clustering (GEPC) method for anomaly detection, represent-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "ing human poses as spatio-temporal graphs. Their framework employs a Spatio-Temporal Graph"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Convolutional Autoencoder (STGCAE) to embed pose graphs, applies deep embedded clustering"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "to produce soft-assignment vectors, and utilizes a Dirichlet Process Mixture Model (DPMM) to"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "compute normality scores."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Liu et al. [146] proposed a Spatial Self-Attention Augmented Graph Convolution (SAA-Graph)"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "module that integrates improved spatiotemporal graph convolutions with Transformer-style self-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "attention to capture both local and global joint information. Their architecture uses a SAA-STGCAE"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "for feature extraction, followed by deep embedded clustering and DPMM for anomaly scoring."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Flaborea et al. [147] introduced COSKAD, a method that encodes skeletal motion using a Space-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Time-Separable Graph Convolutional Network (STS-GCN). The model projects skeletal embeddings"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "into multiple latent spaces—Euclidean, spherical, and hyperbolic—and detects anomalies by mini-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "mizing distances from a learned central point in each space."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Karami et al. [148] presented GiCiSAD, a Graph-Jigsaw Conditioned Diffusion Model for skeleton-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "based video anomaly detection. The framework consists of three novel modules: (1) a Graph Atten-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "tion–based Forecasting module to model spatio-temporal dependencies, (2) a Graph-level Jigsaw"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Puzzle Maker to highlight subtle region-level discrepancies, and (3) a Graph-based Conditional"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Diffusion model for generating diverse human motion patterns to aid anomaly detection."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Pretrained and prompt-guided models. More recently, an increasing number of studies"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "have begun to incorporate pretrained weights and large-scale models to enhance recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "performance in skeleton-based abnormal behavior detection. Sato et al. [149] proposed a prompt-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "guided zero-shot framework for anomaly action recognition, addressing several\nlimitations of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "traditional skeleton-based methods—namely, the reliance on target-domain training, sensitivity"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "to skeleton errors, and the scarcity of normal sample annotations. Their approach employs a"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "PointNet-based permutation-invariant extractor to enable sparse feature propagation and improve"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "robustness against skeletal noise. The framework also integrates cosine similarity between skeleton"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "features and text embeddings (derived from user-provided abnormal action prompts) into anomaly"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "scores via contrastive learning, thereby indirectly incorporating knowledge of normal behavior."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "Liu et al. [150] introduced the Contrastive Language-Skeleton Pre-training framework (Skele-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "tonCLSP), which leverages large language models to enhance skeleton-based action recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "through three key mechanisms: semantic compensation, cross-modal feature integration, and anom-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "aly correction. This framework bridges the semantic gap between textual and skeletal modalities,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:25": "enabling more robust and generalizable recognition of abnormal actions."
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:26": "6\nChallenges and Future Research Directions",
          "Lu et al.": ""
        },
        {
          "111:26": "6.1\nConstructing Diversified Datasets",
          "Lu et al.": ""
        },
        {
          "111:26": "Despite growing interest in recognizing emotional body expressions from 3D skeleton data, the",
          "Lu et al.": ""
        },
        {
          "111:26": "availability of high-quality, large-scale datasets remains limited. Most existing datasets are relatively",
          "Lu et al.": ""
        },
        {
          "111:26": "",
          "Lu et al.": "small in scale and are predominantly annotated using discrete emotion categories (e.g., happiness,"
        },
        {
          "111:26": "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be",
          "Lu et al.": ""
        },
        {
          "111:26": "addressed in dataset development:",
          "Lu et al.": ""
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": "addressed in dataset development:"
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": "generalizability and fairness of recognition systems across diverse populations [57]."
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": "ble 5. While such settings ensure clean data and consistent annotations,"
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": "to complex social and environmental stimuli [152]."
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": ""
        },
        {
          "anger, sadness), as summarized in Table 4. Based on this limitation, several key aspects need to be": "on large manually labeled datasets while improving model generalizability and scalability."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "6.3\nBuilding End-to-End and Efficient Emotion Recognition Frameworks",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Real-time emotion recognition is critical for applications such as human–robot interaction, public",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:27": "safety surveillance, and affective computing on edge devices. However, current skeleton-based emo-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "tion recognition pipelines often rely on multi-stage processes—either acquiring skeletal data from",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "dedicated sensors or extracting skeletons from RGB videos using pose estimation algorithms [90, 91].",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "These intermediate steps not only increase system complexity and latency but also introduce noise",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "and error propagation, ultimately degrading recognition accuracy and computational efficiency.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "To overcome these limitations, future research should explore the development of end-to-end",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "frameworks that map raw input data (e.g., video or depth frames) directly to emotional states,",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "without a heavy dependence on intermediate skeletal representations or extensive preprocessing.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Such approaches could significantly reduce computational overhead and enable more streamlined",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "deployment, particularly in resource-constrained environments [94, 154].",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "In addition, designing lightweight models with strong generalization capabilities and low memory",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "or computational requirements is essential for real-time inference. This necessitates innovations",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:27": "in efficient model architectures, including transformer pruning, knowledge distillation, and edge-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "optimized neural operators, to balance performance and deployment feasibility.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "6.4\nExpanding to Multi-person Emotion Recognition",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Most existing methods focus on single-person emotion recognition; however, many real-world",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "applications—such as public safety, education, and social robotics—demand the ability to understand",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "group-level emotions [155].",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Recognizing collective affect requires modeling not only individual emotional states but also",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "",
          "111:27": "interpersonal dynamics and group context. This introduces a range of challenges, including oc-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "clusions in crowded scenes, variations in individual expressiveness, and the lack of dedicated",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "multi-person skeleton-based emotion datasets.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Future research should investigate relational modeling approaches, such as graph or hypergraph",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "representations, to capture both individual cues and group-level interactions. Such methods could",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "enable the development of more socially aware, context-sensitive, and robust emotion recognition",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "systems suitable for real-world, multi-agent environments.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "6.5\nExpanding to Multimodal Emotion Recognition",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "While 3D skeleton data captures rich information about body movements, emotional expressions",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "are inherently multimodal, encompassing facial expressions, vocal cues, physiological signals, and",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "even brain activity. Relying solely on skeletal data may constrain recognition accuracy, particularly",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "for subtle or ambiguous emotional states.",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "Integrating additional modalities—such as audio, video, or EEG—can provide complementary",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "cues and enhance system robustness [156, 157]. For instance, combining body gestures with",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "speech prosody or facial expressions can substantially improve emotion recognition in naturalistic",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "settings [158]. EEG signals, which reflect internal affective processes, are especially valuable in",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "scenarios where external emotional expressions are weak or intentionally suppressed [159].",
          "111:27": ""
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey": "However, multimodal fusion introduces several key challenges:",
          "111:27": ""
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:28\nLu et al.": "6.6\nLeveraging Large Models for Skeleton-Based Emotion Recognition"
        },
        {
          "111:28\nLu et al.": "Recent advances in large-scale models—such as ChatGPT [160], LLaVa [161], and Gemini [162]—have"
        },
        {
          "111:28\nLu et al.": "opened new avenues for skeleton-based emotion recognition. However, directly applying these"
        },
        {
          "111:28\nLu et al.": "models to 3D skeleton sequences remains challenging due to the modality’s sparse, temporal, and"
        },
        {
          "111:28\nLu et al.": "structured nature. Unlike text or images, skeleton data requires careful preprocessing—such as"
        },
        {
          "111:28\nLu et al.": "transformation into token-like representations or pseudo-images—to align with the input formats"
        },
        {
          "111:28\nLu et al.": "expected by large-scale models [95]."
        },
        {
          "111:28\nLu et al.": "To better harness the capabilities of these models, future research could explore unified archi-"
        },
        {
          "111:28\nLu et al.": "tectures that support not only emotion recognition but also understanding and generation.\nIn"
        },
        {
          "111:28\nLu et al.": "particular, such models could be designed to:"
        },
        {
          "111:28\nLu et al.": "• Interpret: the underlying causes of emotional expressions by incorporating contextual and"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "affective computing in real-world scenarios."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "References"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "[1] A. Kołakowska, A. Landowska, M. Szwoch, W. Szwoch, and M. R. Wrobel, “Emotion recognition and its applications,”"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "Human-Computer Systems Interaction: Backgrounds and Applications 3, pp. 51–62, 2014."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "MIT press, 2000.\n[2] R. W. Picard, Affective computing."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "[3]\nJ. M. Garcia-Garcia, V. M. Penichet, and M. D. Lozano, “Emotion detection: a technology review,” in Proceedings of the"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "XVIII international conference on human computer interaction, 2017, pp. 1–8."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "[4]\nS. Li and W. Deng, “Deep facial expression recognition: A survey,” IEEE Transactions on Affective Computing, vol. 13,"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "no. 3, pp. 1195–1215, 2022."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "[5] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and T. Alhussain, “Speech emotion recognition using deep"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "learning techniques: A review,” IEEE access, vol. 7, pp. 117 327–117 345, 2019."
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "[6] N. Alswaidan and M. E. B. Menai, “A survey of state-of-the-art approaches for emotion recognition in text,” Knowledge"
        },
        {
          "environments. Such systems will be critical for enabling robust, scalable, and human-centered": "and Information Systems, vol. 62, no. 8, pp. 2937–2987, 2020."
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[7] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang, “A review of emotion recognition using physiological"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "signals,” Sensors, vol. 18, no. 7, p. 2074, 2018."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[8] A. Dzedzickis, A. Kaklauskas, and V. Bucinskas, “Human emotion recognition: Review of sensors and methods,”"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Sensors, vol. 20, no. 3, 2020. [Online]. Available: https://www.mdpi.com/1424-8220/20/3/592"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[9] A. Kleinsmith and N. Bianchi-Berthouze, “Affective body expression perception and recognition: A survey,” IEEE"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Transactions on Affective Computing, vol. 4, no. 1, pp. 15–33, 2012."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[10]\nF. Noroozi, C. A. Corneanu, D. Kamińska, T. Sapiński, S. Escalera, and G. Anbarjafari, “Survey on emotional body"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "gesture recognition,” IEEE transactions on affective computing, vol. 12, no. 2, pp. 505–523, 2018."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[11]\nJ. M. Montepare, S. B. Goldstein, and A. Clausen, “The identification of emotions from gait information,” Journal of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Nonverbal Behavior, vol. 11, no. 1, pp. 33–42, 1987."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[12]\nJ. Michalak, N. F. Troje, J. Fischer, P. Vollmar, T. Heidenreich, and D. Schulte, “Embodiment of sadness and depres-"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "sion—gait patterns associated with dysphoric mood,” Psychosomatic medicine, vol. 71, no. 5, pp. 580–587, 2009."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "F. Deligianni, Y. Guo, and G.-Z. Yang, “From emotions to mood disorders: A survey on gait analysis methodology,”\n[13]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "IEEE Journal of Biomedical and Health Informatics, vol. 23, no. 6, pp. 2302–2316, 2019."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[14] T. Wang, S. Liu, F. He, W. Dai, M. Du, Y. Ke, and D. Ming, “Emotion recognition from full-body motion using multiscale"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "spatio-temporal network,” IEEE Transactions on Affective Computing, vol. 15, no. 3, pp. 898–912, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[15] Z. Zhang, “Microsoft kinect sensor and its effect,” IEEE multimedia, vol. 19, no. 2, pp. 4–10, 2012."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[16] Q. Peng, C. Zheng, and C. Chen, “A dual-augmentor framework for domain generalization in 3d human pose"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "estimation,” in CVPR, 2024, pp. 2240–2249."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[17] C. Zheng, W. Wu, C. Chen, T. Yang, S. Zhu, J. Shen, N. Kehtarnavaz, and M. Shah, “Deep learning-based human pose"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "estimation: A survey,” ACM Computing Surveys, 2023."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[18] E. Ghaleb, A. Mertens, S. Asteriadis, and G. Weiss, “Skeleton-based explainable bodily expressed emotion recognition"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "through graph convolutional networks,” in FG, 2021, pp. 1–8."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[19] H. Zhang, P. Yi, R. Liu, and D. Zhou, “Emotion recognition from body movements with as-lstm,” in 2021 IEEE 7th"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "International Conference on Virtual Reality, 2021, pp. 26–32."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[20] C. Beyan, S. Karumuri, G. Volpe, A. Camurri, and R. Niewiadomski, “Modeling multiple temporal scales of full-body"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "movements for emotion classification,” IEEE Transactions on Affective Computing, vol. 14, no. 2, pp. 1070–1081, 2023."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[21]\nS. Xu, J. Fang, X. Hu, E. Ngai, Y. Guo, V. Leung, J. Cheng, and B. Hu, “Emotion recognition from gait analyses: Current"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "research and future directions,” arXiv preprint arXiv:2003.11461, 2020."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[22] M.-A. Mahfoudi, A. Meyer, T. Gaudin, A. Buendia, and S. Bouakaz, “Emotion expression in human body posture and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "movement: A survey on intelligible motion factors, quantification and validation,” IEEE Transactions on Affective"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Computing, vol. 14, no. 4, pp. 2697–2721, 2023."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[23] T. S. Rached and A. Perkusich, “Emotion recognition based on brain-computer interface systems,” Brain-computer"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "interface systems-Recent progress and future prospects, pp. 253–270, 2013."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[24] T. Dalgleish, “The emotional brain,” Nature Reviews Neuroscience, vol. 5, no. 7, pp. 583–589, 2004."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[25] A. Kołakowska, A. Landowska, M. Szwoch, W. Szwoch, and M. R. Wróbel, “Modeling emotions for affect-aware"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "applications,” Information Systems Development and Applications, pp. 55–69, 2015."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[26] P. Ekman, “Universals and cultural differences in facial expressions of emotion.” in Nebraska symposium on motivation."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "University of Nebraska Press, 1971."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[27] H. Gunes, B. Schuller, M. Pantic, and R. Cowie, “Emotion representation, analysis and synthesis in continuous space: A"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "IEEE, 2011, pp. 827–834.\nsurvey,” in 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[28] B. Stephens-Fripp, F. Naghdy, D. Stirling, and G. Naghdy, “Automatic affect perception based on body gait and posture:"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "A survey,” International Journal of Social Robotics, vol. 9, pp. 617–641, 2017."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[29]\nJ. A. Russell and A. Mehrabian, “Evidence for a three-factor theory of emotions,” Journal of research in Personality,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "vol. 11, no. 3, pp. 273–294, 1977."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[30] M. K. Greenwald, E. W. Cook, and P. J. Lang, “Affective judgment and psychophysiological response: dimensional"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "covariation in the evaluation of pictorial stimuli.” Journal of psychophysiology, 1989."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[31] G. F. Wilson and C. A. Russell, “Real-time assessment of mental workload using psychophysiological measures and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "artificial neural networks,” Human factors, vol. 45, no. 4, pp. 635–644, 2003."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[32] A. Mehrabian, “Pleasure-arousal-dominance: A general framework for describing and measuring individual differences"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "in temperament,” Current Psychology, vol. 14, pp. 261–292, 1996."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[33] G. E. Kang and M. M. Gross, “The effect of emotion on movement smoothness during gait in healthy young adults,”"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Journal of biomechanics, vol. 49, no. 16, pp. 4022–4027, 2016."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[34] ——, “Emotional influences on sit-to-walk in healthy young adults,” Human movement science, vol. 40, pp. 341–351,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "2015."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "[35] A. Barliya, L. Omlor, M. A. Giese, A. Berthoz, and T. Flash, “Expression of emotion in the kinematics of locomotion,”"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:29": "Experimental brain research, vol. 225, pp. 159–176, 2013."
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:30": "",
          "Lu et al.": "[36] R. Plutchik, “The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their"
        },
        {
          "111:30": "",
          "Lu et al.": "complexity and provide tools for clinical practice,” American scientist, vol. 89, no. 4, pp. 344–350, 2001."
        },
        {
          "111:30": "",
          "Lu et al.": "[37] H. G. Wallbott, “Bodily expression of emotion,” European journal of social psychology, vol. 28, no. 6, pp. 879–896, 1998."
        },
        {
          "111:30": "[38]",
          "Lu et al.": "S. Dahl and A. Friberg, “Visual perception of expressiveness in musicians’ body movements,” Music Perception, vol. 24,"
        },
        {
          "111:30": "",
          "Lu et al.": "no. 5, pp. 433–454, 2007."
        },
        {
          "111:30": "",
          "Lu et al.": "[39] M. M. Gross, E. A. Crane, and B. L. Fredrickson, “Methodology for assessing bodily expression of emotion,” Journal of"
        },
        {
          "111:30": "",
          "Lu et al.": "Nonverbal Behavior, vol. 34, pp. 223–248, 2010."
        },
        {
          "111:30": "",
          "Lu et al.": "[40] N. Dael, M. Mortillaro, and K. R. Scherer, “Emotion expression in body action and posture.” Emotion, vol. 12, no. 5, p."
        },
        {
          "111:30": "",
          "Lu et al.": "1085, 2012."
        },
        {
          "111:30": "",
          "Lu et al.": "[41] K. H. Kim, S. W. Bang, and S. R. Kim, “Emotion recognition system using short-term monitoring of physiological"
        },
        {
          "111:30": "",
          "Lu et al.": "signals,” Medical and biological engineering and computing, vol. 42, pp. 419–427, 2004."
        },
        {
          "111:30": "",
          "Lu et al.": "[42] M. M. Gross, E. A. Crane, and B. L. Fredrickson, “Effort-shape and kinematic assessment of bodily expression of"
        },
        {
          "111:30": "",
          "Lu et al.": "emotion during gait,” Human movement science, vol. 31, no. 1, pp. 202–221, 2012."
        },
        {
          "111:30": "",
          "Lu et al.": "[43] Y. Ma, H. M. Paterson, and F. E. Pollick, “A motion capture library for the study of identity, gender, and emotion"
        },
        {
          "111:30": "",
          "Lu et al.": "perception from biological motion,” Behavior research methods, vol. 38, no. 1, pp. 134–141, 2006."
        },
        {
          "111:30": "[44]",
          "Lu et al.": "“DanceMotion Capture Database (DMCD),” http://dancedb.eu/, 2021, [Online; accessed April 16, 2025]."
        },
        {
          "111:30": "",
          "Lu et al.": "[45] N. Fourati and C. Pelachaud, “Perception of emotions and body movement in the emilya database,” IEEE Transactions"
        },
        {
          "111:30": "",
          "Lu et al.": "on Affective Computing, vol. 9, no. 1, pp. 90–101, 2016."
        },
        {
          "111:30": "",
          "Lu et al.": "[46] M. Zhang, L. Yu, K. Zhang, B. Du, B. Zhan, S. Chen, X. Jiang, S. Guo, J. Zhao, Y. Wang et al., “Kinematic dataset of"
        },
        {
          "111:30": "",
          "Lu et al.": "actors expressing emotions,” Scientific data, vol. 7, no. 1, p. 292, 2020."
        },
        {
          "111:30": "",
          "Lu et al.": "[47] E. Volkova, S. De La Rosa, H. H. Bülthoff, and B. Mohler, “The mpi emotional body expressions database for narrative"
        },
        {
          "111:30": "",
          "Lu et al.": "scenarios,” PloS one, vol. 9, no. 12, p. e113647, 2014."
        },
        {
          "111:30": "",
          "Lu et al.": "[48] L. Cui, S. Li, and T. Zhu, “Emotion detection from natural walking,” in Human Centered Computing: Second International"
        },
        {
          "111:30": "",
          "Lu et al.": "Springer, 2016, pp. 23–33.\nConference, HCC 2016, Colombo, Sri Lanka, January 7-9, 2016, Revised Selected Papers 2."
        },
        {
          "111:30": "[49]",
          "Lu et al.": "S. Li, L. Cui, C. Zhu, B. Li, N. Zhao, and T. Zhu, “Emotion recognition using kinect motion capture data of human"
        },
        {
          "111:30": "",
          "Lu et al.": "gaits,” PeerJ, vol. 4, p. e2364, 2016."
        },
        {
          "111:30": "[50]",
          "Lu et al.": "S. Yu and X. Li, “Gait-based emotion recognition using spatial temporal graph convolutional networks,” in 2021"
        },
        {
          "111:30": "",
          "Lu et al.": "International Conference on Computer Information Science and Artificial Intelligence (CISAI), 2021, pp. 190–193."
        },
        {
          "111:30": "",
          "Lu et al.": "[51] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d pose estimation using part affinity fields,” in"
        },
        {
          "111:30": "",
          "Lu et al.": "CVPR, 2017, pp. 7291–7299."
        },
        {
          "111:30": "",
          "Lu et al.": "[52] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation learning for human pose estimation,” in"
        },
        {
          "111:30": "",
          "Lu et al.": "CVPR, 2019, pp. 5693–5703."
        },
        {
          "111:30": "",
          "Lu et al.": "[53] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel, W. Xu, and C. Theobalt, “Vnect: Real-time"
        },
        {
          "111:30": "",
          "Lu et al.": "ACM\n3d human pose estimation with a single rgb camera,” in ACM Transactions on Graphics (TOG), vol. 36, no. 4."
        },
        {
          "111:30": "",
          "Lu et al.": "New York, NY, USA, 2017, pp. 1–14."
        },
        {
          "111:30": "",
          "Lu et al.": "[54] T. Randhavane, U. Bhattacharya, K. Kapsaskis, K. Gray, A. Bera, and D. Manocha, “The liar’s walk: Detecting deception"
        },
        {
          "111:30": "",
          "Lu et al.": "with gait and gesture,” arXiv preprint arXiv:1912.06874, 2019."
        },
        {
          "111:30": "",
          "Lu et al.": "[55] M. Poyo Solanas, M. J. Vaessen, and B. de Gelder, “The role of computational and subjective features in emotional"
        },
        {
          "111:30": "",
          "Lu et al.": "body expressions,” Scientific reports, vol. 10, no. 1, p. 6202, 2020."
        },
        {
          "111:30": "",
          "Lu et al.": "[56] T. Sapiński, D. Kamińska, A. Pelikant, C. Ozcinar, E. Avots, and G. Anbarjafari, “Multimodal database of emotional"
        },
        {
          "111:30": "",
          "Lu et al.": "Springer, 2019, pp. 153–163.\nspeech, video and gestures,” in ICPR Workshops."
        },
        {
          "111:30": "",
          "Lu et al.": "[57] A. Kleinsmith, P. R. De Silva, and N. Bianchi-Berthouze, “Cross-cultural differences in recognizing affect from body"
        },
        {
          "111:30": "",
          "Lu et al.": "posture,” Interacting with computers, vol. 18, no. 6, pp. 1371–1389, 2006."
        },
        {
          "111:30": "",
          "Lu et al.": "[58] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and D. Manocha, “Step: Spatial temporal graph"
        },
        {
          "111:30": "",
          "Lu et al.": "convolutional networks for emotion perception from gaits,” in AAAI, 2020, p. 1342–1350."
        },
        {
          "111:30": "[59]",
          "Lu et al.": "I. Habibie, D. Holden, J. Schwarz, J. Yearsley, and T. Komura, “A recurrent variational autoencoder for human motion"
        },
        {
          "111:30": "",
          "Lu et al.": "synthesis,” in 28th British Machine Vision Conference, 2017."
        },
        {
          "111:30": "",
          "Lu et al.": "[60] H. Hicheur, H. Kadone, J. Grèzes, and A. Berthoz, The Combined Role of Motion-Related Cues and Upper Body Posture"
        },
        {
          "111:30": "",
          "Lu et al.": "Berlin, Heidelberg: Springer Berlin Heidelberg, 2013, pp.\nfor the Expression of Emotions during Human Walking."
        },
        {
          "111:30": "",
          "Lu et al.": "71–85. [Online]. Available: https://doi.org/10.1007/978-3-642-36368-9_6"
        },
        {
          "111:30": "",
          "Lu et al.": "[61] W. Sheng and X. Li, “Multi-task learning for gait-based identity recognition and emotion recognition using attention"
        },
        {
          "111:30": "",
          "Lu et al.": "enhanced temporal graph convolutional network,” Pattern Recognition, vol. 114, p. 107868, 2021."
        },
        {
          "111:30": "",
          "Lu et al.": "[62] N. Fourati and C. Pelachaud, “Emilya: Emotional body expression in daily actions database.” in LREC, 2014, pp."
        },
        {
          "111:30": "",
          "Lu et al.": "3486–3493."
        },
        {
          "111:30": "[63]",
          "Lu et al.": "S. Narang, A. Best, A. Feng, S.-h. Kang, D. Manocha, and A. Shapiro, “Motion recognition of self and others on realistic"
        },
        {
          "111:30": "",
          "Lu et al.": "3d avatars,” Computer Animation and Virtual Worlds, vol. 28, no. 3-4, p. e1762, 2017."
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[64] C. M. U. G. Lab, “Cmu graphics lab motion capture database,” 2003, accessed: 2025-04-21.\n[Online]. Available:"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "http://mocap.cs.cmu.edu/"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[65] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6m: Large scale datasets and predictive methods for"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "3d human sensing in natural environments,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "no. 7, pp. 1325–1339, 2014."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[66] A. Kapur, A. Kapur, N. Virji-Babul, G. Tzanetakis, and P. F. Driessen, “Gesture-based affective computing on motion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "capture data,” in Affective Computing and Intelligent Interaction: First International Conference, ACII 2005, Beijing,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "Springer, 2005, pp. 1–7.\nChina, October 22-24, 2005. Proceedings 1."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[67] D. Bernhardt and P. Robinson, “Detecting affect from non-stylised body motions,” in International conference on"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "Springer, 2007, pp. 59–70.\naffective computing and intelligent interaction."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[68] N. Fourati and C. Pelachaud, “Multi-level classification of emotional body expression,” in 2015 11th IEEE International"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "Conference and Workshops on Automatic Face and Gesture Recognition (FG), vol. 1, 2015, pp. 1–8."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[69] ——, “Relevant body cues for the classification of emotional body expression in daily actions,” in 2015 International"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "IEEE, 2015, pp. 267–273.\nConference on Affective Computing and Intelligent Interaction (ACII)."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[70] N. Fourati, C. Pelachaud, and P. Darmon, “Contribution of temporal and multi-level body cues to emotion classification,”"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "in 2019 8th International Conference on Affective Computing and Intelligent Interaction, 2019, pp. 116–122."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[71] A. Crenn, R. A. Khan, A. Meyer, and S. Bouakaz, “Body expression recognition from animated 3d skeleton,” in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "International Conference on 3D Imaging, 2016, pp. 1–7."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[72] A. Crenn, A. Meyer, R. A. Khan, H. Konik, and S. Bouakaz, “Toward an efficient body expression recognition based"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "on the synthesis of a neutral movement,” in Proceedings of the 19th ACM International Conference on Multimodal"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "New York, NY, USA: Association for Computing Machinery, 2017, p. 15–22. [Online].\nInteraction, ser. ICMI ’17."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "Available: https://doi.org/10.1145/3136755.3136763"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[73] A. Crenn, A. Meyer, H. Konik, R. A. Khan, and S. Bouakaz, “Generic body expression recognition based on synthesis"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "of realistic neutral motion,” IEEE Access, vol. 8, pp. 207 758–207 767, 2020."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[74]\nS. Saha, S. Datta, A. Konar, and R. Janarthanan, “A study on emotion recognition from body gestures using kinect"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "IEEE, 2014, pp. 056–060.\nsensor,” in 2014 international conference on communication and signal processing."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "S. Piana, A. Staglianò, F. Odone, and A. Camurri, “Adaptive body gesture representation for automatic emotion\n[75]"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "recognition,” ACM Transactions on Interactive Intelligent Systems, vol. 6, no. 1, pp. 1–31, 2016."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[76] A.-A. Samadani, R. Gorbet, and D. Kulić, “Affective movement recognition based on generative and discriminative"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "stochastic dynamic models,” IEEE Transactions on Human-Machine Systems, vol. 44, no. 4, pp. 454–467, 2014."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[77] N. Fourati and C. Pelachaud, “Collection and characterization of emotional body behaviors,” in Proceedings of the 2014"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "International Workshop on Movement and Computing, 2014, pp. 49–54."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[78] A. Kleinsmith, N. Bianchi-Berthouze, and A. Steed, “Automatic recognition of non-acted affective postures,” IEEE"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 41, no. 4, pp. 1027–1038, 2011."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[79] N. Savva, A. Scarinzi, and N. Bianchi-Berthouze, “Continuous recognition of player’s affective body expression as"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "dynamic quality of aesthetic experience,” IEEE Transactions on Computational Intelligence and AI in games, vol. 4,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "no. 3, pp. 199–212, 2012."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[80] M. Garber-Barron and M. Si, “Using body movement and posture for emotion detection in non-acted scenarios,” in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "IEEE, 2012, pp. 1–8.\n2012 IEEE International Conference on Fuzzy Systems."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[81]\nF. Ahmed, A. H. Bari, and M. L. Gavrilova, “Emotion recognition from body movement,” IEEE Access, vol. 8, pp."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "11 761–11 781, 2019."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[82] D. Glowinski, A. Camurri, G. Volpe, N. Dael, and K. Scherer, “Technique for automatic emotion recognition by body"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "gesture analysis,” in CVPR Workshops, 2008, pp. 1–6."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[83]\nSwiss Center for Affective Sciences, University of Geneva, “The geneva multimodal emotion portrayals (gemep)"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "corpus,” https://www.unige.ch/cisa/gemep, 2025, accessed: 2025-06-23."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[84] D. Glowinski, N. Dael, A. Camurri, G. Volpe, M. Mortillaro, and K. Scherer, “Toward a minimal representation of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "affective gestures,” IEEE Transactions on Affective Computing, vol. 2, no. 2, pp. 106–118, 2011."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[85] D. Avola, L. Cinque, A. Fagioli, G. L. Foresti, and C. Massaroni, “Deep temporal analysis for non-acted body affect"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "recognition,” IEEE Transactions on Affective Computing, vol. 13, no. 3, pp. 1366–1377, 2020."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[86] A. Oğuz and Ö. F. Ertuğrul, “Emotion recognition by skeleton-based spatial and temporal analysis,” Expert Systems"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "with Applications, vol. 238, p. 121981, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[87] T. Wang, S. Liu, F. He, M. Du, W. Dai, Y. Ke, and D. Ming, “Affective body expression recognition framework based on"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "temporal and spatial fusion features,” Knowledge-Based Systems, vol. 308, p. 112744, 2025."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[88] T. Sapiński, D. Kamińska, A. Pelikant, and G. Anbarjafari, “Emotion recognition from skeletal movements,” Entropy,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "vol. 21, no. 7, p. 646, 2019."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "[89]\nS. Karumuri, R. Niewiadomski, G. Volpe, and A. Camurri, “From motions to emotions: classification of affect from dance"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:31": "movements using deep learning,” in Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing"
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:32\nLu et al.": "Systems, 2019, pp. 1–6."
        },
        {
          "111:32\nLu et al.": "[90] C. Mingming, F. Jiandong, and Z. Yudong, “Emotion recognition of human body’s posture in open environment,” in"
        },
        {
          "111:32\nLu et al.": "Ieee, 2020, pp. 3294–3299.\n2020 Chinese Control And Decision Conference (CCDC)."
        },
        {
          "111:32\nLu et al.": "[91] Z. Shen,\nJ. Cheng, X. Hu, and Q. Dong, “Emotion recognition based on multi-view body gestures,” in 2019 ieee"
        },
        {
          "111:32\nLu et al.": "IEEE, 2019, pp. 3317–3321.\ninternational conference on image processing (icip)."
        },
        {
          "111:32\nLu et al.": "[92]\nJ. Shi, C. Liu, C. T. Ishi, and H. Ishiguro, “Skeleton-based emotion recognition based on two-stream self-attention"
        },
        {
          "111:32\nLu et al.": "enhanced\nspatial-temporal\ngraph convolutional network,”\nvol.\n21, no.\n1,\n2021.\n[Online]. Available:\nSensors,"
        },
        {
          "111:32\nLu et al.": "https://www.mdpi.com/1424-8220/21/1/205"
        },
        {
          "111:32\nLu et al.": "[93] A. Shirian, S. Tripathi, and T. Guha, “Dynamic emotion modeling with learnable graphs and graph inception network,”"
        },
        {
          "111:32\nLu et al.": "IEEE Transactions on Multimedia, vol. 24, pp. 780–790, 2021."
        },
        {
          "111:32\nLu et al.": "[94] P. V. Paiva, J. J. Ramos, M. Gavrilova, and M. A. Carvalho, “Skelett-skeleton-to-emotion transfer transformer,” IEEE"
        },
        {
          "111:32\nLu et al.": "Access, 2025."
        },
        {
          "111:32\nLu et al.": "[95] H. Lu, J. Chen, F. Liang, M. Tan, R. Zeng, and X. Hu, “Understanding emotional body expressions via large language"
        },
        {
          "111:32\nLu et al.": "models,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 2, pp. 1447–1455, Apr. 2025. [Online]."
        },
        {
          "111:32\nLu et al.": "Available: https://ojs.aaai.org/index.php/AAAI/article/view/32135"
        },
        {
          "111:32\nLu et al.": "[96] L. Omlor and M. A. Giese, “Extraction of spatio-temporal primitives of emotional body expressions,” Neurocomputing,"
        },
        {
          "111:32\nLu et al.": "vol. 70, no. 10, pp. 1938–1942, 2007, computational Neuroscience: Trends in Research 2007. [Online]. Available:"
        },
        {
          "111:32\nLu et al.": "https://www.sciencedirect.com/science/article/pii/S0925231206004309"
        },
        {
          "111:32\nLu et al.": "[97] G. Venture, “Human characterization and emotion characterization from gait,” in 2010 Annual International Conference"
        },
        {
          "111:32\nLu et al.": "of the IEEE Engineering in Medicine and Biology, 2010, pp. 1292–1295."
        },
        {
          "111:32\nLu et al.": "[98] G. Venture, H. Kadone, T. Zhang, J. Grèzes, A. Berthoz, and H. Hicheur, “Recognizing emotions conveyed by human"
        },
        {
          "111:32\nLu et al.": "gait,” International Journal of Social Robotics, vol. 6, pp. 621–632, 2014."
        },
        {
          "111:32\nLu et al.": "[99] M. Karg, K. Kühnlenz, and M. Buss, “Recognition of affect based on gait patterns,” IEEE Transactions on Systems, Man,"
        },
        {
          "111:32\nLu et al.": "and Cybernetics, Part B (Cybernetics), vol. 40, no. 4, pp. 1050–1061, 2010."
        },
        {
          "111:32\nLu et al.": "[100] M. Karg, R. Jenke, W. Seiberl, K. Kühnlenz, A. Schwirtz, and M. Buss, “A comparison of pca, kpca and lda for feature"
        },
        {
          "111:32\nLu et al.": "extraction to recognize affect in gait kinematics,” in 2009 3rd International Conference on Affective Computing and"
        },
        {
          "111:32\nLu et al.": "IEEE, 2009, pp. 1–6.\nIntelligent Interaction and Workshops."
        },
        {
          "111:32\nLu et al.": "[101] M. Karg, R. Jenke, K. Kühnlenz, and M. Buss, “A two-fold pca-approach for inter-individual recognition of emotions"
        },
        {
          "111:32\nLu et al.": "in natural walking.” in MLDM posters, 2009, pp. 51–61."
        },
        {
          "111:32\nLu et al.": "[102] M. Daoudi, S. Berretti, P. Pala, Y. Delevoye, and A. D. Bimbo, “Emotion recognition by body movement representation"
        },
        {
          "111:32\nLu et al.": "on the manifold of symmetric positive definite matrices,” in International Conference on Image Analysis and Processing,"
        },
        {
          "111:32\nLu et al.": "2017, pp. 550–560."
        },
        {
          "111:32\nLu et al.": "[103] B. Li, C. Zhu, S. Li, and T. Zhu, “Identifying emotions from non-contact gaits information based on microsoft kinects,”"
        },
        {
          "111:32\nLu et al.": "IEEE Transactions on Affective Computing, vol. 9, no. 4, pp. 585–591, 2016."
        },
        {
          "111:32\nLu et al.": "[104]\nF. Ahmed, B. Sieu, and M. L. Gavrilova, “Score and rank-level fusion for emotion recognition using genetic algorithm,”"
        },
        {
          "111:32\nLu et al.": "in 2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC), 2018, pp. 46–53."
        },
        {
          "111:32\nLu et al.": "[105] Z. Zhang, Y. Song, L. Cui, X. Liu, and T. Zhu, “Emotion recognition based on customized smart bracelet with built-in"
        },
        {
          "111:32\nLu et al.": "accelerometer,” PeerJ, vol. 4, p. e2258, Jul. 2016. [Online]. Available: https://doi.org/10.7717/peerj.2258"
        },
        {
          "111:32\nLu et al.": "[106]\nJ. C. Quiroz, E. Geangu, and M. H. Yong, “Emotion recognition using smart watch sensor data: Mixed-design study,”"
        },
        {
          "111:32\nLu et al.": "JMIR Ment Health, vol. 5, no. 3, p. e10153, Aug 2018. [Online]. Available: http://mental.jmir.org/2018/3/e10153/"
        },
        {
          "111:32\nLu et al.": "[107]\nJ. C. Quiroz, M. H. Yong, and E. Geangu, “Emotion-recognition using smart watch accelerometer data: Preliminary"
        },
        {
          "111:32\nLu et al.": "findings,” in Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and"
        },
        {
          "111:32\nLu et al.": "Proceedings of the 2017 ACM International Symposium on Wearable Computers, 2017, pp. 805–812."
        },
        {
          "111:32\nLu et al.": "[108] M. Chiu, J. Shu, and P. Hui, “Emotion recognition through gait on mobile devices,” in 2018 IEEE International Conference"
        },
        {
          "111:32\nLu et al.": "on Pervasive Computing and Communications Workshops (PerCom Workshops), 2018, pp. 800–805."
        },
        {
          "111:32\nLu et al.": "[109] T. Randhavane, U. Bhattacharya, K. Kapsaskis, K. Gray, A. Bera, and D. Manocha, “Identifying emotions from walking"
        },
        {
          "111:32\nLu et al.": "using affective and deep features,” arXiv preprint arXiv:1906.11884, 2019."
        },
        {
          "111:32\nLu et al.": "[110] Y. Bhatia, A. H. Bari, G.-S. J. Hsu, and M. Gavrilova, “Motion capture sensor-based emotion recognition using a"
        },
        {
          "111:32\nLu et al.": "bi-modular sequential neural network,” Sensors, vol. 22, no. 1, p. 403, 2022."
        },
        {
          "111:32\nLu et al.": "[111]\nS. Zhang, J. Zhang, W. Song, L. Yang, and X. Zhao, “Hierarchical-attention-based neural network for gait emotion"
        },
        {
          "111:32\nLu et al.": "recognition,” Physica A: Statistical Mechanics and its Applications, vol. 637, p. 129600, 2024. [Online]. Available:"
        },
        {
          "111:32\nLu et al.": "https://www.sciencedirect.com/science/article/pii/S0378437124001080"
        },
        {
          "111:32\nLu et al.": "[112] U. Bhattacharya, C. Roncal, T. Mittal, R. Chandra, K. Kapsaskis, K. Gray, A. Bera, and D. Manocha, “Take an emotion"
        },
        {
          "111:32\nLu et al.": "Springer,\nwalk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping,” in ECCV."
        },
        {
          "111:32\nLu et al.": "2020, pp. 145–163."
        },
        {
          "111:32\nLu et al.": "[113] X. Sun, K. Su, and C. Fan, “Vfl—a deep learning-based framework for classifying walking gaits into emotions,”"
        },
        {
          "111:32\nLu et al.": "Neurocomputing, vol. 473, pp. 1–13, 2022."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[114] C. Hu, W. Sheng, B. Dong, and X. Li, “Tntc:\ntwo-stream network with transformer-based complementarity for"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "gait-based emotion recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "IEEE, 2022, pp. 3229–3233."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[115] Z. Zhu, C. P. Chen, H. Liu, and T. Zhang, “Temporal group attention network with affective complementary learning"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "IEEE,\nfor gait emotion recognition,” in 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "2024, pp. 3026–3033."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[116] T. Zhang, Y. Chen, S. Li, X. Hu, and C. L. P. Chen, “Tt-gcn: Temporal-tightly graph convolutional network for emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "recognition from gaits,” IEEE Transactions on Computational Social Systems, vol. 11, no. 3, pp. 4300–4314, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[117] Y. Zhai, G. Jia, Y.-K. Lai, J. Zhang, J. Yang, and D. Tao, “Looking into gait for perceiving emotions via bilateral posture"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "and movement graph convolutional networks,” IEEE Transactions on Affective Computing, vol. 15, no. 3, pp. 1634–1648,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[118] V. Narayanan, B. M. Manoghar, V. S. Dorbala, D. Manocha, and A. Bera, “Proxemo: Gait-based emotion learning and"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "multi-view proxemic fusion for socially-aware robot navigation,” in IROS, 2020, pp. 8200–8207."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[119] Y. Zhuang, L. Lin, R. Tong, J. Liu, Y. Iwamoto, and Y.-W. Chen, “G-gcsn: Global graph convolution shrinkage network"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "for emotion perception from gait,” in ACCV Workshops, 2021, pp. 46–57."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[120] H. Lu, S. Xu, S. Zhao, X. Hu, R. Ma, and B. Hu, “Epic: Emotion perception by spatio-temporal interaction context of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "gait,” IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 5, pp. 2592–2601, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "Yin,\nL.\nJing,\nF. Huang, G.\nYang,\nand\nZ. Wang,\n“Msa-gcn: Multiscale\nadaptive\ngraph\nconvolution\n[121] Y."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "network\nfor\ngait\nemotion recognition,”\nvol.\n147,\np.\n110117,\n2024.\n[Online]. Available:\nPattern Recognition,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "https://www.sciencedirect.com/science/article/pii/S0031320323008142"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[122] C. Chen and X. Sun, “Sta-gcn:spatial temporal adaptive graph convolutional network for gait emotion recognition,” in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "2023 IEEE International Conference on Multimedia and Expo (ICME), 2023, pp. 1385–1390."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[123] Q. Zeng and L. Shang, “Gaitcycformer: Leveraging gait cycles and transformers for gait emotion recognition.” in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 9, 2025, pp. 9815–9823."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[124] H. Lu, X. Hu, and B. Hu, “See your emotion from gait using unlabeled skeleton data,” in AAAI, vol. 37, no. 2, jun 2023,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "pp. 1826–1834. [Online]. Available: https://doi.org/10.1609%2Faaai.v37i2.25272"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[125] C. Song, L. Lu, Z. Ke, L. Gao, and S. Ding, “Self-supervised gait-based emotion representation learning from selective"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "strongly augmented skeleton sequences,” arXiv preprint arXiv:2405.04900, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[126] N. Zhao, Z. Zhang, Y. Wang, J. Wang, B. Li, T. Zhu, and Y. Xiang, “See your mental state from your walk: Recognizing"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "anxiety and depression through kinect-recorded gait data,” PloS one, vol. 14, no. 5, 2019."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[127] Y. Yuan, B. Li, N. Wang, Q. Ye, Y. Liu, and T. Zhu, “Depression identification from gait spectrum features based on"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "hilbert-huang transform,” in Human Centered Computing: 4th International Conference, HCC 2018, Mexico, December,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "Springer, 2019, pp. 503–515.\n5–7, 2018, Revised Selected Papers 4."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[128] Y. Wang, J. Wang, X. Liu, and T. Zhu, “Detecting depression through gait data: examining the contribution of gait"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "features in recognizing depression,” Frontiers in psychiatry, vol. 12, p. 661213, 2021."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[129] H. Lu, W. Shao, E. Ngai, X. Hu, and B. Hu, “A new skeletal representation based on gait for depression detection,” in"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "2020 IEEE International Conference on E-health Networking, Application Services (HEALTHCOM), 2021, pp. 1–6."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[130] H. Lu, S. Xu, X. Hu, E. Ngai, Y. Guo, W. Wang, and B. Hu, “Postgraduate student depression assessment by multimedia"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "gait analysis,” IEEE MultiMedia, vol. 29, no. 2, pp. 56–65, 2022."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[131]\nJ. Fang, T. Wang, C. Li, X. Hu, E. Ngai, B.-C. Seet, J. Cheng, Y. Guo, and X. Jiang, “Depression prevalence in postgraduate"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "students and its association with gait abnormality,” IEEE Access, vol. 7, pp. 174 425–174 437, 2019."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[132] T. Wang, C. Li, C. Wu, C. Zhao, J. Sun, H. Peng, X. Hu, and B. Hu, “A gait assessment framework for depression"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "detection using kinect sensors,” IEEE Sensors Journal, vol. 21, no. 3, pp. 3260–3270, 2021."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[133]\nJ. Yang, H. Lu, C. Li, X. Hu, and B. Hu, “Data augmentation for depression detection using skeleton-based gait"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "information,” Medical & Biological Engineering & Computing, p. 2665–2679, jul 2022."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[134] W. Shao, Z. You, L. Liang, X. Hu, C. Li, W. Wang, and B. Hu, “A multi-modal gait analysis-based detection system of"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "the risk of depression,” IEEE Journal of Biomedical and Health Informatics, vol. 26, no. 10, pp. 4859–4868, 2021."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[135] X. Liu, Q. Li, S. Hou, M. Ren, X. Hu, and Y. Huang, “Depression risk recognition based on gait: A benchmark,”"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "Neurocomputing, vol. 596, p. 128045, 2024."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[136] Q. Li, M. Ren, X. Hu, X. Liu, L. Yao, and Y. Huang, “Spatio-temporal multi-granularity for skeleton-based depression"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "risk recognition,” IEEE Journal of Biomedical and Health Informatics, pp. 1–12, 2025."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[137] C. A. Mazefsky, J. Herrington, M. Siegel, A. Scarpa, B. B. Maddox, L. Scahill, and S. W. White, “The role of emotion"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "regulation in autism spectrum disorder,” Journal of the American Academy of Child & Adolescent Psychiatry, vol. 52,"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "no. 7, pp. 679–688, 2013."
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "[138] A. A. Al-Jubouri, I. H. Ali, and Y. Rajihy, “Generating 3d dataset of gait and full body movement of children with"
        },
        {
          "Emotion Recognition from Skeleton Data: A Comprehensive Survey\n111:33": "autism spectrum disorders collected by kinect v2 camera,” Compusoft, vol. 9, no. 8, pp. 3791–3797, 2020."
        }
      ],
      "page": 33
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:34": "",
          "Lu et al.": "[139] Y. Zhang, Y. Tian, P. Wu, and D. Chen, “Application of skeleton data and long short-term memory in action recognition"
        },
        {
          "111:34": "",
          "Lu et al.": "of children with autism spectrum disorder,” Sensors, vol. 21, no. 2, p. 411, 2021."
        },
        {
          "111:34": "[140]",
          "Lu et al.": "S. Zahan, Z. Gilani, G. M. Hassan, and A. Mian, “Human gesture and gait analysis for autism detection,” in Proceedings"
        },
        {
          "111:34": "",
          "Lu et al.": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 3328–3337."
        },
        {
          "111:34": "",
          "Lu et al.": "[141] M. Yang, M. Ni, T. Su, W. Zhou, Y. She, W. Zheng, and B. Hu, “Body posture based detection of autism spectrum"
        },
        {
          "111:34": "",
          "Lu et al.": "disorder in children,” IEEE Sensors Journal, 2025."
        },
        {
          "111:34": "",
          "Lu et al.": "[142] P. K. Mishra, A. Mihailidis, and S. S. Khan, “Skeletal video anomaly detection using deep learning: Survey, challenges,"
        },
        {
          "111:34": "",
          "Lu et al.": "and future directions,” IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 8, no. 2, pp. 1073–1085,"
        },
        {
          "111:34": "",
          "Lu et al.": "2024."
        },
        {
          "111:34": "",
          "Lu et al.": "[143] K. Zhou, T. Wu, C. Wang, J. Wang, and C. Li, “Skeleton based abnormal behavior recognition using spatio-temporal"
        },
        {
          "111:34": "",
          "Lu et al.": "convolution and attention-based lstm,” Procedia Computer Science, vol. 174, pp. 424–432, 2020."
        },
        {
          "111:34": "",
          "Lu et al.": "[144] R. Morais, V. Le, T. Tran, B. Saha, M. Mansour, and S. Venkatesh, “Learning regularity in skeleton trajectories for"
        },
        {
          "111:34": "",
          "Lu et al.": "anomaly detection in videos,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,"
        },
        {
          "111:34": "",
          "Lu et al.": "2019, pp. 11 996–12 004."
        },
        {
          "111:34": "",
          "Lu et al.": "[145] A. Markovitz, G. Sharir, I. Friedman, L. Zelnik-Manor, and S. Avidan, “Graph embedded pose clustering for anomaly"
        },
        {
          "111:34": "",
          "Lu et al.": "detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 539–"
        },
        {
          "111:34": "",
          "Lu et al.": "10 547."
        },
        {
          "111:34": "",
          "Lu et al.": "[146] C. Liu, R. Fu, Y. Li, Y. Gao, L. Shi, and W. Li, “A self-attention augmented graph convolutional clustering networks for"
        },
        {
          "111:34": "",
          "Lu et al.": "skeleton-based video anomaly behavior detection,” Applied Sciences, vol. 12, no. 1, p. 4, 2021."
        },
        {
          "111:34": "",
          "Lu et al.": "[147] A. Flaborea, G. M. D. di Melendugno, S. D’Arrigo, M. A. Sterpa, A. Sampieri, and F. Galasso, “Contracting skeletal"
        },
        {
          "111:34": "",
          "Lu et al.": "kinematics for human-related video anomaly detection,” Pattern Recognition, vol. 156, p. 110817, 2024."
        },
        {
          "111:34": "",
          "Lu et al.": "[148] A. Karami, T. K. K. Ho, and N. Armanfard, “Graph-jigsaw conditioned diffusion model for skeleton-based video"
        },
        {
          "111:34": "",
          "Lu et al.": "IEEE, 2025, pp.\nanomaly detection,” in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)."
        },
        {
          "111:34": "",
          "Lu et al.": "4237–4247."
        },
        {
          "111:34": "[149]",
          "Lu et al.": "F. Sato, R. Hachiuma, and T. Sekii, “Prompt-guided zero-shot anomaly action recognition using pretrained deep"
        },
        {
          "111:34": "",
          "Lu et al.": "skeleton features,” in CVPR, 2023, pp. 6471–6480."
        },
        {
          "111:34": "",
          "Lu et al.": "[150] Y. Liu, R. Liu, W. Xin, Q. Miao, Y. Hu, and J. Qi, “Language-skeleton pre-training to collaborate with self-supervised"
        },
        {
          "111:34": "",
          "Lu et al.": "Springer,\nhuman action recognition,” in Chinese Conference on Pattern Recognition and Computer Vision (PRCV)."
        },
        {
          "111:34": "",
          "Lu et al.": "2024, pp. 409–423."
        },
        {
          "111:34": "",
          "Lu et al.": "[151] M. Dahmane, J. Alam, P.-L. St-Charles, M. Lalonde, K. Heffner, and S. Foucher, “A multimodal non-intrusive stress"
        },
        {
          "111:34": "",
          "Lu et al.": "monitoring from the pleasure-arousal emotional dimensions,” IEEE Transactions on Affective Computing, vol. 13, no. 2,"
        },
        {
          "111:34": "",
          "Lu et al.": "pp. 1044–1056, 2020."
        },
        {
          "111:34": "",
          "Lu et al.": "[152] R. Guo, H. Guo, L. Wang, M. Chen, D. Yang, and B. Li, “Development and application of emotion recognition"
        },
        {
          "111:34": "",
          "Lu et al.": "technology—a systematic literature review,” BMC psychology, vol. 12, no. 1, p. 95, 2024."
        },
        {
          "111:34": "",
          "Lu et al.": "[153] Z. Zhang, F. Liang, W. Wang, R. Zeng, V. C. Leung, and X. Hu, “Skeleton-based pre-training with discrete labels for"
        },
        {
          "111:34": "",
          "Lu et al.": "emotion recognition in iot environments,” IEEE Internet of Things Journal, 2025."
        },
        {
          "111:34": "",
          "Lu et al.": "[154] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, and J. Z. Wang, “Arbee: Towards automated recognition of bodily"
        },
        {
          "111:34": "",
          "Lu et al.": "expression of emotion in the wild,” International Journal of Computer Vision, vol. 128, no. 1, pp. 1–25, 2020."
        },
        {
          "111:34": "",
          "Lu et al.": "[155] Q. Li, Z. Liu, Z. Zhang, Q. Wang, and M. Ma, “Decoding group emotional dynamics in a web-based collaborative"
        },
        {
          "111:34": "",
          "Lu et al.": "environment: A novel\nframework utilizing multi-person facial expression recognition,” International Journal of"
        },
        {
          "111:34": "",
          "Lu et al.": "Human–Computer Interaction, vol. 41, no. 5, pp. 3455–3473, 2025."
        },
        {
          "111:34": "[156]",
          "Lu et al.": "S. Zhang, Y. Yang, C. Chen, X. Zhang, Q. Leng, and X. Zhao, “Deep learning-based multimodal emotion recognition"
        },
        {
          "111:34": "",
          "Lu et al.": "from audio, visual, and text modalities: A systematic review of recent advancements and future prospects,” Expert"
        },
        {
          "111:34": "",
          "Lu et al.": "Systems with Applications, vol. 237, p. 121692, 2024."
        },
        {
          "111:34": "",
          "Lu et al.": "[157] X. Li, Y. Zhang, P. Tiwari, D. Song, B. Hu, M. Yang, Z. Zhao, N. Kumar, and P. Marttinen, “Eeg based emotion"
        },
        {
          "111:34": "",
          "Lu et al.": "recognition: A tutorial and review,” ACM Computing Surveys (CSUR), 2022."
        },
        {
          "111:34": "[158]",
          "Lu et al.": "J. Yan, P. Li, C. Du, K. Zhu, X. Zhou, Y. Liu, and J. Wei, “Multimodal emotion recognition based on facial expressions,"
        },
        {
          "111:34": "",
          "Lu et al.": "speech, and body gestures.” Electronics (2079-9292), vol. 13, no. 18, 2024."
        },
        {
          "111:34": "",
          "Lu et al.": "[159] H. Kim and T. Hong, “Enhancing emotion recognition using multimodal fusion of physiological, environmental,"
        },
        {
          "111:34": "",
          "Lu et al.": "personal data,” Expert Systems with Applications, vol. 249, p. 123723, 2024."
        },
        {
          "111:34": "[160] OpenAI, “Chatgpt: An ai language model,” https://chat.openai.com/, 2024.",
          "Lu et al.": ""
        },
        {
          "111:34": "[161] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” NeurIPS, vol. 36, 2024.",
          "Lu et al.": ""
        },
        {
          "111:34": "",
          "Lu et al.": "[162] G. Research, “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,” arXiv, vol."
        },
        {
          "111:34": "",
          "Lu et al.": "abs/2403.05530, 2024,\ntechnical report detailing the Gemini 1.5 Pro architecture and performance benchmarks."
        },
        {
          "111:34": "",
          "Lu et al.": "[Online]. Available: https://ar5iv.labs.arxiv.org/html/2403.05530"
        },
        {
          "111:34": "Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
          "Lu et al.": ""
        },
        {
          "111:34": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2025.",
          "Lu et al.": ""
        }
      ],
      "page": 34
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wrobel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications"
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "3",
      "title": "Emotion detection: a technology review",
      "authors": [
        "J Garcia-Garcia",
        "V Penichet",
        "M Lozano"
      ],
      "year": "2017",
      "venue": "Proceedings of the XVIII international conference on human computer interaction"
    },
    {
      "citation_id": "4",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "6",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "7",
      "title": "Publication date",
      "authors": [
        "J Acm"
      ],
      "year": "2025",
      "venue": "Emotion Recognition from Skeleton Data: A Comprehensive Survey"
    },
    {
      "citation_id": "8",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "9",
      "title": "Human emotion recognition: Review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "12",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "13",
      "title": "Embodiment of sadness and depression-gait patterns associated with dysphoric mood",
      "authors": [
        "J Michalak",
        "N Troje",
        "J Fischer",
        "P Vollmar",
        "T Heidenreich",
        "D Schulte"
      ],
      "year": "2009",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "14",
      "title": "From emotions to mood disorders: A survey on gait analysis methodology",
      "authors": [
        "F Deligianni",
        "Y Guo",
        "G.-Z Yang"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from full-body motion using multiscale spatio-temporal network",
      "authors": [
        "T Wang",
        "S Liu",
        "F He",
        "W Dai",
        "M Du",
        "Y Ke",
        "D Ming"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Microsoft kinect sensor and its effect",
      "authors": [
        "Z Zhang"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "17",
      "title": "A dual-augmentor framework for domain generalization in 3d human pose estimation",
      "authors": [
        "Q Peng",
        "C Zheng",
        "C Chen"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Deep learning-based human pose estimation: A survey",
      "authors": [
        "C Zheng",
        "W Wu",
        "C Chen",
        "T Yang",
        "S Zhu",
        "J Shen",
        "N Kehtarnavaz",
        "M Shah"
      ],
      "year": "2023",
      "venue": "Deep learning-based human pose estimation: A survey"
    },
    {
      "citation_id": "19",
      "title": "Skeleton-based explainable bodily expressed emotion recognition through graph convolutional networks",
      "authors": [
        "E Ghaleb",
        "A Mertens",
        "S Asteriadis",
        "G Weiss"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition from body movements with as-lstm",
      "authors": [
        "H Zhang",
        "P Yi",
        "R Liu",
        "D Zhou"
      ],
      "year": "2021",
      "venue": "2021 IEEE 7th International Conference on Virtual Reality"
    },
    {
      "citation_id": "21",
      "title": "Modeling multiple temporal scales of full-body movements for emotion classification",
      "authors": [
        "C Beyan",
        "S Karumuri",
        "G Volpe",
        "A Camurri",
        "R Niewiadomski"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "Y Guo",
        "V Leung",
        "J Cheng",
        "B Hu"
      ],
      "year": "2020",
      "venue": "Emotion recognition from gait analyses: Current research and future directions",
      "arxiv": "arXiv:2003.11461"
    },
    {
      "citation_id": "23",
      "title": "Emotion expression in human body posture and movement: A survey on intelligible motion factors, quantification and validation",
      "authors": [
        "M.-A Mahfoudi",
        "A Meyer",
        "T Gaudin",
        "A Buendia",
        "S Bouakaz"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Brain-computer interface systems-Recent progress and future prospects",
      "authors": [
        "T Rached",
        "A Perkusich"
      ],
      "year": "2013",
      "venue": "Brain-computer interface systems-Recent progress and future prospects"
    },
    {
      "citation_id": "25",
      "title": "The emotional brain",
      "authors": [
        "T Dalgleish"
      ],
      "year": "2004",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "26",
      "title": "Modeling emotions for affect-aware applications",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wróbel"
      ],
      "year": "2015",
      "venue": "Information Systems Development and Applications"
    },
    {
      "citation_id": "27",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska symposium on motivation"
    },
    {
      "citation_id": "28",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "H Gunes",
        "B Schuller",
        "M Pantic",
        "R Cowie"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "29",
      "title": "Automatic affect perception based on body gait and posture: A survey",
      "authors": [
        "B Stephens-Fripp",
        "F Naghdy",
        "D Stirling",
        "G Naghdy"
      ],
      "year": "2017",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "30",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "31",
      "title": "Affective judgment and psychophysiological response: dimensional covariation in the evaluation of pictorial stimuli",
      "authors": [
        "M Greenwald",
        "E Cook",
        "P Lang"
      ],
      "year": "1989",
      "venue": "Journal of psychophysiology"
    },
    {
      "citation_id": "32",
      "title": "Real-time assessment of mental workload using psychophysiological measures and artificial neural networks",
      "authors": [
        "G Wilson",
        "C Russell"
      ],
      "year": "2003",
      "venue": "Human factors"
    },
    {
      "citation_id": "33",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "34",
      "title": "The effect of emotion on movement smoothness during gait in healthy young adults",
      "authors": [
        "G Kang",
        "M Gross"
      ],
      "year": "2016",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "35",
      "title": "Emotional influences on sit-to-walk in healthy young adults",
      "year": "2015",
      "venue": "Human movement science"
    },
    {
      "citation_id": "36",
      "title": "Expression of emotion in the kinematics of locomotion",
      "authors": [
        "A Barliya",
        "L Omlor",
        "M Giese",
        "A Berthoz",
        "T Flash"
      ],
      "year": "2013",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "37",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "38",
      "title": "Bodily expression of emotion",
      "authors": [
        "H Wallbott"
      ],
      "year": "1998",
      "venue": "European journal of social psychology"
    },
    {
      "citation_id": "39",
      "title": "Visual perception of expressiveness in musicians' body movements",
      "authors": [
        "S Dahl",
        "A Friberg"
      ],
      "year": "2007",
      "venue": "Music Perception"
    },
    {
      "citation_id": "40",
      "title": "Methodology for assessing bodily expression of emotion",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2010",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "41",
      "title": "Emotion expression in body action and posture",
      "authors": [
        "N Dael",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition system using short-term monitoring of physiological signals",
      "authors": [
        "K Kim",
        "S Bang",
        "S Kim"
      ],
      "year": "2004",
      "venue": "Medical and biological engineering and computing"
    },
    {
      "citation_id": "43",
      "title": "Effort-shape and kinematic assessment of bodily expression of emotion during gait",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2012",
      "venue": "Human movement science"
    },
    {
      "citation_id": "44",
      "title": "A motion capture library for the study of identity, gender, and emotion perception from biological motion",
      "authors": [
        "Y Ma",
        "H Paterson",
        "F Pollick"
      ],
      "year": "2006",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "45",
      "title": "DanceMotion Capture Database (DMCD)",
      "year": "2025",
      "venue": "DanceMotion Capture Database (DMCD)"
    },
    {
      "citation_id": "46",
      "title": "Perception of emotions and body movement in the emilya database",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Kinematic dataset of actors expressing emotions",
      "authors": [
        "M Zhang",
        "L Yu",
        "K Zhang",
        "B Du",
        "B Zhan",
        "S Chen",
        "X Jiang",
        "S Guo",
        "J Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Scientific data"
    },
    {
      "citation_id": "48",
      "title": "The mpi emotional body expressions database for narrative scenarios",
      "authors": [
        "E Volkova",
        "S De La Rosa",
        "H Bülthoff",
        "B Mohler"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "49",
      "title": "Emotion detection from natural walking",
      "authors": [
        "L Cui",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "Human Centered Computing: Second International Conference, HCC 2016"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition using kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "51",
      "title": "Gait-based emotion recognition using spatial temporal graph convolutional networks",
      "authors": [
        "S Yu",
        "X Li"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)"
    },
    {
      "citation_id": "52",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "53",
      "title": "Deep high-resolution representation learning for human pose estimation",
      "authors": [
        "K Sun",
        "B Xiao",
        "D Liu",
        "J Wang"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "54",
      "title": "Vnect: Real-time 3d human pose estimation with a single rgb camera",
      "authors": [
        "D Mehta",
        "S Sridhar",
        "O Sotnychenko",
        "H Rhodin",
        "M Shafiei",
        "H.-P Seidel",
        "W Xu",
        "C Theobalt"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "55",
      "title": "The liar's walk: Detecting deception with gait and gesture",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "The liar's walk: Detecting deception with gait and gesture",
      "arxiv": "arXiv:1912.06874"
    },
    {
      "citation_id": "56",
      "title": "The role of computational and subjective features in emotional body expressions",
      "authors": [
        "M Solanas",
        "M Vaessen",
        "B De Gelder"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "57",
      "title": "Multimodal database of emotional speech, video and gestures",
      "authors": [
        "T Sapiński",
        "D Kamińska",
        "A Pelikant",
        "C Ozcinar",
        "E Avots",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "ICPR Workshops"
    },
    {
      "citation_id": "58",
      "title": "Cross-cultural differences in recognizing affect from body posture",
      "authors": [
        "A Kleinsmith",
        "P De Silva",
        "N Bianchi-Berthouze"
      ],
      "year": "2006",
      "venue": "Interacting with computers"
    },
    {
      "citation_id": "59",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "60",
      "title": "A recurrent variational autoencoder for human motion synthesis",
      "authors": [
        "I Habibie",
        "D Holden",
        "J Schwarz",
        "J Yearsley",
        "T Komura"
      ],
      "year": "2017",
      "venue": "28th British Machine Vision Conference"
    },
    {
      "citation_id": "61",
      "title": "The Combined Role of Motion-Related Cues and Upper Body Posture for the Expression of Emotions during Human Walking",
      "authors": [
        "H Hicheur",
        "H Kadone",
        "J Grèzes",
        "A Berthoz"
      ],
      "year": "2013",
      "venue": "The Combined Role of Motion-Related Cues and Upper Body Posture for the Expression of Emotions during Human Walking",
      "doi": "10.1007/978-3-642-36368-9_6"
    },
    {
      "citation_id": "62",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Emilya: Emotional body expression in daily actions database",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "64",
      "title": "Motion recognition of self and others on realistic 3d avatars",
      "authors": [
        "S Narang",
        "A Best",
        "A Feng",
        "S -H. Kang",
        "D Manocha",
        "A Shapiro"
      ],
      "year": "1762",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "65",
      "title": "Publication date",
      "authors": [
        "J Acm"
      ],
      "year": "2025",
      "venue": "Emotion Recognition from Skeleton Data: A Comprehensive Survey"
    },
    {
      "citation_id": "66",
      "title": "Cmu graphics lab motion capture database",
      "authors": [
        "C Lab"
      ],
      "year": "2003",
      "venue": "Cmu graphics lab motion capture database"
    },
    {
      "citation_id": "67",
      "title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
      "authors": [
        "C Ionescu",
        "D Papava",
        "V Olaru",
        "C Sminchisescu"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Gesture-based affective computing on motion capture data",
      "authors": [
        "A Kapur",
        "A Kapur",
        "N Virji-Babul",
        "G Tzanetakis",
        "P Driessen"
      ],
      "year": "2005",
      "venue": "Affective Computing and Intelligent Interaction: First International Conference, ACII 2005"
    },
    {
      "citation_id": "69",
      "title": "Detecting affect from non-stylised body motions",
      "authors": [
        "D Bernhardt",
        "P Robinson"
      ],
      "year": "2007",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "70",
      "title": "Multi-level classification of emotional body expression",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "71",
      "title": "Relevant body cues for the classification of emotional body expression in daily actions",
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "72",
      "title": "Contribution of temporal and multi-level body cues to emotion classification",
      "authors": [
        "N Fourati",
        "C Pelachaud",
        "P Darmon"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "73",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "International Conference on 3D Imaging"
    },
    {
      "citation_id": "74",
      "title": "Toward an efficient body expression recognition based on the synthesis of a neutral movement",
      "authors": [
        "A Crenn",
        "A Meyer",
        "R Khan",
        "H Konik",
        "S Bouakaz"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ser. ICMI '17",
      "doi": "10.1145/3136755.3136763"
    },
    {
      "citation_id": "75",
      "title": "Generic body expression recognition based on synthesis of realistic neutral motion",
      "authors": [
        "A Crenn",
        "A Meyer",
        "H Konik",
        "R Khan",
        "S Bouakaz"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "76",
      "title": "A study on emotion recognition from body gestures using kinect sensor",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "R Janarthanan"
      ],
      "year": "2014",
      "venue": "2014 international conference on communication and signal processing"
    },
    {
      "citation_id": "77",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "78",
      "title": "Affective movement recognition based on generative and discriminative stochastic dynamic models",
      "authors": [
        "A.-A Samadani",
        "R Gorbet",
        "D Kulić"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "79",
      "title": "Collection and characterization of emotional body behaviors",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 International Workshop on Movement and Computing"
    },
    {
      "citation_id": "80",
      "title": "Automatic recognition of non-acted affective postures",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze",
        "A Steed"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "81",
      "title": "Continuous recognition of player's affective body expression as dynamic quality of aesthetic experience",
      "authors": [
        "N Savva",
        "A Scarinzi",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in games"
    },
    {
      "citation_id": "82",
      "title": "Using body movement and posture for emotion detection in non-acted scenarios",
      "authors": [
        "M Garber-Barron",
        "M Si"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "83",
      "title": "Emotion recognition from body movement",
      "authors": [
        "F Ahmed",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "84",
      "title": "Technique for automatic emotion recognition by body gesture analysis",
      "authors": [
        "D Glowinski",
        "A Camurri",
        "G Volpe",
        "N Dael",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "85",
      "title": "The geneva multimodal emotion portrayals (gemep) corpus",
      "year": "2025",
      "venue": "The geneva multimodal emotion portrayals (gemep) corpus"
    },
    {
      "citation_id": "86",
      "title": "Toward a minimal representation of affective gestures",
      "authors": [
        "D Glowinski",
        "N Dael",
        "A Camurri",
        "G Volpe",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "87",
      "title": "Deep temporal analysis for non-acted body affect recognition",
      "authors": [
        "D Avola",
        "L Cinque",
        "A Fagioli",
        "G Foresti",
        "C Massaroni"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "88",
      "title": "Emotion recognition by skeleton-based spatial and temporal analysis",
      "authors": [
        "A Oğuz",
        "Ö Ertuğrul"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "89",
      "title": "Affective body expression recognition framework based on temporal and spatial fusion features",
      "authors": [
        "T Wang",
        "S Liu",
        "F He",
        "M Du",
        "W Dai",
        "Y Ke",
        "D Ming"
      ],
      "year": "2025",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "90",
      "title": "Emotion recognition from skeletal movements",
      "authors": [
        "T Sapiński",
        "D Kamińska",
        "A Pelikant",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "91",
      "title": "From motions to emotions: classification of affect from dance movements using deep learning",
      "authors": [
        "S Karumuri",
        "R Niewiadomski",
        "G Volpe",
        "A Camurri"
      ],
      "year": "2019",
      "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing J. ACM"
    },
    {
      "citation_id": "92",
      "title": "Emotion recognition of human body's posture in open environment",
      "authors": [
        "C Mingming",
        "F Jiandong",
        "Z Yudong"
      ],
      "year": "2020",
      "venue": "2020 Chinese Control And Decision Conference (CCDC)"
    },
    {
      "citation_id": "93",
      "title": "Emotion recognition based on multi-view body gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "Emotion recognition based on multi-view body gestures"
    },
    {
      "citation_id": "94",
      "title": "Skeleton-based emotion recognition based on two-stream self-attention enhanced spatial-temporal graph convolutional network",
      "authors": [
        "J Shi",
        "C Liu",
        "C Ishi",
        "H Ishiguro"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "95",
      "title": "Dynamic emotion modeling with learnable graphs and graph inception network",
      "authors": [
        "A Shirian",
        "S Tripathi",
        "T Guha"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "96",
      "title": "Skelett-skeleton-to-emotion transfer transformer",
      "authors": [
        "P Paiva",
        "J Ramos",
        "M Gavrilova",
        "M Carvalho"
      ],
      "year": "2025",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "97",
      "title": "Understanding emotional body expressions via large language models",
      "authors": [
        "H Lu",
        "J Chen",
        "F Liang",
        "M Tan",
        "R Zeng",
        "X Hu"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "98",
      "title": "Extraction of spatio-temporal primitives of emotional body expressions",
      "authors": [
        "L Omlor",
        "M Giese"
      ],
      "year": "2007",
      "venue": "Neuroscience: Trends in Research"
    },
    {
      "citation_id": "99",
      "title": "Human characterization and emotion characterization from gait",
      "authors": [
        "G Venture"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "100",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "G Venture",
        "H Kadone",
        "T Zhang",
        "J Grèzes",
        "A Berthoz",
        "H Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "101",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "102",
      "title": "A comparison of pca, kpca and lda for feature extraction to recognize affect in gait kinematics",
      "authors": [
        "M Karg",
        "R Jenke",
        "W Seiberl",
        "K Kühnlenz",
        "A Schwirtz",
        "M Buss"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "103",
      "title": "A two-fold pca-approach for inter-individual recognition of emotions in natural walking",
      "authors": [
        "M Karg",
        "R Jenke",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2009",
      "venue": "MLDM posters"
    },
    {
      "citation_id": "104",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Bimbo"
      ],
      "year": "2017",
      "venue": "International Conference on Image Analysis and Processing"
    },
    {
      "citation_id": "105",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "106",
      "title": "Score and rank-level fusion for emotion recognition using genetic algorithm",
      "authors": [
        "F Ahmed",
        "B Sieu",
        "M Gavrilova"
      ],
      "year": "2018",
      "venue": "2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)"
    },
    {
      "citation_id": "107",
      "title": "Emotion recognition based on customized smart bracelet with built-in accelerometer",
      "authors": [
        "Z Zhang",
        "Y Song",
        "L Cui",
        "X Liu",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ",
      "doi": "10.7717/peerj.2258"
    },
    {
      "citation_id": "108",
      "title": "Emotion recognition using smart watch sensor data: Mixed-design study",
      "authors": [
        "J Quiroz",
        "E Geangu",
        "M Yong"
      ],
      "year": "2018",
      "venue": "JMIR Ment Health"
    },
    {
      "citation_id": "109",
      "title": "Emotion-recognition using smart watch accelerometer data: Preliminary findings",
      "authors": [
        "J Quiroz",
        "M Yong",
        "E Geangu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "110",
      "title": "Emotion recognition through gait on mobile devices",
      "authors": [
        "M Chiu",
        "J Shu",
        "P Hui"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "111",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "112",
      "title": "Motion capture sensor-based emotion recognition using a bi-modular sequential neural network",
      "authors": [
        "Y Bhatia",
        "A Bari",
        "G.-S Hsu",
        "M Gavrilova"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "113",
      "title": "Hierarchical-attention-based neural network for gait emotion recognition",
      "authors": [
        "S Zhang",
        "J Zhang",
        "W Song",
        "L Yang",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "114",
      "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
      "authors": [
        "U Bhattacharya",
        "C Roncal",
        "T Mittal",
        "R Chandra",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "115",
      "title": "Vfl-a deep learning-based framework for classifying walking gaits into emotions",
      "authors": [
        "X Sun",
        "K Su",
        "C Fan"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "116",
      "title": "Publication date",
      "authors": [
        "J Acm"
      ],
      "year": "2025",
      "venue": "Emotion Recognition from Skeleton Data: A Comprehensive Survey"
    },
    {
      "citation_id": "117",
      "title": "Tntc: two-stream network with transformer-based complementarity for gait-based emotion recognition",
      "authors": [
        "C Hu",
        "W Sheng",
        "B Dong",
        "X Li"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "118",
      "title": "Temporal group attention network with affective complementary learning for gait emotion recognition",
      "authors": [
        "Z Zhu",
        "C Chen",
        "H Liu",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "119",
      "title": "Tt-gcn: Temporal-tightly graph convolutional network for emotion recognition from gaits",
      "authors": [
        "T Zhang",
        "Y Chen",
        "S Li",
        "X Hu",
        "C Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "120",
      "title": "Looking into gait for perceiving emotions via bilateral posture and movement graph convolutional networks",
      "authors": [
        "Y Zhai",
        "G Jia",
        "Y.-K Lai",
        "J Zhang",
        "J Yang",
        "D Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "121",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "V Narayanan",
        "B Manoghar",
        "V Dorbala",
        "D Manocha",
        "A Bera"
      ],
      "year": "2020",
      "venue": "IROS"
    },
    {
      "citation_id": "122",
      "title": "G-gcsn: Global graph convolution shrinkage network for emotion perception from gait",
      "authors": [
        "Y Zhuang",
        "L Lin",
        "R Tong",
        "J Liu",
        "Y Iwamoto",
        "Y.-W Chen"
      ],
      "venue": "ACCV Workshops, 2021"
    },
    {
      "citation_id": "123",
      "title": "Epic: Emotion perception by spatio-temporal interaction context of gait",
      "authors": [
        "H Lu",
        "S Xu",
        "S Zhao",
        "X Hu",
        "R Ma",
        "B Hu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "124",
      "title": "Msa-gcn: Multiscale adaptive graph convolution network for gait emotion recognition",
      "authors": [
        "Y Yin",
        "L Jing",
        "F Huang",
        "G Yang",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "125",
      "title": "Sta-gcn:spatial temporal adaptive graph convolutional network for gait emotion recognition",
      "authors": [
        "C Chen",
        "X Sun"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "126",
      "title": "Gaitcycformer: Leveraging gait cycles and transformers for gait emotion recognition",
      "authors": [
        "Q Zeng",
        "L Shang"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "127",
      "title": "See your emotion from gait using unlabeled skeleton data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "AAAI"
    },
    {
      "citation_id": "128",
      "title": "Self-supervised gait-based emotion representation learning from selective strongly augmented skeleton sequences",
      "authors": [
        "C Song",
        "L Lu",
        "Z Ke",
        "L Gao",
        "S Ding"
      ],
      "year": "2024",
      "venue": "Self-supervised gait-based emotion representation learning from selective strongly augmented skeleton sequences",
      "arxiv": "arXiv:2405.04900"
    },
    {
      "citation_id": "129",
      "title": "See your mental state from your walk: Recognizing anxiety and depression through kinect-recorded gait data",
      "authors": [
        "N Zhao",
        "Z Zhang",
        "Y Wang",
        "J Wang",
        "B Li",
        "T Zhu",
        "Y Xiang"
      ],
      "year": "2019",
      "venue": "PloS one"
    },
    {
      "citation_id": "130",
      "title": "Depression identification from gait spectrum features based on hilbert-huang transform",
      "authors": [
        "Y Yuan",
        "B Li",
        "N Wang",
        "Q Ye",
        "Y Liu",
        "T Zhu"
      ],
      "year": "2018",
      "venue": "Human Centered Computing: 4th International Conference"
    },
    {
      "citation_id": "131",
      "title": "Detecting depression through gait data: examining the contribution of gait features in recognizing depression",
      "authors": [
        "Y Wang",
        "J Wang",
        "X Liu",
        "T Zhu"
      ],
      "year": "2021",
      "venue": "Frontiers in psychiatry"
    },
    {
      "citation_id": "132",
      "title": "A new skeletal representation based on gait for depression detection",
      "authors": [
        "H Lu",
        "W Shao",
        "E Ngai",
        "X Hu",
        "B Hu"
      ],
      "year": "2021",
      "venue": "2020 IEEE International Conference on E-health Networking, Application Services (HEALTHCOM)"
    },
    {
      "citation_id": "133",
      "title": "Postgraduate student depression assessment by multimedia gait analysis",
      "authors": [
        "H Lu",
        "S Xu",
        "X Hu",
        "E Ngai",
        "Y Guo",
        "W Wang",
        "B Hu"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "134",
      "title": "Depression prevalence in postgraduate students and its association with gait abnormality",
      "authors": [
        "J Fang",
        "T Wang",
        "C Li",
        "X Hu",
        "E Ngai",
        "B.-C Seet",
        "J Cheng",
        "Y Guo",
        "X Jiang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "135",
      "title": "A gait assessment framework for depression detection using kinect sensors",
      "authors": [
        "T Wang",
        "C Li",
        "C Wu",
        "C Zhao",
        "J Sun",
        "H Peng",
        "X Hu",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "136",
      "title": "Data augmentation for depression detection using skeleton-based gait information",
      "authors": [
        "J Yang",
        "H Lu",
        "C Li",
        "X Hu",
        "B Hu"
      ],
      "year": "2022",
      "venue": "Medical & Biological Engineering & Computing"
    },
    {
      "citation_id": "137",
      "title": "A multi-modal gait analysis-based detection system of the risk of depression",
      "authors": [
        "W Shao",
        "Z You",
        "L Liang",
        "X Hu",
        "C Li",
        "W Wang",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "138",
      "title": "Depression risk recognition based on gait: A benchmark",
      "authors": [
        "X Liu",
        "Q Li",
        "S Hou",
        "M Ren",
        "X Hu",
        "Y Huang"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "139",
      "title": "Spatio-temporal multi-granularity for skeleton-based depression risk recognition",
      "authors": [
        "Q Li",
        "M Ren",
        "X Hu",
        "X Liu",
        "L Yao",
        "Y Huang"
      ],
      "year": "2025",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "140",
      "title": "The role of emotion regulation in autism spectrum disorder",
      "authors": [
        "C Mazefsky",
        "J Herrington",
        "M Siegel",
        "A Scarpa",
        "B Maddox",
        "L Scahill",
        "S White"
      ],
      "year": "2013",
      "venue": "Journal of the American Academy of Child & Adolescent Psychiatry"
    },
    {
      "citation_id": "141",
      "title": "Generating 3d dataset of gait and full body movement of children with autism spectrum disorders collected by kinect v2 camera",
      "authors": [
        "A Al-Jubouri",
        "I Ali",
        "Y Rajihy"
      ],
      "year": "2020",
      "venue": "Compusoft"
    },
    {
      "citation_id": "142",
      "title": "Application of skeleton data and long short-term memory in action recognition of children with autism spectrum disorder",
      "authors": [
        "Y Zhang",
        "Y Tian",
        "P Wu",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "143",
      "title": "Human gesture and gait analysis for autism detection",
      "authors": [
        "S Zahan",
        "Z Gilani",
        "G Hassan",
        "A Mian"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "144",
      "title": "Body posture based detection of autism spectrum disorder in children",
      "authors": [
        "M Yang",
        "M Ni",
        "T Su",
        "W Zhou",
        "Y She",
        "W Zheng",
        "B Hu"
      ],
      "year": "2025",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "145",
      "title": "Skeletal video anomaly detection using deep learning: Survey, challenges, and future directions",
      "authors": [
        "P Mishra",
        "A Mihailidis",
        "S Khan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "146",
      "title": "Skeleton based abnormal behavior recognition using spatio-temporal convolution and attention-based lstm",
      "authors": [
        "K Zhou",
        "T Wu",
        "C Wang",
        "J Wang",
        "C Li"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "147",
      "title": "Learning regularity in skeleton trajectories for anomaly detection in videos",
      "authors": [
        "R Morais",
        "V Le",
        "T Tran",
        "B Saha",
        "M Mansour",
        "S Venkatesh"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "148",
      "title": "Graph embedded pose clustering for anomaly detection",
      "authors": [
        "A Markovitz",
        "G Sharir",
        "I Friedman",
        "L Zelnik-Manor",
        "S Avidan"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "149",
      "title": "A self-attention augmented graph convolutional clustering networks for skeleton-based video anomaly behavior detection",
      "authors": [
        "C Liu",
        "R Fu",
        "Y Li",
        "Y Gao",
        "L Shi",
        "W Li"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "150",
      "title": "Contracting skeletal kinematics for human-related video anomaly detection",
      "authors": [
        "A Flaborea",
        "G Di Melendugno",
        "S D'arrigo",
        "M Sterpa",
        "A Sampieri",
        "F Galasso"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "151",
      "title": "Graph-jigsaw conditioned diffusion model for skeleton-based video anomaly detection",
      "authors": [
        "A Karami",
        "T Ho",
        "N Armanfard"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "152",
      "title": "Prompt-guided zero-shot anomaly action recognition using pretrained deep skeleton features",
      "authors": [
        "F Sato",
        "R Hachiuma",
        "T Sekii"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "153",
      "title": "Language-skeleton pre-training to collaborate with self-supervised human action recognition",
      "authors": [
        "Y Liu",
        "R Liu",
        "W Xin",
        "Q Miao",
        "Y Hu",
        "J Qi"
      ],
      "year": "2024",
      "venue": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV)"
    },
    {
      "citation_id": "154",
      "title": "A multimodal non-intrusive stress monitoring from the pleasure-arousal emotional dimensions",
      "authors": [
        "M Dahmane",
        "J Alam",
        "P.-L St-Charles",
        "M Lalonde",
        "K Heffner",
        "S Foucher"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "155",
      "title": "Development and application of emotion recognition technology-a systematic literature review",
      "authors": [
        "R Guo",
        "H Guo",
        "L Wang",
        "M Chen",
        "D Yang",
        "B Li"
      ],
      "year": "2024",
      "venue": "BMC psychology"
    },
    {
      "citation_id": "156",
      "title": "Skeleton-based pre-training with discrete labels for emotion recognition in iot environments",
      "authors": [
        "Z Zhang",
        "F Liang",
        "W Wang",
        "R Zeng",
        "V Leung",
        "X Hu"
      ],
      "year": "2025",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "157",
      "title": "Arbee: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "158",
      "title": "Decoding group emotional dynamics in a web-based collaborative environment: A novel framework utilizing multi-person facial expression recognition",
      "authors": [
        "Q Li",
        "Z Liu",
        "Z Zhang",
        "Q Wang",
        "M Ma"
      ],
      "year": "2025",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "159",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "160",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "161",
      "title": "Multimodal emotion recognition based on facial expressions, speech, and body gestures",
      "authors": [
        "J Yan",
        "P Li",
        "C Du",
        "K Zhu",
        "X Zhou",
        "Y Liu",
        "J Wei"
      ],
      "year": "2024",
      "venue": "Electronics (2079-9292)"
    },
    {
      "citation_id": "162",
      "title": "Enhancing emotion recognition using multimodal fusion of physiological, environmental, personal data",
      "authors": [
        "H Kim",
        "T Hong"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "163",
      "title": "Chatgpt: An ai language model",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Chatgpt: An ai language model"
    },
    {
      "citation_id": "164",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "165",
      "title": "technical report detailing the Gemini 1.5 Pro architecture and performance benchmarks",
      "authors": [
        "G Research"
      ],
      "year": "2024",
      "venue": "arXiv"
    }
  ]
}