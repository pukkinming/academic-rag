{
  "paper_id": "2303.05134v1",
  "title": "Hierarchical Network With Decoupled Knowledge Distillation For Speech Emotion Recognition",
  "published": "2023-03-09T09:40:45Z",
  "authors": [
    "Ziping Zhao",
    "Huan Wang",
    "Haishuai Wang",
    "Bjorn Schuller"
  ],
  "keywords": [
    "speech emotion recognition",
    "decoupled knowledge distillation",
    "multi-head attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The goal of Speech Emotion Recognition (SER) is to enable computers to recognize the emotion category of a given utterance in the same way that humans do. The accuracy of SER is strongly dependent on the validity of the utterance-level representation obtained by the model. Nevertheless, the \"dark knowledge\" carried by non-target classes is always ignored by previous studies. In this paper, we propose a hierarchical network, called DKDFMH, which employs decoupled knowledge distillation in a deep convolutional neural network with a fused multi-head attention mechanism. Our approach applies logit distillation to obtain higher-level semantic features from different scales of attention sets and delve into the knowledge carried by non-target classes, thus guiding the model to focus more on the differences between sentiment features. To validate the effectiveness of our model, we conducted experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved competitive performance, with 79.1 % weighted accuracy (WA) and 77.1 % unweighted accuracy (UA). To the best of our knowledge, this is the first time since 2015 that logit distillation has been returned to state-of-the-art status.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As speech is one of the most natural and direct ways for humans to express emotions, speech emotion recognition (SER) is widely used in fields including online education  [1] , psychological healthcare  [2] , mobile services  [3] , etc. The goal of speech emotion recognition is to make the computer extract the emotion that the speaker is expressing from a given utterance. With the development of deep learning, algorithms such as convolutional neural network (CNN), recurrent neural network (RNN), and long short-term memory (LSTM) have been applied to SER tasks  [4, 5, 6]  to extract abstract features The present work is supported by the National Natural Science Foundation of China (No. 62071330). and learn correlations between frames. However, there are still substantial challenges associated with extracting highlevel features and precisely classifying emotions. We suggest that logit distillation enables the model to learn knowledge of speech emotion from target to non-target classes, and subsequently acquire higher-level semantic features. However, the cross-entropy in each of these methods focuses only on the target class and ignores the non-target classes.\n\nTo address these challenges, we propose a model based on decoupled knowledge distillation with a fused multi-head attention mechanism, namely DKDFMH. The fused multi-head attention mechanism fuses multiple heads in a feature point to facilitate better characterization of the relationship between features. Our motivation for proposing decoupled knowledge distillation for SER is as follows: transferring \"dark knowledge\" to students via the teacher's soft labels, and increasing logit similarity between the student and the teacher, which enhances the discriminability of various types of sentiment features.\n\nThe main contributions of this paper can be summarized as follows:\n\n1) To the best of our knowledge, this is the first time that decoupled knowledge distillation has been applied to SER.\n\n2) We use decoupled knowledge distillation to overcome the limitations of classical knowledge distillation, utilizing \"dark knowledge\" to reduce emotion misclassification and improve accuracy by about 2.9 % relative to the current stateof-the-art methods on the IEMOCAP dataset.\n\n3) By decoupling knowledge distillation with a fused multi-head attention mechanism, our model can achieve a weighted accuracy (WA) of 79.1 % and unweighted accuracy (UA) of 77.1 % on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "As the field of deep learning has continued to develop, the process of speech emotion recognition has advanced. Acoustic features have progressed from the initial hand-crafted features to the point that specific levels of features can be extracted today. In 2016, Lim et al.  [7]  proposed a method for extracting audio features by combining CNN with RNN, which yielded results with higher accuracy than could be achieved by traditional manual classification methods.\n\nSubsequently, the attention mechanism and transformer facilitated significant development in various fields. Aiming to focus more on emotion-related information and reduce the influence of irrelevant external factors, Chen et al.  [8]  proposed a 3-D ACRNN model that combines CNN, Bi-LSTM, and attention mechanisms. Head Fusion was proposed in  [9]  by fusing multi-attention heads in the same attention map. In the field of SER,  [10, 11, 12]  have shown that the attention mechanism performs well on several datasets, highlighting its effectiveness for sentiment classification.\n\nTo learn long-term dependencies in speech signals, Zhao et al.  [13]  introduce a self-attention-based knowledge transfer network, in which teacher models learn from speech recognition in order to transfer attention to speech emotion recognition. In previous work, knowledge distillation has been applied to image classification  [14]  and speech recognition  [15] , among other fields. In conventional knowledge distillation models, a larger teacher model guides a smaller student model through training by minimizing the loss between the teacher and student models. By contrast, in our approach, the teacher and student models are of the same size, and the weights of the loss values between them can be freely allocated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "In 2015, Hinton et al.  [16]  proposed the concept of Knowledge Distillation (KD) to transfer knowledge by minimizing the Kullback-Leibler divergence between the predicted logit of the teacher and student models. Knowledge distillation was subsequently validated on tasks such as speech recognition and image recognition. KD constructively introduces the distillation temperature T into the softmax equation, along with the classification probability Q = [q 1 , q 2 , . . . , q i , . . . , q C ] ∈ R 1×C ; the probability of the i-th class is written as q i , the number of classes is written as C, the logit of the i-th class is written as z i , such that q i is calculated as follows:\n\ntypically, T is set to 1, with one-hot classification used for the label. However, this form of labeling makes the training of the neural network too absolute, meaning that information contained in the incorrect classes is lost. In contrast, when the temperature of T is set to a value greater than 1, the classes whose probability was previously suppressed to 0 will also have a small probability proportional to the distillation temperature. However, this method still limits the potential of logit distillation, as will be discussed in section 3.3. 1. Illustration of our DKDFMH model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Teacher And Student Models",
      "text": "In cases where the dataset is not too large, and as suggested in the article published by Ji et al.  [17]  in 2021, better results can be obtained when the same model is used for both the teacher and student networks. Therefore, we opted to follow this approach. As shown in Fig.  1 , the proposed model is a convolutional neural network with a fused multi-head attention mechanism, which contains a total of five convolutional layers and one attention layer. The module input is the log filter bank coefficients (logF-Bank) spectrum extracted by the Python speech features audio processing library. In the first layer, two parallel convolution layers with kernel sizes of (10,2) and (2,8) are used to extract textures from the temporal and spectral axes, respectively. Each subsequent convolutional layer is followed by batch normalization, with convolutional layers 2 and 3 followed by a max pooling layer with a kernel size of 2 to scale down the data size. After four convolutional layers, we get an 80-channel output, which is fed into the multi-head attention layer to obtain an attention map that maps several different feature attention points, and finally to the fully connected layer for classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Decoupled Knowledge Distillation",
      "text": "Decoupled Knowledge Distillation (DKD), first proposed by Zhao et al.  [18]  in 2022, is the state-of-the-art logit distillation approach. In this paper, we extend decoupled knowledge distillation to the SER task. DKD decouples the logits output by KD into two parts, as in Eq.2, using the binary probability b and the probability p between non-target classes. T and S represent teachers and students, respectively. The loss function uses Kullback-Leibler (KL) divergence  [19] , which is defined as follows:\n\nwhere the knowledge distillation loss is reformulated into the target and non-target classes. KL(b T ||b S ) represents the similarity of the binary probabilities of the teacher and student for the target class in logit distillation, namely TCKD. KL( p T || p S ) represents the similarity of the teacher's and the student's probabilities for the non-target class, namely NCKD. From this, Eq.2 can be rewritten as follows:\n\nwhere p T t represents the prediction confidence of the teacher network. TCKD is Target Class Knowledge Distillation, while NCKD is Non-Target Class Knowledge Distillation, which transfers \"dark knowledge\" via non-target logits.\n\nHowever, as can be seen from Eq.3, NCKD and p T t are coupled, and the contribution of NCKD to knowledge distillation is suppressed. To solve the above problem, we use two hyperparameters, α and β, which are capable of independently adjusting TCKD and NCKD so as to maximize the instruction they provide to the student network. The DKD loss formula is as follows:\n\nwhere β replaces (1 -p T t ), which would be suppressed. As shown in Fig.  2 , DKD provides weights that enable free balance, allowing the model to learn the most appropriate loss.\n\nIn the SER context, some emotions have characteristics that closely resemble each other, making it quite challenging for conventional models to distinguish between these emotions. For example, happiness and anger are often misclassi-fied in the predictions. Notably, however, our model can acquire higher-level semantic features via DKD loss and thereby guide the teacher model to deliver more \"dark knowledge\" to the student model. Moreover, compared to feature distillation, which requires additional computation, storage, and complex structures to align dimensions, logit distillation is also simpler and more efficient.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We use the well-benchmarked corpus Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [20] , which was collected by the University of Southern California. IEMOCAP contains five sessions, each performed by a pair of subjects (one male and one female) in scripted and improvised scenarios. The dataset contains approximately 12 hours of audiovisual data. The average duration of each voice segment is 4.5 sec.\n\nIn previous experiments  [21, 22] , the accuracy on improvised data was higher than that on scripted data; this may be because the actors delivered more emotionally realistic performances during improvisation. In this paper, we choose to use improvised data, with four types of emotions: angry, happy, neutral, and sad. Due to the imbalanced data distribution and the fact that excitement and happiness data tend to be highly similar in the activation and valence domains, most researchers choose to either replace excitement with happiness or to combine the data for both; our experiments use the former approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Weighted accuracy (WA) and unweighted accuracy (UA) are employed to validate the predictive performance of our proposed model. UA is calculated as the average accuracy of the emotional categories, while WA is calculated as the accuracy of all samples. These two evaluation methods are both widely utilized in contemporary SER research.  We implement our proposed model in PyTorch and randomly split the dataset into 80% for training and 20% for testing. We use LogFBank extracted by 40 filters as the feature input, which yields a 197-dimension feature vector. Compared with MFCC, this approach requires comparatively less computation, and the correlation of each feature is made stronger.\n\nIn the feature extraction process, we set the window length to 0.04 sec and the step size to 0.01 sec. Each utterance is divided into 2-sec segments, while there is a 1-sec overlap between each segment in the training set and a 1.6sec overlap in the testing set. The DKD loss is employed in our experiments; α is set to 1, β is set to 8, and the distillation temperature T is set to 4. The optimizer used is Adam, and the system is trained with a batch size of 32 for 50 epochs. The initial learning rate is 1 × 10 -4 , and the decay rate is 1 × 10 -6 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "Ablation study In this section, we verify the effectiveness of DKDFMH through ablation experiments: the model only using CNN, the model using CNN and multi-head attention, the model with knowledge distillation added, the model with TCKD separately, the model with NCKD separately, and the model with decoupled knowledge distillation.\n\nAs can be seen from Table1, using decoupled knowledge distillation resulted in the highest accuracy. The addition of the knowledge distillation improves the weighted accuracy by nearly 1.0%, indicating that the incorrect classes also contain information. We use the results of the knowledge distillation model as the baseline, then compare this with the results of the experiments using TCKD and NCKD respectively.\n\nIt is obvious from the experimental results that the use of TCKD as a separate model is detrimental to the distillation effect, reducing the accuracy of the model by 1.7% to 75.3%. In contrast, using NCKD separately for distillation is about the same or even better than using knowledge distillation, leading to a 0.7% to 77.7% increase in the weighted accuracy. We can accordingly conclude that knowledge between non-target classes is crucial for logit distillation, NCKD is the main reason why logit distillation is effective but suppressed; after decoupled KD, we can freely adjust the coefficients of TCKD and NCKD to produce better distillation performances. Table  2 . Comparison with state-of-the-art methods (%).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method Wa Ua",
      "text": "CTC+Attention  [10]  67.0 69.0 Head Fusion  [9]  76.2 76.4 HGFM  [23]  66.6 70.5 DAAE+CNN+Attention  [24]  70.1 70.7 HNSD  [25]  70.5 72.5 CNN-ELM+STC attention  [12]  61.3 60.  4  Multi-level Co-att  [11]  71.6 72.7 DKDFMH 79.1 77.1",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we apply decoupled knowledge distillation to SER and a convolutional neural network fusing multi-head attention to the teacher and student networks. A WA of 79.1% and a UA of 77.1% were obtained on the IEMOCAP dataset, proving the great potential of our proposed model. In the future, we will continue to study the application of knowledge distillation in SER. Moreover, in order to more closely replicate real-world SER scenarios, we will try to add noise to the audio data so as to improve the robustness of the model.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of our DKDFMH model.",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of Decoupled Knowledge Distillation.",
      "page": 3
    },
    {
      "caption": "Figure 3: Results of modifying β.",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion Matrix of IEMOCAP.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W\nA": "U\nA\n \nA\nC\nC"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W\nA": "U\nA\n \nA\nC\nC"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A realtime speech emotion recognition system and its application in online learning",
      "authors": [
        "Ling Cen",
        "Fei Wu",
        "Zhu Liang Yu",
        "Fengye Hu"
      ],
      "year": "2016",
      "venue": "Emotions, Technology, Design, and Learning"
    },
    {
      "citation_id": "3",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "Nicholas Cummins",
        "Stefan Scherer",
        "Jarek Krajewski",
        "Sebastian Schnieder",
        "Julien Epps",
        "Thomas Quatieri"
      ],
      "year": "2015",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "A study of speech emotion recognition and its application to mobile services",
      "authors": [
        "Won-Joong Yoon",
        "Youn-Ho Cho",
        "Kyu-Sik Park"
      ],
      "year": "2007",
      "venue": "Proc. International Conference on Ubiquitous Intelligence and Computing (UIC)"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Zixuan Peng",
        "Yu Lu",
        "Shengfeng Pan",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "Proc. 2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "Proc. 2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (AP-SIPA)"
    },
    {
      "citation_id": "9",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "10",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Attentionenhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Zhongtian Bao",
        "Zixing Zhang",
        "Nicholas Cummins",
        "Haishuai Wang",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech, Graz, Austria"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition with coattention based multi-level acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "Proc. 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "Lili Guo",
        "Longbiao Wang",
        "Chenglin Xu",
        "Jianwu Dang",
        "Eng Siong Chng",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Proc. 2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Self-attention transfer networks for speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Zhongtian Bao",
        "Zixing Zhang",
        "Nicholas Cummins",
        "Shihuang Sun",
        "Haishuai Wang",
        "Jianhua Tao",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    },
    {
      "citation_id": "15",
      "title": "Deepvid: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation",
      "authors": [
        "Junpeng Wang",
        "Liang Gou",
        "Wei Zhang",
        "Hao Yang",
        "Han-Wei Shen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "16",
      "title": "Distilling knowledge from ensembles of neural networks for speech recognition",
      "authors": [
        "Yevgen Chebotar",
        "Austin Waters"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Computer Science"
    },
    {
      "citation_id": "18",
      "title": "Show, attend and distill: Knowledge distillation via attention-based feature matching",
      "authors": [
        "Mingi Ji",
        "Byeongho Heo",
        "Sungrae Park"
      ],
      "year": "2021",
      "venue": "Proc. 35th AAAI Conference on Artificial Intelligence (AAAI), virtual"
    },
    {
      "citation_id": "19",
      "title": "Decoupled knowledge distillation",
      "authors": [
        "Borui Zhao",
        "Quan Cui",
        "Renjie Song",
        "Yiyu Qiu",
        "Jiajun Liang"
      ],
      "year": "2022",
      "venue": "Proc. 35th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "20",
      "title": "On information and sufficiency",
      "authors": [
        "Solomon Kullback",
        "Richard Leibler"
      ],
      "year": "1951",
      "venue": "The Annals of Mathematical Statistics"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "22",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "Lorenzo Tarantino",
        "Philip Garner",
        "Alexandros Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Hgfm : A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Yunfeng Xu",
        "Hua Xu",
        "Jiyun Zou"
      ],
      "year": "2020",
      "venue": "Proc. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "Yuan Gao",
        "Jiaxing Liu",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "Proc. 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Qi Cao",
        "Mixiao Hou",
        "Bingzhi Chen",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2021",
      "venue": "Proc. 2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}