{
  "paper_id": "2509.23795v1",
  "title": "An Efficient Transfer Learning Method Based On Adapter With Local Attributes For Speech Emotion Recognition",
  "published": "2025-09-28T10:36:59Z",
  "authors": [
    "Haoyu Song",
    "Ian McLoughlin",
    "Qing Gu",
    "Nan Jiang",
    "Yan Song"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing speech emotion recognition (SER) methods commonly suffer from the lack of high-quality large-scale corpus, partly due to the complex, psychological nature of emotion which makes accurate labeling difficult and time consuming. Recently, transfer learning based methods that exploit the encoders pretrained on large-scale speech corpus (e.g., Wav2Vec2.0 and HuBERT) have shown strong potential for downstream SER tasks. However, task-specific fine-tuning remains necessary for various conversational scenarios of different topics, speakers and languages to achieve satisfactory performance. It generally requires costly encoder retraining for individual SER tasks. To address this issue, we propose to train an adapter with local attributes for efficient transfer learning. Specifically, a weighted average pooling-Transformer (WAP-Transformer) is proposed as a lightweight backbone to enrich the frame-level representation. An adapter with teacher-student branches is exploited for taskagnostic transfer learning, where the student branch is jointly optimized via mask prediction and self-distillation objectives, and the teacher branch is obtained online from the student via exponential moving average (EMA). Meanwhile, local attributes are learned from the teacher branch via unsupervised clustering, which aims to act as a universal model that provides additional semantic-rich supervisions. A statistical attentive pooling (SAP) module is proposed to obtain utterance representation for finetuning. To evaluate the effectiveness of the proposed adapter with local attributes, extensive experiments have been conducted on IEMOCAP. Superior performance has been reported, compared to the previous state-of-the-art methods in similar settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) has gained considerable attention due to its potential in application scenarios like human-computer interaction, affective computing, and mental health monitoring  [1] ,  [2] . The task remains challenging, particularly in cross-session settings, where models must generalize to unseen speakers with diverse styles, prosodies, dialects, languages, and acoustic variations  [3] ,  [4] ,  [5] .\n\nUnlike automatic speech recognition (ASR), where phonetic content is relatively stable across different individuals  [6] , emotional expressions are inherently subjective and influenced by cultural background, speaker physiology, and recording conditions  [7] . It is thus necessary to collect a large-scale high-quality emotion corpus with wide coverage of speakers, languages and topics to guarantee the SER performance. However, due to the expensive annotation cost, existing SER methods  [8] ,  [9] ,  [10] ,  [11] ,  [12]  often struggle with few-shot learning and domain shift issues.\n\nRecently, transfer learning based SER methods, that employ pretrained Wav2Vec2.0 and HuBERT  [13] ,  [14]  as frontend encoders, have demonstrated the ability to address data scarcity issues  [15] ,  [16] ,  [17] . However, it is still necessary to retrain the encoder for individual SER tasks to achieve satisfying performance -something computationally complex and requiring a large memory space. In  [18] , Ren et al. presented a self-distillation method to simultaneously fine-tune a pretrained Wav2Vec2.0 model and train a shallow version of it for fast yet effective SER. In  [3] , a local prototypical mapping network (LPMN) was proposed to characterize the complex distribution of latent embedding spaces via an unsupervised learning method similar to VQ-VAE  [19] . A prototype selection scheme is used to reduce bias caused by irrelevant factors during post-processing for specific SER tasks.\n\nIn this paper, we propose to learn an adapter with local attributes for various SER tasks (e.g., that are cross-session validated using the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus  [20] ). As shown in Fig.  1 , a weighted average pooling Transformer (WAP-Transformer) is first proposed as a lightweight backbone to provide enriched framelevel representation. An adapter with teacher-student branches is then designed for self-supervised transfer learning. The student branch is optimized with a mask prediction objective similar to  [21] ,  [22] ,  [23] , from which the teacher branch are online updated via exponential moving average (EMA). Meanwhile, we perform self-distillation on masked frames and take the teacher network as an online tokenizer, from which the local attributes are simultaneously learned to characterize the frame-level feature distribution via batch-wise unsupervised clustering. In effect, the learned local attributes act as the universal model to enhance the task-agnostic adapter training for various downstream SER tasks, partly addressing the domain shift issues. Furthermore, a statistical attentive pooling (SAP) is proposed to aggregate the output of the adapter into an arXiv:2509.23795v1 [cs.SD] 28 Sep 2025 utterance-level representation, which is classified according to the specific SER task. It is worth noting that only the lightweight adapter and classifier need to be finetuned, which is more efficient than retraining the while encoder. We perform 5-fold cross-validation with cross-session settings for evaluation  [3] ,  [24] . Experimental results on the IEMOCAP  [20]  benchmark demonstrate superior performance, achieving 78.32%, 77.56% and 75.63% unweighted accuracy (UA), weighted accuracy (WA) and F1-score, respectively.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Overview Of Adapter With Local Attributes",
      "text": "The architecture of our proposed adapter with local attributes is shown in Fig.  1 , which consists of: 1) a pretrained encoder i.e.,HuBERT-large-960  [13] , 2) a lightweight WAP-Transformer, and 3) the teacher-student branches.\n\n1) Pretrained encoder: This consists of a 7-layer convolutional block, designed to capture low-level spectral and temporal cues, followed by a deep 24-layer Transformerbased context network with a hidden dimensions of 1024, 16 attention heads, and an intermediate feed-forward dimension of 4096. Pretrained on 960 hours of speech data, this model yields robust and general acoustic representations.\n\n2) Lightweight WAP-Transformer: This consists of 3 transformer layers, with the outputs of each layer being adaptively pooled to enrich the frame-level representation. The aim is to reduce the adapter model size. The WAP-Transformer input is a sequence denoted by X = X emb + E pos , where the embeddings X emb ∈ R T ×D are obtained by feeding the input waveform to the pretrained encoder. Following  [22] , a standard learnable 1D position embedding E pos ∈ R T ×D is added to frame-level, and T is sequence length. The output of the l-th transformer layer is conceptually defined as X l = T ransf ormer(X l-1 ). The output of WAP-transformer Z is\n\n3) Teacher-student branches: A Siamese structure with lightweight WAP-Transformer as backbone is exploited to perform self-supervised transfer learning on a given emotion corpus. As aforementioned, the student branch is optimized with both mask prediction and self-distillation objectives, and the teacher is ensembled from the students over epochs. The training process is detailed as follows, consisting of mask prediction, local attribute learning and frame-level self distillation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Adapter Training 1) Mask Prediction:",
      "text": "The input to the student branch X ′ is the masked version of sequence X, where 40% of positions M are randomly selected and replaced with learnable embedding e [M ] ∈ R D . By feeding X ′ to the student branch i.e., WAP-Transformer E s θ , the reconstructed sequence\n\nis obtained by predicting from the remaining visible ones according to context information. The input to the teacher branch E t θ is X, and the sequence Z t = {z t i } T i=1 is used as the target of the masked prediction for optimizing the student parameters θ s . The mask reconstruction loss is formulated as the mean square error of masked positions, which is\n\nAnd the parameter of teacher θ s is updated using EMA, that is\n\nwhere i denotes the learning iteration, and α is the smoothing hyperparameter, set to 0.999 by default. The large α is used to smooth the rapidly changing student parameters. This makes the teacher model more stable, avoids overfitting to noise, and provides a consistent signal that improves convergence and generalization.\n\nThis EMA smooths out the short-term noise in the student's rapidly changing parameters, producing a more stable teacher signal that accelerates convergence and improves generalization.\n\nIn  [22] ,  [13] , an offline additional tokenizer is further introduced to represent the utterance as a sequence of discrete tokens. We propose to learn the local-attribute prototypes directly from the teacher branch; these prototypes offer semantically rich supervision that complements the maskedprediction objective. Because each prototype is the quantiser output, its index k(z t ) naturally serves as the pseudo-label for frame-level self-distillation, while the student produces its logits by computing the (negative) distances between its own embedding and all prototypes (see next subsection).\n\n2) Local Attributes Learning: Local attributes P = {p 1 , p 2 , . . . , p K } are learned to characterize the frame-level feature distribution, which can be learned by a batch-wise unsupervised clustering method. Given a batch of the extracted frame-level embeddings Z t from the teacher branch, local attributes P can be learned as follows\n\nwhere z t ∈ p i denotes embeddings nearest to the i-th attribute p i . That is, we can assign z t to pseudo-label i, according to Euclidean distance\n\nAnd the corresponding attribute is updated as\n\nThe learned attributes may be considered as a universal model to characterize the distribution of frame-level embeddings. We further perform self-distillation to update the student branch with local attributes, besides the reconstruction loss L rec in eqn.  (1) .\n\n3) Frame-level Self-Distillation: We perform frame-level self-distillation on the masked positions M . From the teacher branch, we can obtain the pseudo-labels of Z t according to eqn.  (4) . For the student branch, the logits can be obtained by computing the cosine similarity between of embeddings Z s and local attributes P . The pseudo cross entropy (CE) loss L pce can then be computed to jointly optimize with L r ec, that is\n\nwhere λ is the hyperparameter for balance reconstruction and pseudo CE loss. A ramp-down scheme from 1.0 to 0.5 is applied to place more emphasis on L rec at the early stage, allowing the student to first learn stable representations by mimicking the teacher. As training progresses, the weight gradually shifts toward L pce , enabling the model to focus more on discriminative learning. The lower bound of 0.5 ensures that L rec still contributes throughout training, preventing the student from completely drifting away from the teacher guidance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Finetune On Ser Task",
      "text": "We follow  [24] ,  [3]  to further finetune the adapter for SER. An SAP module comprising a learnable convolution and a statistic pooling layer is proposed to exploit framelevel attention. The convolution layer weighs the frame-level features, while a statistic pooling layer collects the 1 st -and 2 nd -order weighted statistics. Specifically, let Z ∈ R T ×D be the output features for L frames with D dimensions. By feeding Z into a 1 × 1 convolution layer, followed by softmax activation along the time axes, an attentive weight map A ∈ R T × K can be obtained, with K attention heads. Statistical pooling is performed:\n\nwhere ⊙ denotes the Hadamard product. The µ and σ 2 are L 2 -normalized and concatenated as the input the final emotion classification layer. IEMOCAP 5-fold cross-validation is used for fine-tuning, where the dataset is divided into five sessions (each with 2 speakers, for a total of 10 speakers). In each fold, one session serves as the validation set while the remaining four sessions are used for training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset And Evaluation Metrics",
      "text": "IEMOCAP contains recordings of multiple actors in improvised and scripted dialogues with a wide variation in speaker and session characteristics  [20] . It is widely used for SER research and allows us to compare the effectiveness of adapter to other systems. We assessed performance using WA, UA, and macro F1 scores.\n\nWA computes a weighted average of per-class accuracy, while UA, as a simple average, better reflects fairness in the presence of class imbalance. The macro F1-score, computed as the average F1 across all classes, provides a balanced evaluation by giving equal weight to each class, which is particularly useful under class imbalance.\n\nThe results, presented below, confirm the effectiveness of proposed data augmentation strategy in enhancing robustness.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementation Details",
      "text": "As aforementioned, the adapter module exploits the WAP-Transformer backbone, acting on the output of the pretrained encoder (HuBERT-large-960), as shown in Fig.  1 . This begins with a patch embedding layer that projects 1024-dimensional inputs to 384 dimensions, followed by three Transformer blocks at the 384-dimensional level. A fourth block is a 3-layer MLP then applied for aggregation. A Siamese network based WAP-Transformer is designed as adapter, and corresponding K local attributes are learned as a universal model for selfdistillation. A SAP module with 4 attention heads is used to project a sequence of frame-level embeddings into utterancelevel representation for SER.\n\nAdapter training is conducted with a batch size of 96 over 100 epochs using the Adam optimizer with an initial learning rate of 1e-4 and a cosine learning rate scheduler. Fine-tuning also employs a batch size of 96 over 100 epochs, but uses cross-entropy loss for the primary emotion classification task with improvised dialogues, over five classes (with happy and excited merged to account for limited data). To further address data imbalance and leverage the reconstruction ability of our trained model, we apply data augmentation during fine-tuning by randomly masking parts of the embeddings for minority classes, thereby enhancing sample diversity and balancing class distributions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Comparison With State-Of-The-Art Methods",
      "text": "Table  I  presents 5-fold cross validation results in terms of UA, WA and F1-score, for adapter training and several state-of-the-art systems. Overall results demonstrate that our adapter can outperform existing methods on the challenging IEMOCAP dataset. We achieved a UA of 78.32% and an WA of 77.56%, surpassing previous approaches such as Coattention, W2v2-PT  [25] , Spk-norm  [26] , GLRF  [27] , and LPMN  [3]  in similar settings. For example, The competitive LPMN reported UA and WA values of 76.85% and 74.50% respectively. After a prototype selection for post processing, it can improve to 77.42% and 75.82%. Our proposed WAP-Transformer improves over LPMN-1024-s60 around 0.80% and 2.26% in terms of UA and WA metrics. These gains are particularly impressive given the inherent challenges of IEMOCAP, which include significant cross-session and crossspeaker variability that typically degrade performance. The results validate the effectiveness of combining a large-scaled pre-trained encoder, WAP-Transformer based adapter training, and fine-tuning.\n\nFig.  2  compares the utterance-level embedding spaces obtained from the same HuBERT encoder before and after frame-level self-distillation. (a) Without self-distillation, the clusters corresponding to the four emotions are loosely formed and partially overlapped -particularly between Neutral and Happy. (b) Introducing frame-level self-distillation leads to markedly tighter intra-class cohesion and larger inter-class margins, yielding a much cleaner separation of Sad and Angry from the other emotions while also reducing the number of outliers. These observations confirm that self-distillation effectively drives the encoder to focus on affect-relevant acoustic cues, thereby enhancing the discriminability of the learned representations for downstream SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ablation Study To Explore Key Components",
      "text": "To evaluate the impact of the key contributions in WAP-Transformer, we conduct an ablation study focusing on three critical factors:  (1)      II  indicates that a size of 1024 yields the best performance of 78.32% and 77.56% and 75.63% in UA, WA and F1-score. Reducing the LA size degrades performance, suggesting that a larger size is more effective at capturing fine-grained details, although it may also increase computational cost and the risk of overfitting.\n\nAblation studies collectively confirm that the component of the proposed approach -adapter training, an appropriate LA size, and careful integration of domain information all play a crucial role in improving performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "This paper has proposed WAP-Transformer to effectively reduce the gap between a large-scale pretrained HuBERT model and downstream SER tasks with limited corpus size. WAP-Transformer introduces a lightweight transformer (i.e., WAP-Transformer) based adapter, which contains a teacher and student branch. WAP-Transformer learns local attributes that can effectively capture the semantic-rich information related to speakers, content and styles for different tasks. After adapter training and adjustment, we finetune the adapter to perform SER tasks via an attentive pooling layer and a classification layer. 5-fold cross validation on IEMOCAP demonstrates superior performance of 78.32%, 77.56% and 75.63% for UA, WA and F1-scores respectively. Ablation studies confirm the effectiveness of the main contributions introduced by WAP-Transformer.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , a weighted",
      "page": 1
    },
    {
      "caption": "Figure 1: a) The architecture of adapter training with mask prediction and online self-distillation on extracted frame-level feature",
      "page": 2
    },
    {
      "caption": "Figure 1: , which consists of: 1) a pretrained",
      "page": 2
    },
    {
      "caption": "Figure 1: This begins",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE projections emotion embeddings on unseen test data. (a) Emotion embeddings obtained from pretrained encoder,",
      "page": 4
    },
    {
      "caption": "Figure 2: compares the utterance-level embedding spaces ob-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "jiang_nan@mail.ustc.edu.cn,\nsongy@ustc.edu.cn\nE-mail: qinggu6@mail.ustc.edu.cn,"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "Abstract—Existing speech emotion recognition (SER) methods\nlanguages\nand\ntopics\nto\nguarantee\nthe\nSER performance."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "commonly suffer from the lack of high-quality large-scale corpus,"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "However, due to the expensive annotation cost, existing SER"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "partly\ndue\nto\nthe\ncomplex,\npsychological\nnature\nof\nemotion"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "methods [8],\n[9],\n[10],\n[11],\n[12] often struggle with few-shot"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "which makes\naccurate\nlabeling\ndifficult\nand\ntime\nconsuming."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "learning and domain shift\nissues."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "Recently,\ntransfer\nlearning based methods\nthat\nexploit\nthe\nen-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "Recently,\ntransfer learning based SER methods,\nthat employ\ncoders pretrained on large-scale speech corpus (e.g., Wav2Vec2.0"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "and HuBERT) have shown strong potential for downstream SER\npretrained Wav2Vec2.0\nand HuBERT\n[13],\n[14]\nas\nfront-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "tasks. However,\ntask-specific fine-tuning\nremains necessary\nfor"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "end encoders, have demonstrated the\nability to address data"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "various\nconversational\nscenarios\nof\ndifferent\ntopics,\nspeakers"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "scarcity issues\n[15],\n[16],\n[17]. However,\nit\nis\nstill necessary"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "and languages\nto achieve satisfactory performance.\nIt generally"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "to\nretrain\nthe\nencoder\nfor\nindividual SER tasks\nto\nachieve"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "requires costly encoder retraining for individual SER tasks. To"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "satisfying performance – something computationally complex\naddress\nthis\nissue, we propose\nto\ntrain an adapter with local"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "attributes\nfor efficient\ntransfer learning. Specifically, a weighted\nand\nrequiring\na\nlarge memory\nspace.\nIn\n[18], Ren\net\nal."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "average pooling-Transformer (WAP-Transformer)\nis proposed as"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "presented a self-distillation method to simultaneously fine-tune"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "a lightweight backbone to enrich the frame-level representation."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "a pretrained Wav2Vec2.0 model and train a shallow version"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "An adapter with teacher-student branches\nis exploited for task-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "of\nit\nfor\nfast yet\neffective SER.\nIn [3],\na\nlocal prototypical"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "agnostic\ntransfer\nlearning, where\nthe\nstudent branch is\njointly"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "mapping network (LPMN) was proposed to characterize the\noptimized\nvia mask\nprediction\nand\nself-distillation\nobjectives,"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "and the teacher branch is obtained online from the student via\ncomplex distribution of\nlatent embedding spaces via an unsu-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "exponential moving average (EMA). Meanwhile,\nlocal attributes"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "pervised learning method similar to VQ-VAE [19]. A prototype"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "are learned from the teacher branch via unsupervised clustering,"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "selection scheme is used to reduce bias caused by irrelevant"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "which aims to act as a universal model\nthat provides additional"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "factors during post-processing for specific SER tasks."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "semantic-rich supervisions. A statistical attentive pooling (SAP)"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "In this paper, we propose\nto learn an adapter with local\nmodule is proposed to obtain utterance representation for fine-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "tuning. To evaluate the effectiveness of the proposed adapter with\nattributes\nfor various SER tasks\n(e.g.,\nthat\nare\ncross-session"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "local attributes,\nextensive\nexperiments have been conducted on"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "validated using the Interactive Emotional Dyadic Motion Cap-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "IEMOCAP. Superior performance has been reported, compared"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "ture (IEMOCAP) corpus [20]). As shown in Fig. 1, a weighted"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "to the previous state-of-the-art methods in similar settings."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "average pooling Transformer\n(WAP-Transformer)\nis first pro-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "posed as a lightweight backbone to provide enriched frame-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "I.\nINTRODUCTION"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "level representation. An adapter with teacher-student branches"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "Speech Emotion Recognition (SER) has gained considerable\nis\nthen\ndesigned\nfor\nself-supervised\ntransfer\nlearning. The"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "attention\ndue\nto\nits\npotential\nin\napplication\nscenarios\nlike\nstudent branch is optimized with a mask prediction objective"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "human-computer\ninteraction, affective computing, and mental\nsimilar\nto\n[21],\n[22],\n[23],\nfrom which\nthe\nteacher\nbranch"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "health monitoring\n[1],\n[2]. The\ntask\nremains\nchallenging,\nare online updated via\nexponential moving average\n(EMA)."
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "particularly in cross-session settings, where models must gen-\nMeanwhile, we perform self-distillation on masked frames and"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "eralize\nto\nunseen\nspeakers with\ndiverse\nstyles,\nprosodies,\ntake the teacher network as an online tokenizer, from which the"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "dialects,\nlanguages, and acoustic variations [3],\n[4],\n[5].\nlocal attributes are simultaneously learned to characterize the"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "Unlike automatic speech recognition (ASR), where phonetic\nframe-level\nfeature\ndistribution\nvia\nbatch-wise\nunsupervised"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "content\nis\nrelatively\nstable\nacross\ndifferent\nindividuals\n[6],\nclustering. In effect,\nthe learned local attributes act as the uni-"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "emotional expressions are inherently subjective and influenced\nversal model\nto enhance the task-agnostic adapter training for"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "by\ncultural\nbackground,\nspeaker\nphysiology,\nand\nrecording\nvarious downstream SER tasks, partly addressing the domain"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "conditions\n[7].\nIt\nis\nthus\nnecessary\nto\ncollect\na\nlarge-scale\nshift\nissues. Furthermore, a statistical attentive pooling (SAP)"
        },
        {
          "† School of\nInformation Science and Technology, University of Science and Technology of China, Hefei, China": "high-quality emotion corpus with wide coverage of speakers,\nis\nproposed\nto\naggregate\nthe\noutput\nof\nthe\nadapter\ninto\nan"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: a) The architecture of adapter": "space, b) The structure of WAP-Transformer used as backbone of adapter",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": ".",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "weighted averaged Z = (cid:80)L"
        },
        {
          "Fig. 1: a) The architecture of adapter": "utterance-level\nrepresentation, which\nis\nclassified\naccording",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "l=1 wlX l with learnable parameters"
        },
        {
          "Fig. 1: a) The architecture of adapter": "to\nthe\nspecific SER task.\nIt\nis worth\nnoting\nthat\nonly\nthe",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "W = [w1, ..., wL], where L = 3,"
        },
        {
          "Fig. 1: a) The architecture of adapter": "lightweight adapter and classifier need to be finetuned, which",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "3) Teacher-student\nbranches: A Siamese\nstructure with"
        },
        {
          "Fig. 1: a) The architecture of adapter": "is more efficient\nthan retraining the while encoder.",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "lightweight WAP-Transformer\nas\nbackbone\nis\nexploited\nto"
        },
        {
          "Fig. 1: a) The architecture of adapter": "We perform 5-fold cross-validation with cross-session set-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "perform self-supervised transfer\nlearning on a given emotion"
        },
        {
          "Fig. 1: a) The architecture of adapter": "tings\nfor\nevaluation\n[3],\n[24]. Experimental\nresults\non\nthe",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "corpus. As\naforementioned,\nthe\nstudent branch is optimized"
        },
        {
          "Fig. 1: a) The architecture of adapter": "IEMOCAP [20] benchmark demonstrate superior performance,",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "with both mask prediction and self-distillation objectives, and"
        },
        {
          "Fig. 1: a) The architecture of adapter": "achieving\n78.32%,\n77.56% and\n75.63% unweighted\naccu-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "the teacher\nis ensembled from the students over epochs. The"
        },
        {
          "Fig. 1: a) The architecture of adapter": "racy (UA), weighted accuracy (WA) and F1-score, respectively.",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "training process is detailed as follows, consisting of mask pre-"
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "diction, local attribute learning and frame-level self distillation."
        },
        {
          "Fig. 1: a) The architecture of adapter": "II. METHODOLOGY",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "B. Adapter training"
        },
        {
          "Fig. 1: a) The architecture of adapter": "A. Overview of Adapter with Local Attributes",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "1) Mask Prediction: The input\nto the student branch X ′\nis"
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "the masked version of sequence X, where 40% of positions M"
        },
        {
          "Fig. 1: a) The architecture of adapter": "The\narchitecture\nof\nour\nproposed\nadapter with\nlocal\nat-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "are randomly selected and replaced with learnable embedding"
        },
        {
          "Fig. 1: a) The architecture of adapter": "tributes is shown in Fig. 1, which consists of: 1) a pretrained",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "to the student branch i.e., WAP-\ne[M ] ∈ RD. By feeding X ′"
        },
        {
          "Fig. 1: a) The architecture of adapter": "encoder\ni.e.,HuBERT-large-960 [13], 2)\na\nlightweight WAP-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "Transformer Es\nthe\nreconstructed sequence Z s = {zs\ni }T"
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "i=1\nθ ,"
        },
        {
          "Fig. 1: a) The architecture of adapter": "Transformer, and 3)\nthe teacher-student branches.",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "is\nobtained\nby\npredicting\nfrom the\nremaining\nvisible\nones"
        },
        {
          "Fig. 1: a) The architecture of adapter": "1) Pretrained\nencoder:\nThis\nconsists\nof\na\n7-layer\ncon-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "according\nto\ncontext\ninformation. The\ninput\nto\nthe\nteacher"
        },
        {
          "Fig. 1: a) The architecture of adapter": "volutional block, designed to capture\nlow-level\nspectral\nand",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "branch Et\nis X, and the sequence Z t = {zt\nis used as\ni }T\ni=1"
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "θ"
        },
        {
          "Fig. 1: a) The architecture of adapter": "temporal\ncues,\nfollowed\nby\na\ndeep\n24-layer Transformer-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "the target of\nthe masked prediction for optimizing the student"
        },
        {
          "Fig. 1: a) The architecture of adapter": "based context network with a hidden dimensions of 1024, 16",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "parameters θs. The mask reconstruction loss is formulated as"
        },
        {
          "Fig. 1: a) The architecture of adapter": "attention heads, and an intermediate feed-forward dimension",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "the mean square error of masked positions, which is"
        },
        {
          "Fig. 1: a) The architecture of adapter": "of 4096. Pretrained on 960 hours of\nspeech data,\nthis model",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "1"
        },
        {
          "Fig. 1: a) The architecture of adapter": "yields robust and general acoustic representations.",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "(cid:88) i\n∥zt\n(1)\nLrec =\ni − zs\ni ∥2"
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "∥M ∥"
        },
        {
          "Fig. 1: a) The architecture of adapter": "2) Lightweight WAP-Transformer:\nThis\nconsists\nof\n3",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "∈M"
        },
        {
          "Fig. 1: a) The architecture of adapter": "transformer layers, with the outputs of each layer being adap-",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "And the parameter of\nteacher θs\nis updated using EMA,\nthat"
        },
        {
          "Fig. 1: a) The architecture of adapter": "tively pooled to enrich the frame-level representation. The aim",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "is"
        },
        {
          "Fig. 1: a) The architecture of adapter": "is\nto reduce\nthe\nadapter model\nsize. The WAP-Transformer",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "θi+1\n= αθi\n(2)\nt\nt + (1 − α)θi+1"
        },
        {
          "Fig. 1: a) The architecture of adapter": "input\nis\na\nsequence denoted by X = Xemb + Epos, where",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": ""
        },
        {
          "Fig. 1: a) The architecture of adapter": "the embeddings Xemb ∈ RT ×D are obtained by feeding the",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "where i denotes the learning iteration, and α is the smoothing"
        },
        {
          "Fig. 1: a) The architecture of adapter": "input waveform to the pretrained encoder. Following [22], a",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "hyperparameter, set\nto 0.999 by default. The large α is used to"
        },
        {
          "Fig. 1: a) The architecture of adapter": "standard learnable 1D position embedding Epos ∈ RT ×D is",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "smooth the rapidly changing student parameters. This makes"
        },
        {
          "Fig. 1: a) The architecture of adapter": "added to frame-level, and T is\nsequence length. The output",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "the teacher model more stable, avoids overfitting to noise, and"
        },
        {
          "Fig. 1: a) The architecture of adapter": "of\nthe l-th transformer\nlayer\nis conceptually defined as X l =",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "provides\na\nconsistent\nsignal\nthat\nimproves\nconvergence\nand"
        },
        {
          "Fig. 1: a) The architecture of adapter": "T ransf ormer(X l−1). The output of WAP-transformer Z is",
          "training with mask prediction and online self-distillation on extracted frame-level": "",
          "feature": "generalization."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "rapidly changing parameters, producing a more stable teacher",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "We\nfollow [24],\n[3]\nto\nfurther\nfinetune\nthe\nadapter\nfor"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "signal\nthat accelerates convergence and improves generaliza-",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "SER. An SAP module\ncomprising\na\nlearnable\nconvolution"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "tion.",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "and\na\nstatistic\npooling\nlayer\nis\nproposed\nto\nexploit\nframe-"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "In\n[22],\n[13],\nan\noffline\nadditional\ntokenizer\nis\nfurther",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "level attention. The convolution layer weighs\nthe frame-level"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "introduced to represent\nthe utterance as a sequence of discrete",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "features, while a statistic pooling layer collects\nthe 1st- and"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "tokens. We\npropose\nto\nlearn\nthe\nlocal-attribute\nprototypes",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "∈ RT ×D\n2nd-order weighted\nstatistics. Specifically,\nlet Z"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "directly from the\nteacher branch;\nthese prototypes offer\nse-",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "be\nthe\noutput\nfeatures\nfor L frames with D dimensions."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "mantically\nrich\nsupervision\nthat\ncomplements\nthe masked-",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "By feeding Z into a 1 × 1 convolution layer,\nfollowed by"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "prediction objective. Because each prototype is\nthe quantiser",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "softmax activation along the\ntime\naxes,\nan attentive weight"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "output,\nits\nindex k(zt) naturally serves\nas\nthe pseudo-label",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "map A ∈ RT × K can be obtained, with K attention heads."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "for\nframe-level self-distillation, while the student produces its",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "Statistical pooling is performed:"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "logits by computing the (negative) distances between its own",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "µ = Z T A"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "embedding and all prototypes (see next subsection).",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "(7)"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "P\n=\n2) Local\nAttributes\nLearning:\nLocal\nattributes",
          "C. Finetune on SER task": "σ2 = ZT (A ⊙ A) − (ZT A ⊙ ZT A)"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "learned to characterize\nthe\nframe-level\n{p1, p2, . . . , pK} are",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "where ⊙ denotes\nthe Hadamard product. The µ and σ2\nare"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "feature\ndistribution, which\ncan\nbe\nlearned\nby\na\nbatch-wise",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "the final emotion\nL2-normalized and concatenated as the input"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "unsupervised clustering method. Given a batch of the extracted",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "classification layer.\nIEMOCAP 5-fold cross-validation is used"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "frame-level\nembeddings Z t\nfrom the\nteacher\nbranch,\nlocal",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "for fine-tuning, where the dataset\nis divided into five sessions"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "attributes P can be learned as follows",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "(each with 2 speakers, for a total of 10 speakers). In each fold,"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "(cid:88)",
          "C. Finetune on SER task": "one session serves as\nthe validation set while the remaining"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "K(cid:88) k\n(3)\n∥zt − pk∥2\nLVQ =",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "four sessions are used for\ntraining."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "=1\nzt∈pk",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "III. EXPERIMENTS"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "where zt ∈ pi denotes embeddings nearest to the i−th attribute",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "is, we can assign zt\nto pseudo-label\ni, according to\npi. That",
          "C. Finetune on SER task": "A. Dataset and Evaluation Metrics"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "Euclidean distance",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "IEMOCAP contains recordings of multiple actors in impro-"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "vised and scripted dialogues with a wide variation in speaker"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "i = arg min\n(4)\n∥zt − pk∥2\n2,",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "k",
          "C. Finetune on SER task": "and\nsession\ncharacteristics\n[20].\nIt\nis widely\nused\nfor SER"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "And the corresponding attribute is updated as",
          "C. Finetune on SER task": "research and allows us to compare the effectiveness of adapter"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "to other\nsystems. We\nassessed performance using WA, UA,"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "p′\n(5)\ni ← pi + ηt(zt − pi)",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "and macro F1 scores."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "WA computes\na weighted\naverage\nof\nper-class\naccuracy,"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "The learned attributes may be considered as a universal model",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "while UA, as a simple average, better\nreflects\nfairness\nin the"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "to characterize the distribution of frame-level embeddings. We",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "presence of class\nimbalance. The macro F1-score, computed"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "further perform self-distillation to update the student branch",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "as\nthe\naverage\nF1\nacross\nall\nclasses,\nprovides\na\nbalanced"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "with local\nattributes, besides\nthe\nin\nreconstruction loss Lrec",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "evaluation\nby\ngiving\nequal weight\nto\neach\nclass, which\nis"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "eqn.(1).",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "particularly useful under class imbalance."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "3) Frame-level Self-Distillation: We perform frame-level",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "The\nresults, presented below,\nconfirm the\neffectiveness of"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "self-distillation on the masked positions M . From the teacher",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "proposed data augmentation strategy in enhancing robustness."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "branch, we can obtain the pseudo-labels of Z t\naccording to",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "eqn. (4). For the student branch,\nthe logits can be obtained by",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "B.\nImplementation details"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "computing the\ncosine\nsimilarity between of\nembeddings Z s",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "As aforementioned,\nthe adapter module exploits the WAP-"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "and local\nattributes P . The pseudo cross\nentropy (CE)\nloss",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "Transformer backbone, acting on the output of\nthe pretrained"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "that\nLpce can then be computed to jointly optimize with Lrec,",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "encoder (HuBERT-large-960), as shown in Fig. 1. This begins"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "is",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "with a patch embedding layer\nthat projects 1024-dimensional"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "(6)\nL = λLrec + (1 − λ)Lpce",
          "C. Finetune on SER task": ""
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "",
          "C. Finetune on SER task": "inputs\nto\n384\ndimensions,\nfollowed\nby\nthree\nTransformer"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "where λ is the hyperparameter\nfor balance reconstruction and",
          "C. Finetune on SER task": "blocks at the 384-dimensional level. A fourth block is a 3-layer"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "0.5\npseudo CE loss. A ramp-down\nscheme\nfrom 1.0\nto\nis",
          "C. Finetune on SER task": "MLP then applied for aggregation. A Siamese network based"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "applied to place more\nat\nthe\nearly stage,\nemphasis on Lrec",
          "C. Finetune on SER task": "WAP-Transformer\nis designed as adapter, and corresponding"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "allowing\nthe\nstudent\nto first\nlearn\nstable\nrepresentations\nby",
          "C. Finetune on SER task": "K local attributes are learned as a universal model\nfor\nself-"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "mimicking\nthe\nteacher. As\ntraining\nprogresses,\nthe weight",
          "C. Finetune on SER task": "distillation. A SAP module with 4 attention heads\nis used to"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "gradually shifts toward Lpce, enabling the model to focus more",
          "C. Finetune on SER task": "project a sequence of\nframe-level embeddings into utterance-"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "on discriminative learning. The lower bound of 0.5 ensures that",
          "C. Finetune on SER task": "level\nrepresentation for SER."
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "still contributes\nthroughout\ntraining, preventing the stu-\nLrec",
          "C. Finetune on SER task": "Adapter\ntraining is conducted with a batch size of 96 over"
        },
        {
          "This EMA smooths out\nthe short-term noise in the student’s": "dent from completely drifting away from the teacher guidance.",
          "C. Finetune on SER task": "100 epochs using the Adam optimizer with an initial\nlearning"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)": "Fig. 2:\nt-SNE projections emotion embeddings on unseen test data.",
          "(b)": "(a) Emotion embeddings obtained from pretrained encoder,"
        },
        {
          "(a)": "(b) Emotion embeddings obtained from our proposed adapter.",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "TABLE I: Validation scores of the proposed WAP-Transformer"
        },
        {
          "(a)": "rate of 1e-4 and a cosine learning rate scheduler. Fine-tuning",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "compared with state-of-the-art methods on IEMOCAP."
        },
        {
          "(a)": "also employs\na batch size of 96 over 100 epochs, but uses",
          "(b)": ""
        },
        {
          "(a)": "cross-entropy loss for\nthe primary emotion classification task",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "Method\nUA\nWA\nF1"
        },
        {
          "(a)": "with improvised dialogues, over five classes (with happy and",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "Co-att\n[25]\n72.70\n71.64\n–"
        },
        {
          "(a)": "excited merged to account for limited data). To further address",
          "(b)": "W2v2-PT [15]\n–\n67.20\n–"
        },
        {
          "(a)": "",
          "(b)": "Spk-norm [26]\n–\n74.20\n–"
        },
        {
          "(a)": "data imbalance and leverage the reconstruction ability of our",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "GLRF [27]\n73.31\n72.81\n72.92"
        },
        {
          "(a)": "trained model, we apply data augmentation during fine-tuning",
          "(b)": "LPMN-1024-s60 [3]\n77.42\n75.82\n75.71"
        },
        {
          "(a)": "",
          "(b)": "LPMN [3]\n76.85\n74.50\n74.88"
        },
        {
          "(a)": "by randomly masking parts of\nthe\nembeddings\nfor minority",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "WAP-Transformer\n(Ours)\n78.32\n77.56\n75.63"
        },
        {
          "(a)": "classes,\nthereby\nenhancing\nsample\ndiversity\nand\nbalancing",
          "(b)": ""
        },
        {
          "(a)": "class distributions.",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "and partially overlapped – particularly between Neutral\nand"
        },
        {
          "(a)": "C. Comparison with state-of-the-art methods",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "Happy.\n(b)\nIntroducing\nframe-level\nself-distillation\nleads\nto"
        },
        {
          "(a)": "Table\nI\npresents\n5-fold\ncross\nvalidation\nresults\nin\nterms",
          "(b)": "markedly\ntighter\nintra-class\ncohesion\nand\nlarger\ninter-class"
        },
        {
          "(a)": "of UA, WA and F1-score,\nfor\nadapter\ntraining\nand\nseveral",
          "(b)": "margins, yielding a much cleaner separation of Sad and Angry"
        },
        {
          "(a)": "state-of-the-art\nsystems. Overall\nresults demonstrate that our",
          "(b)": "from the other emotions while also reducing the number of"
        },
        {
          "(a)": "adapter can outperform existing methods on the challenging",
          "(b)": "outliers. These observations confirm that self-distillation effec-"
        },
        {
          "(a)": "IEMOCAP dataset. We\nachieved\na UA of\n78.32% and\nan",
          "(b)": "tively drives\nthe encoder\nto focus on affect-relevant acoustic"
        },
        {
          "(a)": "WA of 77.56%,\nsurpassing previous approaches\nsuch as Co-",
          "(b)": "cues,\nthereby\nenhancing\nthe\ndiscriminability\nof\nthe\nlearned"
        },
        {
          "(a)": "attention, W2v2-PT [25], Spk-norm [26], GLRF [27],\nand",
          "(b)": "representations for downstream SER."
        },
        {
          "(a)": "LPMN [3]\nin similar\nsettings. For example, The competitive",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "D. Ablation study to explore key components"
        },
        {
          "(a)": "LPMN reported UA and WA values of 76.85% and 74.50%",
          "(b)": ""
        },
        {
          "(a)": "respectively. After a prototype selection for post processing,",
          "(b)": "To evaluate\nthe\nimpact of\nthe key contributions\nin WAP-"
        },
        {
          "(a)": "it can improve to 77.42% and 75.82%. Our proposed WAP-",
          "(b)": "Transformer, we conduct an ablation study focusing on three"
        },
        {
          "(a)": "Transformer\nimproves\nover LPMN-1024-s60\naround\n0.80%",
          "(b)": "critical\nfactors:\n(1)\nthe\neffect of\nadapter\ntraining via WAP-"
        },
        {
          "(a)": "and\n2.26% in\nterms\nof UA and WA metrics. These\ngains",
          "(b)": "Transformer,\n(2)\nthe influence of\nlocal attribute (LA)\nsize in"
        },
        {
          "(a)": "are particularly impressive given the\ninherent\nchallenges of",
          "(b)": "the VQ module Table II\nsummarizes\nthe performance under"
        },
        {
          "(a)": "IEMOCAP, which include significant cross-session and cross-",
          "(b)": "different configurations."
        },
        {
          "(a)": "speaker\nvariability\nthat\ntypically\ndegrade\nperformance. The",
          "(b)": "Overall,\nthese ablation experiments reveal some useful find-"
        },
        {
          "(a)": "results validate the effectiveness of combining a large-scaled",
          "(b)": "ings. Firstly,\nadapter\ntraining can significantly boost perfor-"
        },
        {
          "(a)": "pre-trained encoder, WAP-Transformer based adapter training,",
          "(b)": "mance with WAP-Transformer. Without adapter\ntraining,\nthe"
        },
        {
          "(a)": "and fine-tuning.",
          "(b)": "model achieved a UA of 75.73%, a WA of 74.91%, and an F1"
        },
        {
          "(a)": "Fig. 2 compares\nthe utterance-level embedding spaces ob-",
          "(b)": "score of 72.90%. All metrics increased after adapter\ntraining,"
        },
        {
          "(a)": "tained\nfrom the\nsame HuBERT\nencoder\nbefore\nand\nafter",
          "(b)": "to 78.32% (UA), 77.56% (WA), and 75.63% (F1) respectively"
        },
        {
          "(a)": "frame-level\nself-distillation.\n(a) Without\nself-distillation,\nthe",
          "(b)": "– around 2% absolute\nincrease\nfor\neach. This\nconfirms\nthat"
        },
        {
          "(a)": "clusters corresponding to the four emotions are loosely formed",
          "(b)": "adapter training benefits from WAP-Transformer to learn local"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "p. 713, 2020."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[9] C.-S. Ahn, C. Kasun, S. Sivadas, and J. Rajapakse, “Recurrent multi-"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "head attention fusion network for combining audio and text\nfor speech"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "emotion recognition,” in Proc.\nInterspeech, pp. 744–748, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[10] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "recognition with co-attention based multi-level acoustic information,” in"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Proc. of\nICASSP, pp. 7367–7371, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[11] C. Fu, C. Liu, C. T. Ishi, and H. Ishiguro, “Maec: Multi-instance learning"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "with\nan\nadversarial\nauto-encoder-based\nclassifier\nfor\nspeech\nemotion"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "recognition,” in Proc. of\nICASSP, pp. 6299–6303, 2021."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[12]\nS. Mao, P. Ching, and T. Lee, “Deep Learning of Segment-Level Feature"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Representation with Multiple\nInstance Learning\nfor Utterance-Level"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Speech Emotion Recognition,” in Proc. of\nInterspeech, pp. 1686–1690,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "2019."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[13] W. N. Hsu, B. Bolte, Y. H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "A. Mohamed,\n“Hubert: Self-supervised speech representation learning"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "by masked prediction of hidden units,” pp. 3451–3460, 2021."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[14] A. Baevski, Y.\nZhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "A framework\nfor\nself-supervised\nlearning\nof\nspeech\nrepresentations,”"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Advances in neural\ninformation processing systems, vol. 33, pp. 12449–"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "12460, 2020."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[15]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "using wav2vec 2.0 embeddings,” arXiv preprint arXiv:2104.03502, 2021."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[16]\nL. Zhang, B. Schuller, and M. Chen, “Multimodal emotion recognition:"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Challenges and opportunities,” in Proc. of ICASSP, pp. 8094–8098, 2019."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[17] A. Geetha, T. Mala, D. Priyanka,\nand E. Uma,\n“Multimodal\nemotion"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "recognition with deep learning:\nadvancements,\nchallenges,\nand future"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "directions,” Information Fusion, vol. 105, p. 102218, 2024."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[18]\nZ. Ren, T. T. Nguyen, Y. Chang, and B. W. Schuller, “Fast yet effective"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "speech emotion recognition with self-distillation,” in Proc. of\nICASSP,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "pp. 1–5, 2023."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[19] A. van den Oord, O. Vinyals,\nand K. Kavukcuoglu,\n“Neural discrete"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "representation learning,” CoRR, vol. abs/1711.00937, 2017."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[20] Busso, Carlos, Bulut, Murtaza, Lee, C. Chun, Kazemzadeh, Abe, Mower,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Emily, Kim, Samuel, Chang, J. N., Lee, Sungbok, Narayanan, and S. S.,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "“IEMOCAP:\ninteractive\nemotional\ndyadic motion\ncapture\ndatabase,”"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Language Resources and Evaluation, vol. 42, no. 4, p. 335, 2008."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[21] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar,\nand R. Girshick,\n“Masked"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Proc.\nof\nautoencoders\nare\nscalable\nvision\nlearners,”\nin\nthe CVPR,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "pp. 16000–16009, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[22] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: Bert pre-training of image"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "transformers,” arXiv preprint arXiv:2106.08254, 2021."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[23]\nZ. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu,"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "“Simmim: A simple framework for masked image modeling,” in Proc."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "of CVPR, pp. 9653–9663, June 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[24] Y.-X. Xi, Y. Song, L.-R. Dai,\nI. McLoughlin,\nand L. Liu,\n“Frontend"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "attributes disentanglement\nfor speech emotion recognition,” in Proc. of"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "ICASSP, pp. 7712–7716, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[25] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech emotion"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "recognition with co-attention based multi-level acoustic information,” in"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "Proc. of\nICASSP, pp. 7367–7371, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[26]\nI. Gat, H. Aronowitz, W. Zhu, E. Morais,\nand R. Hoory,\n“Speaker"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "normalization for self-supervised speech emotion recognition,” in Proc."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "of\nICASSP, pp. 7342–7346, 2022."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "[27] C. Ding, J. Li, D. Zong, B. Li, T. Zhang, and Q. Zhou, “Stable speech"
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": "emotion recognition with head-k-pooling loss,” INTERSPEECH, 2023."
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        },
        {
          "recognition and analysis of iemocap database,” Electronics, vol. 9, no. 5,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "ponents and training settings."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "K attributes\nUA (%)\nWA (%)\nF1-score"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "K = 1024\n78.32\n77.56\n75.63"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "K = 512\n77.22\n76.93\n74.26"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "K = 256\n77.00\n76.21\n75.14"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "K = 128\n76.10\n76.28\n73.80"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "WAP-Transformer"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "(w/o Adapter Training)\n75.73\n74.91\n72.90"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "attributes,\ncapturing semantic-rich information like\nspeakers,"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "phonetic content and styles."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "When we consider\nlocal attributes (LA), Table II\nindicates"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "that a size of 1024 yields the best performance of 78.32% and"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "77.56% and 75.63% in UA, WA and F1-score. Reducing the"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "LA size degrades performance, suggesting that a larger size is"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "more effective at capturing fine-grained details, although it may"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "also increase computational cost and the risk of overfitting."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "Ablation studies collectively confirm that\nthe component of"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "the proposed approach – adapter\ntraining, an appropriate LA"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "size, and careful\nintegration of domain information all play a"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "crucial\nrole in improving performance."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "IV. CONCLUSION"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "This paper has proposed WAP-Transformer to effectively re-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "duce the gap between a large-scale pretrained HuBERT model"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "and downstream SER tasks with limited corpus\nsize. WAP-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "Transformer\nintroduces a lightweight\ntransformer\n(i.e., WAP-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "Transformer)\nbased\nadapter, which\ncontains\na\nteacher\nand"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "student branch. WAP-Transformer\nlearns\nlocal attributes\nthat"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "can effectively capture the semantic-rich information related to"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "speakers, content and styles for different\ntasks. After adapter"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "training and adjustment, we finetune\nthe\nadapter\nto perform"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "SER tasks via an attentive pooling layer and a classification"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "layer. 5-fold cross validation on IEMOCAP demonstrates su-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "perior performance of 78.32%, 77.56% and 75.63% for UA,"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "WA and F1-scores\nrespectively. Ablation studies confirm the"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "effectiveness of\nthe main contributions\nintroduced by WAP-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "Transformer."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "REFERENCES"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": ""
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "[1] B. Schuller, A. Batliner, S. Steidl, and D. Seppi, “Recognising realistic"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "emotions and affect\nin speech: State of\nthe art and lessons learnt\nfrom"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "the first challenge,” Speech Communication, vol. 53, no. 9-10, pp. 1062–"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "1087, 2011."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "[2]\nT. Reitmaier, E. Wallington, D. Kalarikalayil Raju, O. Klejch, J. Pearson,"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "M.\nJones,\nP. Bell,\nand\nS. Robinson,\n“Opportunities\nand\nchallenges"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "of\nautomatic\nspeech\nrecognition\nsystems\nfor\nlow-resource\nlanguage"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "speakers,” in Proc. of CHI, pp. 1–17, 2022."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "[3] Y. Xi, Y. Song, L. Dai, H. Song, and I. McLoughlin, “An effective local"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "prototypical mapping network for speech emotion recognition,” in Proc."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "of\nInterspeech, pp. 1055–1059, 2024."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "[4] Y. Yin, B. Huang, Y. Wu,\nand M. Soleymani,\n“Speaker-invariant\nad-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "ICMI\nversarial domain adaptation for emotion recognition,” in Proc. of"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "2020, pp. 481–490, 2020."
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "[5]\nS. Li,\nP.\nSong, L.\nJi, Y.\nJin,\nand W. Zheng,\n“A generalized\nsub-"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "space distribution adaptation framework for cross-corpus speech emotion"
        },
        {
          "TABLE II: Ablation studies on the impact of WAP–LA com-": "recognition,” in Proc. of\nICASSP, 2023."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Opportunities and challenges of automatic speech recognition systems for low-resource language speakers",
      "authors": [
        "T Reitmaier",
        "E Wallington",
        "D Kalarikalayil Raju",
        "O Klejch",
        "J Pearson",
        "M Jones",
        "P Bell",
        "S Robinson"
      ],
      "year": "2022",
      "venue": "Proc. of CHI"
    },
    {
      "citation_id": "3",
      "title": "An effective local prototypical mapping network for speech emotion recognition",
      "authors": [
        "Y Xi",
        "Y Song",
        "L Dai",
        "H Song",
        "I Mcloughlin"
      ],
      "year": "2024",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Speaker-invariant adversarial domain adaptation for emotion recognition",
      "authors": [
        "Y Yin",
        "B Huang",
        "Y Wu",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Proc. of ICMI 2020"
    },
    {
      "citation_id": "5",
      "title": "A generalized subspace distribution adaptation framework for cross-corpus speech emotion recognition",
      "authors": [
        "S Li",
        "P Song",
        "L Ji",
        "Y Jin",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "authors": [
        "G Hinton",
        "L Deng",
        "D Yu",
        "G Dahl",
        "A -R. Mohamed",
        "N Jaitly",
        "A Senior",
        "V Vanhoucke",
        "P Nguyen",
        "T Sainath"
      ],
      "year": "2012",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "7",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech communication"
    },
    {
      "citation_id": "8",
      "title": "Attention-lstm-attention model for speech emotion recognition and analysis of iemocap database",
      "authors": [
        "Y Yu",
        "Y.-J Kim"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "9",
      "title": "Recurrent multihead attention fusion network for combining audio and text for speech emotion recognition",
      "authors": [
        "C.-S Ahn",
        "C Kasun",
        "S Sivadas",
        "J Rajapakse"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Maec: Multi-instance learning with an adversarial auto-encoder-based classifier for speech emotion recognition",
      "authors": [
        "C Fu",
        "C Liu",
        "C Ishi",
        "H Ishiguro"
      ],
      "year": "2021",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "12",
      "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
      "authors": [
        "S Mao",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "14",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "16",
      "title": "Multimodal emotion recognition: Challenges and opportunities",
      "authors": [
        "L Zhang",
        "B Schuller",
        "M Chen"
      ],
      "year": "2019",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Fast yet effective speech emotion recognition with self-distillation",
      "authors": [
        "Z Ren",
        "T Nguyen",
        "Y Chang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Neural discrete representation learning",
      "authors": [
        "A Van Den Oord",
        "O Vinyals",
        "K Kavukcuoglu"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Bulut",
        "Murtaza",
        "C Lee",
        "Chun",
        "Kazemzadeh",
        "Abe",
        "Mower",
        "Emily",
        "Kim",
        "Chang Samuel",
        "J Lee",
        "Sungbok",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "21",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proc. of the CVPR"
    },
    {
      "citation_id": "22",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "S Piao",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Beit: Bert pre-training of image transformers",
      "arxiv": "arXiv:2106.08254"
    },
    {
      "citation_id": "23",
      "title": "Simmim: A simple framework for masked image modeling",
      "authors": [
        "Z Xie",
        "Z Zhang",
        "Y Cao",
        "Y Lin",
        "J Bao",
        "Z Yao",
        "Q Dai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "24",
      "title": "Frontend attributes disentanglement for speech emotion recognition",
      "authors": [
        "Y.-X Xi",
        "Y Song",
        "L.-R Dai",
        "I Mcloughlin",
        "L Liu"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "26",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu",
        "E Morais",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "27",
      "title": "Stable speech emotion recognition with head-k-pooling loss",
      "authors": [
        "C Ding",
        "J Li",
        "D Zong",
        "B Li",
        "T Zhang",
        "Q Zhou"
      ],
      "year": "2023",
      "venue": "Stable speech emotion recognition with head-k-pooling loss"
    }
  ]
}