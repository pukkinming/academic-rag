{
  "paper_id": "2005.07757v1",
  "title": "\"I Have Vxxx Bxx Connexxxn!\": Facing Packet Loss In Deep Speech Emotion Recognition",
  "published": "2020-05-15T19:33:40Z",
  "authors": [
    "Mostafa M. Mohamed",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Packet Loss",
    "Matched Condition",
    "End-to-End Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In applications that use emotion recognition via speech, frame-loss can be a severe issue given manifold applications, where the audio stream loses some data frames, for a variety of reasons like low bandwidth. In this contribution, we investigate for the first time the effects of frame-loss on the performance of emotion recognition via speech. Reproducible extensive experiments are reported on the popular RECOLA corpus using a state-of-the-art end-to-end deep neural network, which mainly consists of convolution blocks and recurrent layers. A simple environment based on a Markov Chain model is used to model the loss mechanism based on two main parameters. We explore matched, mismatched, and multi-condition training settings. As one expects, the matched setting yields the best performance, while the mismatched yields the lowest. Furthermore, frame-loss as a data augmentation technique is introduced as a general-purpose strategy to overcome the effects of frame-loss. It can be used during training, and we observed it to produce models that are more robust against frame-loss in run-time environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There is a rise of affective computing applications which predict emotions through speech or other signals like images. Such applications depend heavily on the quality of the audio streams of speech to correctly predict the emotions. In streaming applications, there are a variety of factors that could result in lower quality of data received, like lower data rate and packet loss  [1] , or varying throughput in mobile communication  [2] . In such applications, any issue like this that might happen, would cause a drop in the input streams which might lead to severe degradation in the performance of the application. Such a degradation could happen for a variety of reasons, for example, the dependency of some models on the audio context to predict the emotions of the succeeding time points, also when some models assume the continuity of the input speech. These are typical assumptions made by neural network models like  [3, 4] , because of the design of recurrent neural networks  [5] .\n\nThe impact of disturbances during automatic 'speech emotion recognition' (SER) has been investigated for speech in the presence of noise  [6] [7] [8] , reverberation  [7, 9] , or in narrowband transmission  [10]  and coded speech  [10, 11] . However, to the authors' best knowledge, no work exists that investigates the impact of packet (or frame) loss on SER. There are only a few papers addressing SER in VoIP setting  [12] , yet, not systematically investigating packet loss impact. This seems surprising, given that a main application of SER is found in call centres, and SER is currently finding its way onto mobile phones  [13] . Packet loss and its impact on speech processing has largely been studied so far in the context of automatic speech recognition  [14]  and enhanced in  [15] .\n\nThe main aim of this paper is to examine the effects of these frame-loss cases on the performance of models for emotion recognition via speech. In addition to that, an attempt to enhance such models to become more robust against frame-loss will be made.\n\nThis paper is divided as follows: Section 2 contains the details of the approach, Section 3 contains the experimental settings and the results, and Section 4 provides the conclusion of the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "The approach mainly uses an end-to-end model which predicts emotions (defined as two dimensions arousal and valence). The model is trained and tested under a variety of settings that are simulated by a mechanism modelling lossy environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Packet Loss Generation Model",
      "text": "In order to model the lossy and non-lossy packets -or, more precisely, frames in our case -in a given sequence, we adapt the Markov Chain  [16]  M(pL, pN) as shown in Figure  2 . This is a standard approach for packet loss modelling  [17] ; note, however, that also more complex models, e. g., three states have been used  [18] , for example, to model burst behaviour. Other models are also reviewed in a recent survey  [19] . Given a sequence of t frames, we can use M to sample a binary sequence of length t. This can be achieved by starting at the state N , then transitioning between the states N (for no-loss) and L (for loss) based on the transition probabilities pL and pN. This is done until t states are enumerated. Then, the sampled sequence of states is directly transformed into the binary string, by replacing N by 1 and L by 0.\n\nThe sampled binary string can be used to select elements from the given sequence, where the frames at positions with corresponding character '1' are the only frames taken. For example, a binary sequence 01101 would select the frames y2, y3, y5 from the sequence y1, y2, y3, y4, y5.\n\nThe intuition behind this model is that it can mimic a variety of possibilities. The value of pN models the overall stability of the system, in particular, how unlikely it is that a frame-loss error might occur. Additionally, the value of pL models the intensity of frame-loss when it occurs. High values of pL correspond to persistent errors that stay long. Different combinations of these can correspond to different possibilities as shown in Table  1 . An environment with a low bandwidth could be thought of as to have low values for both parameters, which mirrors a scenario of frequent non-persistent frame-loss issues. If both parameters have high values, this mirrors an environment with a low chance of a persistent breakdown event.\n\nFurthermore, we will need to drop frames from two sequences simultaneously, mainly when one is an input audio sequence X and the other is Y which consists of the output labels sequence. Even though, both correspond to the same duration of time, still the sample rate of X is higher than that of Y . For simplicity, it is assumed that the sample rate of X is a multiple of the sample rate of Y , with a multiplicative factor r. Based on that assumption, if we acquired a binary string MY from the model M to drop the frames of the output labels Y , then we can construct a mask MX to drop the corresponding elements of X. The mask MX is constructed by repeating each element of MY for r times in place. For example, if r = 3 and the mask '1011' is used to drop frames from Y , then X is dropped using '111000111111'. This mechanism ensures that the dropping of frames corresponds to the same time tags. Eventually, the given Markov Chain M(pL, pN ) will sample binary strings that have an expected fraction of losses  [20] :",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "The dataset that is used in the experiment is the RECOLA dataset  [21] . The training data consists of the 16 training tracks, 15 validation tracks, and 15 test tracks. Each track consists of 5 minutes of audio  [21] , recorded at 44.1 kHz. Each track is labelled across time and the labels were collected at a frequency of 25 Hz. Each track contains one student participant with a mean age of 22 years. The speakers spoke in a variety of languages which consisted of 33 French, 8 Italian, 4 German, and 1 Portuguese speakers. In our experiments, the audio tracks are down-sampled to 16 kHz, and the labels are down-sampled by a factor of 5 using median pooling. Since the labels for the test portion were not freely accessible at the time of the experiments, the validation portion is used for testing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "There needs to be a model that can recognise emotions via speech, where emotions are defined by two main dimensions arousal and valence. For this purpose, an end-to-end deep model is used, due to its simplicity and strong performance.\n\nThere is one model architecture that is adapted in all the experiments, based on a variant of the model introduced by  [3] , with slightly different hyperparameters. The model's architecture is depicted in Figure  1 . It starts with a batch normalisation layer  [22] , followed by three convolution blocks, then a bidirectional LSTM layer  [23] , and finally a time-distributed fully-connected layer  [24]  (using tanh activation function) with two output features. Bidirectional LSTMs have shown to be effective in ASR  [25] . Each of the convolution blocks or the recurrent layers are followed by a dropout layer (dropout rate 0.5) to reduce overfitting  [26] . Each convolution block consists of a 1D convolution layer (with ReLU activation function) followed by a max-pooling layer. The convolution layers have filter sizes 27, 14, and 3 respectively. The number of output channels are 64, 128, and 128 respectively. The pooling sizes are 40, 20, and 4 respectively. The bidirectional LSTM consists of 64 output units. The sizes of the pooling layers are chosen to reduce the input sample rate from 16 kHz to an output sample rate of 5 Hz. Accordingly, the kernel layers have a padding to preserve the input length. Then, their filters' sizes are chosen to render the overlap rate R ≈ 0.4 as advised in  [3] . The overlap rate is calculated by the formula:\n\nDuring training, the input and output data are segmented into frames of 20 seconds, in order to reduce the time complexity needed by the LSTM layers to operate on long sequences. The training is performed using the Adadelta optimisation algorithm  [27]  with a learning rate of 0.5, for 200 epochs and a mini-batch size of 16. Similar to  [3] , the loss function that is used for training is a function that would maximise the concordance correlation coefficient (CCC)  [28] . The function is 1-ρc(y, ŷ), where ρc is the CCC, defined by the formula:\n\nwhere σ 2 x , σ 2 y are the variances of x and y respectively, µx, µy are the means of x and y respectively, and σ 2 xy is the covariance of x and y. The loss function uses the CCC on the time dimension of the data, then averages the values across examples and emotions features, in order to ensure that both emotion dimensions are optimised adequately.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "The effects of frame-loss on emotion recognition are investigated under four different settings: matched, mismatched, multi-conditions, and augmentation. The main difference between these settings is the training environment. Table  2  shows the validation CCC scores for all the chosen settings. The testing environment is the same for all of them; it considers several combinations of the two parameters pN and pL. Depending on the chosen values for both parameters and the training setting, a corresponding model is chosen to be tested using CCC (in Equation  3 ). The testing is done by applying the frame-loss (in the corresponding settings only) individually on each of the five minutes tracks, then predicting the labels for the remaining frames. The comparison between labels and predictions is then done individually for each emotion dimension, by calculating CCC on the concatenation of all tracks (since they might have different lengths after applying frame-loss).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mismatched Training",
      "text": "In the mismatched setting, the training is run on the clean data without any application of frame-loss, and the same model is used for all test combinations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Conditions Training",
      "text": "In the multi-conditions training settings, for each training batch, two values pN and pL are sampled uniformly from [0.05, 1] and [0, 1] respectively. Then, accordingly, a frame-loss mask is sampled using the Markov Chain M. The sampled mask is used to drop frames for all the examples in the batch. Only one model is trained in this setting, and it is used for all test combinations. During sampling, pN is clipped to be at least 0.05 to prevent extremely high loss of training data which degrades the training quality severely.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Matched Training",
      "text": "The training environment in the matched settings relies on partial multi-conditions training, because there are many test combinations of the two parameters pN and pL, and it would be impractical to train a model for each of those combinations. Consequently, the values are clustered in three categories: low, medium, and high, with values in the ranges [0, 1/3), [1/3, 2/3) and [2/3, 1], respectively. Using these categories, there are nine combinations for models to be trained. In each combination, based on the chosen categories, values for both pN and pL are sampled uniformly for each batch (according to the corresponding categories' ranges). Similar to the multi-conditions setting, according to sampled values of pN and pL, a mask is generated using the introduced Markov Chain M to drop the frames of the whole batch. respectively, while low corresponds to the range [0, 1/3) for pL, and [0.05, 1/3) for pN.  [3]  is shown in the second row.\n\nbe at least 0.05 to prevent the severe degradation of training quality. However, still some residues of the degradation is visible in the last row of Table  2 . During the testing, depending on the categories in which each of the testing values of pN and pL lie in, the model with the corresponding matching category is chosen for testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Augmentation Training",
      "text": "In this setting, one of the models from the matched training setting is used, when pL is low and pN is high. This one model is then used for all the test combinations. This setup is similar to the multi-conditions setup, with one key difference, which is the model used for testing. The main aim of this setting is to examine the effectiveness of a frame-loss as a data augmentation technique  [29]  which can be used during training with the aim to improve the results or allow the model to be more robust in degraded run-time environments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "The results in Figure  3  are comparing the scores to a single dimension, which is the ratio of dropped frames after applying the frame-loss. The results of the testing are shown in details in Figure  4 , where the different combinations of valence/arousal and the three training settings (matched, mismatched, and multi-conditions) are examined. According to Figure  3 , it can be seen that generally, the matched setting has the overall best performance, while the mismatched has the worst overall performance. The    performance of the matched setting is expected since the model gets trained on data which is the most similar to the test data, in comparison to the other settings. In addition, for a low drop-rate < 0.5, the multi-conditions setting tends to have the worst performance, while the matched and mismatched settings are more or less on par.\n\nThe previous results were the main motivation to examine the augmentation setting, which tries to combine the advantages of the mismatched settings and multi-conditions, without matching the training and testing. In that case, one model is trained with parameters that cause a low drop-rate. The aim is to achieve the high performance of the matched settings for the low drop-rate, and resembles some of the high performance of the multi-conditions setting on the high drop-rate. The results according to Figure  3  show that this is indeed the case. The augmentation setting achieves nearly similar performance like the matched setting for drop-rate < 0.5, while making some improvement over the mismatched setting for higher drop-rate.\n\nAfter examining the results of the different settings, a strategy to overcome the frame-loss effects is to try to match the setting of the training environment to match the deployment environment. In case this matching is hard to be performed, a data augmentation technique can be a general purpose technique to use. For particular environments with severe degradation in the audio's quality, the training with multi-conditions setting can then be used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, the effects of frame-loss on the performance of automatic speech emotion recognition were examined. A Markov Chain model was utilised to model environments with frame-loss, where an audio stream can lose data packets during transmission. For such an examination, an end-to-end deep model was used for the experiments. The model mainly consists of convolution blocks and recurrent layers and the dataset RECOLA was chosen for the experiments.\n\nThe experiments had mainly three settings: matched, mismatched, and multi-conditions settings. In all of the settings, the models were tested with a variety of possibilities of frame-loss, while the training was the crucial difference between the different settings. In the mismatched setting, the model was trained on clean data. In the matched setting, a variety of models were trained based on low, mid, or high values of the parameters. In the multi-conditions settings, one model was trained using a mixture of all parameters' combinations.\n\nThe results have shown that the matched settings had the best overall performance while the mismatched setting had the worst overall performance. The multi-conditions setting was on par with the matched settings for lossy data (with frame-loss rate > 0.5). However, it was the worst on data with low frame-loss rate < 0.5. On the other hand, the matched and mismatched settings had an on par performance for data with low frame-loss rates < 0.5.\n\nAn additional setting was experimented to test out a general purpose solution for the frame-loss problem, namely training with frame-loss as a data augmentation mechanism, just using parameters that lead to low frame-loss rates. The augmentation has been shown as a compromise strategy to combine the advantages of the mismatched and multi-conditions settings, without matching the training to the test environments. It has shown a performance on low frame-loss rates which is on par to the matched setting, while for high frame-loss rates it has shown an improvement over the mismatched setting.\n\nFuture work should investigate the use of Packet Loss Concealment (PLC) methods  [30]  in the context of SER instead of classical PLC  [31] . This could include recent deep learning approaches including such from the image processing domain  [32]  originally tailored for occlusion restoration, as it has repeatedly been shown that audio can well be modelled as an 'image' using the spectogram or related representations  [33] .",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: End-to-end model for speech emotion recognition.",
      "page": 1
    },
    {
      "caption": "Figure 2: Markov Chain M(pL, pN) that samples a binary",
      "page": 2
    },
    {
      "caption": "Figure 3: CCC scores for arousal and valence compared against different frame-drop rates, for the different training settings.",
      "page": 3
    },
    {
      "caption": "Figure 3: are comparing the scores to a single",
      "page": 3
    },
    {
      "caption": "Figure 4: , where the different combinations of valence/arousal",
      "page": 3
    },
    {
      "caption": "Figure 3: , it can be seen that generally,",
      "page": 3
    },
    {
      "caption": "Figure 4: CCC scores for valence and arousal, for the three matched, mismatched, and multi-conditions settings. pN is the probability",
      "page": 4
    },
    {
      "caption": "Figure 3: show that this is indeed the case. The",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".507\n.505\n.501\n.541\n.486\n.487\n.421\n.515\n.508\n.512\n.546\n.460\n.425\n.288\n.514\n.509\n.499\n.528\n.454\n.402\n.214\n.534\n.515\n.509\n.476\n.470\n.354\n.083": ".469\n.480\n.483\n.462\n.301\n.244\n.051\n.466\n.478\n.464\n.466\n.293\n.241\n.136\n.486\n.469\n.468\n.431\n.329\n.225\n.203"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Perceived video streaming quality under initial buffering and rebuffering degradations",
      "authors": [
        "X Tan",
        "J Gustafsson",
        "G Heikkilä"
      ],
      "year": "2006",
      "venue": "Proceedings MESAQIN Conference"
    },
    {
      "citation_id": "3",
      "title": "A Deep Learning Approach for Location Independent Throughput Prediction",
      "authors": [
        "J Schmid",
        "M Schneider",
        "A Hb",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings IEEE International Conference on Connected Vehicles and Expo (ICCVE)"
    },
    {
      "citation_id": "4",
      "title": "End-to-End Speech Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proceedings International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville",
        "Deep Learning"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Emotion Recognition in the Noise Applying Large Acoustic Feature Sets",
      "authors": [
        "B Schuller",
        "D Arsić",
        "F Wallhoff",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Proceedings 3rd International Conference on Speech Prosody"
    },
    {
      "citation_id": "8",
      "title": "Recognition of Non-Prototypical Emotions in Reverberated and Noisy Speech by Non-Negative Matrix Factorization",
      "authors": [
        "F Weninger",
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Special Issue on Emotion and Mental State Recognition from Speech"
    },
    {
      "citation_id": "9",
      "title": "Spectral and Cepstral Audio Noise Reduction Techniques in Speech Emotion Recognition",
      "authors": [
        "J Pohjalainen",
        "F Ringeval",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia, MM"
    },
    {
      "citation_id": "10",
      "title": "Affective Speaker State Analysis in the Presence of Reverberation",
      "authors": [
        "B Schuller"
      ],
      "year": "2011",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "11",
      "title": "The Effect of Narrow-band Transmission on Recognition of Paralinguistic Information from Human Vocalizations",
      "authors": [
        "E Marchi",
        "S Frühholz",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Objective study of the performance degradation in emotion recognition through the AMR-WB+ codec",
      "authors": [
        "A Albin",
        "E Moore"
      ],
      "year": "2015",
      "venue": "Proceedings INTERSPEECH, 16th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "13",
      "title": "Integration of Negative Emotion Detection into a VoIP Call Center System",
      "authors": [
        "T.-L Pao",
        "C.-F Chang",
        "R.-C Tsao"
      ],
      "year": "2012",
      "venue": "Proceedings on the International Conference on Artificial Intelligence (ICAI)"
    },
    {
      "citation_id": "14",
      "title": "Real-time Tracking of Speakers' Emotions, States, and Traits on Mobile Platforms",
      "authors": [
        "E Marchi",
        "F Eyben",
        "G Hagerer",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings INTERSPEECH, 17th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "Robust speech recognition in burst-like packet loss",
      "authors": [
        "B Milner"
      ],
      "year": "2001",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Speech Prediction Using an Adaptive Recurrent Neural Network with Application to Packet Loss Concealment",
      "authors": [
        "R Lotfidereshgi",
        "P Gournay"
      ],
      "year": "2018",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Pattern recognition and machine learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern recognition and machine learning"
    },
    {
      "citation_id": "18",
      "title": "The Gilbert-Elliott model for packet loss in real time services on the Internet",
      "authors": [
        "G Haßlinger",
        "O Hohlfeld"
      ],
      "year": "2008",
      "venue": "Proceedings 14th GI/ITG Conference-Measurement, Modelling and Evalutation of Computer and Communication Systems"
    },
    {
      "citation_id": "19",
      "title": "An analysis of packet loss models for distributed speech recognition",
      "authors": [
        "B Milner",
        "A James"
      ],
      "year": "2004",
      "venue": "Proceedings INTERSPEECH, 8th International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "20",
      "title": "MAC-Layer Packet Loss Models for Wi-Fi Networks: A Survey",
      "authors": [
        "C Da Silva",
        "C Pedroso"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Packet loss concealment with recurrent neural networks for wireless inertial pose tracking",
      "authors": [
        "X Xiao",
        "S Zarar"
      ],
      "year": "2018",
      "venue": "Proceedings IEEE 15th International Conference on Wearable and Implantable Body Sensor Networks (BSN)"
    },
    {
      "citation_id": "22",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "23",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "24",
      "title": "Long Short-term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "25",
      "title": "Learning internal representations by error propagation",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1985",
      "venue": "Learning internal representations by error propagation"
    },
    {
      "citation_id": "26",
      "title": "A comprehensive study of deep bidirectional LSTM RNNS for acoustic modeling in speech recognition",
      "authors": [
        "A Zeyer",
        "P Doetsch",
        "P Voigtlaender",
        "R Schlter",
        "H Ney"
      ],
      "year": "2017",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "28",
      "title": "Adadelta: an adaptive learning rate method",
      "authors": [
        "M Zeiler"
      ],
      "year": "2012",
      "venue": "Adadelta: an adaptive learning rate method",
      "arxiv": "arXiv:1212.5701"
    },
    {
      "citation_id": "29",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "30",
      "title": "The effectiveness of data augmentation in image classification using deep learning",
      "authors": [
        "L Perez",
        "J Wang"
      ],
      "year": "2017",
      "venue": "The effectiveness of data augmentation in image classification using deep learning",
      "arxiv": "arXiv:1712.04621"
    },
    {
      "citation_id": "31",
      "title": "Hidden Markov model-based packet loss concealment for voice over IP",
      "authors": [
        "C Rodbro",
        "M Murthi",
        "S Andersen",
        "S Jensen"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Efficient HMM-Based Estimation of Missing Features, with Applications to Packet Loss Concealment",
      "authors": [
        "B Borgström",
        "P Borgström",
        "A Alwan"
      ],
      "year": "2010",
      "venue": "Proceedings INTERSPEECH, 11th Annual Conference of the International Speech Communication Association. Makuhari, Chiba, Japan: ISCA"
    },
    {
      "citation_id": "33",
      "title": "Latent Convolutional Models",
      "authors": [
        "S Athar",
        "E Burnaev",
        "V Lempitsky"
      ],
      "year": "2018",
      "venue": "Latent Convolutional Models"
    },
    {
      "citation_id": "34",
      "title": "An Image-based Deep Spectrum Feature Representation for the Recognition of Emotional Speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings 25th ACM International Conference on Multimedia"
    }
  ]
}