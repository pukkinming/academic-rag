{
  "paper_id": "2105.07484v1",
  "title": "Leveraging Semantic Scene Characteristics And Multi-Stream Convolutional Architectures In A Contextual Approach For Video-Based Visual Emotion Recognition In The Wild",
  "published": "2021-05-16T17:31:59Z",
  "authors": [
    "Ioannis Pikoulis",
    "Panagiotis P. Filntisis",
    "Petros Maragos"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multistream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pretraining a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The interpretation, perception and recognition of human affect has been the subject of rigorous studies and analyses across several scientific disciplines such as biology, psychology, sociology, neurology and last but not least, computer science. While the aforementioned cognitive sciences focus on the extraction of the available affective information, the fields of computer vision and machine learning aim at automating the recognition process through the development of novel techniques and algorithms which are capable of producing effective and robust encodings of such information.\n\nThe majority of past efforts in visual emotion recognition have been mostly limited to the analysis of facial expressions  [37] ,  [10] ,  [13] ,  [25] ,  [44] , while some studies have either incorporated information relative to body pose  [11] ,  [5] ,  [2] ,  [8]  or have attempted to perform emotion recognition exclusively on the basis of body movements and gestures  [1] ,  [14] ,  [28] ,  [31] ,  [32] . While some of these approaches perform well in certain specified settings, they fail to interpret real-world scenarios. This is because emotion recognition systems are, more often than not, expected to operate on instances of people whose facial features are fully visible and their body joints are unoccluded, something which does not generally conform to reality.\n\nEvidence from psychology related studies suggest that visual context, in addition to facial expression and body pose, provides important information to the perception of people's emotions. Dudzik et al.  [7]  propose two sources of context as means of interpreting emotional behavior, namely perceiver knowledge/experience and perceivable encoding context. Wieser and Brosch  [39]  highlight situational context as the primary aspect of the latter, with features that mainly revolve around the visual scenes in which the depicted emotional behaviors are embedded. Barrett and Kensinger  [3]  report that the structural features of the face, when viewed in isolation, often prove to be insufficient for perceiving emotion. Furthermore, empirical findings suggest that the categorization of facial expressions is speeded up at the sight of congruent scenes  [29] , while both positive and negative contexts result in significantly different ratings of faces compared with those presented in neutral contexts  [24] .\n\nIn this work, we aim at extending the concept of contextbased visual emotion recognition in the dynamic setting of video sequences. Our approach to the problem rests on the late fusion of Temporal Segment Networks (TSN)  [38]  and a Spatial-Temporal Graph Convolutional Network (ST-GCN)  [41] . We extend the original TSN framework by incorporating multiple input streams that encode bodily, contextual, facial and generic scene-related features, enhancing our model's joint understanding of emotion and the depicted surrounding environments. To the best of our knowledge, our approach is the first to explicitly infuse scene characteristics as well as make effective use of multi-stream optical flow in an emotion recognition process. Extensive ablation experiments, based on the recently assembled and challenging Body Language Dataset (BoLD), are carried out so as to study the various contributions of our methods.\n\nThe remainder of the paper is structured as follows: Firstly, we provide an overview of the most notable related work in the domain of context and skeleton-based visual emotion recognition. Subsequently, we analyze our proposed model architecture. Next, we present our experimental results on the BoLD dataset, followed by conclusive remarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Kosti et al.  [17]  made one of the first notable contributions towards context-based emotion recognition by introducing the EMOTIC dataset as well as providing a baseline model that was trained and evaluated on the latter. Their baseline model consisted of three modules: two ConvNet feature extractors (one for each of the body and context input streams) and one fusion network. A notable improvement in recognition performance over the baseline model came along the EmotiCon framework, as it was introduced by Mittal et al.  [23] . Their main contributions are associated with the incorporation of multiple modalities in the task of context-based emotion recognition, including the face, pose, inter-agent interactions and socio-dynamic context.\n\nSubsequently, researchers shifted their attention towards video-based emotion recognition in context. Lee et al.  [19]  introduced the Context-Aware Emotion Recognition benchmark which is comprised of 13.201 TV video clips and a total of 1.1M frames. Moreover, a baseline model was proposed, featuring a face and a context encoding stream which were merged using an adaptive-fusion network.\n\nAfter the recent assemble of the Body Language Dataset (BoLD), Luo et al.  [22]  furthered their contributions by comparing various network configurations and finally providing a baseline model for the task of categorical and continuous emotion prediction. Among the examined methodologies were: motion-based descriptors, i.e. histograms of optical flow and motion boundary histograms, skeleton-based learning through an ST-GCN and Laban Movement Analysis  [18]  as well as pixel-level learning through two-stream convolutional and TSN architectures. Filntisis et al.  [9]  proposed the incorporation of a contextual feature encoding branch and the addition of visual-semantic embedding loss based on Global Vectors (GloVe)  [27]  word embeddings, achieving state-ofthe-art performance on BoLD.\n\nAs graph-based neural networks have proved to be powerful tools for determining human actions  [41] ,  [34] ,  [20] ,  [35] ,  [21] , there have also been attempts to adapt them for the task of emotion recognition. Bhattacharya et al.  [4]  introduced a classifier network for the task of emotion recognition through gaits, as well as a realistic gait generator, with the ST-GCN architecture being the common denominator between the two. Sheng et al.  [33]  proposed an Attention Enhanced Temporal Graph Convolutional Network (AT-GCN) as part of a multi-tasking framework which can jointly learn representations relative to both emotion and identity recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Model Architecture",
      "text": "A complete schematic diagram of our proposed network ensemble is shown in Fig.  1 . The backbone of our network implementation resides in a combination of the two-stream convolutional  [36]  and TSN  [38]  architectures, both of which were initially proposed for video-based action recognition. During TSN training, any given input video sequence V is firstly divided into K segments {S 1 , ..., S K } of equal durations. The TSN operates on a set of K snippets, with each snippet constituting an instance that has been randomly sampled from the corresponding segment. More formally, the output of a temporal segment network is modeled as follows:\n\nwhere {T 1 , ..., T K } denote the snippets, W denotes the network trainable parameters, F denotes the snippet-level network predictions, G denotes a segmental consensus function and H denotes a prediction function. Firstly, we will present the TSN structure regarding the RGB modality, along with our proposed extensions for the enhancement of emotion understanding. Next, we will do the same for the Optical Flow modality, and finally we will present the part of the architecture relative to skeletonbased learning. We choose to utilize 18-layer ResNets  [12]  as our primary feature extractors for all the subsequent convolutional branches. ResNets constitute state-of-the-art ConvNet backbones, offering a valuable trade-off between performance and computational complexity. Moreover, the ResNet-18 variant produces 512-dim deep feature vector representations for each given input image.\n\nA. TSN-RGB 1) Body: A single RGB image usually encodes static appearance at a specific point in time, lacking the contextual information about previous and next frames. We begin by training a standard TSN using the RGB modality and the body crops of each frame instance. For the calculation of the necessary body bounding boxes, we make use of the coordinates of 18 body joints that have been successfully tracked along the entirety of each video sequence and are being provided by the distributors of the dataset. The body branch feature extractor is pre-trained on ImageNet  [6] .\n\n2) Context: We incorporate a context stream in the form of RGB frames whose primary depicted agents have been masked out. For the acquisition of the masks we use the body bounding boxes that we have previously calculated and multiply them element-wise with a constant value of zero. Contextual feature extraction is a scene-centric task, and therefore we choose to initialize the corresponding ConvNet backbone using the Places365-Standard  [45] , a large-scale database of photographs, labeled with scene semantic categories.\n\n3) Face: We introduce an input stream which explicitly operates on extracted face crops. For the localization and extraction of faces we use the five body joints that correspond to the eyes, ears and nose of each depicted instance. These joints are used to calculate the smallest bounding box that contains the head of the agent. As the pose of an agent might result in partial or complete occlusion of their facial features, the successful extraction of the face region is not guaranteed. The ConvNet backbone of the face branch receives manual pre-training on AffectNet  [25]  which constitutes the largest facial expression database, containing over 1M images, annotated on both categorical and dimensional level.\n\n4) Scenes and Attributes: The depicted scene along with its attributes hold valuable information relative to human emotion understanding, especially in cases where primary sources of affective information, such the face and body, are occluded. Therefore, we aspire to enrich our model's perception of context by directly extracting the Places365 scene-specific scores and the corresponding Scene UNderstanding (SUN)  [26]  attributes through an 18-layer Wide-ResNet  [43]  which has been jointly pre-trained on both of the aforementioned databases.\n\nThe Places  [45]  database is a quasi-exhaustive repository of 10M scene photographs, labeled with 476 scene semantic categories. We use only a subset of the latter, namely the Places365-Standard which features 1.8M images and 365 scene categories. Moreover, the SUN attribute database  [26]  constitutes a subset of the SUN categorical database  [40] , comprised of 14,000 images that are annotated using a taxonomy of 102 scene attributes. Some of the categories that are included in the Places365 dataset are: amusement park, basketball court, cemetery, jail, cell, lecture room, museum, office, sauna, soccer field, etc. Some of the scene attributes included in the SUN dataset are: competing, socializing, working, exercise, praying, open-area, enclosed-area, stressful, etc. It is quite evident that the environment and scene depicted in an image can be closely related with the emotions of the people that are present. For example, an image of a funeral that is located at a cemetery, suggests a strong correlation between the above oppressive setting and the generally negative and sad feelings shared among the depicted people. Provided that our model is capable of leveraging the hinted correlations, incorporating scene specific information can potentially boost its overall emotion recognition performance.\n\nGiven an input image, the feature extractor, through each last convolutional block produces feature maps Z ∈ R 512×14×14 . After the application of an average pooling layer, a deep feature vector representation is formed and fed into a FC layer with weights W scenes ∈ R 512×365 , producing class confidence scores ŷscenes ∈ R 1×365 . An additional set of pre-trained weights, namely W attr ∈ R 512×102 can be used for the prediction of confidence scores ŷattr ∈ R 1×102 for 102 scene attributes that are included in the SUN dataset. The corresponding scene and attribute classification probabilities are calculated after the row-wise application of the softmax function. Subsequently, the produced scene and attribute probability scores are concatenated with the extracted deep features from all the aforementioned input streams. After the initialization of the feature extractor with the aforementioned pre-trained models, weight parameters are kept frozen during the training phase.\n\nThe inclusion of all input streams of the TSN-RGB results in a 2003-dim concatenated feature vector.\n\n5) Loss Functions: For the training of the continuous emotion prediction branch, we use a standard mean squared error (MSE) loss L cont along the three emotional dimensions of valence, arousal and dominance. As far as categorical emotion prediction is concerned, the ground truth targets are provided in the form of confidence scores. Firstly, we apply a sigmoid function to the barebones extracted class scores and then impose an MSE loss between the predicted and ground truth confidence scores. We denote this loss term as L cat1 . Secondly, after binarizing the ground truth confidence scores with a threshold of 0.5, we apply a binary cross-entropy loss between the produced and ground truth multi-hot target labels. We denote this term as L cat2 . We also enforce semantic congruity between the extracted visual embeddings and the categorical label word embeddings from a 300-dim GloVe  [27]  model, pre-trained on Wikipedia and Gigaword 5 data, in the same manner as in  [9] . More specifically, given an input image X, we transform the concatenated visual embeddings f v (X) into the same dimensionality as the word embeddings f t (y i ) through a linear transformation W emb , with y being the corresponding multi-hot target vector and i being the categorical label class index. We later apply an MSE loss between the transformed visual embeddings and the average of word embeddings that correspond only to the positive labels of the given ground truth target vector and denote this term as L emb :\n\n(2)\n\nwhere P denotes the set of positive class labels for a given target vector y and |P| denotes the cardinality of that set.\n\nThe whole network can be trained in an end-to-end manner by minimizing the combined loss function:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Tsn-Flow",
      "text": "Similarly to the case of temporal stream ConvNets in the original two-stream convolutional architecture  [36] , we experiment with training a TSN on stacked optical flow fields. Optical flow extraction is carried out via the TVL1 algorithm  [42] . This form of dense optical flow is known to effectively encode motion between consecutive frames. We denote this model as TSN-Flow.\n\nIn all our subsequent implementations, we stack bidirectional optical flow fields from L = 5 consecutive frames for each snippet. After decomposing each displacement vector into its horizontal and vertical components, we end up with a 10-channel input volume per segment, per input stream. To begin with, we train a standard TSN using the Optical Flow modality and the body crops of each frame instance. The usage of body joint coordinates for the localization and extraction of the necessary bounding boxes remains the same as in the case of the RGB modality. Body-oriented dense optical flow encodes the movement of the primary agent depicted in each instance. Additionally, we incorporate a context stream, in the form of stacked optical flow fields whose primary depicted agents have been masked out. The context input stream effectively encodes the motion of any occasional secondary agent or object. Lastly, we introduce an input stream that focuses solely on the head and face movements of the primary agent. This is achieved by training an additional temporal ConvNet on small fragments of dense flow that correspond to the head region of each agent.\n\nThe features extracted using optical flow streams have distributions that greatly differ from their RGB counterparts. As optical flow values are discretized in the interval [0, 255], therefore sharing the same range with RGB images, we use RGB models to initialize the parameters of the temporal ConvNets. The feature extractors for all TSN-Flow streams were pre-trained on ImageNet  [6] . Consequently, the weights of the first convolutional layer are modified so as to handle the input of optical flow fields. More specifically, the weights are averaged across the RGB channels and replicated by the number of channels of the temporal stream inputs.\n\nThe inclusion of all the aforementioned input streams results in a 1536-dim concatenated feature vector per input volume. During training we employ the same combined loss function as the one used for its RGB counterpart.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Skeleton-Based Learning",
      "text": "As for the final source of affective information, we shift our attention towards the Human Skeleton and attempt to incorporate a Spatial-Temporal Graph Convolutional Network (ST-GCN)  [41] , as it was originally proposed for skeletonbased action recognition. We choose to deploy the vanilla ST-GCN consisting of 9 layers of spatial-temporal graph convolution operators (ST-GCN units). The features extracted from the last ST-GCN unit undergo average pooling and with the use of 1 × 1 convolutions, the final predictions are produced.\n\n1) Joint Labeling Strategies: The main variable setting of the ST-GCN configurations is the joint labeling strategy that is being used during the construction of the graph adjacency matrix, namely uniform, distance or spatial. With uniform being the simplest labeling strategy, all joints that are connected through a limb belong in the same subset, resulting in K v = 1 total subsets. The distance labeling strategy extends the concept of neighboring joints, as pairs of joints that are connected through a sequence of limbs are also taken into consideration, leading to a total of K v = D + 1 subsets, where D is the maximum allowed distance between two neighboring joints (we choose D = 1 for simplicity). Lastly, according to the spatial labeling strategy, neighboring joints are distinguished based on their individual distances from a fixed root (the neck), resulting in K v = 3 subsets.\n\n2) Forward Propagation: In the spatial-temporal case, the input feature map H in of a ST-GCN unit is represented as a tensor of shape (C in , T in , V ), where C in denotes the number of input channels, T in denotes the number of frames in the skeleton sequence and V denotes the number of nodes. Firstly, the input tensor undergoes a (K v • C out ) × 1 × 1 spatial graph convolution operation, with C out being the desired number of output channels and K v being the number of joint subsets that are formed based on the chosen labeling strategy. The resulting tensor is reshaped into (K v , C out , T in , V ) and multiplied with the normalized adjacency matrix D -1 2 ÂD -1 2 , where Â = I + A (I denotes the identity matrix) and D is a diagonal matrix with elements D ii = j Âij . In case of the distance and spatial partitioning strategies (K v > 1), the adjacency matrix A is formed by stacking K v matrices A k , with each one corresponding to one of the K v joint subsets. If we ignore interlayer nonlinearities, then the aforementioned spatial convolution operation is equivalent to the original GCN  [16]  formula:\n\nwhere W k are C out × C in × 1 × 1 weight matrices (the multiplication is replicated T in times in the temporal dimension and V times in the spatial dimension). D ii k = j Âij k + α is the normalized diagonal matrix and α is set to 0.001 to avoid empty rows. Additionally, learnable edge importance weighting can be implemented simply by multiplying element-wise the adjacency matrices Âk of Eq. 4 with a weight mask M, namely Âk M. The output feature map resulting from the spatial graph convolution undergoes a C out × Γ × 1 temporal convolution, with Γ denoting the temporal kernel size, completing the processing pipeline of a ST-GCN unit.\n\nTraining in the case of the ST-GCN is driven by a combined loss function similar to the one described in Eq. 3, with the sole difference being the exclusion of the categorical label embedding loss L emb .\n\n3) Data Augmentation: Joint coordinates are normalized in the range [0, 1] using the largest joint bounding box within each sequence. We then strictly follow the proposed methodologies of  [41] . Firstly, we find the maximum sequence length T within our dataset and pad every clip with zeros until it reaches that specified length. During the training phase, padding is applied randomly within the sequence while during inference the paddings are placed always at the end for consistency. Additionally, during training we perform random affine transformations on the skeleton sequences of all frames, with the aim of simulating camera movement.\n\nAfter augmentation, input data is represented by tensors of size (C, T, V ). For each frame of a sequence, BoLD provides 18 tuples that contain 2D joint coordinates plus a detection confidence score associated with each joint, therefore in our case C = 3 and V = 18. In order to further reduce the effect of over-fitting, we also pre-train the ST-GCN on the Kinetics dataset  [15]  which has been extensively used for skeleton-based action recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Evaluations A. Dataset",
      "text": "All of our upcoming experimental results are based on the standard train, validation and test splits of the Body Language Dataset (BoLD) 1 which was assembled by Luo et al.  [22]  and constitutes a database that focuses on bodily expressions of emotion. BoLD is comprised of 9,876 movie video clips of body movements, depicting a total of 13,239 human characters. The annotation of the dataset was performed using a crowdsourcing pipeline based on the Amazon Mechanical Turk. Instances are annotated in both categorical and dimensional level, utilizing the 26 emotional categories of the EMOTIC  [17]  dataset and the VAD  [30]  dimensional model, respectively.\n\nAs far as evaluation metrics are concerned, for categorical emotion prediction, average precision (AP), i.e. the area under the precision-recall curve as well as the area under the receiver operating characteristic (ROC-AUC) are used. For continuous emotion regression along the VAD dimensions, the coefficient of determination (R 2 ) is used. Performance comparison among different models is carried out on the basis of an aggregatory emotion recognition score (ERS) which is calculated as follows:\n\nwhere mR 2 denotes the mean coefficient of determination along the VAD dimensions while mAP and mRA denote the 1 https://cydar.ist.psu.edu/emotionchallenge mean average precision and the mean ROC-AUC over the 26 emotion categories respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Configuration Details",
      "text": "For all experiments regarding TSN configurations, we use K train = 3 segments during training and K val = 25 segments during validation while the segmental consensus function has been chosen to be average pooling. Both the TSNs and ST-GCN are trained for 25 epochs with a batch size of 16, using the SGD optimizer, momentum equal to 0.9 and weight decay equal to 10 -5 . The initial learning rate is set to 10 -3 in the case of TSNs and 5•10 -3 in the case of the ST-GCN. The learning rates are reduced by a factor of 0.1 whenever the monitored loss on the validation set plateaus. No particular data augmentation technique was applied for either the TSN-RGB or TSN-Flow, as the built-in variations of the BoLD dataset proved sufficient for avoiding over-fitting.\n\nApart from the previously described methodologies, we experiment with the partial batch-normalization scheme, or Partial BN for short, as proposed in  [38] . More specifically, after the initialization with pre-trained models, in every ConvNet feature extractor, we freeze the mean and variance of parameters of all batch normalization layers, except for the first one. This method is expected to work especially well in the case of temporal ConvNets and reduce the effect of overfitting. More specifically, as the distribution of optical flow is different from the RGB images, the distribution of activation values in the first convolutional layer will also differ from the ones inherited through their initialization with RGB pretrained models.\n\nAll of our results were generated on a single NVIDIA GeForce RTX 2080 Ti GPU. All the code was implemented using PyTorch 2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Ablation Studies",
      "text": "Tables I and II present performance comparisons among all TSN-RGB and TSN-Flow configurations, respectively, which we have considered. The second columns describe the various input streams that are being included, with \"B\" denoting the body, \"C\" denoting the context, \"F\" denoting the face, \"S\" denoting the Places365 scene categories and \"A\" denoting the corresponding SUN attributes.\n\n1) TSN-RGB: Both the inclusion of the context and face streams are conducive to an increase in ERS score, with the latter showcasing a more considerable boost in performance over the bare-bones body stream. The sole inclusion of either the Places365 scene-specific or the SUN attribute scores is conducive to higher recognition scores, with the latter seemingly being more beneficial. However, it is the combined usage of the two that results in the biggest improvement in performance, in both the categorical and continuous tasks, as expected. Moreover, while the addition of L emb leads to a trivial performance boost, the application of the Partial BN regularization scheme tops off our previously best performing network, reaching a maximum of 0.2579 ERS on the BoLD validation set. It seems as if the continuous re-estimation of mean and variance parameters of batchnormalization layers that are located deeper within the network, becomes obsolete and may in fact have a negative impact on generalization performance, provided that the model's parameters have been previously initialized through a proper pre-training procedure.\n\nThe beneficial influence of scene and attribute related features in human emotion understanding becomes more evident in cases where the facial characteristics and poses of the depicted agents are occluded. This is further highlighted in Fig.  2  which includes instances that were randomly selected from the validation set. Each instance is accompanied by its top-5 predicted scene categories, top-9 predicted attributes, ground truth and predicted (regressed) emotion categories (VAD values) as well as the corresponding Jaccard similarity coefficient (JC), for each model configuration. Correct category recognition is indicated in green. In all cases, the incorporation of scene and attribute characteristics, on top of the existing bodily, contextual and facial features, results in more emotions being correctly recognised. In addition, emotions that are semantically related, i.e. peace-happinesspleasure (e) and sadness-suffering-pain (f), are jointly predicted, even though some of them have not been included by the annotators.\n\n2) TSN-Flow: The introduction of either the context or face stream leads to a marginal improvement over the barebones temporal body stream. A more considerable boost in performance is achieved through the inclusion of all three input streams. These findings validate our intuitive decision to follow a multi-stream approach for encoding motion through Optical Flow, analogously to the case of the RGB modality. As mentioned in the case of TSN-RGB, the addition of the categorical label embedding loss L emb improves the network's performance in both categorical and continuous tasks, while with the application of the Partial BN scheme, the resulting model tops off all previous configurations reaching a maximum of 0.2398 ERS.\n\n3) ST-GCN: As far as skeleton-based learning is concerned, Table  III      Skeleton, effectively forming a network ensemble. The score fusion methods which will be considered include: maximum, simple average and weighted average. Table  IV  summarizes the results. A weighted average of the TSN-RGB, TSN-Flow and ST-GCN, with a weight ratio of 2:2:1 respectively, leads to the best result of 0.2897 ERS on the validation set. More importantly, our implementation surpasses the current stateof-the-art of 0.2439 ERS on the BoLD validation set, as it was recently achieved in  [9] . Figs.  3  and 4  summarize the results for the 26 discrete emotion categories and the continuous VAD dimensions relative to the AP and R 2 performance metrics, respectively. Subsequently, we evaluate our best performing network ensemble on the official BoLD test set. A comparative study regarding the performance and complexity of our proposed model and earlier published works is presented in Table  V . The proposed network ensemble manages to surpass the current state-of-the-art of 0.2624 ERS, as achieved in  [9] , by a considerable margin on all metrics, thus verifying the superiority of our technique. As far as complexity is concerned, our model does a good job at maintaining a lower number of trainable parameters through the efficient utilization of multiple input streams and shallow feature extractors (ResNet-18, Wide-ResNet-18), in comparison with previous implementations that made use of deeper ConvNet backbones  [9]  (ResNet-50 & 101) and disentangled the various input streams all together  [22] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "This study employs two major components of action recognition related literature, namely Temporal Segment Networks (TSN) and Spatial-Temporal Graph Convolutional Networks (ST-GCN) with the aim of extending the concept of context-based emotion recognition in the dynamic setting of video sequences. The most notable contribution of this paper is the extension of the original TSN architecture with the inclusion of multiple input streams that effectively encode bodily, contextual, facial and generic scene-related features, enhancing our model's perception of visual context and emotion in general. Our intuitive modifications regarding the incorporation of scene and attribute classification scores, as well as multi-stream optical flow, combined with a properly pre-trained ST-GCN, have led to significant improvements over the state-of-the-art techniques with relation to the challenging Body Language Dataset (BoLD).\n\nLastly, a possible future research direction might be proposals for further exploitation of the categorical label dependencies that reside within the datasets and may lead to an additional improvement in categorical emotion prediction. Also, the Depth modality has been left relatively unexplored on the subject of context-based emotion recognition in videos. Currently, published data with relation to BoLD are quite scarce and there is definitely a lot of room for improvement. However, our existing results undoubtedly prove that we have made significant steps in the right direction.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The backbone of our network",
      "page": 2
    },
    {
      "caption": "Figure 1: Complete schematic diagram of the proposed network ensemble, featuring an ST-GCN module and three TSN input streams (body, context, face)",
      "page": 3
    },
    {
      "caption": "Figure 2: Top-5 predicted scene categories, top-9 predicted attibutes, ground truth and predicted (regressed) emotion categories (VAD values) as well as",
      "page": 6
    },
    {
      "caption": "Figure 2: which includes instances that were randomly selected",
      "page": 6
    },
    {
      "caption": "Figure 3: Average precision (AP) scores per emotion category, as obtained",
      "page": 7
    },
    {
      "caption": "Figure 4: Coefﬁcient of determination (R2) per emotional dimension, as",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "mR2↑",
          "Classiﬁcation": "mAP↑",
          "ERS": ""
        },
        {
          "Model": "TSN-RGB",
          "Features": "B\nBC\nBCF\nBCFS\nBCFA",
          "Lemb": "No",
          "Partial BN": "No",
          "Regression": "0.0300",
          "Classiﬁcation": "0.1419",
          "ERS": "0.1983"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0362",
          "Classiﬁcation": "0.1468",
          "ERS": "0.2053"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0531",
          "Classiﬁcation": "0.1615",
          "ERS": "0.2222"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0597",
          "Classiﬁcation": "0.1736",
          "ERS": "0.2347"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0685",
          "Classiﬁcation": "0.1763",
          "ERS": "0.2388"
        },
        {
          "Model": "",
          "Features": "BCFSA",
          "Lemb": "No",
          "Partial BN": "No",
          "Regression": "0.0710",
          "Classiﬁcation": "0.1762",
          "ERS": "0.2404"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "Yes",
          "Partial BN": "No",
          "Regression": "0.0713",
          "Classiﬁcation": "0.1779",
          "ERS": "0.2416"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "Yes",
          "Partial BN": "Yes",
          "Regression": "0.0969",
          "Classiﬁcation": "0.1839",
          "ERS": "0.2579"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "mR2↑",
          "Classiﬁcation": "mAP↑\nmRA↑",
          "ERS": ""
        },
        {
          "Model": "TSN-Flow",
          "Features": "B\nBC\nBF",
          "Lemb": "No",
          "Partial BN": "No",
          "Regression": "0.0560",
          "Classiﬁcation": "0.1431\n0.5778",
          "ERS": "0.2082"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0661",
          "Classiﬁcation": "0.1415\n0.5882",
          "ERS": "0.2155"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "",
          "Partial BN": "",
          "Regression": "0.0649",
          "Classiﬁcation": "0.1497\n0.5971",
          "ERS": "0.2190"
        },
        {
          "Model": "",
          "Features": "BCF",
          "Lemb": "No",
          "Partial BN": "No",
          "Regression": "0.0795",
          "Classiﬁcation": "0.1524\n0.6054",
          "ERS": "0.2292"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "Yes",
          "Partial BN": "No",
          "Regression": "0.0888",
          "Classiﬁcation": "0.1563\n0.6135",
          "ERS": "0.2369"
        },
        {
          "Model": "",
          "Features": "",
          "Lemb": "Yes",
          "Partial BN": "Yes",
          "Regression": "0.0947",
          "Classiﬁcation": "0.6149\n0.1554",
          "ERS": "0.2398"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scenes\nAttributes\nGround Truth\nBCF\nBCFSA": "Temple/Asia\nPagoda\nChalet\nPalace\nHunting Lodge"
        },
        {
          "Scenes\nAttributes\nGround Truth\nBCF\nBCFSA": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scenes\nAttributes\nGround Truth\nBCF\nBCFSA": "Martial arts gym\nClean room\nLocker room\nArtists loft\nElevator lobby"
        },
        {
          "Scenes\nAttributes\nGround Truth\nBCF\nBCFSA": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "b": "",
          "Stage/Indoor\nDiscotheque\nBallroom\nOrchestra pit\nMovie theater": "",
          "Enclosed area\nNo horizon\nIndoor lighting\nCloth\nCongregating\nSocialising\nMan-made\nSpectating\nStressful": "",
          "Engagement\nConﬁdence\nPleasure\nSensitivity\nV: 0.8234\nA: 0.5572\nD: 0.8015": "",
          "Engagement\nV: 0.5003\nA: 0.6079\nD: 0.5651": "JC = 0.25",
          "Engagement\nPleasure\nExcitement\nAnticipation\nV: 0.5601\nA: 0.5537\nD: 0.5086": "JC = 0.33"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Oast house\nCottage\nTree farm\nKasbah\nVillage": "",
          "Natural light\nFoliage\nOpen area\nVegetation\nLeaves\nTrees\nNo horizon\nMan-made\nShrubbery": "",
          "Affection\nEsteem\nSympathy\nV: 0.5764\nA: 0.4758\nD: 0.7499": "",
          "Peace \nHappiness\nV: 0.5658\nA: 0.4353\nD: 0.5420": "JC = 0.0",
          "Esteem\nPeace\nEngagement\nHappiness\nPleasure\nV: 0.6856\nA: 0.4592\nD: 0.5874": "JC = 0.143"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Catacomb\nArch. Excavation\nGrotto\nTrench\nBasement": "",
          "No horizon\nMan-made\nDirt\nNatural light\nEnclosed area\nDry\nDirty\nRugged scene\nAged": "",
          "Anticipation\nSympathy\nSensitivity\nSadness\nSuffering\nV: 0.3343\nA: 0.4304\nD: 0.4720": "",
          "Suffering\nFear\nV: 0.4828\nA: 0.5799\nD: 0.6331": "JC = 0.167",
          "Anticipation\nSensitivity\nSadness\nSuffering, Pain\nEngagement\nV: 0.5499\nA: 0.5384\nD: 0.6444": "JC = 0.57"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Beer hall\nPub/Indoor\nBanquet hall\nDining hall\nBar": "",
          "No horizon\nEnclosed area\nMan-made\nIndoor lighting\nSocialising\nCloth\nCongregating\nEating\nStressful": "",
          "Peace\nAnticipation\nHappiness\nPleasure\nV: 0.8083\nA: 0.3500\nD: 0.6623": "",
          "Peace\nV: 0.5699\nA: 0.5485\nD: 0.6299": "JC = 0.25",
          "Peace\nHappiness\nV: 0.6095\nA: 0.4758\nD: 0.6141": "JC = 0.50"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Network Ensembles": "",
          "Score Fusion": "",
          "Regression": "mR2↑",
          "Classiﬁcation": "mAP↑\nmRA↑",
          "ERS": ""
        },
        {
          "Network Ensembles": "TSN-RGB+TSN-Flow\n(ours)",
          "Score Fusion": "Maximum",
          "Regression": "0.0939",
          "Classiﬁcation": "0.1840\n0.6543",
          "ERS": "0.2566"
        },
        {
          "Network Ensembles": "",
          "Score Fusion": "Average",
          "Regression": "0.1444",
          "Classiﬁcation": "0.1883\n0.6661",
          "ERS": "0.2858"
        },
        {
          "Network Ensembles": "TSN-RGB+TSN-Flow\n+ST-GCN (ours)",
          "Score Fusion": "Maximum",
          "Regression": "0.0652",
          "Classiﬁcation": "0.1809\n0.6493",
          "ERS": "0.2402"
        },
        {
          "Network Ensembles": "",
          "Score Fusion": "Average",
          "Regression": "0.1438",
          "Classiﬁcation": "0.1933\n0.6658",
          "ERS": "0.2867"
        },
        {
          "Network Ensembles": "",
          "Score Fusion": "Weighted Average",
          "Regression": "0.1489",
          "Classiﬁcation": "0.6682\n0.1929",
          "ERS": "0.2897"
        },
        {
          "Network Ensembles": "Filntisis et al.\n[9]",
          "Score Fusion": "Average",
          "Regression": "0.0917",
          "Classiﬁcation": "0.1656\n0.6266",
          "ERS": "0.2439"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gesture-based affective computing on motion capture data",
      "authors": [
        "A Asha] Kapur",
        "A Ajay] Kapur",
        "N Virji-Babul",
        "G Tzanetakis",
        "P Driessen"
      ],
      "year": "2005",
      "venue": "Int. Conf. Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "2",
      "title": "Introducing the Geneva multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "T Bänziger",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "3",
      "title": "Context is routinely encoded during emotion perception",
      "authors": [
        "L Barrett",
        "E Kensinger"
      ],
      "year": "2010",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "4",
      "title": "STEP: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. AAAI Conf. Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition through multiple modalities: face, body gesture, speech",
      "authors": [
        "G Castellano",
        "L Kessous",
        "G Caridakis"
      ],
      "year": "2008",
      "venue": "Affect and Emotion in Human-Computer Interaction"
    },
    {
      "citation_id": "6",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conf. Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "7",
      "title": "Context in human emotion perception for automatic affect detection: A survey of audiovisual databases",
      "authors": [
        "B Dudzik",
        "M.-P Jansen",
        "F Burger",
        "F Kaptein",
        "J Broekens",
        "D Heylen",
        "H Hung",
        "M Neerincx",
        "K Truong"
      ],
      "year": "2019",
      "venue": "2019 8th Int. Conf. Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Fusing body posture with facial expressions for joint recognition of affect in child-robot interaction",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "P Koutras",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "9",
      "title": "Emotion understanding in videos through body, context, and visual-semantic embedding loss",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2020",
      "venue": "Proc. 16th Eur. Conf. Computer Vision Workshops (ECCVW) -Workshop on Bodily Expressed Emotion Understanding (BEEU)"
    },
    {
      "citation_id": "10",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Int. Conf. Neural Inf. Processing"
    },
    {
      "citation_id": "11",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Proc. Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "14",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Syst., Man, and Cybern"
    },
    {
      "citation_id": "15",
      "title": "The Kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The Kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "16",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "17",
      "title": "Context based emotion recognition using EMOTIC dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "18",
      "title": "The mastery of movement",
      "authors": [
        "R Laban",
        "L Ullmann"
      ],
      "year": "1971",
      "venue": "The mastery of movement"
    },
    {
      "citation_id": "19",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Seungryong] Kim",
        "S Sunok] Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "20",
      "title": "Actionalstructural graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "M Li",
        "S Chen",
        "X Chen",
        "Y Zhang",
        "Y Wang",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Disentangling and unifying graph convolutions for skeleton-based action recognition",
      "authors": [
        "Z Liu",
        "H Zhang",
        "Z Chen",
        "Z Wang",
        "W Ouyang"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild. Int",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2019",
      "venue": "Journal Computer Vision (IJCV)"
    },
    {
      "citation_id": "23",
      "title": "EmotiCon: Context-aware multimodal emotion recognition using Frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "The Kuleshov Effect: The influence of contextual framing on emotional attributions",
      "authors": [
        "D Mobbs",
        "N Weiskopf",
        "H Lau",
        "E Featherstone",
        "R Dolan",
        "C Frith"
      ],
      "year": "2006",
      "venue": "Social Cognitive and Affective Neuroscience"
    },
    {
      "citation_id": "25",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "SUN attribute database: Discovering, annotating, and recognizing scene attributes",
      "authors": [
        "G Patterson",
        "J Hays"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. 2014 Conf. Empirical Methods Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "A set of full-body movement features for emotion recognition to help children affected by autism spectrum condition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "A Camurri",
        "F Odone"
      ],
      "year": "2013",
      "venue": "In IDGEI Int. Workshop"
    },
    {
      "citation_id": "29",
      "title": "Recognition of facial expressions is influenced by emotional scene gist",
      "authors": [
        "R Righart",
        "B Gelder"
      ],
      "year": "2008",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "31",
      "title": "A study on emotion recognition from body gestures using Kinect sensor",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "R Janarthanan"
      ],
      "year": "2014",
      "venue": "2014 Int. Conf. Communication and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition from skeletal movements",
      "authors": [
        "T Sapiński",
        "D Kamińska",
        "A Pelikant",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "33",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Two-stream adaptive graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "L Shi",
        "Y Zhang",
        "J Cheng",
        "H Lu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "An attention enhanced graph convolutional lstm network for skeleton-based action recognition",
      "authors": [
        "C Si",
        "W Chen",
        "W Wang",
        "L Wang",
        "T Tan"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Advances Neural Inf. Processing Syst. (NIPS)"
    },
    {
      "citation_id": "37",
      "title": "The Toronto face database",
      "authors": [
        "J Susskind",
        "A Anderson",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Tech. Rep"
    },
    {
      "citation_id": "38",
      "title": "Temporal Segment Networks: Towards good practices for deep action recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Van Gool"
      ],
      "year": "2016",
      "venue": "Eur. Conf. Computer Vision (ECCV)"
    },
    {
      "citation_id": "39",
      "title": "Faces in context: A review and systematization of contextual influences on affective face processing",
      "authors": [
        "M Wieser",
        "T Brosch"
      ],
      "year": "2012",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "40",
      "title": "SUN database: Large-scale scene recognition from abbey to zoo",
      "authors": [
        "J Xiao",
        "J Hays",
        "K Ehinger",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "42",
      "title": "A duality based approach for realtime TV-L 1 optical flow",
      "authors": [
        "C Zach",
        "T Pock",
        "H Bischof"
      ],
      "year": "2007",
      "venue": "29th DAGM Symp. Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Wide residual networks",
      "authors": [
        "S Zagoruyko",
        "N Komodakis"
      ],
      "year": "2016",
      "venue": "Wide residual networks",
      "arxiv": "arXiv:1605.07146"
    },
    {
      "citation_id": "44",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Image Processing"
    },
    {
      "citation_id": "45",
      "title": "Places: An image database for deep scene understanding",
      "authors": [
        "B Zhou",
        "A Khosla",
        "A Lapedriza",
        "A Torralba",
        "A Oliva"
      ],
      "year": "2016",
      "venue": "Places: An image database for deep scene understanding",
      "arxiv": "arXiv:1610.02055"
    }
  ]
}