{
  "paper_id": "2112.05851v4",
  "title": "Short And Long Range Relation Based Spatio-Temporal Transformer For Micro-Expression Recognition",
  "published": "2021-12-10T22:10:31Z",
  "authors": [
    "Liangfei Zhang",
    "Xiaopeng Hong",
    "Ognjen Arandjelovic",
    "Guoying Zhao"
  ],
  "keywords": [
    "Emotion recognition",
    "long-term optical flow",
    "temporal aggregator",
    "self-attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Being spontaneous, micro-expressions are useful in the inference of a person's true emotions even if an attempt is made to conceal them. Due to their short duration and low intensity, the recognition of micro-expressions is a difficult task in affective computing. The early work based on handcrafted spatio-temporal features which showed some promise, has recently been superseded by different deep learning approaches which now compete for the state of the art performance. Nevertheless, the problem of capturing both local and global spatio-temporal patterns remains challenging. To this end, herein we propose a novel spatio-temporal transformer architecture -to the best of our knowledge, the first purely transformer based approach (i.e. void of any convolutional network use) for micro-expression recognition. The architecture comprises a spatial encoder which learns spatial patterns, a temporal aggregator for temporal dimension analysis, and a classification head. A comprehensive evaluation on three widely used spontaneous micro-expression data sets, namely SMIC-HS, CASME II and SAMM, shows that the proposed approach consistently outperforms the state of the art, and is the first framework in the published literature on micro-expression recognition to achieve the unweighted F1-score greater than 0.9 on any of the aforementioned data sets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions play an important role in interpersonal communication and their recognition is one of the most significant tasks in affective computing. Though there some disagreement on this remains, a notable number of psychologists believe that although due to different cultural environments individuals use different languages to communicate, the expression of their emotions is rather universal  [1] . Correctly recognizing facial expressions is important in general communication and can help understanding people's mental state and emotions.\n\nWhen colloquially used, the term 'facial expressions' refers to what are more precisely technically termed facial macro-expressions (MaEs). While crucial for human interaction, providing a universal and non-verbal means of articulating emotion  [2] , facial macro-expressions can be effected voluntarily which means that they can be used to deceive. In other words, a person's macro-expression may not accurately represent their truly felt emotion. However, whatever the conscious effort, felt emotions effect short-lasting contraction of facial muscles which are expressed involuntarily under psychological inhibition. The resulting minute, sudden, and transient expressions are referred to as microexpressions (MEs). After being first observed and recognized as a phenomenon of interest by Haggard and Isaacs  [3] , and then further elaborated on by a case study reported by Ekman and Friesen  [4] , MEs began to be researched more widely by psychologists, and in the last decade attracting interest within the field of computer vision  [5] . In contrast to MaEs, MEs are subtle. They are exhibited for 0.04s to 0.2s  [1] , and with lesser facial movement. These characteristics make MEs harder to be recognized than MaEs, whether manually (i.e. by humans) or automatically (i.e. by computers).\n\nThe seminal work by Pfister, et al. and the release of the database of micro-expression movie clips, namely SMICsub (Spontaneous Micro-expression Corpus)  [6] , effected a marked empowerment of computer scientists in the realm of micro-expression recognition (MER). The first generation of solutions built upon the well-established computer vision tradition and introduced a series of handcrafted features, such as Local Binary Pattern-Three Orthogonal Planes (LBP-TOP)  [7] , 3 Dimensional Histograms of Oriented Gradients (3DHOG)  [8] , Histograms of Image Gradient Orientation (HIGO)  [9]  and Histograms of Oriented Optical Flow (HOOF)  [10]  and their variations. The next generation shifted focus towards Convolutional Neural Network (CNN) based deep learning methods  [11] ,  [12] ,  [13] ,  [14] ,  [15] . Early work by and large uses convolutional kernels to extract spatial information from micro-expression video frames. This kind of pixel level operators can be considered as capturing \"short-range\", local spatial relationships. \"Long-range\", global relationships between different spatial regions have also been proposed and studied, notably by means of Graph Convolutional Network (GCN) based architectures  [16] ,  [17] ,  [18] ,  [19] ,  [20] . The activations of Facial Action Units (AUs) are generally used as nodes to build graphs. The relationships between different AU engagements are combined with image features to improve the discriminatory power in the context of MER. However, though these approaches consider global spatial relations so as to assist learning, they can only learn these after local features are extracted, i.e. they are unable to learn both kinds of relations jointly. In order to capture automatically both short-and longrange relations at the same time, we apply Multi-head Selfattention Mechanism (MSM) instead of a Convolutional Kernel as the cornerstone of our deep learning MER architecture. As shown in Fig.  1 , the relations between block 1 and N will hardly ever be learnt by CNN but has been considered at the beginning of MSM. MSM based networks are called Transformer. Short-range and long-range relationships between elements of a sequence can be learned in a parallelized manner because transformers utilize sequences in their entirety, as opposed to processing sequence elements sequentially like recurrent networks. Most recently, transformer networks came to the attention of the CV community. By dividing them into smaller constituent patches, twodimensional images can be converted into one-dimensional sequences, translating the spatial relationships into the relationships between sequence elements (image patches). In this way, transformer networks can be simply applied to vision problems and on various tasks they have outperformed CNNs  [21] . Examples include segmentation  [22] , image super-resolution  [23] , image recognition  [24] ,  [25] , video understanding  [26] ,  [27]  and object detection  [28] ,  [29] .\n\nMost MER research in the published literature is video based, as Ben et al. elaborated  [30] , though there is a small but notable body of work on single-frame analysis  [31] ,  [32] ,  [33] . This statistic reflects the consensus that for best performance both spatial and temporal information need be considered. In particular, absolute and relative facial motions are extracted and analysed through spatial and temporal features respectively. Most handcrafted methods in existence use the same kind of operator to detect spatial and temporal information from different dimensions by considering the frames as 3D data. The resulting spatiotemporal features with uniform format are used together to implement video based MER. In deep learning based methods, spatial features are mainly extracted by means of a convolutional neural network. Some concatenate spatial features extracted from each frame and others use recurrent neural networks to derive temporal information. To integrate various spatio-temporal relations, our design makes use of long-term temporal information in spatial data (i.e. each frame of video sample) prior to the spatial encoder, and a temporal aggregation block to fuse both short-and long-term temporal relationships afterwards.\n\nIn this work we show how a transformer based deep learning architecture can be applied to MER in a manner which outperforms the current state of the art. The main contributions of the present work are as follows:  We evaluate our approach on the three well known and popular ME databases, Spontaneous Micro-Expression Corpus (SMIC)  [34] , Chinese Academy of Sciences Micro-Expression II (CASME II)  [35]  and Spontaneous Actions and Micro-Movements (SAMM)  [36] , in both Sole Database Evaluation (SDE) and Composite Database Evaluation (CDE) settings and achieve state of the art results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Micro-Expression Recognition",
      "text": "Since the publication of the SMIC data set in 2013, the volume of research on automatic micro-expression recognition has been increasing steadily over the years. From the handcrafted computer vision methods in the early years to the deep learning approaches more recently, the main ideas of micro-expression feature extraction could be categorized as primarily pursuing either a spatial strategy or a temporal one.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spatial Features",
      "text": "The fundamental challenge of computer vision is that of extracting semantic information from images or videos. Whatever the approach, the extraction of some kind of spatial features is central to addressing this challenge. Microexpression recognition is no exception. In a manner similar to many gradient based features applied previously on generic computer vision tasks, Polikovsky et al.  [8]  proposed the use of a gradient feature adapted to MER to describe local dynamics of the face. The magnitudes of local gradient projections in the XY plane is used to construct histograms across different regions, which are used as spatial features. LBP quickly became the most popular operator for microexpression analysis after Pfister et al.  [6]  first applied it to MER. This operator describes local appearance in an image. The key idea behind it is that the relative brightness of neighbouring pixels can be used to describe local appearance in a geometrically and photometrically robust manner. Its widespread use and favourable performance often make it the default baseline method when new data sets are published, or a new ME related task proposed. As for deep learning approaches, CNN model can be thought as a combination of two components: a feature extraction part and a classification part. The convolution and pooling layers perform spatial feature extraction.\n\nFurther to local appearance based features, numerous other strategies have been described for spatial feature extraction in micro-expression analysis. One of the simplest and commonest of these employs facial Region Of Interest (ROI) segmentation. Polikovsky et al.  [8]  segmented each face sample into 12 regions according to the Facial Action Coding System (FACS)  [37] , each region corresponding to an independent facial muscle complex, and applied appearance normalization to individual regions. Others have modified or extended this strategy, e.g. employing different methods for segmentation or different salient regions -11  [38] , 16  [39] , 36  [10]  instead of 12 of Polikovsky et al. Spatial feature operators are applied with each ROI rather the whole image, thus providing a more nuanced description of the face. In recent years, a more principled equivalent of this strategy (in that it is learnt, rather than predetermined by a human), can be found in the form of attention blocks applied within neural networks to improve their ability to learn spatial features. These blocks can generate weight masks for feature maps, helping a network pay greater attention to significant regions. Most recently, GCNs have also been used within deep learning frameworks as a means of capturing spatial information, often using AUs as correponding to graph nodes. For example, Lei et al.  [20]  segment node patches based on facial landmarks and fuse them with an AU GCN. Xie et al.  [18]  infer AU node features from the backbone features by global average pooling and use them to build an AU relation graph for GCN layers. These optimization measures use a priori knowledge (AUs in FACS) to enhance the extracted spatial features. Long-range spatial relationships are not directly learnt by such networks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Features",
      "text": "Since one of the most characteristic aspects of microexpressions is their sudden occurrence, temporal features cannot be ignored. While some methods in the literature do use only the single, apex frame instead of all frames in each ME sample  [31] ,  [32] ,  [33] ,  [40] , most employ all in the range between the onset frame and the offset, thus treating all temporal changes within this time period on the same footing. Some go further and employ temporal frame interpolation (as indeed we do herein) so as to increase the frame count  [6] ,  [9] ,  [10] ,  [12] ,  [39] .\n\nA vast number of handcrafted feature based approaches treat raw video data as a 3D spatio-temporal volume, treating the temporal dimension as no different than the spatial ones. In other words, they apply the same kind of operator used to extract spatial features on pseudo-images formed by a cut through the 3D volume comprising one spatial dimension and the temporal dimension. For example, in LBP-TOP, LBP operators are applied on XT and Y T planes to extract temporal features, and their histogram across the three dimensions forms the final representation. 3DHOG similarly treats videos as spatio-temporal cuboids with no distinction made between the three dimensions, but arguably with even greater uniformity than LBP-TOP in that the descriptor itself is inherently 3D based. Similar in this regard are optical flow based features, which too inherently combine local spatial and temporal elements -the use of optical strain  [41] , flow orientation  [10]  or its magnitude  [31]  are all variations on this theme.\n\nAs an alternative to the use of raw appearance imagery as input to a deep learning network, the use of preprocessed data in the form of optic flow matrices has been proposed by some authors  [15] ,  [19] ,  [42] . In this manner, proximal temporal information is exploited directly. On the other hand, the learning of longer range temporal patterns has been approached in a variety of ways by different authors. Some extract temporal patterns simply by treating video sequences as 3-dimensional matrices  [16] ,  [41] ,  [43] , rather than 2-dimensional ones which naturally capture single images. Others employ structures such as the recurrent neural network (RNN) or the LSTM  [12] ,  [44] . In addition to the use of off-the-shelf recurrent deep learning strategies, recently there has been an emergence of methods which apply domain specific knowledge so as to make the learning particularly effective for micro-expression analysis  [15] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Transformers In Computer Vision",
      "text": "For approximately a decade now, convolutional neural networks have established themselves as the backbone of most deep learning algorithms in computer vision. However, convolution always operates on fixed size windows and is thus unable to extract distal relations. The idea of a transformer was first introduced in the context of NLP. It relies on a self-attention mechanism, learning the relationships between elements of a sequence. Transformers are able to capture 'long-term' dependence between sequence elements which is challenging for conventional recurrent models to encode. By dividing an image into sub-images and imposing a consistent ordering on them, a planar image can be converted into a sequence, so spatial dependencies can be learned in the same way as temporal features. For this reason, transformer based deep learning architectures have recently gained significant attention from the computer vision community and are starting to play an increasing role in a number of computer vision tasks.\n\nA representative example in the context of object detection is the DEtection TRansformer (DETR)  [28]  framework which uses transformer blocks first, for regression and classification, but the visual features are still extracted by a CNN based backbone. The Image Generative Pre-Training (iGPT) approach of Chen et al.  [45]  attempts to exploit the strengths of transformers somewhat differently, pretraining BERT (Bidirectional Encoder Representations from Transformers)  [46] , originally proposed for language understanding, and thereafter fine tuning the network with a small classification head. iGPT uses pixels instead language tokens within BERT, but suffers from significant information loss effected by a necessary image resolution reduction. In the context of classification, the Vision Transformer (ViT) approach of Dosovitskiy et al.  [24]  applies transformer encoding of image patches as a means of extracting visual features directly. It is the first pure vision transformer, and in its spirit and design, follows the original transformer  [47]  architecture faithfully. As such, it facilitates the application of scalable transformer architectures used in NLP effortlessly.\n\nFollowing these successes, transformers have been applied to a variety of computer vision tasks, including those in the realm of affective computing  [48] ,  [49] . Notable examples include facial action unit detection  [50]  and facial image-based macro-expression recognition  [51] . However, none of the existing approaches to micro-expression recognition adequately make use of both the spatial and temporal information due to the design difficulties posed by the challenges we discussed in the previous sections.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "In the present work we propose a method that takes advantage both of the physiological understanding of microexpressions and their characteristics, as well as of the transformer framework. The approach overcomes many of the weaknesses of the existing MER methods in the literature as discussed in the previous section. Importantly, our method is able to extract and thus benefit both from proximal (i.e. short-range) and distal (i.e. long-range) spatio-temporal features. Each element of the proposed framework is laid out in detail next, corresponds to each sub-section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Long-Term Optical Flow",
      "text": "Optical flow describes the apparent motion of brightness patterns between frames, caused by the relative movement of the content of a scene and the camera used to image it  [52] . If the camera is static, optical flow can be used to infer both the direction and the magnitude of an imaged object's movement from the change in the appearance of pixels between frames  [53] . Optical flow is inherently temporally local, i.e. save for practical considerations (numerical, efficiency, etc.) it is computed between consecutive frames of sequence. This introduces a problem when micro-expression videos are considered, created by the already noted limited motion exhibited during the expressions. Therefore, herein we propose to calculate optical flow between each sample frame and the onset frame instead of consecutive frames, see Fig.  3 . To see the reasons behind this choice, consider Fig.  4  which shows optical flow fields of consecutive frames starting with the micro-expression onset frame. It can be readily observed that the fields are rather similar up to the apex frame, which can be attributed to the aforementioned brevity of the expression, with a similar trend thereafter but in the opposite direction. In contrast, our, temporally non-local modified optical flow -long-term optical flow in a manner of speaking -exhibits a much more structured pattern, always being in the same direction, increasing in magnitude up to the apex frame and declining in magnitude thereafter. This results in much more stable and discriminative features associated with each micro-expression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spatial Feature Extraction",
      "text": "The key idea underlying the proposed method lies in the extraction of long-range spatial relations from each frame using a transformer encoder, with images as before being treated as sequences of constituent patches. More specifically, input frames are first represented as vector sequences with local spatial features of each image patch. The resulting sequences are then fed into the transformer encoder for long-term spatial feature extraction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input Embedding And Short-Range Spatial Relation Learning",
      "text": "The standard transformer receives a 1D sequence as input. To handle 2D images, we represent each image as a sequence of rasterized 2D patches. Herein we do not use appearance images, that is the original video sequence frames, as input but rather the corresponding optical flow fields. An input  embedding block is proposed as a means of representing input images as vector sequences for input to the transformer encoder.\n\nThe general input embedding mechanism considers the image X ∈ R H×W ×C as a sequence of non-overlapping P × P pixel patches, where H, W , and C are respectively the height, the width, and the channel count of the input. Different from the \"separate and flat\" linear patch embedding proposed by Dosovitskiy et al.  [24] , we first extract local spatial features in patch regions with a patch-wise fully connected layer. Patches of image X are represented as X p ∈ R N ×(P 2 ,C) . As shown in Fig.  5 , we extract the short-range spatial features from image X to feature map X ∈ R After that, a learnable D-dimensional vector is concatenated with the sequence, as the class token (Z 0 [0] = x class ), whose state as the output of the transformer encoder (Z L T [0]). The effective input sequence length for the transformer encoder is thus N + 1. Then a position embedding is added to each vector in the sequence. The whole input embedding procedure can be described as follows:\n\nwhere Z 0 ∈ R (N ×D) is the input of the transformer encoder.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Long-Range Spatial Relation Learning By Transformer Encoder",
      "text": "After short-range spatial relation are extracted from the input long-term optical flow fields of each frame and embedded as vectors, they are passed to a transformer encoder for further long-range spatial feature extraction.\n\nOur encoder contains L T transformer layers; herein we use L T = 12, adopting this value from the ViT-Base model of Dosovitskiy et al.  [24]  (the pre-trained encoder we use in experiments).Each layer involves two blocks, a Multihead Self-attention Mechanism (MSM) and a Position-Wise fully connected Feed-Forward network (PWFF), as shown in Fig.  6 . Layer Normalisation (LN) is applied before each block and residual connections after each block  [54] ,  [55] .\n\nThe output of the transformer layer can be written as follows:\n\nwhere Z l is the output of layer l. The PWFF block contains two layers with the Gaussian Error Linear Unit (GELU) nonlinear activation function. The feature embedding dimension thereby first increases from D to 4D and then reduces back to D, which equals 768 in our experiments.\n\nMulti-head attention allows the model to focus simultaneously on information content from different parts of the sequences, so both long-range and short-range spatial relations can be learnt. An attention function is mapping a query and a set of key-value pairs to the output, a weighted sum of the values. The weights are computed using a compatibility function of the queries with the corresponding keys, and they are all vectors. The self-attention function is computed on a set of queries simultaneously. The queries, keys and values can be grouped together and represented as matrices Q, K and V , so the computation of the matrix of outputs can be written as:\n\nwhere W Q , W K , W V ∈ R D×Dm are learnable matrices and SA is the self-attention module. MSM can be seen as a type of self-attention with M heads in parallel operation and a projection of their concatenated outputs:\n\nwhere\n\n, so as to keep the number of parameters constant with changing M .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Temporal Aggregation",
      "text": "After extracting both local and global spatial features associated with each frame using a transformer encoder, we introduce an aggregation block to extract temporal features before performing the ultimate classification. The aggregation function ensures that our transformer model can be trained and applied to the spatial feature sets of each frame, subsequently processing the temporal relations between frames in each sample. Since facial movement during micro-expressions is almost imperceptible, all frames from a single video sample are rather similar one to another. Nevertheless, it is still possible to identify reliably a number of salient frames, such as the apex frame, that play a particularly important role in the analysis of a micro-expression.  Long Short-Term Memory (LSTM)  [56]  is a type of recurrent neural network with feedback connections, which overcomes two well-known problems associated with RNNs: the vanishing gradient problem, and the sensitivity to the variation of the temporal gap length between salient events in a processed sequence. The elements of the input are the sets of outputs from the transformer encoder for each frame. The inputs are not concatenated, and the input sequence length is thus dependent on the number of frames in each ME video sample.\n\nWe used three LSTM layers in the aggregation block. The computation details of each layer are:\n\nwhere F is the number of chosen frames in each video sample, L A is the total number of layers in both the transformer encoder and the LSTM aggregator. Z t l denotes the outputs of the layer l after t frames have been processed. After all frames are processed in this manner, the result is a single feature set describing the entire micro-expression video sample. Finally, these features are fed into an MLP which is used for the ultimate MER classification. The details of how previous output join the latter training are presented in Fig.  7 . We also design a comparative experiment to demonstrate the effectiveness of the LSTM aggregator, the details of which are described in the Section 4.3.2.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Network Optimization",
      "text": "Following the aggregation block, our network contains two fully connected layers which facilitate the final classification achieved using the SoftMax activation function. Cross Entropy loss is used as the objective function for training:\n\nwhere N is the number of the ME video samples and C the number of emotion classes. The value of y ic is 1 when the true class of sample i is equal to c and 0 otherwise. Similarly, p ic is the predicted probability that sample i belongs to class c. When using gradient descent to optimize the objective function during network training, as the parameter set gets closer to its optimum, the learning rate should be reduced. Herein we achieve this using cosine annealing  [57] , i.e. using the the cosine function to modulate the learning rate which initially decreases slowly, and then rather rapidly before stabilizing again. This learning rate adjustment is particularly important in the context of the problem at hand, considering that the number of available micro-expression video samples is not large even in the largest corpora, readily learning to overfitting if due care is not taken.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Evaluation",
      "text": "In this section we describe the empirical experiments used to evaluate the proposed method. We begin with a description of the data sets used, follow up with details on the data preprocessing performed, relevant implementation details, and evaluation metrics, and conclude with a report of the results and a discussion of the findings.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Databases",
      "text": "Following the best practices in the field, for our evaluation we adopt the use of three large data sets, namely the Spontaneous Micro-Expression Corpus (SMIC)  [34] , the Chinese Academy of Sciences Micro-Expression II data set (CASME II)  [35] , and the Spontaneous Actions and Micro-Movement database (SAMM)  [36] , thus ensuring sufficient diversity of data, evaluation scale, and ready and fair comparison with other methods in the literature. All video samples in these databases capture spontaneously exhibited, rather than acted micro-expressions (see Zhang and Arandjelović  [5]  for discussion), which is important for establishing the realworld applicability of findings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Smic",
      "text": "The Spontaneous Micro-Expression Corpus (SMIC) is the earliest published spontaneous micro-expression database  [34] . It comprises three distinct parts captured by cameras of different types, namely a conventional visual camera (VIS), a near-infrared camera (NIR) and a highspeed camera (HS). These subsets are designed to study micro-expression analysis tasks in various application scenarios. To achieve uniformity with the other two corpora, namely CASME-II and SAMM which are described next, which only contain high-speed camera videos, it is the HS subset from SMIC that we make use of herein. The SMIC-HS contains 164 video sequences (samples) from 16 subjects of 3 ethnicities. Using two human labellers, these videos are categorized as corresponding to either negative (70), positive  (51) , or surprised (43) expression, and both raw and cropped frames are provided.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Casme Ii",
      "text": "The Chinese Academy of Sciences Micro-Expression II (CASME II) data set contains 247 micro-expression video samples from 26 Chinese participants. The full videos have the resolution of 640 × 480 pixels. Cropped facial frames in 280 × 340 pixel resolution (higher than both CASME and SMIC-HS), extracted using the same face registration and alignment method as for SMIC, are also provided. The micro-expression samples in CASME II are labelled by 2 coders to 5 classes, namely Happiness  (33) , Disgust  (60) , Surprise  (25) , Repression  (27) , and Others (102).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Samm",
      "text": "The Spontaneous Actions and Micro-Movement (SAMM) database is the newest MER corpus. The 159 microexpression short videos in the corpus were collected using 32 participants of 13 ethnicities, with an even gender distribution (16 male and 16 female), at 200 fps and the resolution of 2040 × 1088 pixels, with the face region size being approximately 400 × 400 pixels. The samples are assigned to one of 8 emotion classes, namely Anger (57), Happiness  (26) , Other  (26) , Surprise  (15) , Contempt  (12) , Disgust  (9) , Fear  (8)  and Sadness (6).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Face Cropping",
      "text": "As noted in the previous section, cropped face images are explicitly provided in both SMIC-HS and CASME II data sets, with the same registration method used in both; no cropped faces are provided as part of SAMM. In order to maintain data consistency across different databases, in our experiments we employ a different face extraction approach. In particular, we utilize the Ensemble of Regression Trees (ERT)  [58]  algorithm implemented in DLib  [59]  to localize salient facial loci (68 of them) in a uniform manner regardless of which data set a specific video sample came from.\n\nIn the case of SMIC-HS and CASME II videos, the original authors' face extraction process consists of facial landmarks detection in the first frame of a micro-expression clip and then the detected face being registered to the model face using a LWM transformation. Motivated by the short duration of MEs, the faces in all remaining frames of the video sample are registered using the same matrix.\n\nHowever, in this paper we employ an alternative strategy. The primary reason lies in the need for sufficient and representative data diversity, which is particularly important in deep learning. In particular, the original face extraction method just described, often results in the close resemblance of samples which increases the risk of model overfitting. Therefore, herein we instead simply use a nonreflective 2D Euclidean transformation, i.e. one comprising only rotation and translation. By doing so, at the same time we ensure the correct alignment of salient facial points and maintain information containing facial contour variability. Furthermore, unlike the authors of SMIC-HS and CASME II, we do not perform facial landmark detection in the first frame of a micro-expression sample, but rather in the apex, thereby increasing the registration accuracy of the most informative parts of the video. As shown in Fig.  8 , points 27-30 can be used to determine the centre line of the nose that can be considered as the vertical symmetry line of the entire face area. Point 30 is set as the centre point, and the square size s (in pixels) is computed by adding the vertical distance from the centre point of the eyebrows  (19)  to the lowest point of the chin (8), y apex  [8]  -y apex  [19]  , to the height of chin, y apex  [8]  -y apex  [57]  , so that nearly the entire face is included in the cropped image: s = (y apex  [8]  -y apex  [19]  ) + (y apex  [8]  -y apex  [57]  ).\n\n(16)",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Temporal Interpolation",
      "text": "Considering the short duration of micro-expressions, even when samples are acquired using high-speed cameras, in some instances only a small number (cc. 10) of frames is available. In an attempt to extract accurate temporal information, we also apply frame interpolation from raw videos, effectively synthetically augmenting data. In previous work, the Temporal Interpolation Model (TIM) relies on a path graph to characterize the structure of a sequence of frames, popularly used in several handcrafted feature based methods  [9] ,  [13] ,  [60] , whereas Liu et al.  [10]  use simple linear interpolation. Herein we propose a novel approach to interpolation so that its result is smoother in terms of optical flow, it being the nexus of our entire MER methodology. Most existing optical flow based methods produce artifacts on motion boundaries by estimating bidirectional optical flows, scaling and reversing them to approximate intermediate flows. We adopt the Real-time Intermediate Flow Estimation (RIFE) method  [61] , which uses an endto-end trainable neural network, IFNet, which speedily and directly estimates the intermediate flows.\n\nOriginal RIFE interpolates one frame between two given consecutive frames, so we apply it recursively to interpolate multiple intermediate frames. Specifically, given any two consecutive input frames I 0 , I 1 , we apply RIFE once to get intermediate frame Î0.5 at t = 0.5. We then apply RIFE to interpolate between I 0 and Î0.5 to get Î0.25 , and so on. In our experiment, we prioritize interpolation in the temporal vicinity of the apex frame. The interpolated queue can be expressed as Îa-0.5 , Îa+0.5 , Îa-1.5 , Îa+1.5 , . . . , Îo+0.5 or Îf-0.5 , where a, o and f are frame indices of the apex, onset, and offset frames respectively. Recall that the apex frames are specified explicitly in CASME II and SAMM, and for SMIC-HS we choose the middle frame of each sample video as the apex. If the number of interpolation frames is lower than the reference count (the average number of frames in this period across the database), we use the same method on the updated frame sequence iteratively to generate further intermediate frames.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation Details",
      "text": "In the spatial feature extraction procedure, we employed base ViT blocks, with 12 Encoder layers, hidden size of 768, MLP size of 3072, and 12 heads. For initialization, we use the official ViT-B/16 model  [24]  pre-trained on ImageNet  [62] . We resize our input images to 384 × 384 pixels and split each image into patches with 16 × 16 pixels, so that the number of patches is 24 × 24. 768-dimensional vectors are passed though all transformer encoder layers.\n\nFor temporal aggregation, we select 11 frames (apex, and five preceding and succeeding it) per sample as inputs for the mean aggregator and LSTM aggregator. We have tried other options with different number of frame, but it didn't work any better. We only use long-term optical flow in experiments, as motivated by the arguments discussed in Section 3.1. For learning parameters, the initial learning rate and weight decay are set to be 1e-3 and 1e-4, respectively. The momentum for Stochastic Gradient Decent (SGD) is set to 0.9, with the batch size 4 for all experiments. All the experiments were conducted with PyTorch.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mean Versus Lstm Aggregator",
      "text": "We compare our LSTM aggregator with an alternative which uses the simple mean operator for temporal aggregation.\n\nAfter each frame is processed by spatial encoder, the corresponding output is used in the computation by the mean aggregation layer (layer L T + 1):\n\nIn a manner similar to that described previously in the context of the LSTM Aggregator, outputs of each frame from our transformer encoder are taken as inputs to the temporal feature extraction module. Compared to the mean operator, LSTM has the advantage of larger expressive capability, resulting in different extracted relationships between different frames. Within the specific context of our work, this means that its ability to distinguish between emotions is also different, with LSTM expected to perform better.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following previous work and the Micro-Expressions Grand Challenges (MEGCs), we conducted experiments on SMIC-HS, CASME II, and SAMM, evaluating the classification performance using the corresponding original emotion classes, as well as the composite corpus formed using all three data sets and relabelled using three classes as proposed in MEGC 2019  [63] . All results are reported using LOSO crossvalidation. Evaluation is repeated multiple times by holding out test samples of each subject group while the remaining samples are used for training. In this way we best mimic real-world situations and in particular assess the robustness to variability in ethnicity, gender, emotional sensitivity, etc. 4.3.3.1 Sole Database Evaluation (SDE): In the first part of our empirical evaluation, experiments are conducted on three databases individually, using the corresponding original emotion labels, excepting the very rare (and thus underrepresented) classes in CASME II and SAMM. SMIC-HS uses 3 class labels whereas the other two sets both use 5. We use accuracy and macro F1-score to assess the recognition performance.\n\n4.3.3.2 Composite Database Evaluation (CDE): In the second part of our empirical evaluation, experiments are conducted on the composite database with 3 emotion classes (negative, positive, and surprise). The composite database, that is the database obtained by merging SMIC, CASME II, and SAMM contains the total of 68 subjects, 16 from SMIC, 24 from CASME II and 28 from SAMM. LOSO cross-validation is applied on each database separately and together on the composite database. Unweighted F1score (UF1), also known as the macro F1-score and Unweighted Average Recall (UAR) are used to assess performance:\n\nwhere N c is the total number of samples of class c across all subjects.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results And Discussion",
      "text": "We compare the performance of the proposed approach with baseline handcrafted feature extraction methods and the most prominent recent deep learning based methods on the widely used micro-expression databases, SMIC-HS, CASME II, and SAMM, described in the previous section, both in the SDE and the CDE settings. To ensure uniformity and fairness of the comparison, the SDE results for all methods were obtained in identical conditions, i.e. for the identical number of samples, the number of labels (classes), and using the same cross-validation approach. The details of the performance of our SLSTT on different emotion categories are shown in Fig.  9 .\n\nAs can be readily seen in Table  1  which presents a comprehensive overview of our experimental results in the SDE setting, the method proposed in the present paper performs best (n.b. shown in bold) in all but one testing scenario, in which it is second best (n.b. second best performance is denoted by square brackets), trailing marginally behind the method introduced by Sun et al.  [69] . What is more, in most cases our method outperforms rivals by a significant margin.\n\nMoving next to the results of our experiments in the CDE setting, these are summarized in Table  2 . It can be readily seen that our method's performance is again shown to be excellent. In particular, in most cases our method again comes out either at the top or second best (as before the former being shown in bold and the latter denoted by square brackets enclosure). The only existing method in the literature which remains competitive against ours is that of Lei et al.  [20] . To elaborate in further detail, our approach achieved the best results both in terms of UF1 and UAR on CASME II, and on UF1 on the full composite database, and second best on UAR on the composite database and on UF1 on SMIC-HS. The performance of all methods on CASME II is consistently higher than when applied on other data sets, which suggests that the challenge of MER is increased with ethnic diversity of participants -this should be born in mind in future research and any comparative analysis. It is insightful to observe that in contrast with the results in the SDE setting already discussed (see Table  1 ), our method does not come out as dominant in the context of CDE. This suggests an important conclusion, namely that our method is particularly capable of nuanced learning over finer grained classes and that its superiority is less able to come through in a simpler setting when only 3 emotional classes as used.\n\nTaking into account the results from both the sole and the composite database experiments, it is useful to observe that when only short-range patterns are utilized, convolutional neural network approaches do not outperform methods based on handcrafted feature. It is the inclusion of longrange spatial learning that is key, as shown by the marked improvement in performance of the corresponding methods. Yet, the proposed method's exceeds even their performance, owing to its use of a multi-head self-attention mechanism, thus demonstrating its importance in MER. The superiority of our short-and long-range relation based spatiotemporal transformer is further corroborated by the results shown in the latest two rows in both Table  1  and Table  2  which summarize our comparison of the proposed LSTM aggregator with the simpler mean operator aggregator.\n\nIn CASME II, distinguishing whether a micro-expression is Disgust or Others is inherently difficult because the database contains multiple inconsistently labelled samples with only AU4 activated -some of them are labelled as Others, some as Disgust. It is also worth noting that in SAMM, some AU labels ('AU12 or 14') for the Contempt class were not manually verified, which also causes confusion with the Happiness class (mostly with AU12 labelled). In part, these labelling issues emerge from the fact that the mapping between facial action unit activations and emotions (as understood by psychologists) is not a bijection. It is also the case that imperfect information is made use of because only visual data is used. Hence, it should be understood that the theoretical highest accuracy of automated microexpression recognition on the MER corpora currently used for research purposes is not 100%. The micro-expression databases containing multi-modal signals  [74] ,  [75] , which have begun emerging recently, seem promising in overcoming some of the limitations of the existing corpora, and we intend to make use of them in our future work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a novel transformer based spatiotemporal deep learning framework for micro-expression recognition, which is the first deep learning work in the field entirely void of convolutional neural network use. In our framework both short-and long-term relations between pixels in spatial and temporal directions of the sample videos can be learned. We use transformer encoder layers with multi-head self-attention mechanism to learn spatial relations from visualized long-term optical flow frames and design a temporal aggregation block for temporal relations. Extensive experimental results using three large MER databases, both in the context of sole database evaluation and composite database evaluation settings and the Leave One Subject Out cross validation protocol, consistently demonstrate that our approach is effective and outperforms the current state of the art. These findings strongly motivate further research on the use of transformer based architectures rather than convolutional neural networks in micro-expression analysis, and we hope that our theoretical contributions will help direct such future efforts.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of the different spatial feature extraction methods of",
      "page": 2
    },
    {
      "caption": "Figure 1: , the relations between block 1",
      "page": 2
    },
    {
      "caption": "Figure 2: The framework of proposed Short and Long range relation based",
      "page": 2
    },
    {
      "caption": "Figure 2: To the best of our knowledge, ours",
      "page": 2
    },
    {
      "caption": "Figure 3: Different computing mechanism between short- and long-term",
      "page": 4
    },
    {
      "caption": "Figure 3: To see the reasons behind this choice, consider Fig. 4 which",
      "page": 4
    },
    {
      "caption": "Figure 4: Illustration of optic ﬂow computed between the onset and the apex",
      "page": 5
    },
    {
      "caption": "Figure 6: Layer Normalisation (LN) is applied before each",
      "page": 5
    },
    {
      "caption": "Figure 5: Long-term optical ﬂow ﬁelds are as inputs of the Input Embedding blocks. After short-range spatial feature extraction, patch and position",
      "page": 6
    },
    {
      "caption": "Figure 6: Detailed structure of a Transformer Encoder layer. The output of",
      "page": 6
    },
    {
      "caption": "Figure 7: We also design a comparative experiment to",
      "page": 6
    },
    {
      "caption": "Figure 7: The repeating module in an LSTM aggregator layer.",
      "page": 7
    },
    {
      "caption": "Figure 8: The 68 facial landmarks used by our method, shown for the onset",
      "page": 8
    },
    {
      "caption": "Figure 9: As can be readily seen in Table 1 which presents a com-",
      "page": 9
    },
    {
      "caption": "Figure 9: Confusion matrices corresponding to each of our experiments. Only one is shown for SMIC-HS because the SDE and the CDE are identical",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SMIC-HS": "Acc(%)",
          "CASME II": "Acc(%)",
          "SAMM": "Acc(%)"
        },
        {
          "SMIC-HS": "53.66\n44.51\n57.93\n64.02\n60.37\n68.29",
          "CASME II": "46.46\n46.56\n59.51\n58.39\n–\n67.21",
          "SAMM": "–\n–\n–\n–\n–\n–"
        },
        {
          "SMIC-HS": "59.76\n63.41\n–\n–\n49.40\n76.06\n64.63",
          "CASME II": "62.96\n70.78\n49.20\n42.71\n65.90\n72.61\n[75.20]",
          "SAMM": "52.94\n57.35\n48.90\n–\n48.50\n–\n55.88"
        },
        {
          "SMIC-HS": "73.17\n[75.00]",
          "CASME II": "73.79\n75.81",
          "SAMM": "[66.42]\n72.39"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Composite": "UF1",
          "SMIC-HS": "UF1",
          "CASME II": "UF1",
          "SAMM": "UF1"
        },
        {
          "Composite": "0.588\n0.630",
          "SMIC-HS": "0.200\n0.573",
          "CASME II": "0.703\n0.781",
          "SAMM": "0.395\n0.521"
        },
        {
          "Composite": "0.589\n0.425\n0.516\n0.505\n0.720\n0.652\n0.732\n0.735\n0.789\n0.631\n0.705\n[0.791]",
          "SMIC-HS": "0.461\n0.460\n0.411\n0.410\n0.682\n0.582\n0.665\n0.680\n0.746\n0.553\n0.598\n0.719",
          "CASME II": "0.625\n0.291\n0.589\n0.559\n0.876\n0.707\n0.862\n0.838\n0.829\n0.798\n0.809\n[0.880]",
          "SAMM": "0.476\n0.565\n0.414\n0.410\n0.541\n0.621\n0.587\n0.659\n0.775\n0.496\n0.677\n[0.775]"
        },
        {
          "Composite": "0.788\n0.816",
          "SMIC-HS": "0.719\n[0.740]",
          "CASME II": "0.844\n0.901",
          "SAMM": "0.625\n0.715"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "2",
      "title": "Quantification of advanced dementia patients' engagement in therapeutic sessions: An automatic video based approach using computer vision and machine learning",
      "authors": [
        "L Zhang",
        "O Arandjelović",
        "S Dewar",
        "A Astell",
        "G Doherty",
        "M Ellis"
      ],
      "year": "2020",
      "venue": "Proceedings International Conference of the IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "3",
      "title": "Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy",
      "authors": [
        "L Gottschalk",
        "A Auerbach",
        "E Haggard",
        "K Isaacs"
      ],
      "year": "1966",
      "venue": "Methods of Research in Psychotherapy"
    },
    {
      "citation_id": "4",
      "title": "Nonverbal Leakage and Clues to Deception",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Review of Automatic Microexpression Recognition in the Past Decade",
      "authors": [
        "L Zhang",
        "O Arandjelović"
      ],
      "year": "2021",
      "venue": "Machine Learning and Knowledge Extraction"
    },
    {
      "citation_id": "6",
      "title": "Recognising spontaneous facial micro-expressions",
      "authors": [
        "T Pfister",
        "X Li",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "7",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Facial micro-expressions recognition using high speed camera and 3D-Gradient descriptor",
      "authors": [
        "S Polikovsky",
        "Y Kameda",
        "Y Ohta"
      ],
      "year": "2009",
      "venue": "IET Seminar Digest"
    },
    {
      "citation_id": "9",
      "title": "Towards reading hidden emotions: A comparative study of spontaneous micro-expression spotting and recognition methods",
      "authors": [
        "X Li",
        "X Hong",
        "A Moilanen",
        "X Huang",
        "T Pfister",
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
      "authors": [
        "Y Liu",
        "J Zhang",
        "W Yan",
        "S Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Selective deep features for micro-expression recognition",
      "authors": [
        "D Patel",
        "X Hong",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of 23rd international conference on pattern recognition (ICPR"
    },
    {
      "citation_id": "12",
      "title": "Enriched longterm recurrent convolutional network for facial micro-expression recognition",
      "authors": [
        "H Khor",
        "J See",
        "R Phan",
        "W Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of 13th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "13",
      "title": "Micro-expression recognition based on 3D flow convolutional neural network",
      "authors": [
        "J Li",
        "Y Wang",
        "J See",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "14",
      "title": "Spontaneous facial micro-expression recognition via deep convolutional network",
      "authors": [
        "Z Xia",
        "X Feng",
        "X Hong",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proceedings of 8th International Conference on Image Processing Theory, Tools and Applications"
    },
    {
      "citation_id": "15",
      "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "MER-GCN: Micro-Expression Recognition Based on Relation Modeling with Graph Convolutional Networks",
      "authors": [
        "L Lo",
        "H Xie",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of 3rd International Conference on Multimedia Information Processing and Retrieval"
    },
    {
      "citation_id": "17",
      "title": "FACS-Based Graph Features for Real-Time Micro-Expression Recognition",
      "authors": [
        "A Buhari",
        "C.-P Ooi",
        "V Baskaran",
        "R Phan",
        "K Wong",
        "W.-H Tan"
      ],
      "year": "2020",
      "venue": "Journal of Imaging"
    },
    {
      "citation_id": "18",
      "title": "Au-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Micro-Expression Classification Based on Landmark Relations With Graph Attention Convolutional Network",
      "authors": [
        "A Kumar",
        "B Bhanu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "20",
      "title": "Micro-Expression Recognition Based on Facial Graph Representation Learning and Facial Action Unit Fusion",
      "authors": [
        "L Lei",
        "T Chen",
        "S Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "21",
      "title": "Transformers in Vision: A Survey",
      "authors": [
        "S Khan",
        "M Naseer",
        "M Hayat",
        "S Zamir",
        "F Khan",
        "M Shah"
      ],
      "year": "2021",
      "venue": "Transformers in Vision: A Survey",
      "arxiv": "arXiv:2101.01169"
    },
    {
      "citation_id": "22",
      "title": "Cross-modal self-attention network for referring image segmentation",
      "authors": [
        "L Ye",
        "M Rochan",
        "Z Liu",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Learning texture transformer network for image super-resolution",
      "authors": [
        "F Yang",
        "H Yang",
        "J Fu",
        "H Lu",
        "B Guo"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "Others"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "25",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "H Touvron",
        "M Cord",
        "M Douze",
        "F Massa",
        "A Sablayrolles",
        "H Jégou"
      ],
      "year": "2020",
      "venue": "Training data-efficient image transformers & distillation through attention",
      "arxiv": "arXiv:2012.12877"
    },
    {
      "citation_id": "26",
      "title": "VideoBERT: A joint model for video and language representation learning",
      "authors": [
        "C Sun",
        "A Myers",
        "C Vondrick",
        "K Murphy",
        "C Schmid"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Video action transformer network",
      "authors": [
        "R Girdhar",
        "J Joao Carreira",
        "C Doersch",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
      "authors": [
        "X Zhu",
        "W Su",
        "L Lu",
        "B Li",
        "X Wang",
        "J Dai"
      ],
      "year": "2020",
      "venue": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
      "arxiv": "arXiv:2010.04159"
    },
    {
      "citation_id": "30",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S.-J Wang",
        "K Kpalma",
        "W Meng",
        "Y.-J Liu"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "31",
      "title": "Less is more: Micro-expression recognition from video using apex frame",
      "authors": [
        "S Liong",
        "J See",
        "K Wong",
        "R Phan"
      ],
      "year": "2018",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "32",
      "title": "OFF-ApexNet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S Liong",
        "W Yau",
        "Y Huang",
        "L Tan"
      ],
      "year": "2019",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "33",
      "title": "Joint Local and Global Information Learning With Single Apex Frame Detection for Micro-Expression Recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Society"
    },
    {
      "citation_id": "34",
      "title": "A Spontaneous Micro-expression Database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2013",
      "venue": "Proceedings of 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG"
    },
    {
      "citation_id": "35",
      "title": "CASME II: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W Yan",
        "X Li",
        "S Wang",
        "G Zhao",
        "Y Liu",
        "Y Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "36",
      "title": "SAMM: A Spontaneous Micro-Facial Movement Dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Facial Action Coding System -Investigator's Guide",
      "authors": [
        "P Ekman",
        "W Friesen",
        "J Hager"
      ],
      "year": "2002",
      "venue": "Facial Action Coding System -Investigator's Guide"
    },
    {
      "citation_id": "38",
      "title": "Facial Action Unit Detection with Local Key Facial Sub-region Based Multi-label Classification for Micro-expression Analysis",
      "authors": [
        "L Zhang",
        "O Arandjelović",
        "X Hong"
      ],
      "year": "2021",
      "venue": "ACM international conference on Multimedia Workshops"
    },
    {
      "citation_id": "39",
      "title": "Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Directional Features",
      "authors": [
        "S.-J Wang",
        "W.-J Yan",
        "G Zhao",
        "X Fu",
        "C.-G Zhou"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "A novel apex-time network for cross-dataset micro-expression recognition",
      "authors": [
        "M Peng",
        "C Wang",
        "T Bi",
        "Y Shi",
        "X Zhou",
        "T Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of 8th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "41",
      "title": "Shallow triple stream three-dimensional CNN (STSTNet) for micro-expression recognition",
      "authors": [
        "S Liong",
        "Y Gan",
        "J See",
        "H Khor",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "42",
      "title": "A neural microexpression recognizer",
      "authors": [
        "Y Liu",
        "H Du",
        "L Zheng",
        "T Gedeon"
      ],
      "year": "2019",
      "venue": "Proceedings of 14th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "43",
      "title": "Spontaneous facial micro-expression recognition using 3d spatiotemporal convolutional neural networks",
      "authors": [
        "S Reddy",
        "S Karri",
        "S Dubey",
        "S Mukherjee"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "44",
      "title": "Micro-expression recognition with expression-state constrained spatio-temporal feature representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM Multimedia Conference"
    },
    {
      "citation_id": "45",
      "title": "Pre-Trained Image Processing Transformer",
      "authors": [
        "H Chen",
        "Y Wang",
        "T Guo",
        "C Xu",
        "Y Deng",
        "Z Liu",
        "S Ma",
        "C Xu",
        "C Xu",
        "W Gao"
      ],
      "year": "2021",
      "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Google"
      ],
      "year": "2018",
      "venue": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "47",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Transformer encoder with multimodal multi-head attention for continuous affect recognition",
      "authors": [
        "H Chen",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Emotion transformer fusion: Complementary representation properties of eeg and eye movements on recognizing anger and surprise",
      "authors": [
        "Y Wang",
        "W.-B Jiang",
        "R Li",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "50",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "51",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Detection of dynamic background due to swaying movements from motion features",
      "authors": [
        "D.-S Pham",
        "O Arandjelović",
        "S Venkatesh"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "53",
      "title": "Cctv scene perspective distortion estimation from low-level motion features",
      "authors": [
        "O Arandjelović",
        "D.-S Pham",
        "S Venkatesh"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "54",
      "title": "Learning deep transformer models for machine translation",
      "authors": [
        "Q Wang",
        "B Li",
        "T Xiao",
        "J Zhu",
        "C Li",
        "D Wong",
        "L Chao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "55",
      "title": "Adaptive input representations for neural language modeling",
      "authors": [
        "A Baevski",
        "M Auli"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "56",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "57",
      "title": "SGDR: Stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "58",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "J Sullivan"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "60",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S Wang",
        "B Li",
        "Y Liu",
        "W Yan",
        "X Ou",
        "X Huang",
        "F Xu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "61",
      "title": "RIFE: Real-time intermediate flow estimation for video frame interpolation",
      "authors": [
        "H Zhewei",
        "Z Tianyuan",
        "H Wen",
        "S Boxin",
        "Z Shuchang"
      ],
      "year": "2020",
      "venue": "RIFE: Real-time intermediate flow estimation for video frame interpolation",
      "arxiv": "arXiv:2011.06294"
    },
    {
      "citation_id": "62",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "63",
      "title": "MEGC 2019the second facial micro-expressions grand challenge",
      "authors": [
        "J See",
        "M Yap",
        "J Li",
        "X Hong",
        "S.-J Wang"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "64",
      "title": "Spontaneous facial micro-expression analysis using Spatiotemporal Completed Local Quantized Patterns",
      "authors": [
        "X Huang",
        "G Zhao",
        "X Hong",
        "W Zheng",
        "M Pietikäinen"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "65",
      "title": "Dualstream shallow networks for facial micro-expression recognition",
      "authors": [
        "H Khor",
        "J See",
        "S Liong",
        "R Phan",
        "W Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of International Conference on Image Processing"
    },
    {
      "citation_id": "66",
      "title": "Facial Micro-Expression Recognition Using Spatiotemporal Local Binary Pattern with Integral Projection",
      "authors": [
        "X Huang",
        "S Wang",
        "G Zhao",
        "M Piteikainen"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Learning from hierarchical spatiotemporal descriptors for micro-expression recognition",
      "authors": [
        "Y Zong",
        "X Huang",
        "W Zheng",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "68",
      "title": "Micro-attention for microexpression recognition",
      "authors": [
        "C Wang",
        "M Peng",
        "T Bi",
        "T Chen"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "69",
      "title": "Dynamic Micro-Expression Recognition Using Knowledge Distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Geme: Dual-stream multi-task gender-based micro-expression recognition",
      "authors": [
        "X Nie",
        "M Takalkar",
        "M Duan",
        "H Zhang",
        "M Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "71",
      "title": "Revealing the Invisible with Model and Shrinking for Composite-Database Micro-Expression Recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "72",
      "title": "CapsuleNet for microexpression recognition",
      "authors": [
        "N Van Quang",
        "J Chun",
        "T Tokuyama"
      ],
      "year": "2019",
      "venue": "Proceedings of 14th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "73",
      "title": "Dual-inception network for crossdatabase micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "L Xue"
      ],
      "year": "2019",
      "venue": "Proceedings of 14th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "74",
      "title": "Cas (me) 3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity",
      "authors": [
        "J Li",
        "Z Dong",
        "S Lu",
        "S.-J Wang",
        "W.-J Yan",
        "Y Ma",
        "Y Liu",
        "C Huang",
        "X Fu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "75",
      "title": "4dme: A spontaneous 4d micro-expression dataset with multimodalities",
      "authors": [
        "X Li",
        "S Cheng",
        "Y Li",
        "M Behzad",
        "J Shen",
        "S Zafeiriou",
        "M Pantic",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}