{
  "paper_id": "2107.03783v1",
  "title": "Use Of Affective Visual Information For Summarization Of Human-Centric Videos",
  "published": "2021-07-08T11:46:04Z",
  "authors": [
    "Berkay Köprü",
    "Engin Erzin"
  ],
  "keywords": [
    "Affective computing",
    "video summarization",
    "continuous emotion recognition",
    "neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Increasing volume of user-generated human-centric video content and their applications, such as video retrieval and browsing, require compact representations that are addressed by the video summarization literature. Current supervised studies formulate video summarization as a sequence-to-sequence learning problem and the existing solutions often neglect the surge of human-centric view, which inherently contains affective content. In this study, we investigate the affective-information enriched supervised video summarization task for human-centric videos. First, we train a visual input-driven state-of-the-art continuous emotion recognition model (CER-NET) on the RECOLA dataset to estimate emotional attributes. Then, we integrate the estimated emotional attributes and the high-level representations from the CER-NET with the visual information to define the proposed affective video summarization architectures (AVSUM). In addition, we investigate the use of attention to improve the AVSUM architectures and propose two new architectures based on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We conduct video summarization experiments on the TvSum database. The proposed AVSUM-GRU architecture with an early fusion of high level GRU embeddings and the temporal attention based TA-AVSUM architecture attain competitive video summarization performances by bringing strong performance improvements for the human-centric videos compared to the state-of-the-art in terms of F-score and self-defined face recall metrics.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimedia applications and services have seen a surge in recent years. There is more than 500 hours upload per minute to video streaming platforms such as Youtube and Twitch. Especially user-generated human-centric videos allocate major part of the available videos in these video streaming platforms. Such a massive video resource creates two crucial needs: (i) personalized human-computer interactions (HCI), and (ii) efficient representations and retrieval of video contents. Emotion recognition addresses the former by providing affect aware applications  [1] , while video summarization addresses the latter  [2] .\n\nEmotion recognition is a critical task that enables personalized HCI applications and understanding of personal choices. Humans are emotional creatures, even for many cognitive tasks decision making processes are driven primarily by emotions  [3] . Emotions are represented both in discrete and continuous domains. Discrete categorical emotions, such as happiness and sadness, can also be represented in the 3-dimensional continuous affect space of activation, valence, and dominance attributes, which are the indicators of activeness-passiveness, positivenessnegativeness, and dominance-submissiveness, respectively  [4] ,  [5] .\n\nIn the literature, estimation of activation, valence and dominance attributes is referred as Continuous Emotion Recognition (CER). Although CER is widely studied by the speech processing community  [6] ,  [7] ,  [8] , it has been studied over the visual channels  [9] ,  [10] ,  [11] . In  [9] , a deep attention based convolutional network is proposed to detect facial expressions and emotions. Aspandi et al.  [10]  proposed an adversarial training approach to jointly estimate whether the image is fake or not while estimating the activation and valence (AV) attributes. In  [11] , VGG-16 driven visual features are used as the input of a stacked convolutional recurrent neural network (CRNN) for affect recognition in the wild.\n\nCompact and efficient representations of videos can be extracted by selecting subsets of frames from the videos through video summarization techniques. Video summarization is categorized regarding the output type which could be video frames or video fragments  [2] ,  [12] ,  [13] . The former, finding key frames, is known as video storyboard generation. The latter, chronologically stitched segment selection, is referred as video skimming. Due to the discrete nature of storyboard generation, it lacks in smoothness and naturalness. However, since it does not require synchronization, it offers more degrees of freedom in data organization than video skimming  [2] .\n\nEarly video summarization approaches are unsupervised and made use of low level similarity measures between the frames  [14] ,  [15] ,  [16] ,  [17] ,  [18] ; while recent unsupervised studies apply Generative Adversarial Networks (GANs)  [19]  and attention  [20] ,  [21]  to solve the video summarization problem. They use GANs to reconstruct the input video from the selected key frames for unsupervised video summarization. Also low-level measures, such as color histogram intersections  [16] ,  [17]  and mutual information  [18] , are investigated for unsupervised video summarization.\n\nSupervised approaches for video summarization have recently become popular that tend to perform better in other computer vision tasks such as object detection and segmentation. In  [12] ,  [22] ,  [23] ,  [24] , the authors model the video summarization task as a sequence-to-sequence mapping problem. In  [12] , Long Short-Term Memory (LSTM) is adopted to tackle with variable range dependencies within the encoder-decoder type architectures. In contrast to recurrent models,  [22]  proposes a fully convolutional solution, where all frames are processed together. Recent approaches  [23] ,  [24] , succesfully adapted attention mechanisms into the video-summarization task.\n\nCoupling of affective computing and video summarization has not been extensively studied in the literature. In two related video summarization studies, key frames are selected based on the physiological responses of the viewers that are expected to be highly correlated with the viewers' emotional states  [25] ,  [26] . In  [25] , the facial activity of the viewer is tracked, and then the frames are ranked to extract personal highlights from the videos by using heuristics. Money and Agius track physiological responses of the viewers, such as heart rate and electrodermal response, to analyze sub-segments of the videos  [26] . These studies leverage physiological information that are highly related to emotional states of the humans to generate personal highlights/summaries. On the other hand, these approaches require at least one subject as a video viewer and do not provide feasible solutions for the automatic video summarization.\n\nIn this study, we address the use of affective information, which is extracted from visual data, for the summarization of human-centric videos. Unlike studies evaluating viewers' perspective  [25] ,  [26] , we model affective states from humans in the video. That makes automatic affective video summarization feasible and specific for human-centric videos. For affective video summarization, we adopt a twostep approach. First, affective information is extracted using the convolutional recurrent neural networks. Then, we explore the affective information and attention mechanisms to enrich a fully convolutional network based video summarization. To summarize, the main contributions of this study are as follows:\n\n• We formulate a novel end-to-end learning problem for affective-information enriched video summarization targeting human-centric videos.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We model affective information in terms of emotional attributes and learned embeddings extracted from the CER module.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "We investigate the use of attention mechanisms in video summarization for human-centric videos and develop temporal and spatial attention-based frameworks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We carry out affective video summarization evaluations on the state-of-the-art using both standard and self-defined evaluation metrics.\n\nThe rest of this paper is organized as follows. Section 2 reviews related work, and Section 3 describes main building blocks of the proposed framework. Section 4 presents the experiments conducted together with the performance evaluations. Finally, conclusion is presented in Section 5.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "Our proposed affective video summarization framework integrates emotion recognition into video summarization. In this regard, we first point out several emotion recognition studies that relates video summarization. Then, we briefly discuss unsupervised and supervised video summarization literature.\n\nEmotion recognition studies formulate the problem as discrete emotion recognition (DER)  [27] ,  [28] ,  [29]  or continuous emotional attribute regression  [1] ,  [30] . In  [27] , stacked CNN-RNN architecture is proposed to extract local and global features to classify emotions. For the CER problems, Schmitt et al. investigates RNNs with CCC loss function after extracting low-level descriptors (LLDs) such as melfrequency cepstral coefficients (MFCCs) and zero crossing rate from speech signal  [30] . Recently, multi-modal approaches are explored for CER and DER  [1] ,  [31] . Tzirakis et al. design an end-to-end network utilizing raw video, audio and text, where visual features are extracted using 3 stage High-Resolution Network and audio features are extracted via multiple 1-D convolutional layers  [31] . Contextual features are extracted from text by first generating point-wise n-grams using convolutional layers, then linearly projecting these sub-features with multiple heads to increase the diversity. Finally, extracted features are fused using attention. In  [1] , visual information is expressed using facial attributes and audio information is represented by the MFCCs. Audio-visual information is fused at the feature level, and a CRNN model is trained with multi-task learning for the CER problem.\n\nTwo recent emotion recognition studies are interesting in the context of video summarization  [32] ,  [33] . Xu et al. investigates information transfer from image and textual data of videos for emotion recognition, emotion attribution and emotion-oriented summarization  [32] . First, they learn video representations using Image Transfer Encoding and textual representations using zero-shot learning from auxiliary datasets. Then, they perform a categorical emotion recognition using the Support Vector Machine (SVM) classifier. Later, emotion attribution sets the contribution of each frame to the video's overall emotion. Finally, video summarization is formulated as a selection of key frames by maximizing an emotion attribute based score function.\n\nIn the second related study, Tu et al. train a joint model to capture emotion attribution and recognition using multitask learning  [33] . Later, similar to the first study  [32] , video summarization task is formulated as a post-processing optimization problem and solved using MINMAX dynamic programming. Note that both of these studies formulate summarization task as a optimization problem which is executed in post-processing. Hence their video summarization frameworks are not learning based and they do not explore how affective information alter behavior of the proposed summarization architecture. Furthermore, the proposed solutions are not evaluated on the state-of-the-art video summarization datasets.\n\nScarcity of the labeled data leads unsupervised learning studies for the video summarization problem. In a recent study, Jung et al. address unsupervised video summarization problem by first learning discriminative features over a Variational Auto Encoder (VAE) and GAN-based architecture using variance loss to alleviate ineffective feature learning  [34] . Then, they define a chunk and stride network (CSNet) to overcome the difficulty of learning for longlength videos. In another study, Zhao et al. presents a dual learning framework for the unsupervised video summarization  [35] . They integrate the summary generation and video reconstruction tasks using multi-task learning so to reward the summary generator under the assistance of the video reconstructor. Zhou et al. formulates summarization task as sequential decision-making using an end-to-end reinforcement learning based framework  [36] . They utilize a reward function that jointly accounts for diversity and representativeness of the generated summaries in an unsupervised setting.\n\nSupervised studies on video summarization adapts encoder-decoder architectures  [22] ,  [23] ,  [24] ,  [37] . In  [37] , video is modeled as a 3-dimensional tensor, and 3D convolutional networks are used in the encoder to extract shallow and deep spatio-temporal features. Then, extracted multilevel features are fed into an LSTM based decoder. The proposed architecture is trained with the Sobolev loss, which constrains the derivative of the sequential data. With the great success of attention modules  [38] ,  [39] , recent works in video summarization adapt attention into the decoder part. Ji et al. extract visual features using the GoogleNet and later encode with a Bidirectional LSTM network  [23] . Encoded vectors combine Bahdanau attention  [38] , and then feed into an LSTM based decoder. Later the Bidirectional LSTM approach is extended to prevent semantic information loss  [40] . In the extended architecture, an additional network analyzing the semantic information loss is added and used as a feedback mechanism from the decoder to the encoder using the Huber loss.\n\nAlthough we apply supervised learning for video summarization task, our study differs from these studies in the exploitation of affective information for the video summarization task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this paper, we investigate the use of affective information for enriching video summarization by capturing emotionally salient regions of the human-centric videos. First, we state and formulate the video summarization problem and define the visual feature extraction for both the CER and video summarization tasks. Then, an end-to-end framework for the CER is presented. Emotional attributes and highlevel embeddings from the CER framework are later used as affective representations by the video summarization framework. Finally, we introduce the proposed affective video summarization architectures by first defining a video summarization baseline and then enriching this baseline with the fusion of affective information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Statement",
      "text": "Video summarization is widely formulated as either a binary classification or a frame-level regression task. In the binary classification task, summarization outputs are either key-frames  [12] ,  [22]  or key-shots  [12] ,  [41]  from the video. On the other hand, frame-level importance scores are extracted in the regression task  [12] ,  [23] .\n\nIn this study, we formulate the video summarization as a binary classification task where the positive labels correspond to the selected key-frames. The summarization network receives a feature matrix, v ∈ R N ×D , and emits an output matrix, s ∈ R N ×C , where N is the number of frames in the video, D is the dimensionality of the framelevel visual feature, and C is the number of classes. We take C = 2 representing the positive and negative classes for the key-frame selection and these two nodes output class probability values at the output of the network. Then the positive class for key-frame selection or the negative class for frame skip are set by picking the node with higher probability. Eventually, the video summary is constructed from the key-frames that are labeled as positive.\n\nIn this study, we extract the visual information using the GoogleNet  [42] . Output of the pool5 layer of the pre-trained GoogleNet is used as the visual feature and represented as e i ∈ R D at frame i with dimension D = 1024.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cer Network (Cer-Net)",
      "text": "The continuous emotion recognition problem is set as the continuous regression of an emotional attribute from the temporal visual features. For this purpose, we construct a CER network (CER-NET), which consists of two back-toback convolutional layers, a max pool layer in the temporal domain, a Gated Recurrent Unit (GRU) layer, and a fully connected layer as shown in Figure  1 . We use the CER-NET to train two separate networks to estimate the activation and valence (AV) attributes separately. A temporal visual feature matrix is defined to be the input of the CER-NET. For this purpose, visual features around the i-th frame are cascaded to define the temporal visual feature as\n\nat frame i, where T is the temporal window size and it is set as T = 2∆ = 20 frames.\n\nWe refer the group of back-to-back two convolutional and a max-pool layers as the conv layer for the sake of simplicity. In CER-NET, the conv layer models the spatial relations and provides a compact representation of the temporal window. In this manner, both temporally related features are highlighted and dimensionality of the representation is reduced. Dimensionality reduction is the key to complexity reduction and it also prevents overfitting. The compact representation from the conv layer is fed into GRU to model long-term temporal relations.\n\nAt the inference phase, CER-NET receives f CER i and provides the GRU layer outputs as g i ∈ R G and the fully connected layer outputs as ŷi ∈ R. We define the GRU layer output, g i , as an affective embedding, which can carry affective information to the video summarization model. On the other hand, the fully connected layer output, ŷi , represents the estimated emotional attribute (A or V) at frame i and delivers another source of affective information.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Affective Video Summarization",
      "text": "The proposed affective-information enriched video summarization (AVSUM) is based on a fully convolutional neural network (FCN) for semantic segmentation  [43] , which is adapted by  [22]  into video summarization (SUM-FCN). In the following, we briefly describe SUM-FCN and then present two fusion architectures to combine affective information with video summarization, which are later extended by temporal and spatial attention mechanisms.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sum-Fcn",
      "text": "SUM-FCN adopts an encoder-decoder architecture where the encoder is based on fully convolutional layers and the decoder is based on deconvolutions. Figure  2  includes the architecture of the SUM-FCN compromising encoderbottleneck-decoder layers driven by the visual input v. The encoder compresses the temporal information while increasing spatially, and the bottleneck passes it to the decoder to reconstruct necessary information from this compact representation. SUM-FCN receives the visual features for the whole video at once and provides the summarization outputs at once. Video frame rate is typically down-sampled before the summarization. Let v be the input of SUM-FCN, then v = [e 1 , . . . , e N ] ∈ R N ×D where N is number of frames in the down-sampled stream. Given v, SUM-FCN emits s ∈ R N ×C , where C is the dimension of the summarization annotations and set as C = 2 as defined in section 3.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Feature Fusion For Avsum",
      "text": "Affective information is extracted from the two CER-NET models, which are trained to estimate the activation and valence (AV) attributes separately. Two types of affective information cues are extracted from the CER-NET models: (i) the estimated AV attributes (f AV ), and (ii) the learned high-level CER-NET representations based on the GRU embeddings (g AV ). The estimated AV attribute vector f AV j is constructed as a column vector from the estimated activation and valence attributes as f AV j = [ŷ A j , ŷV j ] ∈ R 2 at frame j in the down-sampled stream. Similarly, g AV j is constructed by concatenating the outputs of the GRU layers from the two CER-NET models as\n\nthe emotional attribute f AV and the affect embedding f GRU representations of the video are defined as\n\nThe first proposed AVSUM architecture, referred as AVSUM-GRU, combines the affect embedding f GRU with the visual input v. Figure  2  presents the AVSUM-GRU architecture receiving the v GRU input as\n\nwhere ⊕ operator is representing the feature vector combining over the whole video. Alternatively, we define the AVSUM-SCAV architecture, as illustrated in Figure  2 , by combining the emotional attribute f AV with the visual input v to receive v AV as\n\nand incorporating a long skip-connection by concatenating the emotional attribute f AV to the final layer of the summarization network. AVSUM-SCAV inserts a skip-connection to the output of deconv2 layer and has a final fully connected mlp layer to reduce the dimension from C + 2 to C.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Temporal Attention For Avsum",
      "text": "We adapt multi-headed attention (MHA) mechanism into the AVSUM architecture to efficiently model temporal dependencies across the video frames. Figure  3  depicts the MHA based temporal attention structure, referred as TA-AVSUM, where MHA is placed to the output of conv4 layer which emits X ∈ R M ×S . Here, M and S are respectively the temporal and spatial dimensions of X.\n\nMHA receives three inputs as Query (Q), Key (K) and Value (V), then outputs a weighted summation of the rows of V. The weights are calculated from the similarity between the Q and K. In this context, the MHA is defined as\n\nwhere\n\nh and W O are the learned linear projections and H is the number of heads. We employ multi-headed self-attention, by setting Q, K and V to X. Hence, the learned projections matrices are formed as",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spatial Attention For Avsum",
      "text": "Motivated by the Squeeze and Excitation Networks  [44] , which attend to different channels of an image, we propose the fourth AVSUM network with spatial domain attention and refer it as SA-AVSUM. Figure  3  depicts the SA-AVSUM structure where we employ attention to the output of the deconv2 layer. Unlike TA-AVSUM, we adopt single-headed attention, which is formulated in  (6) . Then, K and V are set to transpose of ŝAVSUM and Q is set from the affective embedding f GRU .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Training",
      "text": "Training of the CER-NET and video summarization models are executed in two phases. First the CER-NET models are trained, later they are fixed and integrated for the affective video summarization to train the AVSUM-GRU, AVSUM-SCAV, TA-AVSUM, and SA-AVSUM models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cer-Net",
      "text": "The CER-NET models are trained separately to estimate the activation and valence attributes using the concordance correlation coefficient (CCC) based loss function. The loss function of the CER-NET is defined as the negated CCC value,\n\nwhere y is the ground truth and ŷ is the estimated attribute.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Summarization Networks",
      "text": "The key frame selection problem has an imbalanced nature, since only a small number of frames are selected for the summary  [22] . In order to overcome the imbalance problem, a weighted binary cross entropy loss is defined as\n\nwhere z j ∈ 0, 1 is the binary ground truth, ẑ is the predicted score and w zj is the weight of the j th frame. The weights for the binary target are defined as",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "Experimental evaluations of the proposed models and comparisons with the state-of-the-art are performed using two datasets. In this section, we first introduce the datasets and evaluation metrics, then explain implementation details. Finally, experimental results are presented and discussed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We train and evaluate the CER-NET on the RECOLA dataset, which is a popular multi-modal dataset for emotion recognition  [45] . The RECOLA dataset is composed of multimodal recordings of dyadic conversations from 27 French speakers. From these 27 recordings, 18 of them are annotated, and the rest of the records are used for testing. The annotations are at the rate of 40 msec and from 6 different annotators. In total, we use 90 minutes of recordings from the RECOLA dataset in this study. Experimental evaluations on the proposed AVSUM architectures are executed on the frequently used TvSum dataset  [41] . Note that there is no available video summarization dataset containing only human-centric videos in the literature. The TvSum dataset contains 50 user generated videos from 10 different categories, such as vehicle tire changing, sandwich making, grooming an animal, etc. The demographics of the dataset in terms of face including frames in the summary and in total video clip is presented in Figure  4 . We categorize videos into human-centric and rest regarding the number of faces in the summary where we set the threshold to 80 frames labeling 15 videos as human-centric. Ground truths are provided by the framelevel importance score for each video from 20 raters. We follow the approach in  [12] ,  [22]  to convert the frame-level importance scores into the keyshot-based summaries.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Emotional attribute estimation with the CER-NET is evaluated using the CCC metric following  [1] , which is the negative of the L CER loss in  (8) .\n\nFor the video summarization task, following  [12] ,  [22] ,  [23] , F-score is used as the evaluation metric. For the kth video, let the ground truth binary summarization vector be s k and estimated binary summarization vector to be ŝk where s k , ŝk ∈ R N . Then precision P k and recall R k for the k-th video are calculated as\n\nwhere j runs over the frames. Video level F-score is defined as the harmonic mean of the precision and recall,\n\nThen, the final F-score metric F 1 is defined as the unweighted average of the video level F-score values as\n\nwhere K is the number of videos in the dataset. Since the affective information is learned from a dataset of human-centric videos, capability of capturing affective frames of the video during the summarization is expected to be better with human-centric videos. By highlighting this fact, we define two new metrics for the evaluation of the affective video summarization. The first metric computes normalized F1 score differences with the baseline over the videos with the highest number of face appearances. To define this metric, let us first define the normalized F1 score difference for the k-th video with respect to the SUM-FCN baseline model as\n\nwhere F 1 k refers to the F1 score of the model in evaluation. Also assume that all the videos in the dataset are sorted with the highest number of face appearances in descending order and are indexed with k l for l = 1, . . . , K. Then, the cumulative F1 score difference metric for the Top-L humancentric videos is defined as\n\nAssociated with the ∆F 1 L , we also compute the F1 score of the Top-L human-centric videos and refer it as F 1 L . Human-centric nature of the videos can be associated with the face appearances in the video frames. Motivated with this fact, we set a second metric for evaluation of the affective video summarization as the recall rate of face appearing frames in the extracted summary. Let d k F be the binary vector representing whether a frame includes a face appearance or not for the k-th video. Binary face appearance vectors are extracted by the histogram of oriented gradients (HOG) based face detector  [46] . Then, the recall rate of face appearing frames, let's refer it as face recall, R is defined as\n\nwhere\n\nis the Hadamard product of d k F and s k . We also study statistical differences of a given affective feature dimension, f , across face appearing and notappearing frames. For this purpose, Kullback-Leibler (KL) divergence of f in these two classes is defined as\n\nwhere P f and Q f are respectively probability distributions of the affective feature dimension f across face appearing and face not-appearing frames. Note that the affective feature dimension f is driven from the affective feature set as",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cer-Net",
      "text": "The window size T is selected as 20 frames. The cnn1 and cnn2 layers have 10 filters, max-pooling layer reduces the temporal dimension from 20 to 5, gru layer has 10 cells and FCN layer has 1 node. Hence, the dimensionality of f AV is 2 and g AV is 20 (G = 10). Annotated recordings of the RECOLA dataset are used during the training of the CER-NET. RECOLA recordings are divided as 10% for the test, 10% for the validation, and 80% for the training. We applied Adam optimizer with a learning rate of 10 -4 and with the batch size of 256.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Video Summarization Networks",
      "text": "Following  [12] ,  [22] , TvSum videos are downsampled to 2 fps, and frames are fed into GoogleNet. For the AVSUM-SCAV, the input feature dimension is set as 1026. The input dimension of the AVSUM-GRU, TA-AVSUM, and SA-AVSUM models became 1044 with the GRU embeddings.\n\nFor the TA-AVSUM, we set the number of heads H = 4.\n\nWe mimic fixed size cropping in semantic segmentation by uniformly sampling the video frames and using N = 320  [22] . We adopted the leave one-group out cross-validation technique to compare the performances. At each fold, 9 videos are selected, and the rest of the videos are used for training. The training is held for 50 epochs with a batch size of 5 videos. During the training phase, Adam optimizer with the learning rate of 10 -3 is used. For each training fold, a model achieving the highest F-score (F 1), and a model achieving the highest face recall (R) are selected for the performance evaluations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cer-Net Performance",
      "text": "Table  1  presents the CCC performance of the CER-NET emotion recognition model on the RECOLA dataset. The proposed architecture performs better at estimating the activation than the valence. The end-to-end CER-NET architecture outperforms CER-MTL Facial model  [1]  in both activation and valence by achieving CCC of 0.40 and 0.17 respectively. The end-to-end visual network in [47] performs strongly for the valence estimation and fairly close with the CER-NET for the activation estimation. CER-NET outperforms [47] by 4% at estimating the activation, while [47] achieves CCC of 0.48 for valence and CER-NET achieves 0.17. Different than CER-NET, CER-MTL Facial receives visual information as facial activation units and optical flow vectors. Due to the input dimension, CER-NET has more trainable coefficients leading to a more complex structure than the CER-MTL Facial.\n\nIn comparison with these two baseline visual models  [1] , [47], CER-NET performs competitively in representing affective information on the visual channel.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cumulative Avsum Performance",
      "text": "In this section, we first present performance evaluations of the proposed affective video summarization models in terms of cumulative F-score and face recall metrics. Then, the video level performances are investigated to better highlight characteristics of videos that have improved summarization performance with the affective cues.\n\nTable  2  presents cumulative F-score (F 1) and face recall (R) together with the Top-15 F-score (F 1 15 ) and face recall (R 15 ) performances for the proposed AVSUM and the baseline SUM-FCN models. In each column, top-two scoring performances are highlighted in bold. Recall that we apply two model selection criteria based on F-score and face recall maximization. Each model selection criterion is observed to favor its related performance metric in the evaluations. That is, maximization of F 1 (Max F 1) yields higher F-score while maximization of R (Max R) yields higher face recall R.\n\nObserving the cumulative F 1 performances, AVSUM-GRU model is competitive for both model selection criteria and observed as the best performing model with the Cumulative F-score (F 1) and face recall (R) together with the Top-15 F-score (F 1 15 ) and face recall (R 15 ) performances (top two models are in bold) of the AVSUM and the SUM-FCN models with the maximization of F 1 and R model selection criteria",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sum-Fcn [22]",
      "text": "57 Affective information is modeled with the continuous emotion recognition task trained on the RECOLA dataset. Since RECOLA is an human-centric dataset, which includes facial videos, we also choose to evaluate the video summarization performance for the Top-L human-centric videos, where L is set as 15 with the discussion in section 4.1. Table  2  presents the Top-15 F-score F 1 15 and face recall R 15 performances. While attention based SA-AVSUM and TA-AVSUM models perform best for the F 1 15 score with the Max R criterion, AVSUM-SCAV and AVSUM-GRU models perform best with the Max F 1 criterion. Overall, the best performance is 60.60% F 1 15 score with the Max F 1 criterion for the AVSUM-SCAV model. Observing the Top-15 face recall R 15 performances, while TA-AVSUM model is competitive with the baseline SUM-FCN model with the Max F 1 criterion, it performs significantly superior with the Max R criterion achieving 70.31% face recall R 15 rate.\n\nTable  2  highlights two runner up models, AVSUM-GRU and TA-AVSUM. AVSUM-GRU model sustains strong Fscore rates with the Max F 1 criterion, especially for humancentric videos targeted with F 1 15 performance. Alternatively, while temporal attention based TA-AVSUM model performs strongly for the face recall with the Max R criterion and attains 70.31% face recall R 15 rate, it also sustains a competitive performance for the F-score and face recall rates with the Max F 1 criterion. Hence temporal attention is observed to better integrate the affective information for the affective video summarization.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Explainability",
      "text": "We conduct explainability evaluations to better understand contributions of the affective feature dimensions and the proposed model architectures for the affective video summarization. First, we present KL divergence analysis for the affective feature dimensions. Then, we investigate video level summarization performances of the AVSUM models in terms of the cumulative F-score difference metric ∆F 1 L for the Top-L human-centric videos. In Figure  5 , the first two KLD values are for the activation and valence attributes, and the later values are color coded for the dimensions of g A and g V . Note that all the dimensions of the g A , except the fourth, exhibit higher KLD values than the activation attribute, and at certain dimensions KLD is almost 4 times higher than the KLD of the activation. A similar trend can be observed for the valence embedding vector g V , where five dimensions exhibit higher KLD values than the valence attribute, and the largest KLD is extracted for the 6th dimension of the g V . These higher KLD values for the GRU based feature dimensions can be observed as the discriminative cues for the human-centric videos that also contribute to the performance of the proposed AVSUM architectures.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Video Level Evaluations",
      "text": "Figure  6  depicts the performance comparison of the AVSUM models with the ∆F 1 L metric for the Top-30 human-centric videos and yields valuable insights. Note that positive accumulation of ∆F 1 L metric indicates a better performance than the baseline SUM-FCN model. In Figure  6 , the AVSUM-SCAV, TA-AVSUM and SA-AVSUM models perform better than the baseline till the Top-15 human-centric videos, whereas AVSUM-GRU model sustains a higher performance till the Top-20 human-centric videos. Similar trends of the AVSUM-SCAV and SA-AVSUM ∆F 1 L performances can be due to the common late fusion mechanism in these architectures. As L increases, we can assume human-centric characteristic of the videos is getting weaker. Hence both AVSUM-SCAV and SA-AVSUM and as well as AVSUM-GRU models are observed to performed better for the top human-centric videos and their performances start degrading as human-centric characteristic is getting weaker.\n\nWe also investigate video level F-score performances for the proposed AVSUM models. Figure  7  presents scatter plot of the video level F 1 scores on the left column and video level R scores on the right column, where the diagonal in each figure represents similar performances of the compared models. Furthermore, the Top-15 human-centric videos are color coded in blue to observe their comparative performances.\n\nVideo level F-score performances of the AVSUM-GRU vs SUM-FCN tend to cluster around the diagonal that makes these two models to be most similar in terms of F-score performance. Furthermore, majority of the Top-15 humancentric videos are on or above the diagonal, which indicates a stronger performance for the human-centric videos. Unlike, F1-score, face recall performance comparison depicted by Figure  7 (b) has a scattered behavior providing improvements to some videos while degrading other. However, it is seen that majority of the Top-15 human-centric videos are affected positively. This observation is in line with the F 1 15 performance of AVSUM-GRU.\n\nVideo level F-score performances of the AVSUM-SCAV vs SUM-FCN have a higher deviation from the diagonal. However, almost all the Top-15 human-centric videos are on or above the diagonal. This indicates a strong performance improvement for the human-centric videos. In terms of face recall on Figure  7 (d), majority of the videos are accumulated around the diagonal, indicating that AVSUM-SCAV and SUM-FCN are most similar in terms of face recall.\n\nVideo level F-score performances of the TA-AVSUM and SA-AVSUM models have also high deviation from the diagonal. However, like AVSUM-SCAV majority of the Top-15 human-centric videos are on or above the diagonal for both. Similar to F-score, face recall comparisons of TA-AVSUM and SA-AVSUM have a scattered behavior depicted by Figure  7 (f), and 7(h). However, different than SA-AVSUM, scattered points accumulated on the positive side for TA-AVSUM, stating a major performance improvement which is inline with its best achieving R 15 performance for the Max R criterion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we proposed a new affective information enriched end-to-end video summarization framework for human-centric videos. As a first step, we modeled affective information in terms of AV attributes and GRU embeddings, which were extracted from the CER models. The CER-NET, a CER model achieving state-of-the-art CCC performance, was introduced. We explored the use of affective information with the proposed AVSUM-SCAV and AVSUM-GRU fusion models and attention mechanisms based TA-AVSUM and SA-AVSUM models. Experimental investigations of the proposed models were conducted on the RECOLA and TvSum datasets.\n\nWe observed that with the fusion of affective information, F-score performance of the video summarization on the human-centric videos can be improved. To further analyze the effect of injected features we defined a face recall (R) metric and showed that AVSUM-GRU and AVSUM-SCAV models outperform SUM-FCN with more than 1% increase in face recall R. The AVSUM-GRU model has strong performance improvements for the human-centric videos on F 1 score and face recall R, and as well it is the most competitive model with the baseline SUM-FCN. On the other hand, we observed that attention enhanced mechanisms exhibits strong performance gains with the Max R criterion for the human-centric videos. We should also note that affective GRU embedding features exhibit higher KLD across withface and without-face frames.\n\nComparing the proposed AVSUM models, AVSUM-GRU has a consistent and competitive performance regardless of the model selection criterion at F 1 and R metrics and has a good balance between the performance improvement at Top-15 and the performance degradation at the remaining videos. On the other hand, temporal attention based TA-AVSUM performs competitive with the Max F 1 criterion and attains strong improvement with the Max R criterion. The proposed AVSUM models integrate affective information to the summarization architectures and attain important video summarization improvements for the humancentric videos. Compilation of affective human-centric video datasets for video summarization tasks stays as a critical and valuable future study. As a future work, we would like to collect a dataset which is labeled for both video summarization and CER. To extend current study, we would like to investigate multi-modal architectures for both CER and human-centric video summarization components.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: CER-NET: The continuous emotion recognition network",
      "page": 3
    },
    {
      "caption": "Figure 1: We use the CER-NET",
      "page": 3
    },
    {
      "caption": "Figure 2: An overview of the proposed AVSUM-GRU and AVSUM-SCAV architectures. AVSUM-GRU combines high-level affective information fGRU",
      "page": 4
    },
    {
      "caption": "Figure 2: presents the AVSUM-GRU",
      "page": 5
    },
    {
      "caption": "Figure 2: , by combining the emotional attri-",
      "page": 5
    },
    {
      "caption": "Figure 3: depicts the",
      "page": 5
    },
    {
      "caption": "Figure 3: depicts the SA-AVSUM",
      "page": 5
    },
    {
      "caption": "Figure 3: An overview of the proposed TA-AVSUM and SA-AVSUM architectures. TA-AVSUM applies multi-head-self attention to the output of conv4",
      "page": 6
    },
    {
      "caption": "Figure 4: We categorize videos into human-centric and",
      "page": 6
    },
    {
      "caption": "Figure 4: Number of face including frames in the summary and total video",
      "page": 6
    },
    {
      "caption": "Figure 5: KL divergence D(Pf||Qf) of each affective feature dimension",
      "page": 8
    },
    {
      "caption": "Figure 5: depicts the KL divergence (KLD) of the affective",
      "page": 8
    },
    {
      "caption": "Figure 6: depicts the performance comparison of the AVSUM",
      "page": 8
    },
    {
      "caption": "Figure 6: , the AVSUM-",
      "page": 8
    },
    {
      "caption": "Figure 6: Performance comparison of the AVSUM models with the ∆F1L",
      "page": 9
    },
    {
      "caption": "Figure 7: presents scatter plot",
      "page": 9
    },
    {
      "caption": "Figure 7: (b) has a scattered behavior providing improve-",
      "page": 9
    },
    {
      "caption": "Figure 7: (d), majority of the videos are accumulated",
      "page": 9
    },
    {
      "caption": "Figure 7: (f), and 7(h). However, different than SA-AVSUM,",
      "page": 9
    },
    {
      "caption": "Figure 7: Video level F1 score and R score comparisons of the AVSUM models against the baseline SUM-FCN: F1 scores are with Max F1 criterion,",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "encoder": "encoder"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "57.46 60.00 53.20 65.52": "57.50 60.25 54.04 59.14"
        },
        {
          "57.46 60.00 53.20 65.52": "56.64 60.60 52.11 63.80"
        },
        {
          "57.46 60.00 53.20 65.52": "57.47 59.95 53.22 65.55"
        },
        {
          "57.46 60.00 53.20 65.52": "55.92 59.98 49.25 62.38"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Activation\nValence",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "A",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "Column_2": "A",
          "Column_3": "VSUM-SCAV",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "Column_2": "A\nTA",
          "Column_3": "VSUM-GRU\n-AVSUM",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "SA",
          "Column_2": "SA",
          "Column_3": "-AVSUM",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T": "",
          "Column_2": "T",
          "op-15": "op-15",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "T": "R",
          "Column_2": "R",
          "op-15": "est",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Top-15\nRest",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T": "",
          "Column_2": "T",
          "op-15": "op-15",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "T": "R",
          "Column_2": "R",
          "op-15": "est",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Top-1\nRest",
          "Column_7": "5",
          "Column_8": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T": "",
          "Column_2": "T",
          "op-15": "op-15",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "T": "R",
          "Column_2": "R",
          "op-15": "est",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Top-1\nRest",
          "Column_7": "5",
          "Column_8": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T": "",
          "Column_2": "T",
          "op-15": "op-15",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "T": "R",
          "Column_2": "R",
          "op-15": "est",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "Top-15\nRest",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal continuous emotion recognition using deep multi-task learning with correlation loss",
      "authors": [
        "E Erzin"
      ],
      "year": "2020",
      "venue": "Multimodal continuous emotion recognition using deep multi-task learning with correlation loss",
      "arxiv": "arXiv:2011.00876"
    },
    {
      "citation_id": "2",
      "title": "Video summarization using deep neural networks: A survey",
      "authors": [
        "E Apostolidis",
        "E Adamantidou",
        "A Metsai",
        "V Mezaris",
        "I Patras"
      ],
      "year": "2021",
      "venue": "Video summarization using deep neural networks: A survey",
      "arxiv": "arXiv:2101.06072v1"
    },
    {
      "citation_id": "3",
      "title": "Emotion and decision making",
      "authors": [
        "J Lerner",
        "Y Li",
        "P Valdesolo",
        "K Kassam"
      ],
      "year": "2015",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "4",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "5",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "6",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "ICASSP 2016, IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "INTERSPEECH 2017, Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "INTERSPEECH 2019, Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Deep-Emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "A Abdolrashidi"
      ],
      "year": "2019",
      "venue": "Deep-Emotion: Facial expression recognition using attentional convolutional network",
      "arxiv": "arXiv:1902.01019"
    },
    {
      "citation_id": "10",
      "title": "Adversarial-based neural networks for affect estimations in the wild",
      "authors": [
        "D Aspandi",
        "A Mallol-Ragolta",
        "B Schuller",
        "X Binefa"
      ],
      "year": "2020",
      "venue": "Adversarial-based neural networks for affect estimations in the wild",
      "arxiv": "arXiv:2002.00883"
    },
    {
      "citation_id": "11",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "12",
      "title": "Video summarization with long short-term memory",
      "authors": [
        "K Zhang",
        "W.-L Chao",
        "F Sha",
        "K Grauman"
      ],
      "year": "2016",
      "venue": "ECCV 2016, European Conference on Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "A user attention model for video summarization",
      "authors": [
        "Y.-F Ma",
        "L Lu",
        "H.-J Zhang",
        "M Li"
      ],
      "year": "2002",
      "venue": "MULTIMEDIA '02, Tenth ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Reconstructing storyline graphs for image recommendation from web community photos",
      "authors": [
        "G Kim",
        "E Xing"
      ],
      "year": "2014",
      "venue": "CVPR 2014, IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Keyframe-based video summarization using Delaunay clustering",
      "authors": [
        "P Mundur",
        "Y Rao",
        "Y Yesha"
      ],
      "year": "2006",
      "venue": "International Journal on Digital Libraries"
    },
    {
      "citation_id": "16",
      "title": "Motion-based video representation for scene change detection",
      "authors": [
        "C Ngo",
        "T Pong",
        "H Zhang",
        "R Chin"
      ],
      "year": "2000",
      "venue": "ICPR 2000, International Conference on Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Automatic video summarization by graph modeling",
      "authors": [
        "C Ngo",
        "Y Ma",
        "H Zhang"
      ],
      "year": "2003",
      "venue": "ICCV 2003, IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Information theory-based shot cut/fade detection and video summarization",
      "authors": [
        "Z Cernekova",
        "I Pitas",
        "C Nikou"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised video summarization with adversarial LSTM networks",
      "authors": [
        "B Mahasseni",
        "M Lam",
        "S Todorovic"
      ],
      "year": "2017",
      "venue": "CVPR 2017, 30th IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised video summarization via attention-driven adversarial learning",
      "authors": [
        "E Apostolidis",
        "E Adamantidou",
        "A Metsai",
        "V Mezaris",
        "I Patras"
      ],
      "year": "2020",
      "venue": "Unsupervised video summarization via attention-driven adversarial learning"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised video summarization with attentive conditional generative adversarial networks",
      "authors": [
        "X He",
        "Y Hua",
        "T Song",
        "Z Zhang",
        "Z Xue",
        "R Ma",
        "N Robertson",
        "H Guan"
      ],
      "year": "2019",
      "venue": "Multimedia 2019, 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Video summarization using fully convolutional sequence networks",
      "authors": [
        "M Rochan",
        "L Ye",
        "Y Wang"
      ],
      "year": "2018",
      "venue": "ECCV 2018, European Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Video summarization with attention-based encoder-decoder networks",
      "authors": [
        "Z Ji",
        "K Xiong",
        "Y Pang",
        "X Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "24",
      "title": "Exploring global diverse attention via pairwise temporal relation for video summarization",
      "authors": [
        "P Li",
        "Q Ye",
        "L Zhang",
        "L Yuan",
        "X Xu",
        "L Shao"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Looking at the viewer: Analysing facial activity to detect personal highlights of multimedia contents",
      "authors": [
        "H Joho",
        "J Staiano",
        "N Sebe",
        "J Jose"
      ],
      "year": "2011",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "26",
      "title": "Analysing user physiological responses for affective video summarisation",
      "authors": [
        "A Money",
        "H Agius"
      ],
      "year": "2009",
      "venue": "Displays"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "28",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP 2017, IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition from 3D log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Continuous emotion recognition in speech -do we need recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "in IN-TERSPEECH 2019, Annual Conference of the International Speech Communication Association. ISCA"
    },
    {
      "citation_id": "31",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization",
      "authors": [
        "B Xu",
        "Y Fu",
        "Y Jiang",
        "B Li",
        "L Sigal"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "A Multi-Task Neural Approach for Emotion Attribution, Classification, and Summarization",
      "authors": [
        "G Tu",
        "Y Fu",
        "B Li",
        "J Gao",
        "Y Jiang",
        "X Xue"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Discriminative feature learning for unsupervised video summarization",
      "authors": [
        "Y Jung",
        "D Cho",
        "D Kim",
        "S Woo",
        "I Kweon"
      ],
      "year": "2019",
      "venue": "33rd AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Property-constrained dual learning for video summarization",
      "authors": [
        "B Zhao",
        "X Li",
        "X Lu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "36",
      "title": "Deep reinforcement learning for unsupervised video summarization with diversityrepresentativeness reward",
      "authors": [
        "K Zhou",
        "Y Qiao",
        "T Xiang"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Spatiotemporal modeling for video summarization using convolutional recurrent neural network",
      "authors": [
        "Y Yuan",
        "H Li",
        "Q Wang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "ICLR 2015, 3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "39",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "40",
      "title": "Deep attentive and semantic preserving video summarization",
      "authors": [
        "Z Ji",
        "F Jiao",
        "Y Pang",
        "L Shao"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "41",
      "title": "TVSum: Summarizing web videos using titles",
      "authors": [
        "Y Song",
        "J Vallmitjana",
        "A Stent",
        "A Jaimes"
      ],
      "year": "2015",
      "venue": "CVPR 2015, IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "CVPR 2015, IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "authors": [
        "J Long",
        "E Shelhamer",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "CVPR 2018, IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and af",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "venue": "Introducing the RECOLA multimodal corpus of remote collaborative and af"
    }
  ]
}