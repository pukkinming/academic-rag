{
  "paper_id": "2111.12324v1",
  "title": "How Speech Is Recognized To Be Emotional -A Study Based On Information Decomposition",
  "published": "2021-11-24T08:15:53Z",
  "authors": [
    "Haoran Sun",
    "Lantian Li",
    "Thomas Fang Zheng",
    "Dong Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The way that humans encode their emotion into speech signals is complex. For instance, an angry man may increase his pitch and speaking rate, and use impolite words. In this paper, we present a preliminary study on various emotional factors and investigate how each of them impacts modern emotion recognition systems. The key tool of our study is the SpeechFlow model presented recently, by which we are able to decompose speech signals into separate information factors (content, pitch, rhythm). Based on this decomposition, we carefully studied the performance of each information component and their combinations. We conducted the study on three different speech emotion corpora and chose an attention-based convolutional RNN as the emotion classifier. Our results show that rhythm is the most important component for emotional expression. Moreover, the cross-corpus results are very bad (even worse than guess), demonstrating that the present speech emotion recognition model is rather weak. Interestingly, by removing one or several unimportant components, the cross-corpus results can be improved. This demonstrates the potential of the decomposition approach towards a generalizable emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recognizing emotion from speech signals is highly desirable for designing a comfortable human-machine interface. After three decades of research, speech emotion recognition (SER) has gained significant improvement  [1] . Early research mostly focused on extracting emotion-related features, forming some 'standard' feature sets such as GeMAPS  [2]  and COMPARE  [3] . By these emotion features, simple classifiers such as hidden Markov model (HMM) or support vector machines (SVM) were employed to determine the emotion  [4] ,  [5]  . Recently, deep learning methods gained much popularity, in particular the end-to-end architecture based on deep neural nets (DNN)  [6] -  [13] . Good performance was reported with various types of DNN models, including convolutional neural network (CNN)  [14] , deep belief network  [15] , longshort term memory (LSTM)  [8]  and variational auto-encoders (VAE)  [16] . The main advantage of deep learning models is that they can learn emotional cues automatically, so potentially discover features more powerful than human engineering.\n\nIn spite of the notable advance in performance, how a speech is recognized by the machine to be emotional is still far from clear. One reason is that human emotions in audio data are very complex, and the expression and perception of a particular emotion is impacted by various factors such as gender, speakers, age, culture, and languages  [17] . From the signal processing perspective, Murray et al  [18]  identified that the quality of voice, the timing of pronunciation units, and the pitch contour are mostly affected in emotional speech. This study is very inspiring and guided the long-standing research on emotion sensitive features. However, it is still not easy to identify by which information factors in the speech signal that machines recognize human's emotion, even if we can test and compare the SER performance with individual features and their combinations. This is because there is no guarantee that these features faithfully reflect the underlying information factors (e.g., prosody patterns), and there is no guarantee that the complex temporal/frequency dependencies within the original speech signal can be recovered by reintegrating the separately extracted features. This is in particular true with the DNN-based end-to-end model, as the decision is made largely in a black box.\n\nIn this paper, we try to answer the following question: \"How an end-to-end DNN model determines emotions\". Our main tool is a speech factorization model called SpeechFlow  [19] . By this model, speech signals can be decomposed into separate information factors, and these factors can be put together to recover the original speech. This analysis-and-synthesis tool offers us an interesting opportunity to manipulate the information load in speech signals, allowing us testing the impact of each individual information factor and their combinations. In this preliminary study, we decompose speech signal into three components: content, rhythm, and pitch. This decomposition was motivated by the importance of timing (rhythm) and pitch in human's emotion perception, as found by Murray  [18] , as well as the intrinsic association of emotion status and linguistic content  [20] -  [22] . Fig.  1  illustrates the full diagram of our approach.\n\nWe chose the attention-based convolutional RNN (ACRNN)  [10]  as the SER backbone, due to its good performance reported in the literature. The ACRNN model was trained with IEMOCAP, a popular emotion speech dataset in English, and was tested on IEMOCAP, SAVEE and CSLT-ESDB, where SAVEE and CSLT-ESDB are two datasets in English and Chinese respectively. The results show that among the three information factors (content, rhythm, pitch), rhythm is the most discriminative and generalizable. When the test data is highly mismatched with the training data, for example in the cross-lingual test, removing some information factors may increase rather than decrease the performance. This suggests that to achieve reasonable generalization, information control deserves deeper investigation.\n\nOur contribution is two-fold:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methods",
      "text": "In this section, we firstly revisit the SpeechFlow model presented in  [19] . This model can decompose speech signals into separate information factors, and then reconstruct the original speech from these factors. Importantly, we can remove any individual factor by setting its value to be a constant, which allows us freely manipulating the information load of the speech signal. The second component of our architecture is an attention-based convolutional RNN (ACRNN)  [10] . This model can use as the backbone of our SER system, and test the SER performance of speech signals with different information load. The entire architecture of this work is illustrated in Fig.  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Speechflow For Speech Information Decomposition",
      "text": "SpeechFlow is a speech factorization model proposed recently  [19] . This model can decompose speech signals into separate information factors in an unsupervised way. As shown in Fig.  1  (left), speech S is decomposed by SpeechFlow into four information factors: rhythm Z r , pitch Z f , content Z c and timbre Z t , by four neural encoders: a rhythm encoder E r , a pitch encoder E f , a content encoder E c and a timbre encoder E t .\n\nThe training objective of the model is to reconstruct the original speech S from these information factors, with a decoder D. Formally, the reconstructed speech Ŝ is obtained by:\n\nand the objective function L is formulated as:\n\nwhere ||•|| denotes the 2 -norm. To ensure that the information factors posses their own desired information after model training, the encoders need some special designs, as shown below.\n\nFirstly, the input to the rhythm encoder E r , content encoder E c and timbre encoder E t is speech S, whereas the input to pitch encoder E f is the normalized pitch contour P. Note that P has been normalized to posses the same mean and variation for all the speakers, so it involves only rhythm and pitch information, without information about speaker trait and linguistic content.\n\nSecondly, a random resampling operation (RR) along the temporal axis is conducted before speech S is fed to the content encoder E c and pitch encoder E f . This operation randomly shrinks or stretches the duration of each speech segment. This operation makes the two encoders lose the true rhythm information, so the complete rhythm information can only pass through the rhythm encoder E r and represented by Z r .\n\nThirdly, the speaker identity vector is used as the timbre information. We choose a pre-trained speaker recognition model as the timbre encoder E t . This model is based on deep speaker embedding  [23] ,  [24] , and the produced speaker vectors are used as Z t . Note that Z t involves only the timber information.\n\nFinally, the dimensions of all the information factors are much lower than the input speech. This limited dimensionality forms the so-called information bottleneck, which means that none of the individual factors can represent the whole signal, and all the factors must cooperate together to achieve the learning goal, i.e., reconstruct the original speech. To make this cooperation effective and economic, each factor has to focus on the information that it can easily supply and others cannot, thus leading to the desired information decomposition  [19] .\n\nPut them together, the entire encoding process can be formulated as follows:\n\nwhere RR denotes random resampling.\n\nSpeechFlow offers a powerful analysis-and-synthesis tool, by which we can freely manipulate the information factors, hence modifying the information load in the reconstructed speech. For example, we can set any information factor to be a constant, so that remove the corresponding information from the reconstructed speech. In this study, we focus on the impact of three information factors: content (Z c ), rhythm (Z r ), and pitch (Z f ). The impact of the timbre factor Z t will be left for future investigation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Acrnn-Based Emotion Recognition",
      "text": "If the SpeechFlow model has been well-trained, we can use it to manipulate the information load in speech signals. This allows us to conduct an ablation study to evaluate which information factor is the most important for emotion recognition. We first generate speech signals with particular information involved, and then fed the speech into an attentionbased convolutional RNN (ACRNN)  [10]  classifier for emotion recognition, as shown in Fig.  1 (right) .\n\nThe ACRNN structure consists of four components, as detailed below. More details about ACRNN can be found in  [10] . Note that for each configuration of the information factor selection in SpeechFlow, we need retrain the ACRNN model. This is the case even we select all the information factors (i.e., do not intentionally remove any information). III. RELATED WORK Speech factorization has been extensively used in speech coding, speech synthesis, and voice conversion  [25] -  [28] , but the application in SER is rare. Li et al.  [29]  proposed a cascade factorization approach, where content and speaker traits are sequentially extracted in prior, and these information factors are used as conditional inputs of an end-to-end SER system. Peri et al.  [30]  employed an adversarial training to purify emotion embeddings, by using both audio and visual streams, though it is more a feature extraction approach rather than a complete factorization approach.\n\nThe research on the decision process of DNN in SER is also not extensive. Jalal et al.  [31]  investigated how an attentionbased DNN model focuses on the important segment of the speech signal when performing SER. This work is different from ours as we focus on important information factors rather than important segments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments A. Data",
      "text": "The corpora used in our experiments are summarized in Table  I . Considering that the sets of emotion labels are not completely the same for different corpora, we chose to use the four overlapped emotion classes: Angry, Happy, Sad and Neutral. All the speech signals were uniformly formatted to 16kHz, 16-bits to ensure data consistency.\n\nSince English and Chinese are different in pronunciation, we trained separate SpeechFlow models for the two languages, using VCTK and AISHELL-3 respectively. Two ACRNN SER models were trained, one with IEMOCAP and the other with CSLT-ESDB  1  . For each training, the entire corpus was split into a training set (80%), a validation set (10%) and a test set (10%). The speakers and utterances in the validation and test sets did not appear in the training set. The SAVEE dataset is too small to be used for model training, we therefore used it as a test set only.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Configurations 1) Speechflow:",
      "text": "The SpeechFlow model was implemented using the source code published online 2  . We mostly followed the settings in the original repository, including network structures, data preprocessing steps, and the training scheme. The only change we made is the timbre encoder, which is onehot speaker codes. This one-hot code cannot be extended to represent speakers outside of the training dataset, therefore not suited for our task.\n\nTo solve this problem, we introduced a speaker encoder to realize the function of the timbre encoder. The speaker encoder is able to generate continuous vectors to represent the trait of speakers. These continuous vectors, often called speaker vectors, are generalizable to speakers in any database.\n\nIn our experiments, we chose the d-vector model to implement the speaker (timbre) encoder  [23] ,  [24] . The model was constructed following the Kaldi SITW recipe  [37] . 2) ACRNN: We built the ACRNN SER model using the public source code online 3 . The input feature was 80dimensional mel-spectrograms, to match the output of the SpeechFlow model. Other configurations were the same as in  [10] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Qualitative Analysis Of Speechflow",
      "text": "We firstly verify how SpeechFlow decomposes and recovers speech signals. Specifically, we first decompose the spectrogram of a speech signal into individual information factors, and then remove a particular factor by setting the input of the corresponding encoder to be zero. Finally, the spectrogram can be recovered by the decoder of the SpeechFlow model. The reconstructed spectrograms, when different information factors are removed, are shown in Fig.  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Basic Results",
      "text": "In this section, we report the SER performance when different information factors are removed. As the first step, 3 https://github.com/xuanjihe/speech-emotion-recognition.\n\nwe chose IEMOCAP as the training data to build the ACRNN model, and tested the performance on IEMOCAP and SAVEE. The SpeechFlow model was trained using VCTK. We use the unweighted average recall (UAR) as the metric, and the results are shown in Table  II . Note that '' denotes that this factor was preserved while \"\" denotes that this factor was removed. For example, in system 4, the ACRNN model was trained with speech reconstructed by preserving the content factor Z c only, and the validation and test data were processed in the same way. Firstly, we observe that system 2, where all the information factors are preserved, can obtain performance comparable to system 1 on both the two test sets. Besides, if all the information factors are removed (system 3), the decision is completely random.  4  These results double confirmed that SpeechFlow can decompose speech signals into information factors and these factors can be put together to recover the original speech.\n\nSecondly, we find that rhythm obtains better performance than content and pitch (compare system 5 vs.  4  and 6 ). This observation is consistent in both the within-corpus test (IEMOCAP) and the cross-corpus test (SAVEE). It indicates that rhythm is a salient feature used by the ACRNN model: it is not only discriminative, but also generalizable.\n\nThirdly, we observe that combining rhythm with other factors did not lead to clear advantage, and sometimes may lead to performance loss. This suggests that DNNs may rely on one or a few information to perform the decision, rather than the full information set. This behavior, however, may be related to the limited training data, and deeper investigation with a larger dataset will give a more convincing conclusion.\n\nTo assist the analysis for the SER performances, the confusion matrices for the IEMOCAP test and the SAVEE test are shown in Fig.  3  and Fig.  4 , respectively. It can be found that the performance tendency is quite similar on the two test sets. For example, for system 4 (content only), all the utterances tend to be recognized as sad. Moreover, the inter-emotion confusion is more balanced with system 5 (rhythm only) compared to system 4 (content only) and system 6 (pitch only), double confirming the superiority of rhythm information in DNNbased SER.      II  when tested on SAVEE. The meaning of the labels and numbers are the same as in Fig.  3 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Cross-Corpus Results",
      "text": "In this section, we design a cross-lingual test using the IEMOCAP and CSLT-ESDB datasets, which are in different languages. VCTK and AISHELL-3 were firstly employed to train an English SpeechFlow and a Chinese SpeechFlow respectively. Then, IEMOCAP was used to train the English ACRNN model, where the VCTK-based SpeechFlow was used to perform information selection, and CSLT-ESDB was used to train the Chinese ACRNN models, where the AISHELL-3based SpeechFlow was used for information selection. The UAR results are reported in Table  III , where IEMO and ESDB are the abbreviations of IEMOCAP and CSLT-ESDB, respectively. Firstly, it can be observed that although the results of the within-lingual tests are highly promising, the performance of the cross-lingual tests is very bad, sometimes even worse than guess (25%). Besides, by removing one or several unimportant components, the cross-lingual performance can be increased rather than decreased (ref. to System 2 and System 5, 7, 9, with IEMO training and ESDB test). This demonstrates the potential of the decomposition approach towards a generalizable emotion recognition, and also suggests that information selection/control might be important for cross-lingual SER.\n\nNevertheless, since all the results are so bad in the crosslingual tests, no conclusions can be said convincing. Perhaps the only thing we can make sure is that the present DNNbased SER system is not well generalized and more research is required.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this paper, we presented a preliminary study on how DNN-based SER models make decisions on emotions. To answer this question, we employed the SpeechFlow model to decompose speech signals into separate information factors, and then comprehensively studied the impact of each information factor and their combinations to the SER performance. Our results on three emotion corpora showed that rhythm has more discrimination and generalization capability on SER, at least in within-corpus tests. However, with more challenging mismatch such as in the cross-lingual test, all these factors and their combinations failed to deliver reasonable performance. This demonstrated that the current speech emotion model is still unreliable and can not be applied 'in the wild'. As for the future work, we will study more powerful factorization models, in order to pursue more independent factors. Moreover, we need larger emotion datasets to verify the observations in this study. Finally, the impact of cultural discrepancy deserves deeper investigation, by which we may explain the weak performance in our cross-lingual test.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the full diagram of our",
      "page": 1
    },
    {
      "caption": "Figure 1: The full diagram of the work. The SpeechFlow model decomposes speech signals into three information factors: content Zc, rhythm Zr, pitch Zf.",
      "page": 2
    },
    {
      "caption": "Figure 2: The spectrogram reconstructed by SpeechFlow. The model was",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices of systems in Table II when tested on IEMOCAP.",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrices of systems in Table II when tested on SAVEE.",
      "page": 5
    },
    {
      "caption": "Figure 3: E. Cross-corpus results",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "The INTER-SPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "The INTER-SPEECH 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "4",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Deep temporal models using identity skip-connections for speech emotion recognition",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "9",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "arxiv": "arXiv:1706.00612"
    },
    {
      "citation_id": "10",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "11",
      "title": "Mmann: Multimodal multilevel attention neural network for horror clip detection",
      "authors": [
        "X Cheng",
        "X Zhang",
        "M Xu",
        "T Zheng"
      ],
      "year": "2018",
      "venue": "Mmann: Multimodal multilevel attention neural network for horror clip detection"
    },
    {
      "citation_id": "12",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2018",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "13",
      "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "14",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech",
      "arxiv": "arXiv:1904.03833"
    },
    {
      "citation_id": "15",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "16",
      "title": "Random deep belief networks for recognizing emotions from speech signals",
      "authors": [
        "G Wen",
        "H Li",
        "J Huang",
        "D Li",
        "E Xun"
      ],
      "year": "2017",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational autoencoders for learning latent representations of speech emotion: A preliminary study",
      "arxiv": "arXiv:1712.08708"
    },
    {
      "citation_id": "18",
      "title": "Att-net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "19",
      "title": "Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion",
      "authors": [
        "I Murray",
        "J Arnott"
      ],
      "year": "1993",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised speech decomposition via triple information bottleneck",
      "authors": [
        "K Qian",
        "Y Zhang",
        "S Chang",
        "M Hasegawa-Johnson",
        "D Cox"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Intonation as an interface between language and affect",
      "authors": [
        "D Grandjean",
        "T Bänziger",
        "K Scherer"
      ],
      "year": "2006",
      "venue": "Progress in brain research"
    },
    {
      "citation_id": "22",
      "title": "Emotional speech processing: Disentangling the effects of prosody and semantic cues",
      "authors": [
        "M Pell",
        "A Jaywant",
        "L Monetta",
        "S Kotz"
      ],
      "year": "2011",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "23",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Deep neural networks for small footprint text-dependent speaker verification",
      "authors": [
        "E Variani",
        "X Lei",
        "E Mcdermott",
        "I Moreno",
        "J Gonzalez-Dominguez"
      ],
      "year": "2014",
      "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Gaussian-constrained training for speaker verification",
      "authors": [
        "L Li",
        "Z Tang",
        "Y Shi",
        "D Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Learning latent representations for speech generation and transformation",
      "authors": [
        "W.-N Hsu",
        "Y Zhang",
        "J Glass"
      ],
      "year": "2017",
      "venue": "Learning latent representations for speech generation and transformation",
      "arxiv": "arXiv:1704.04222"
    },
    {
      "citation_id": "27",
      "title": "Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations",
      "authors": [
        "J.-C Chou",
        "C -C. Yeh",
        "H -Y. Lee",
        "L.-S Lee"
      ],
      "year": "2018",
      "venue": "Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations",
      "arxiv": "arXiv:1804.02812"
    },
    {
      "citation_id": "28",
      "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y.-J Zhang",
        "S Pan",
        "L He",
        "Z.-H Ling"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "30",
      "title": "Deep factorization for speech signal",
      "authors": [
        "L Li",
        "D Wang",
        "Y Chen",
        "Y Shi",
        "Z Tang",
        "T Zheng"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Disentanglement for audio-visual emotion recognition using multitask setup",
      "authors": [
        "R Peri",
        "S Parthasarathy",
        "C Bradshaw",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain"
      ],
      "year": "2020",
      "venue": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition"
    },
    {
      "citation_id": "33",
      "title": "CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2017",
      "venue": "University of Edinburgh. The Centre for Speech Technology Research (CSTR)"
    },
    {
      "citation_id": "34",
      "title": "AISHELL-3: A multi-speaker mandarin TTS corpus and the baselines",
      "authors": [
        "X Yao",
        "Hui Shi",
        "Bu"
      ],
      "year": "2015",
      "venue": "AISHELL-3: A multi-speaker mandarin TTS corpus and the baselines"
    },
    {
      "citation_id": "35",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "36",
      "title": "Emotional adaptive training for speaker verification",
      "authors": [
        "F Bie",
        "D Wang",
        "T Zheng",
        "J Tejedor",
        "R Chen"
      ],
      "year": "2013",
      "venue": "2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "37",
      "title": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems",
      "authors": [
        "W Wang"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "38",
      "title": "The Kaldi speech recognition toolkit",
      "authors": [
        "D Povey",
        "A Ghoshal",
        "G Boulianne",
        "L Burget",
        "O Glembek",
        "N Goel",
        "M Hannemann",
        "P Motlicek",
        "Y Qian",
        "P Schwarz"
      ],
      "year": "2011",
      "venue": "IEEE 2011 workshop on automatic speech recognition and understanding"
    }
  ]
}