{
  "paper_id": "2101.10785v1",
  "title": "Developing Emotion Recognition For Video Conference Software To Support People With Autism",
  "published": "2021-01-26T13:54:36Z",
  "authors": [
    "Marc Franzen",
    "Michael Stephan Gresser",
    "Tobias Müller",
    "Sebastian Mauser"
  ],
  "keywords": [
    "emotion recognition",
    "communication aids",
    "computer science",
    "artificial intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We develop an emotion recognition software for the use with a video conference software for autistic individuals which are unable to recognize emotions properly. It can get an image out of the video stream, detect the emotion in it with the help of a neural network and display the prediction to the user. The network is trained on facial landmark features. The software is fully modular to support adaption to different video conference software, programming languages and implementations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Autism is a spectrum-condition, where the affected person can have a wide variety of impacts on various abilities. Some individuals have repetitive and stereotypical interests which makes them prefer doing recurring and monotonous work. Other individuals have impacts on social skills, like the ability to interact and communicate well with other people.  [1]   [2]  Oftentimes, they lack the ability to detect emotions in the faces of their conversation partners.  [3]  This makes it difficult for them to estimate the course of an interview and therefore we want to support them with our software.\n\nThe software recognizes the emotion of a communication partner during a video conference and displays the result to the autistic individual. As a result, the software may compensate for a vital part of the social skillset of an autistic individual.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Solutions",
      "text": "In terms of emotion recognition of a facial image, there are already different commercial and non-commercial solutions with different accuracies available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Commercial Solution",
      "text": "In the commercial space, Affectiva is a company that develops emotion measurement technology and offers it as a service for other companies to use in their products. Their core product is the AFFDEX algorithm  [4] , that is used in Affectiva's AFFDEX SDK, which is available for purchase on their website  [5] . It is mainly meant for market research, but it is also used in other environments, such as the automotive industry to monitor emotions and reactions of a driver.  [6]  AFFDEX has already found its way into a similar context as this project. With the help of AR glasses, children and adults can be assisted in learning \"crucial social and cognitive skills\" with a special focus on emotion recognition.  [7]  This also supports our motivation, since many people with autism have problems understanding emotions.\n\nAlso, AFFDEX has a high accuracy in detecting \"key emotions\", more precisely in the \"high 90th percentile\".  [8]  As the AFFDEX SDK is a commercial solution, it is not easily accessible and has a price point of 25000 USD  [5] . Therefore, this SDK is not the ideal solution for this research project and other solutions must be considered.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Existing Solutions",
      "text": "As an overview, we consult the paper  [9] , which is summarized in the following paragraphs. It distinguishes between three different steps: Image Acquisition, Feature Extraction, Classification.\n\na) Image Acquisition: This step contains the acquisition of images from various sources, including \"a database, a live video stream or other sources, in 2D or 3D\".  [9]  The data source can either be static by using still images or dynamic by using image sequences.\n\nAfterwards, there might be pre-processing applied to the data. This could be de-noising, scaling and cropping to optimize the data for the next step.\n\nb) Feature Extraction: The extraction of features from facial data is an essential step, as they describe the \"physical phenomena\"  [9]  on which the detection of facial expressions is based on.\n\nThe better the selection of features and their representation of the face, the more robust the recognition afterwards is.\n\nThe available methods can be grouped into appearance features, geometric features, a hybrid approach using both and a template-based approach. c) Classification: In the final classification step, the detected facial expression is assigned to a predefined expression.\n\nThe available classifiers can typically be split into parametric and non-parametric machine-learning-methods, where either a predetermined function is given and parameters for this function are learned or the mapping function is learned without predetermining the form of a function.\n\nNon-machine-learning methods might be feasible but are not discussed in this overview-paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Examples",
      "text": "a) Linear Directional Patterns: A solution referenced in  [9]  uses Linear Directional Patterns (LDP) representing the changes in the image as a histogram, which results in stable facial features.\n\nThese features are then fed into a Support Vector Machine (SVM) that maps these features to their corresponding emotions.\n\nThey state, that they reach an accuracy between 80% and 99% depending on the used parameters and test-scenarios.\n\nb) Active Appearance Model: Another already existing project uses an Active Appearance Model (AAM) to gather features which are then used to detect emotions in an image.  [10]  The AAM takes the statistical model of a face, representing its shape and tries to match this model to the image that is currently processed. More precisely, the model consists of a set of connected points (landmark coordinates) which are then iteratively deformed until they fit onto the current image. This process can be improved by training a model with images and their respective landmark coordinates.  [11]  In that project, the extracted features are then used to calculate a mean parameter vector for each of seven emotions. These mean parameter vectors are then compared to the Euclidean distance from the face parameters of the current image. The accuracy of this approach is at around 90% for the emotions fear, joy, disgust and neutral, but around 60-80% for surprise, anger and sadness.  [10]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Proposed Solution",
      "text": "Our hypothesis is that the direct matching from faces to emotions, as well as the manual feature engineering like in the above solutions with e.g. the Euclidean distance can lead to a lesser accurate detection than could be theoretically possible.\n\nFor similar tasks, neural networks are often considered, because of their ability to learn relevant correlations in the data on their own and it could perform better because of additional learned features.\n\nBecause of this, we propose the use of a neural network as our classifier to match faces to emotions, because they can learn features, a human would not consider to be relevant in the face.\n\nTo circumvent the problem to match raw image data to emotions, we take the common approach to use stable features similar to AAM in  [10]  as the input to our neural network which then matches these to the corresponding emotions. This enables a more diverse prediction, without the need to distinguish between e.g. genders, skin-color and age. Our solution is divided into four different kinds of modules (See Tab. I).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method",
      "text": "Each module runs in a separate process and uses ZeroMQ for communication. ZeroMQ is a programming-language independent library, which enables us to open sockets on the localhost of a machine and then use TCP connections to exchange data  [12] . We implement this as a \"Request-Reply\" pattern, where one module requests data by sending \"ready\" over the socket, where the other module then sends the data in the previously defined format. If a module stops, it sends \"done\" and quits. As soon as another module receives \"done\" instead of \"ready\", it itself sends the \"done\" message to its adjacent module and quits.\n\nBy implementing this pattern, we achieve an exchangeability, in which we can replace any module from any programming language, which supports OpenCV and ZeroMQ.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Module: Input B. Module: Model",
      "text": "The model expects any image type as a byte-stream and creates an OpenCV image object from that. To improve the performance, we scale this image down and thus reduce the amount of data for further processing. It is then serialized and forwarded to the controller.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Module: Controller",
      "text": "The controller module expects a serialized OpenCV image from the model. It then de-serializes it and starts processing it in two steps, as shown in figure  1  and outlined in the following paragraphs. 1) Facial landmark detection: For facial landmarkdetection we use the technique from  [15]  that is included in the dlib library  [16]  and adapted the implementation from  [17] , which is using the pre-trained detector inside the library. We choose this method from the summary  [18]  because it incorporates the face detection and the facial landmark detection in a single solution while providing real-time capabilities and an open source.\n\n2) Emotion detection: From the gathered facial landmark points, we use artificial intelligence as a mapping function between the facial landmark features and the given emotion. The neural network learns the correlation between certain points of a face and the corresponding emotion while ignoring irrelevant points. As an example, humans would rate, that the position of the tip of the nose does not correspond as strongly to emotions as the corners of the mouth do. a) Dataset: We use the Dataset \"facial_expressions\"  [14]  from this challenge  [19] . This dataset has 13718 images, each labeled manually by the submitter with one of 8 facial expressions: anger, contempt, disgust, fear, happiness, sadness, surprise and neutral. We choose this dataset because it contains natural images of celebrities, as these reflect real emotions better than datasets crafted with actors. For better training we remove the data submitted by \"jhamski\" and \"628\" as these images vary in resolution and color space and are sometimes distorted. 456 images were not included into the dataset because the chosen facial landmark recognition could not detect a face in them. We also move random pictures of every category from the training dataset to a validation set to compare this solution against others (see Section IV). These images are not used to train the network.\n\nThe exact composition of the full possible training and validation sets is shown in table II.\n\nOur final dataset consists of 12309 gray-scale images, each with a resolution of 350x350 pixels. The composition is unevenly distributed and thus not suitable for training neural networks. This is verified through initial tests, where all classes with small amounts of images are ignored in any predictions.\n\nHowever, we determine, that the emotions contempt, disgust, fear and sadness are not important in a business video call. In most cases, happiness and neutral indicate if a con-ference is going well. Therefore anger and surprise are less important.\n\nSo, for the scope of this project we decide to leave them out for now and focus on happiness and neutral as facial expressions. If this works well, it should be expandable to other emotions with larger datasets. This leaves us with 11228 images for training and 382 for validation. For training we use two common implementations of neural networks and some variation in the fed data which results in three total tested solutions.\n\nWe use TensorFlow  [20]  with Keras  [21]  as the framework to create all neural networks.\n\nFully-Connected Neural Network:\n\nThe first tested network is a dense neural network (DNN) to create a multi-layer perceptron (MLP). It has the following architecture:\n\n• Input Layer with X-& Y-Coordinates for each of the 68 detected facial landmarks (136 inputs) • Dense hidden Layer with 1024 neurons, followed by a dropout layer with rate 0.5 • Dense hidden Layer with 512 neurons, followed by a dropout layer with rate 0.5 • Dense hidden Layer with 256 neurons, followed by a dropout layer with rate 0.5 • Dense output layer with a node for each of the 2 possible emotions As this model hits a plateau between 75% and 80% accuracy on the training and validation dataset, we suspect this could be caused by the changing absolute coordinates between images. So as a second attempt we add additional features and convert all absolute coordinates to relative ones.\n\nFully-Connected Neural Network with modified features:\n\nAs the most relevant changes happen relative to the center of the respective portion of the face, the center point of each portion is calculated. This was added as an additional feature and all points of this portion were added relative to that center.\n\nWe decide to split the face in 4 parts: mouth, nose, left eye, right eye. The eyebrows are given from the center of the corresponding eye, because we think that the movement of an eyebrow relative to the eye (raising eyebrows) is the most significant change in an emotion.\n\nAdditionally, the positions of the landmarks depend on the shape of a face. As an accommodation, we add the width, height and the center point of the face outline as features for the neural network. With these, the network can potentially predict the emotion independently from the geometry of the face.\n\nThis modified dataset uses the same DNN architecture with the only difference being the input layer to accommodate for the additional features.\n\nThis yields slightly better results on both datasets, as later discussed in IV.\n\nConvolutional Neural Network: Convolutional Neural Networks (CNN) are usually used for image recognition, as they can make use of the spatial position of features.  [22]  So we think this concept might be applied here, since the position of each facial landmark can correspond to certain emotions.\n\nFor the dataset to be used with a CNN, all points of each image were projected into a 350x350 matrix, initially filled with zeros, where each added '1' corresponds to a facial landmark position.\n\nThe CNN has the following architecture:\n\n• Input layer with a 350x350 matrix with a depth of 1 for the single channel representing either a facial landmark at the given point or not • 2D-Convolution layer with 32 neurons and a kernel size of 3 • Maximum 2D-Pooling layer with a size of 2x2\n\n• Dropout layer with a rate of 0.25 to combat overfitting • Flattening layer to prepare the data for the fully connected layer • Dense output layer with a node for each of the 2 possible emotions To combat overfitting, additionally to the dropout layer, we use data augmentation since in this case it is easy to flip each matrix horizontally to get twice as many data rows as before.\n\nCommon parameters: All neural networks were trained using the optimizer Adam  [23]  with a learning rate of 1 * 10 -6 . The used activation function was always Rectified Linear Unit (relu), except for the output layer, which used softmax. The range of the full width and height (350px each) of all coordinates for the DNNs was mapped to values from 0 to 1.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture Creation:",
      "text": "For a basis on which we can improve on, we use the model architectures provided by the Keras framework as examples  [24] .\n\nWith the Multilayer Perceptron and Convolutional Neural Network as starting points for our further development, we adapt them to our data format and reach the final presented networks by training, testing and altering the models.\n\nThe base remains conceptually the same, with minor modifications, like adding or removing layers and adjusting parameters, like the neuron-count or the dropout rate. c) Use: With the trained model, we run inference on the solution. The trained model is simply loaded using the Keras framework and inference is called using the given methods provided by the Keras Model class.\n\nThe result of this inference is then sent as a string for the emotion to the view module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Module: View",
      "text": "The view accepts any kind of string and displays it in a window on the screen. In future work, there could be an implementation of this bachelor's thesis  [1] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "To evaluate the software, we define the following office scenario: The opponent has a webcam with a resolution of 720p30 and the user has a PC with a recent (as of 2019, 9th Generation) Intel Core i5 processor.\n\nTo compare our solution with a representative of the commercial sector, we selected the AFFDEX algorithm. It is implemented in the demo-program \"AffdexMe\"  [25] . We install it on the same machine we use for our own solution. Instead of the webcam input we use a stream of the desktop to simulate how AffdexMe would perform with the same input.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Performance",
      "text": "As AffdexMe does not provide interfaces for time measurement, we use a camera with a high framerate to count the milliseconds passed between the image being visible for AffdexMe and the program detecting the face and emotion.\n\nIn our solution, we also measure the time between the image reaching the controller module and the detected emotion leaving the controller module for the different types of networks.\n\nAll tests are executed on the same machine with an Intel Core i5 5500U and 8 GB of RAM.\n\nThe performance of each software varies greatly with the input resolution. For AffdexMe the input-images have a resolution of 525 x 525 pixels, while our software takes a screenshot with a resolution of 1366 x 768 pixels. This has to be taken into consideration when evaluating the results in table  III .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Precision",
      "text": "We compare all of our 3 optional solutions with each other and the solution from Affectiva  [4] . We separated random images from the dataset for validation, based on our rating of importance of each emotion for a business video conference. We use two metrics to evaluate the results. For one, we collect the certainty for the emotions in AffdexMe and our solutions that correspond to the labeled emotions of the dataset and calculate the average per emotion. As Affectiva does not have a \"neutral\" emotion, we use the strongest detected emotion and calculate the reciprocal value. This is shown in table IV. As the other metric we calculate the accuracy of the predictions by each solution and AffdexMe. This is achieved by measuring the percentage of all correctly classified emotions on our dataset for validation. These results are shown in table V.  With modified features it performs better overall, but only by a small margin. We suspect this is because the relative features have a slightly more relevant changes in their values and it might also be caused by the additional width and height features.\n\nThe training begins to plateau at about 3000 epochs. After that it only increases slowly. They might not be fully fitted yet, but are close to being fully trained, where it does not increase anymore or even decreases.\n\nAs the modified features train faster and get better overall results, this might be the preferred option when continuing to develop these methods further.\n\nLike the DNN, the CNN continues improving over the whole 500 trained epochs and also does not reach a point yet where any accuracy starts to decrease. From this we can also argue, that with more training time these results can be further improved. As training on the CNN takes 72 to 73 seconds per epoch, it is stopped at 500 epochs, which is equivalent to 10 hours of training time, because of the time constraint of this project.\n\nWe suspect it would get better quickly on the training dataset but would stay in the same range on the validation dataset, as it already plateaus early on and increases only by a small margin towards the end.\n\nAs it is shown in figure  3  the accuracy of the CNN is after 1/6th of the steps already more accurate on the training dataset. At the end of our training it is less accurate on the validation data than the DNN at its end.\n\nTo perform similar or better on the validation set than the DNNs, we think the CNNs architecture would need some modification and fine tuning. c) Discussion: As the DNN is smaller in file size of the model and is faster for inference on a single image, this is the preferred type for our solution for now, based on these results.\n\nAs already mentioned, we choose the DNN with relative feature points, as it is slightly better. The time for calculating relative coordinates from the absolute ones is negligible as the few needed array operations lay in the margin of error when measuring runtime.\n\nAlthough the results show, that we do not reach accuracies on the validation-dataset in the higher 90th percentile, like the shown solutions in II-C, we think that our solution has the benefit of using natural emotions for training.\n\nWe see many of the existing solutions use images of acted emotions, while our solution is trained on non-acted, everyday images of famous people.\n\nTo support this claim, we conducted several real-world tests with our solution in different conditions. They show a subjectively higher accuracy than our validation dataset.\n\nAdditionally, our solution cannot be easily compared with many other solutions, as we do not use the same datasets for validation. With different images, our accuracy can improve significantly when compared to the other solutions.\n\nFinally, our solutions are more accurate on our chosen dataset than \"AffdexMe\", but also slightly slower. As already mentioned, this is due to the different resolutions.\n\nAlso, based on our experience using the AFFDEX algorithm, we think that it benefits from a stream of moving images. However, we use static images to test it to be able to compare the results to our solution.\n\nFurthermore, our solution has the benefit compared to AFFDEX, that we are just training on the relevant emotions, while their solution is influenced by additional emotions which can result in more false predictions.\n\nV. FUTURE IMPROVEMENTS a) Multi-modal information: Because our social interaction is not just based on visual feedback alone, we propose the idea of combining several sources of human reactions as a result of the current emotional state.\n\nThis might improve the overall precision and confidence in the recognition system.\n\nIn case of video conference software, voice data might be taken into consideration. This was already evaluated in more detail in  [26]  and  [27] .\n\nAdditionally, chat messages can reflect the current emotional state and might also be used as another source of information, as discussed in  [28] .\n\nThese sources might be used as a starting point for further investigation.\n\nb) Adaption to video conference software: Because this project is built with modularity in mind, it is easy to provide adaptions for various video conference software and different User-Interfaces like the one presented in  [1] . c) Larger Dataset: Our dataset consists of 12309 images with labels. As the labels are distributed very unequally, future work can concentrate on gathering a larger and more evenly distributed dataset to improve the detection of the less important emotions as well as helping the classifier to distinct between similar emotions like anger and disgust.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We conclude that this software is a big step forward for the effective employment of individuals with autism.\n\nAlthough our solution is limited to two distinct emotions, we think it can already help such individuals to achieve the everyday task of video conferences in an office environment.\n\nAnd, as we stated before, with more data and further learning this can be improved to include more emotions.\n\nIt might be a good approach before deploying such a software, to discuss how this affects the emotional climate in the office, as well as what the effects of a wrong classification could be, if decisions are based on these judgements.\n\nOverall, it is a helpful software that could already be used in its current state when all other hurdles about privacy, fairness and the possible impacts of misclassifications can be overcome.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Steps for detecting emotions. Photo of face from dataset [14]",
      "page": 3
    },
    {
      "caption": "Figure 2: DNN: Accuracy on training and validation data over 5000 epochs",
      "page": 5
    },
    {
      "caption": "Figure 3: CNN: Accuracy on training and validation data over 500 epochs",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Module": "main",
          "Description": "Starts the other modules and waits for\nthem to exit."
        },
        {
          "Module": "input",
          "Description": "Gathers\nthe\nimage\nin some way and\nsends\nthe image-data as\n.jpg over\nthe\nsocket."
        },
        {
          "Module": "model",
          "Description": "Receives the image-data and constructs\na standardized OpenCV image object."
        },
        {
          "Module": "controller",
          "Description": "Receives\nthe OpenCV image\nobject,\nperforms\nthe emotion recognition and\nsends the conclusion as string."
        },
        {
          "Module": "view",
          "Description": "Receives\nthe string and displays\nit on\nthe screen."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Anger",
          "Training": "169",
          "Validation": "45",
          "Importance": "medium"
        },
        {
          "Emotion": "Contempt",
          "Training": "9",
          "Validation": "-",
          "Importance": "low"
        },
        {
          "Emotion": "Disgust",
          "Training": "12",
          "Validation": "-",
          "Importance": "low"
        },
        {
          "Emotion": "Fear",
          "Training": "11",
          "Validation": "-",
          "Importance": "low"
        },
        {
          "Emotion": "Happiness",
          "Training": "4961",
          "Validation": "191",
          "Importance": "high"
        },
        {
          "Emotion": "Neutral",
          "Training": "6267",
          "Validation": "191",
          "Importance": "high"
        },
        {
          "Emotion": "Sadness",
          "Training": "116",
          "Validation": "-",
          "Importance": "low"
        },
        {
          "Emotion": "Surprise",
          "Training": "288",
          "Validation": "49",
          "Importance": "medium"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Average": "737,1ms",
          "Standard Deviation": "35,3ms"
        },
        {
          "Average": "727,4ms",
          "Standard Deviation": "25,5ms"
        },
        {
          "Average": "767,8ms",
          "Standard Deviation": "40,7ms"
        },
        {
          "Average": "367,8ms",
          "Standard Deviation": "116,9ms"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happiness": "191",
          "Neutral": "191",
          "Total": "382"
        },
        {
          "Happiness": "69,48%",
          "Neutral": "75,04%",
          "Total": "72,26%"
        },
        {
          "Happiness": "72,39%",
          "Neutral": "76,79%",
          "Total": "74,59%"
        },
        {
          "Happiness": "55,27%",
          "Neutral": "62,46%",
          "Total": "58,87%"
        },
        {
          "Happiness": "52,36%",
          "Neutral": "26,54%",
          "Total": "39,45%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happiness": "191",
          "Neutral": "191",
          "Total": "382"
        },
        {
          "Happiness": "74,35%",
          "Neutral": "86,91%",
          "Total": "80,63%"
        },
        {
          "Happiness": "78,01%",
          "Neutral": "86,39%",
          "Total": "82,20%"
        },
        {
          "Happiness": "58,64%",
          "Neutral": "75,39%",
          "Total": "67,02%"
        },
        {
          "Happiness": "59,26%",
          "Neutral": "52,15%",
          "Total": "55,70%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "User Experience Design für Anwendungen mit Künstlicher Intelligenz",
      "authors": [
        "S Bobek"
      ],
      "venue": "User Experience Design für Anwendungen mit Künstlicher Intelligenz"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Germany Weingarten"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Autismus | auticon erklärt wie wir das Autismus-Spektrum sehen",
      "authors": [
        "Auticon"
      ],
      "year": "2019",
      "venue": "Autismus | auticon erklärt wie wir das Autismus-Spektrum sehen"
    },
    {
      "citation_id": "4",
      "title": "Recognition of emotions in autism: A formal meta-analysis",
      "authors": [
        "M Uljarevic",
        "A Hamilton"
      ],
      "year": "2013",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "5",
      "title": "Affdex sdk: A crossplatform real-time multi-face expression recognition toolkit",
      "authors": [
        "D Mcduff",
        "A Mahmoud",
        "M Mavadati",
        "M Amr",
        "J Turcot",
        "R Kaliouby"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems, ser. CHI EA '16"
    },
    {
      "citation_id": "7",
      "title": "Pricing -Affectiva Developer Portal",
      "authors": [
        "Affectiva"
      ],
      "year": "2019",
      "venue": "Pricing -Affectiva Developer Portal"
    },
    {
      "citation_id": "8",
      "title": "Predicting ad liking and purchase intent: Largescale analysis of facial responses to ads",
      "authors": [
        "D Mcduff",
        "R Kaliouby",
        "J Cohn",
        "R Picard"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Success Stories -Brain Power -Autism Support",
      "authors": [
        "Affectiva"
      ],
      "year": "2019",
      "venue": "Success Stories -Brain Power -Autism Support"
    },
    {
      "citation_id": "10",
      "title": "Determining Accuracy",
      "authors": [
        "Affectiva"
      ],
      "year": "2019",
      "venue": "Determining Accuracy"
    },
    {
      "citation_id": "11",
      "title": "Face expression recognition: A brief overview of the last decade",
      "authors": [
        "C.-D Cȃleanu"
      ],
      "year": "2013",
      "venue": "2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI)"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition using facial expressions with active appearance models",
      "authors": [
        "M Ratliff",
        "E Patterson"
      ],
      "year": "2008",
      "venue": "Proc. of HRI"
    },
    {
      "citation_id": "13",
      "title": "Active appearance models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "1998",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "14",
      "title": "ZeroMQ | Get started",
      "year": "2019",
      "venue": "The ZeroMQ authors"
    },
    {
      "citation_id": "15",
      "title": "mss • PyPI",
      "authors": [
        "T2"
      ],
      "year": "2019",
      "venue": "mss • PyPI"
    },
    {
      "citation_id": "16",
      "title": "GitHub -muxspace/facial_expressions: A set of images for classifying facial expressions",
      "authors": [
        "B Rowe"
      ],
      "year": "2019",
      "venue": "GitHub -muxspace/facial_expressions: A set of images for classifying facial expressions"
    },
    {
      "citation_id": "17",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "J Sullivan"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "dlib C++ Library",
      "authors": [
        "D King"
      ],
      "year": "2019",
      "venue": "dlib C++ Library"
    },
    {
      "citation_id": "19",
      "title": "Facial landmarks with dlib, OpenCV, and Python",
      "authors": [
        "A Rosebrock"
      ],
      "year": "2019",
      "venue": "Facial landmarks with dlib, OpenCV, and Python"
    },
    {
      "citation_id": "20",
      "title": "List of Code",
      "authors": [
        "Y Wu"
      ],
      "year": "2016",
      "venue": "List of Code"
    },
    {
      "citation_id": "21",
      "title": "Emotion Detection From Facial Expressions",
      "year": "2019",
      "venue": "Emotion Detection From Facial Expressions"
    },
    {
      "citation_id": "22",
      "title": "TensorFlow",
      "authors": [
        "Tensorflow"
      ],
      "year": "2019",
      "venue": "TensorFlow"
    },
    {
      "citation_id": "23",
      "title": "Keras: The Python Deep Learning library",
      "year": "2019",
      "venue": "Keras: The Python Deep Learning library"
    },
    {
      "citation_id": "24",
      "title": "Convolutional neural network",
      "authors": [
        "Wikipedia"
      ],
      "year": "2019",
      "venue": "Convolutional neural network"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "26",
      "title": "Guide to the Sequential model -Examples",
      "year": "2019",
      "venue": "Guide to the Sequential model -Examples"
    },
    {
      "citation_id": "27",
      "title": "AffdexMe -Affectiva Developer Portal",
      "authors": [
        "Affectiva"
      ],
      "year": "2019",
      "venue": "AffdexMe -Affectiva Developer Portal"
    },
    {
      "citation_id": "28",
      "title": "Multi-modal emotion recognition from speech and text",
      "authors": [
        "Z.-J Chuang",
        "C.-H Wu"
      ],
      "year": "2004",
      "venue": "International Journal of Computational Linguistics & Chinese Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Facial emotion recognition using multi-modal information",
      "authors": [
        "L Silva",
        "T Miyasato",
        "R Nakatsu"
      ],
      "year": "1997",
      "venue": "Proceedings of the IEEE Intelligent Conf. Information, Comm. And Signal Processing",
      "doi": "10.1109/ICICS.1997.647126"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition from text using semantic labels and separable mixture models",
      "authors": [
        "C.-H Wu",
        "Z.-J Chuang",
        "Y.-C Lin"
      ],
      "year": "2006",
      "venue": "ACM transactions on Asian language information processing (TALIP)"
    }
  ]
}