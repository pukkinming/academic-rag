{
  "paper_id": "2304.08981v2",
  "title": "‚Ä¢ Human-Centered Computing ‚Üí Human Computer Interaction (Hci)",
  "published": "2023-04-18T13:23:42Z",
  "authors": [
    "Zheng Lian",
    "Haiyang Sun",
    "Licai Sun",
    "Kang Chen",
    "Mingyu Xu",
    "Kexin Wang",
    "Ke Xu",
    "Yu He",
    "Ying Li",
    "Jinming Zhao",
    "Ye Liu",
    "Bin Liu",
    "Jiangyan Yi",
    "Meng Wang",
    "Erik Cambria",
    "Guoying Zhao",
    "Bj√∂rn W. Schuller",
    "Jianhua Tao"
  ],
  "keywords": [
    "Multimodal Emotion Recognition Challenge (MER 2023)",
    "multilabel learning",
    "modality robustness",
    "semi-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition has become an important research topic due to its wide-ranging applications in human-computer interaction. Over the past few decades, researchers have proposed various approaches  [1] [2] [3] . But due to their low robustness in complex environments, existing techniques do not fully meet the demands in practice. To this end, we launch a Multimodal Emotion Recognition Challenge (MER 2023), which aims to improve system robustness from three aspects: multi-label learning, modality robustness, and semi-supervised learning.\n\nAnnotating with both discrete and dimensional emotions is common in current datasets  [4, 5] . Existing works mainly utilize multitask learning to predict all labels simultaneously  [6, 7] . However, these works ignore the correlation between discrete and dimensional emotions. For example, valence is a dimensional emotion that reflects the degree of pleasure. For negative emotions (such as anger and sadness), the valence score should be less than 0; for positive emotions (such as happiness), the valence score should be greater than 0. To fully exploit the multi-label correlation, we launch the MER-MULTI sub-challenge, which encourages participants to exploit the appropriate loss function  [8]  or model structure  [9]  to boost recognition performance.\n\nMany factors may lead to modality perturbation, which increases the difficulty of emotion recognition. Recently, researchers have proposed various strategies to deal with this problem  [10] [11] [12] . But due to the lack of benchmark datasets, existing works mainly rely on their own simulated missing conditions to evaluate modality robustness. To this end, we launch the MER-NOISE sub-challenge, which provides a benchmark test set focusing on more realistic modality perturbations such as background noise and blurry videos. In this sub-challenge, we encourage participants to use data augmentation  [13]  or other more advanced techniques  [14, 15] .\n\nMeanwhile, it is difficult to collect large amounts of emotionlabeled samples due to the high annotation cost. Training with limited data harms the generalization ability of recognition systems. To address this issue, researchers have exploited various pre-trained models for video emotion recognition  [16, 17] . However, task similarity impacts the performance of transfer learning  [18] . Existing video-level pre-trained models mainly focus on action recognition rather than expression videos  [19] . In this paper, we extract human-centered video clips from movies and TV series that contain emotional expressions. We then launch the MER-SEMI sub-challenge, encouraging participants to use semi-supervised learning  [19, 20]  to achieve better performance.\n\nTherefore, MER 2023 consists of three sub-challenges: MER-MULTI, MER-NOISE, and MER-SEMI. Different from existing challenges (such as AVEC  [21] [22] [23] [24] [25] [26] [27] [28] [29] , EmotiW  [30] [31] [32] [33] [34] [35] [36] [37] , and MuSE  [38] [39] [40] ), we mainly focus on system robustness, and provide a common platform and benchmark test sets for performance evaluation. We plan to organize a series of challenges and related workshops that bring together researchers from all over the world to discuss recent research and future directions in this field.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Challenge Dataset",
      "text": "MER 2023 employs an extended version of CHEAVD for performance evaluation. Due to the small size of CHEAVD, we implement a fully automatic strategy to collect large amounts of unlabeled video clips; due to the low annotation consistency of CHEAVD, we adopt a stricter data selection approach and split the dataset into reliable and unreliable parts. As for reliable samples, we further divide them into three subsets: Train&Val, MER-MULTI, and MER-NOISE. As for unreliable samples, we treat them as unlabeled data and merge them with automatically-collected samples to form MER-SEMI. Statistics of each subset are shown in Table  1 . Figure  1  summarizes the distribution of discrete emotions. Despite some imbalance, our dataset still exhibits a relatively high balance compared to other mainstream benchmarks such as MELD  [41]  and CMU-MOSEI  [5] . Figure  2  further reveals the relationship between discrete emotions and valences. Valence serves as an indicator of pleasure, and the value from small to large means the sentiment from negative to positive. From this figure, we observe that the valence distribution of different discrete labels is quite reasonable. Negative emotions (such as anger, sadness, and worry) predominantly exhibit valences below 0. Conversely, positive emotions (such as happiness) primarily exhibit valences above 0. The valence associated with neutral centers around 0. Notably, surprise is a fairly complex emotion that contains multiple meanings such as sadly surprised, angrily surprised, or happily surprised. Hence, its valence ranges from negative to positive. These findings ensure the high quality of our labels and demonstrate the necessity of incorporating both discrete and dimensional annotations, as they can help us distinguish some subtle differences in emotional states.\n\nTo download the dataset, participants should fill out an End User License Agreement (EULA)  4  , which requires participants to use this dataset only for academic research and not to edit or upload samples to the Internet. For each track, participants can submit 20 times per day with a maximum of 200 times: MER-MULTI 5  , MER-NOISE  6  , and MER-SEMI  7  . At the end of the challenge, each team is required to submit a paper describing their approach. For each paper, the program committee will conduct a double-blind review of the scientific quality, novelty, and technical quality. To continue using this dataset after the challenge, please sign a new EULA  8  and send it to our official email address  9  . We will provide the test set labels to facilitate further usage. We believe this dataset can serve as a new benchmark in robust multimodal emotion recognition, especially for the Chinese research community.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Participants And Outcome",
      "text": "This year's challenge attracts the registration of 76 teams from varying academic institutions. Due to the inherent class imbalance of discrete emotions (see Figure  1 ), we choose the weighted average F-score as our evaluation metric, consistent with previous works  [42, 43] . For dimensional emotions, we select the widely utilized mean square errors as the evaluation metric. To further evaluate comprehensive performance, we define a combined metric that incorporates both discrete and dimension predictions:\n\nwhere metric ùëí and metric ùë£ represent the metrics for discrete emotions and valences, respectively. In MER-MULTI and MER-NOISE, participants are required to provide predictions for both discrete and dimensional emotions. Therefore, we use the combined metric for performance evaluation. In MER-SEMI, we only evaluate discrete results on the labeled subset. Therefore, we use the weighted average F-score as the evaluation metric.\n\nFor each sub-challenge, we perform an initial attempt to explore a range of multimodal features and establish a competitive baseline system 10 . To ensure reproducibility, we primarily utilize open-source pre-trained models for feature extraction and a simple yet effective multi-layer perceptron for emotion recognition. In MER-MULTI and MER-NOISE, our baseline system achieves 0.56 and 0.41 on the combined metric, respectively. In MER-SEMI, we only evaluated discrete emotions and our baseline system reaches 86.40% on the weighted average F-score.\n\nTable  2  ‚àº Table  4  show the leaderboards for the three subchallenges. Excitingly, we witness that most teams exceed our baseline performance. The team named \"sense-dl-lab\" emerges as the winner across all three sub-challenges. Their system outperforms our baseline by 0.1405 on MER-MULTI and 0.2746 on MER-NOISE. For MER-SEMI, their system reaches 89.11% on the evaluation metric, outperforming our baseline by 2.36%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "This paper summarizes MER 2023, a multimodal emotion recognition challenge focused on system robustness. MER 2023 consists of three sub-challenges: (1) MER-MULTI requires participants to predict both discrete and dimensional emotions. This multi-scale labeling process can help distinguish some subtle differences in emotional states; (2) MER-NOISE simulates data corruption in realworld environments for modality robustness evaluation; (3) MER-SEMI requires participants to train more powerful classifiers using large amounts of unlabeled data. In the future, we plan to increase both labeled and unlabeled samples in our corpus. Additionally, we hope to organize a series of challenges and related workshops that bring together researchers from all over the world to discuss recent research and future directions in multimodal emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of discrete emotions (Train&Val).",
      "page": 2
    },
    {
      "caption": "Figure 2: Empirical PDF on the valence for different discrete",
      "page": 2
    },
    {
      "caption": "Figure 1: summarizes the distribution of discrete emotions. De-",
      "page": 2
    },
    {
      "caption": "Figure 2: further reveals the relationship",
      "page": 2
    },
    {
      "caption": "Figure 1: ), we choose the weighted average",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rank": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n‚Äì\n16\n17\n18\n19\n20\n21\n22\n23",
          "Team": "sense-dl-lab\nAIPL-BME-SEU\nUSTC-qw\nAI4AI\nT_MERG\nSZTU-MIPS\nDesheng\nSuda_iai\nEmotion recognition group\nSUST-EiAi-Team\nFudanDML\nMCI-SCUT\nBeihang University\nADDD\nWinner\nBaseline\nTUA1\nSCUTer\nCiL Fighting!\nCCNUNLP\nQuaint Critters\nEmo.avi\nSDNU_AIASC\nCognitist",
          "Combined (‚Üë)": "0.7005\n0.6860\n0.6846\n0.6783\n0.6765\n0.6702\n0.6675\n0.6087\n0.6025\n0.5988\n0.5880\n0.5819\n0.5713\n0.5673\n0.5655\n0.56\n0.5561\n0.5551\n0.5453\n0.5446\n0.5422\n0.4977\n0.4639\n0.1612"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rank": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n‚Äì\n17\n18\n19\n20\n21\n22\n23",
          "Team": "sense-dl-lab\nAIPL-BME-SEU\nAI4AI\nLeVoice\nSZTU-MIPS\nUSTC-qw\nVoice of Soul\nDesheng\nBeihang University\nSUST-EiAi-Team\nTriple Six\nFudanDML\nDelta\nUSTBJDL822\nMCI-SCUT\nTrailblazers\nBaseline\nADDD\nCiL Fighting!\nSuda_iai\nT_MERG\nQuaint Critters\nTUA1\nUSTC-IAT-United",
          "Combined (‚Üë)": "0.6846\n0.6694\n0.6371\n0.6256\n0.6247\n0.6162\n0.6140\n0.5707\n0.5462\n0.5455\n0.5444\n0.5378\n0.5339\n0.5075\n0.5003\n0.4669\n0.41\n0.4055\n0.3863\n0.3744\n0.3666\n0.3648\n0.0723\n-0.4608"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rank": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n‚Äì\n11\n12\n13\n14\n15",
          "Team": "sense-dl-lab\nSZTU-MIPS\nSUST-EiAi-Team\nDesheng\nAI4AI\nUSTC-IAT-United\nSCUTer\nVoice of Soul\nBeihang University\nAIPL-BME-SEU\nBaseline\nADDD\nBig Data and Intelligence Cognition\nTUA1\nT_MERG\nWearing Instruments Lab",
          "Discrete (‚Üë)": "0.8911\n0.8855\n0.8853\n0.8841\n0.8811\n0.8775\n0.8726\n0.8703\n0.8691\n0.8689\n0.8675\n0.8661\n0.8537\n0.8507\n0.8486\n0.0661"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Vincent Michalski",
        "Kishore Konda",
        "Roland Memisevic",
        "Christopher Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "2",
      "title": "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Xavier Bouthillier",
        "Pascal Lamblin",
        "Caglar Gulcehre",
        "Vincent Michalski",
        "Kishore Konda",
        "S√©bastien Jean",
        "Pierre Froumenty",
        "Yann Dauphin",
        "Nicolas Boulanger-Lewandowski"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "3",
      "title": "Recent trends in deep learning based natural language processing",
      "authors": [
        "Tom Young",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "7",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "8",
      "title": "Exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
      "authors": [
        "Huang-Cheng Chou",
        "Chi-Chun Lee",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Emotional reaction analysis based on multi-label graph convolutional networks and dynamic facial expression recognition transformer",
      "authors": [
        "Kexin Wang",
        "Zheng Lian",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Yin Fan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "10",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "arxiv": "arXiv:2208.07589"
    },
    {
      "citation_id": "13",
      "title": "Analyzing modality robustness in multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Yingting Li",
        "Bo Cheng",
        "Shuai Zhao",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2022",
      "venue": "Proceedings of the North American Chapter"
    },
    {
      "citation_id": "14",
      "title": "Deep partial multi-view learning",
      "authors": [
        "Changqing Zhang",
        "Yajie Cui",
        "Zongbo Han",
        "Joey Zhou",
        "Huazhu Fu",
        "Qinghua Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised representation learning with future observation prediction for speech emotion recognition",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the Interspeech"
    },
    {
      "citation_id": "17",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "Rui Mao",
        "Qian Liu",
        "Kai He",
        "Wei Li",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "A survey of transfer learning",
      "authors": [
        "Karl Weiss",
        "Taghi Khoshgoftaar",
        "Dingding Wang"
      ],
      "year": "2016",
      "venue": "Journal of Big data"
    },
    {
      "citation_id": "19",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Doll√°r",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Florian Eyben",
        "Gary Mckeown",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "22",
      "title": "Avec 2012: the continuous audio/visual emotion challenge",
      "authors": [
        "Bj√∂rn Schuller",
        "Michel Valster",
        "Florian Eyben",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2012",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "23",
      "title": "Avec 2013: the continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "Michel Valstar",
        "Bj√∂rn Schuller",
        "Kirsty Smith",
        "Florian Eyben",
        "Bihan Jiang",
        "Sanjay Bilakhia",
        "Sebastian Schnieder",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "24",
      "title": "Avec 2014: 3d dimensional affect and depression recognition challenge",
      "authors": [
        "Michel Valstar",
        "Bj√∂rn Schuller",
        "Kirsty Smith",
        "Timur Almaev",
        "Florian Eyben",
        "Jarek Krajewski",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "25",
      "title": "Avec 2015: The 5th international audio/visual emotion challenge and workshop",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Bj√∂rn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "27",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Jonathan Gratch",
        "Roddy Cowie",
        "Stefan Scherer",
        "Sharon Mozgai",
        "Nicholas Cummins",
        "Maximilian Schmitt",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "28",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Denis Lalanne",
        "Adrien Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "29",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition in the wild challenge 2013",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Michael Wagner",
        "Tom Gedeon"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Karan Sikka",
        "Tom Gedeon"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "32",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "Abhinav Dhall",
        "Roland Ov Ramana Murthy",
        "Jyoti Goecke",
        "Tom Joshi",
        "Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "33",
      "title": "Emotiw 2016: Video and group-level emotion recognition challenges",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "34",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Shreya Ghosh",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "35",
      "title": "Emotiw 2018: Audio-video, student engagement and group-level affect prediction",
      "authors": [
        "Abhinav Dhall",
        "Amanjot Kaur",
        "Roland Goecke",
        "Tom Gedeon"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "36",
      "title": "Automatic emotion, engagement and cohesion prediction tasks",
      "authors": [
        "Abhinav Dhall",
        "Emotiw"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "37",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "Abhinav Dhall",
        "Garima Sharma",
        "Roland Goecke",
        "Tom Gedeon"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "38",
      "title": "Muse 2020 challenge and workshop: Multimodal sentiment analysis, emotiontarget engagement and trustworthiness detection in real-life media: Emotional car reviews in-the-wild",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Georgios Rizos",
        "Panagiotis Tzirakis",
        "Xinchen Du",
        "Felix Hafner",
        "Lea Schumann",
        "Adria Mallol-Ragolta",
        "Bj√∂rn Schuller",
        "Iulia Lefter"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "39",
      "title": "The muse 2021 multimodal sentiment analysis challenge: sentiment, emotion, physiologicalemotion, and stress",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Christ",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Eva-Maria Messner",
        "Erik Cambria",
        "Guoying Zhao",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "40",
      "title": "Muse 2022 challenge: Multimodal humour, emotional reactions, and stress",
      "authors": [
        "Shahin Amiriparian",
        "Lukas Christ",
        "Andreas K√∂nig",
        "Eva-Maria Me√üner",
        "Alan Cowen",
        "Erik Cambria",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Decn: Dialogical emotion correction network for conversational emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    }
  ]
}