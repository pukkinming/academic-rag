{
  "paper_id": "2210.14495v1",
  "title": "Two-Stage Dimensional Emotion Recognition By Fusing Predictions Of Acoustic And Text Networks Using Svm",
  "published": "2022-10-26T05:49:13Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "keywords": [
    "automatic speech emotion recognition",
    "affective computing",
    "late fusion",
    "multimodal fusion",
    "dimensional emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic speech emotion recognition (SER) by a computer is a critical component for more natural human-machine interaction. As in human-human interaction, the capability to perceive emotion correctly is essential to take further steps in a particular situation. One issue in SER is whether it is necessary to combine acoustic features with other data, such as facial expressions, text, and motion capture. This research proposes to combine acoustic and text information by applying a late-fusion approach consisting of two steps. First, acoustic and text features are trained separately in deep learning systems. Second, the prediction results from the deep learning systems are fed into a support vector machine (SVM) to predict the final regression score. Furthermore, the task in this research is dimensional emotion modeling, because it can enable a deeper analysis of affective states. Experimental results show that this two-stage, late-fusion approach obtains higher performance than that of any one-stage processing, with a linear correlation from one-stage to two-stage processing. This latefusion approach improves previous early fusion results measured in concordance correlation coefficients score.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human emotion is important for responding properly in a particular situation for both human-human communication and future machinehuman communication. Emotion can be recognized from many modalities: facial expressions, speech, and motion of body parts. In the absence of visual features, speech is the only way to recognize emotion, as in the case of a telephone call or a call-center application  (Petrushin, 1999) . By identifying caller emotions automatically from a system, appropriate feedback can be applied quickly and precisely. Speech is a modality in which both acoustic and verbal information can be extracted to recognize human emotion. Unfortunately, most speech emotion recognition (SER) systems use only acoustic features for predicting categorical emotions. In contrast, this research proposes to use both acoustic and text features to improve dimensional SER performance. Text can be extracted from speech, and it may contribute to emotion recognition. For example, an interlocutor can perceive emotion not only from prosodic information but also from semantics.  Grice (2002)  stated in his implicature theory that what is implied derives from what is said. For example, if someone says that he is angry but looks happy, then the implication is that he is indeed angry. Hence, it is necessary to use linguistic information to determine expressed emotion from speech. A fusion of acoustic and linguistic information from speech is viable since (spoken) text can be obtained from speech-to-text technology. This bimodal features fusion strategy may improve the performance of SER over acoustic-only SER.\n\nBesides the categorical approach, emotion can also be analyzed via a dimensional approach. In dimensional emotion, affective states are lines in a continuous space. Some researchers have used a two-dimensional (2D) space comprising valance (positive or negative) and arousal (excited or apathetic).\n\nOther researchers have proposed a 3D emotional space by adding either dominance (degree of power over emotion) or liking/disliking. Although it is rare, a 4D emotional space has also been studied by adding expectancy or naturalness. after used by DNNs is a reason to use SVM over DNN.\n\nThis study aims to evaluate the combination of acoustic and text features to improve the performance of dimensional automatic SER by using two-stage processing. Current research on pattern recognition has also shown that the use of multimodal features from audio, visual, and motion-capture data increases performance as compared to using a single modality  (Hu and Flaxman, 2018; Yoon et al., 2018; Tripathi and Beigi, 2018) . Meanwhile, research on big data has revealed that the use of more data will improve performance for results from the same algorithm  (Halevy et al., 2009) . By using both acoustic and text features, SER should obtain improved performance over acoustic-only and textonly recognition. This assumption is also motivated by the fact that human emotion perception uses multimodal sensing, peculiarly verbal and non-verbal information. Many technologies, such as human-robot interaction, can potentially benefit from such improvement in emotion recognition.\n\nThe main contributions of this study then are: (1) a proposal of two-stage processing for dimensional emotion recognition from acoustic and text features using LSTM and SVM, and a comparison of the results with unimodal results and another fusion method on the same metric and dataset scenario; (2) an evaluation of different acoustic and text features to find the best pair of acoustic-text pair based on evaluated features, including a frame-based acoustic feature and utterance-based statistical functions with and without silent pause features; (3) evaluation of speaker-dependent vs. speaker-independent scenarios in dimensional speech emotion recognition from text features; and (4) evaluation of using text features on a dataset that originally contains target sentences but removed to avoid the effect of these target sentences.\n\nThe rest of this paper is organized as follows. \"Related work\" reviews closely related work to this research, including the difference between this study and previous research, \"Datasets and features\" outlines the datasets and feature sets used in this research, \"Two-stage bimodal emotion recognition\" explains the method to achieve the results, \"Results and discussion\" shows the results and its discussion, and finally \"Conclusions\" concludes this study and proposes future work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Speech emotion recognition (SER) began to be seriously researched as part of human-computer interaction with the work by e.g.,  Kleine-cosack (2006) . The amount of research has grown as datasets have become publicly available, including the Berlin EMO-DB, IEMOCAP, MSP-IMPROV, and RAVDESS datasets.\n\nTo enable the analysis and comparison with previous research, we include the following literature reviews of related work. We focus on comparing previous work that used the same or similar datasets as this work does (specifically, IEMOCAP, MSP-IMPROV, or both), and especially on research that focused on dimensional rather than categorical emotion. While the focus here is on bimodal emotion recognition using both acoustic and text data, some work on speech-only or text-only emotion recognition is briefly described.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Acoustic Emotion Recognition",
      "text": "Recognition of emotion within speech signals has been actively developed since the success of recognizing emotion via facial expressions. From categorical emotion detection, the paradigm of SER has shifted to predicting degrees of emotion attributes or dimensional emotions. One of the earliest papers on (categorical) SER  (Petrushin, 1999)  explored how well humans and computers recognize emotion in speech. Since then, research on categorical emotion recognition has grown following the development of affective research in psychology.  Jin and Wang (2005)  reported a first trial on SER in categorical and twodimensional (2D) spaces. They found that acoustic features are helpful in describing and distinguishing emotion through the concept of emotion modeling (2D space). In 2009,  Giannakopoulos et al. (2009)  re-investigated the association of speech signals with an emotion wheel (continuous space). They proposed a method to estimate the degrees of valence and arousal. Their method, including a proposed feature set, could estimate both valence and arousal, with an error close to that of average human annotation.  Grimm et al. (2007)  used a fuzzy-logic estimator and a rule base derived from acoustic features in speech, such as pitch, energy, speaking rate, and spectral characteristics, to describe emotion primitives (valence, arousal, and dominance). They obtained a moderate to high correlation (0.42 < r <0.85) between their method and human annotation.\n\nUsing the IEMOCAP dataset, Parthasarathy and Busso tried to train a neural network system to predict valence, arousal, and dominance simultaneously  (Parthasarathy and Busso, 2017) . They proposed a multitask learning (MTL) system based on the mean squared error (MSE) to balance the prediction of the three emotion dimensions. They found that by combining a shared layer and an independent layer, the MTL system's best performance exceeded that of the traditional single-task learning (STL) method.\n\nAbdelWahab and Busso (2018) proposed using a domain-adversarial neural network (DANN) to solve the problem of mismatch between training and test data in dimensional SER. Using the DANN, they obtained performance that significantly improved that of a source-trained DNN. Thus, they addressed the importance of minimizing the mismatch between the source (training) and target (test) data. Furthermore, using the DANN showed that creating a flexible, discriminant feature representation can reduce the gap in the feature space between the source and target domains. Some of the above results on dimensional SER showed that recognizing valence is more difficult than recognizing arousal. To overcome this issue,  Sridhar et al. (2018)  used higher regularization (dropout) for valence than for the other dimensions when training SER through a DNN. Their system analysis showed that a higher dropout is needed for predicting valence. By using higher regularization, models could identify more general acoustic patterns that were observed across speakers.  Elbarougy and Akagi (2014)  used a three-layer model based on human perception for the same purpose.  Li and Akagi (2019)  improved on that work by combining acoustic features for multilingual emotion recognition.\n\nAlthough some improvements have been achieved, El  Ayadi et al. (2011)  addressed the SER issue of whether it suffices to use acoustic features for modeling emotions or it is necessary to combine them with other types of features, such as linguistic discourse information or facial features. Text features can be obtained through automatic speech recognition (ASR) and may be helpful in significantly improving SER performance. In particular, text features are expected to improve the performance of valence recognition, for which acoustic features have typically failed to achieve high performance. Moreover, text features are commonly used for sentiment analysis, which is similar to valence prediction.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Text Emotion Recognition",
      "text": "As mentioned above, one area of research on text processing focuses on sentiment analysis. This area is closely related to recognizing valence, i.e., the polarity or semantic orientation of an event, object, or situation  (Jurafsky and Martin, 2017) . Although early research sought to recognize sentiment in text, extension to recognize categorical and dimensional emotion has been attempted in recent years. As in other areas of research on pattern recognition, some researchers in text processing have used unsupervised learning to detect emotion in text  (Mäntylä et al., 2016; Mohammad, 2016) , while others have used supervised learning based on machine learning  (Alm et al., 2005; Yang et al., 2016) .  Alm et al. (2005)  used a bag-of-words (BoW) model and other text features from text datasets to predict emotion within those datasets. Using a multiclass linear classifier, they obtained encouraging results that suggest a potential direction for future research. Their proposed method could predict basic emotion from text with an accuracy close to 70%.  Kim et al. (2010)  used unsupervised learning to predict categorical emotion from three different datasets: SemEval, ISEAR, and Fairy Tales. Using three different techniques, they found that the best performance was achieved with categorical classification based on non-negative matrix factorization (NMF).  Atmaja and Akagi (2019)  used a deep-learning-based classification model and improved the precision, recall, and F-score results for the ISEAR dataset from 0.528, 0.417, and 0.372 to 0.56, 0.54, and 0.54, respectively. They showed the effectiveness of the deep-learning-based method for categorical emotion recognition on a larger dataset, while on a smaller dataset, the unsupervised approach achieved better results. Apart from categorical emotion recognition,  Atmaja and Akagi (2019)  also performed dimensional emotion recognition on the same dataset used for the categorical task. Similar to the categorical task, the results showed fewer errors when the size of the training set was increased.  Mäntylä et al. (2016)  used emotion words from an affective lexicon to mine valence, arousal, and dominance in text communication. Specifically, they used text communication data from a software development situation, including issues and comments captured through issue repository technology. They used the measure of valence, arousal, and dominance (VAD) to detect the productivity and burnout of the software developers. The results showed that increased emotions in terms of VAD correlated with increased productivity. Their results also complemented previous results showing that VAD can be measured from text, though at first, only the sentiment (valence) was used in text processing.\n\nResearch on text emotion recognition has usually used written language (from chats, Twitter, forum threads, etc.), which differs from spoken language.\n\nAlso, most such work on text processing has detected only the valence, i.e., only one emotion dimension. Because speech transcription converts spoken language to a written form, it should contain more emotional information than written plain text. Evaluation of the other emotion dimensions (arousal and dominance) is also necessary to determine the impact on those dimensions, along with evaluation of the combination with acoustic features for that purpose.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Bimodal Emotion Recognition",
      "text": "Using bimodal or multimodal features for emotion recognition is not new. Among many modalities, audio and visual features are the most used for extracting emotion information. When only speech is conveyed, however, two types of information can be extracted: acoustic and text features. Among many research papers, the reports by  Eyben et al. (2010) ,  Karadogan and Larsen (2012) ,  Ye and Fan (2014) ,  Jin et al. (2015) ,  Aldeneh et al. (2017) ,  Yoon et al. (2018) ,  Atmaja et al. (2019), and Zhang et al. (2019)  are the most related to this paper.  Eyben et al. (2010)  proposed an online method to detect not only valence and arousal but also the time when those emotion attributes are detected.\n\nThey used a recurrent neural network (RNN) based on long short-term memory (LSTM) to recognize a framewise valence-arousal continuum with time.\n\nBy adding a keyword spotter, they were able to improve the performance by using regression analysis. The results were measured in Pearson correlation coefficient (PCC). They also found that keywords like \"again,\" \"angry,\" \"assertive,\" and \"very\" were related to activation, while typical keywords correlated to valence were \"good,\" \"great,\" \"lovely,\" and \"totally.\" Similar to that idea,  Karadogan and Larsen (2012)  used affective words from Affective Norms for English Words (ANEW) to determine a valence-arousal value and combine it with a result from acoustic features. The latter paper also obtained similar improvement over using a single modality.  Ye and Fan (2014)  used bimodal features from acoustic and text information to recognize emotion within speech. The acoustic features were trained in two parallel classifiers: an SVM and a backpropagation network. The text features were trained in two serial classifiers, which were both Naive Bayes classifiers.\n\nThe second classifier acted as a filter for unreliable parts from the first classifier. Decision-level fusion (late fusion) was then implemented by combining the acoustic and text features with tree-weighting factors for the SVM, backpropagation network, and text classifiers. The resulting fusion method obtained 93% accuracy, as compared to 83% from the acoustic features only and 89% from the text features only. The task was categorical emotion detection from a Chinese database. Similar to that approach for a categorical task,  Jin et al. (2015)  used the IEMOCAP dataset to test combinations of acoustic and text features for SER. The novelty of their method was the use of an emotion vector for lexical features, which improved the accuracy in four-class emotion recognition from 53.5% (acoustic) and 57.4% (text) to 69.2% (acoustic + text).  Aldeneh et al. (2017)  used acoustic and lexical features to detect the degree of valence from speech. They used 40 mel-filterbanks (MFBs) as acoustic features and word vectors as text features. Continuous valence values were then converted to three categorical classes: negative, neutral, and positive. Using that approach, they improved the weighted accuracy from 64.5% (text) and 58.9% (acoustic) to 69.2% (acoustic + text).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "The term multimodal used in this subsection refers to the use of three or more modalities for emotion recognition. These modalities are usually visual, audio, text, gesture, and eye gaze. In a recent survey  (Yang and Hirschberg, 2018) , it is confirmed that the use of multimodal classifiers can outperform unimodal classifiers. Motivated by human multimodal experiences, multimodal processing not only applies to emotion recognition but also for other applications, e.g., audio-visual speech recognition, event detection, and media summarization. The lack of multimodal emotion recognition itself is the necessity of several modalities, mainly audio and video. In some cases, only audio data are available, e.g., telephone calls, voice assistants, and audio messages. Due to privacy and other reasons, the use of videos and other modalities may limit obtained data for predicting emotional states. In the case of speech, acoustic and linguistic features can be extracted back to obtain bimodal features.\n\nApart from the advantages and disadvantages of bimodal/multimodal recognition over using a single modality, there is a need to develop a new method for SER. The reasons are (1) some prior research did not predict all emotional attributes  (Eyben et al., 2010; Karadogan and Larsen, 2012; Zhang et al., 2019) , while other studies predicted emotion categories instead of attributes; (2) instead of predicting continuous emotion attribute scores, some studies switched to a categorical task for simplicity  (Zhang et al., 2019; Aldeneh et al., 2017) ; and (3) some of the reported results are not up to date and showed low improvement  (Eyben et al., 2010; Karadogan and Larsen, 2012) .\n\nAlthough we have limited our work to using both acoustic and text features, other researchers have already proposed another solution to solve the issues above, namely, using audiovisual emotion recognition. Nevertheless, there is still a need to propose and evaluate methods using both acoustic and text features because some target applications only involve speech data. In these voice-based applications, no visual or written-text information is acquired. To maximize the resources for extracting emotion within speech, this paper exploits both acoustic and text features (obtained via speech transcription) and combines them for dimensional emotion regression. By using both kinds of information in a two-stage process, we expect the proposed method's performance to be close to or exceed the performance obtained by using visual information.\n\nNote here that visual information, particularly facial expressions, has been reported to have more influence on dimensional emotion than other modalities do  (Fabien Ringeval et al., 2018) .",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Data And Feature Sets",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Datasets",
      "text": "Datasets for investigating our proposal to use two-stage processing for dimensional SER must meet certain requirements. The requirements are that (1) the dataset has both speech data and text transcription (to speed up text data acquisition), (2) the dataset is already annotated with dimensional labels, and\n\n(3) the dataset is publicly available. The following two datasets satisfy these requirements.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP, which stands for interactive emotional dyadic motion capture database, contains recordings of dyadic conversations with markers on the face, head, and hands. The recordings thus provide detailed information about the actors's facial expressions and hand movements during both scripted and spontaneous spoken communication scenarios  (Busso et al., 2008) . This research only uses the acoustic and text features because the goal is bimodal speech emotion recognition. The IEMOCAP dataset is freely available upon request, including its labels for categorical and dimensional emotion. We use the dimensional emotion labels, which are average scores for two evaluators because they enable deeper analysis of emotional states. The dimensional emotion scores, for va-lence, arousal, and dominance, are meant to range from 1 to 5 as a result of Self-Assessment Manikin (SAM) evaluation. We have found some labels with scores lower than 1 or higher than 5; however, we remove those data (seven samples). All labels are then converted from the 5-point scale to a floating-point values in range [-1, 1] when they are fed to a DNN system.\n\nThe total length of the IEMOCAP dataset is about 12 hours, or 10039 turns/utterances, from ten actors in five dyadic sessions (two actors each). The speech modality used to extract acoustic features is a set of files in the dataset with a single channel per sentence. The sampling rate of the speech data was 16 kHz. For text data, we use manual transcription in the dataset without additional preprocessing.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Msp-Improv",
      "text": "MSP-IMPROV, developed by the Multimodal Signal Processing (MSP) Lab at the University of Texas, Dallas, is a multimodal emotional database obtained by applying lexical and emotion control in the recording process while also promoting naturalness. The dataset provides audio and visual recordings, while text transcriptions are obtained via automatic speech recognition (ASR) provided by the authors. As with IEMOCAP, we use speech and text data with dimensional emotion labels. The annotation method for the recordings was the same as for IEMOCAP, i.e., SAM evaluation, with rating by at least five evaluators.\n\nWe treat missing evaluations as neutral speech (i.e., a score of 3 for valence, arousal, and dominance). Also as with IEMOCAP, all labels are converted to floating-point values in a range [-1, 1] from the original 5-point scale.\n\nThe MSP-IMPROV dataset was designed within a dialogue framework to elicit target sentences that had the same semantic content but were produced with different emotional expressions. In one recording, the target sentences were produced ad lib; for another recording, the target sentences were read.\n\nThese two recordings are referred to as \"Target-improvised\" and \"Target-read\", respectively. For our purposes, since our goal is to examine the effect of both linguistic and acoustic information on emotional ratings, these recordings were not appropriate for our study. However, we were able to use two sets of recordings that did not have the same semantic content, which is called \"Other-improvised\" and \"Natural-interaction\". The former included conversations of the actors during improvisation sessions; the latter included the exchanges during the breaks, while the actors were not acting, which also were being recorded. A similar protocol was used by  Zhang et al. (2019) , and we followed their lead in referring to this subset of the MSP-IMPROV dataset as MSP-I+N (MSP improvised and natural interaction), or MSPIN. In our work, we included the text transcriptions used by  Zhang et al.  (transcriptions are provided by the authors of the dataset); for the additional utterances not included in the Zang study, transcriptions were obtained using Mozilla's DeepSpeech  (Mozilla, 2019) . We thus use 7166 utterances from a total of 8438. The speech data in the dataset was sampled in mono at 44.1 kHz, with one file per utterance/sentence. We split each dataset into two partitions to observe any differences between a speaker-dependent (SD) partition and a speaker-independent partition made by leaving one session out (LOSO) for each dataset. For example, for the IEMO-CAP dataset, the last session (i.e., session 5), which is recorded from two different actors (out of 10) is only used for testing. Similarly, for MSP-I+N, all utterances from session 6 (two speakers out of 12) are used for the test set.\n\nOur rule for data splitting is to divide between the training + development and test sets in a ratio close to 80:20. This rule is applied for both the SD and LOSO partitions. Then, of the training + development data, 80% is used for training and the remaining 20% is used for development, as shown in Figure  1 . Both methods are evaluated with the same unseen test sets to compare the performance and measure the improvement. Note that we did not use cross validation (but instead divided into training and test data) for evaluation since the number of samples for both datasets is adequate (10039 and 7166 samples).\n\nThis strategy is also utilized to keep the same test set for LSTM (one-stage processing) and SVM (two-stage processing) which is difficult if the samples are shuffled/cross-validated.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Feature Sets",
      "text": "Most research on SER, or generally on pattern classification, focuses on two main topics: feature extraction, as in  Batliner et al. (2011) , and classification/regression methods, as in  Albornoz et al. (2011) . While this research focuses on the second topic, we also evaluate state-of-the-art feature sets used for SER. For acoustic features, we evaluate three feature sets: the Geneva Minimalistic Acoustic Parameter Set (GeMAPS), statistical functions from GeMAPS, and the same functions from GeMAPS with a silence feature. For text features, aside from the original word vectors extracted from the text transcription, we also evaluate two word embeddings that are pretrained on a larger corpus: the Word2Vec embedding  (Mikolov et al., 2013)  and GloVe embedding  (Pennington et al., 2014) . These feature sets are explained below.\n\nAcoustic features. The type of acoustic features extracted from a speech signal is the most important part of an SER system. GeMAPS is an effort to standardize the acoustic features used for voice research and affective computing  (Eyben et al., 2016a) . The feature set consists of 23 acoustic low-level descriptors (LLDs) such as fundamental frequency (f 0 ), jitter, shimmer, and formants, as listed in Table  1 . As an extension of GeMAPS, eGeMAPS includes statistical functions derived from the LLDs, such as the minimum, maximum, mean, and other values. Since these features are extracted on frame-based processing, the feature size becomes large for one utterance (e.g., 3409 × 23 for IEMOCAP), which is suitable for deep learning methods like LSTM. Including the LLDs, the total number of features in eGeMAPS is 88. These statistical values are often called high-level statistical functions (HSF).  Schmitt and Schuller (2018)  found, however, that using only the mean and standard deviation (std) from the LLDs achieved a better result than using eGeMAPS and audiovisual features.\n\nThese global features may represent more emotion information within speech than frame-based features. We thus coded these two statistical functions (47 values) from the LLDs as the HSF1 feature set. We also investigate the effect of including a silence feature in this SER research, as explained below. We define the combination of HSF1 with the silence feature as HSF2.\n\nSilence, in this paper, is defined as the proportion of silent frames among all frames in an utterance. In human communication, the proportion of silence in speaking depends on the speaker's emotion. For example, a happy speaker may have fewer silences (or pauses) than a sad speaker. The proportion of silence in an utterance can be calculated as\n\nwhere N s is the number of frames categorized as silence (silent frames), and N t is the total number of frames. A frame is categorized as silent if it does not exceed a threshold value defined by multiplying a factor by a root mean square (RMS) energy, X rms . Mathematically, this is formulated as\n\nwhere X rms is defined as\n\nThis silence feature is similar to the disfluency feature proposed in  Moore et al. (2014) . In that paper, the author divided the total duration of disfluency by the total utterance length for n words. Figure  2  illustrates the calculation of our silence feature. If X rms from a frame is below th, then it is categorized as silent, and the calculation of equation 1 is applied.\n\nText features. To process a word sequence in a computational model, the text must be converted to numerical values. The resulting text feature is commonly known as word embedding and is a vector representation of a word. Numerical values in the form of a vector are used to enable a computer to process text data, as it can only process numerical values. The values are points (numeric data) in a space whose number of dimensions is equal to the vocabulary size. The word representations embed those points in a feature space of lower dimension. In   the original space, every word is represented by a one-hot vector, with a value of 1 for the corresponding word and 0 for other words. The element with a value of 1 is converted to a point in the range of the vocabulary size.\n\nIn addition to directly converting the text in the transcriptions of the datasets (IEMOCAP and MSP-I+N) to sequences, two pretrained word embeddings are used to weight the original word embeddings. As mentioned above, the two word-embedding models are Word2Vec  (Mikolov et al., 2013)  and GloVe  (Pennington et al., 2014) . Hence, we have three different text features for word",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Two-Stage Bimodal Emotion Recognition",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Acoustic Emotion Recognition System",
      "text": "Most SER research uses only acoustic features. Our approach to acoustic SER is similar to that research. The contribution of our acoustic network is the evaluation of mean + std features at the utterance level and the use of a silence feature with the statistical functions to investigate any improvement. This evaluation is a continuation of (Tris Atmaja and Akagi, 2020) with extension on different feature sets and datasets. As explained in section 3.2, we evaluate three acoustic feature sets: LLDs, HSF1, and HSF2.\n\nThe LLD features are the 23 acoustic features listed in Table  1 .  The tuning of hyper-parameters follows the previous research  (Atmaja et al., 2019; Atmaja and Akagi, 2020) . A batch size of 8 was used with a maximum of 50 epochs. An early stop criterion with ten patiences stops the training process if no improvement were made in 10 epochs (before the maximum epoch). The last highest-score model was used to predict the development data. An RMSprop optimizer was used with its default learning rate, i.e., 0.001. Table  2  shows the setups on acoustic and text networks. These setups were obtained based on experiments with regard to the size of networks. For instance, the smaller acoustic networks with HSF features employed tanh output activation function and did not use the dropout rate, while the larger acoustic networks (with LLD)\n\nand text networks employed linear activation function and dropout rate.\n\nFor the HSF1 and HSF2 inputs on acoustic networks, the same setup applies.\n\nThese two feature sets are very small as compared to the LLDs: HSF1 has a size of 1 × 46, while HSF2 has a size of 1 × 47. This big difference in input size\n\n(1:1800) leads to faster computation on HSF1 and HSF2 than on the LLDs. Note that although Figure  3  shows HSF2 as the input feature, the same architecture also applies to the LLDs and HSF1.  The idea of using LSTM is to hold the last output in memory and use that output as a successive step. For instance, LLD with (3409, 23) feature size will process the first time step 1 to the last time step 3409. For HSF1 and HSF2, which contains a single time stamp, the data is processed only once  ([1, 46]  and  [1, 47]  for HSF1 and HSF2). Here, the only difference from multi-time steps is that the network performs three passes (forget gate, input gate, and output gate) instead of a single pass (see  Eyben et al. (2010) ). This information will include all information from the networks' memory.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Text Emotion Recognition System",
      "text": "The text network, shown in Figure  4  for the MSP-I+N dataset, uses the same input size for the three different text features. The WE, WE with pretrained Word2Vec, and WE with pretrained GloVe embedding have 300 dimensions for each word. The longest sequence in the IEMOCAP dataset is 100 sequences (words), while for MSP-I+N, the longest is 300 sequences. Hence, the input feature sizes for the LSTM layers are 100 × 300 for IEMOCAP and 300 × 300 for MSP-I+N with its corresponding number of samples. The same three LSTM layers are stacked as in the acoustic network, but the last LSTM layer only returns the last output. A dense layer with a size of 128 nodes is added after the LSTM layers and before the last three dense layers. Between the dense layers is a dropout layer with the same probability of 0.3 to avoid overfitting.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Multitask Learning",
      "text": "The task here in dimensional emotion recognition is to simultaneously predict the degrees of three emotion attributes, i.e., the degrees of valence, arousal, and dominance, for any given utterance. As the main target metric is the concordance correlation coefficient (CCC), the loss function is the CCC loss (CCCL).\n\nCCC also measures the performance of regression analyses by comparing gold standard labels and predictions. CCCL computes the score difference between the labels and predicted values for the three attributes. The CCC and CCCL are formulated as the following:\n\nwhere ρ is the Pearson correlation coefficient between the predicted emotion degree x and the true emotion degree y, σ 2 is the variance, and µ is the mean.\n\nAs the learning process minimizes three variables, we use the following multitask learning approach to optimize the CCC score:\n\nwhere α and β are respective CCCL parameters for valence (V) and arousal (A).\n\nThe parameter for dominance (D) is obtained by subtracting α and β from 1.\n\nThe same parameter range [0, 1] with 0.1 steps is investigated for α and β for both the acoustic and text networks, resulting in different optimal parameters, which are obtained by using linear search. Note that only positive values of CCCL D 's parameters are used to investigate the optimal parameters.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Svm-Based Late Fusion",
      "text": "We choose an SVM as the final classifier to fuse the outputs of the acoustic and text networks because of its effectiveness in handling smaller data (as compared to a DNN) and its computation speed. The datapoints produced by LSTM processing as the input of SVM are small; i.e.,  1600, 1538, 1147, and 1148  for IEMOCAP-SD, IEMOCAP-LOSO, MSPIN-SD and MSPIN-LOS0, respectively. The SVM then applies a regression analysis to map them to the given labels. Figure  5  shows the architecture of this two-stage emotion recognition system using DNNs and an SVM. Each prediction from the acoustic and text networks is fed into the SVM. From two values (e.g., valence predictions from the acoustic and text networks), the SVM learns to generate a final predicted degree (e.g., for valence). The concept of using the SVM as the final classifier can be summarized as follows.\n\nSuppose that two valence prediction outputs from the acoustic and text networks,\n\nx ter [i]], are generated by the DNNs, and that y i is the corresponding valence label. The problem in dimensional SER fusing acoustic and text results is to minimize the following:\n\nwhere w is a weighting vector, C is a penalty parameter, ζ and ζ * is the distance between misclassified points and the corresponding marginal boundary (above or below). Here, φ is the kernel function. We choose a radial basis function (RBF) kernel because of its flexibility in modeling a nonlinear process with a dimensional emotion model close to this kernel. The function φ for the RBF kernel is formulated as\n\nwhere γ defines how much influence a single training has on the model. All parameters in this SVM are obtained empirically via linear search in a specific range. Although the explanation above uses valence, the same also applies for arousal and dominance.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Reproducibility",
      "text": "The experimental code was written in Python, and, for the sake of research reproducibility, it is available in the following repository: https://github.com/bagustris/two-stage-ser.\n\nThe DNN part was implemented using Keras by  Chollet and Others (2015)  and Tensorflow, while the SVM-based fusion was implemented using the scikit-learn toolkit by  Pedregosa et al. (2011) . To obtain consistent results for each run, some fixed numbers are initialized at the beginning, as can be found in the repository above.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Results And Discussions",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Results From Single Modality",
      "text": "Before presenting the bimodal feature-fusion results, it is important to show the results of unimodal emotion recognition. The goals here are (1) to observe the (relative) improvement of bimodal feature fusion over using a single modality, and (2) to observe the effects of different features on different emotion attributes.\n\nTables  3  and 4  list the single-modality results of dimensional emotion recognition from the acoustic and text networks, respectively. In general, acousticbased SER gave better results than text-based SER in terms of the average CCC score. For particular emotion attributes, the text network gave a higher CCC score for valence prediction than those obtained by the acoustic network, except on the MSPIN datasets. This confirms the previous finding by  Karadogan and Larsen (2012)  that valence is better estimated by semantic features, while arousal is better predicted by acoustic features. In addition, we found that the dominance dimension was better predicted by acoustic features than by text features. This finding can be inferred from both tables, in which the CCC scores for the dominance dimension are frequently higher from the acoustic network than from the text network.\n\nThe exception of a higher valence score on the MSPIN-SD dataset by the acoustic network can be seen as the effect of either the DNN architecture or the dataset's characteristics. In Chen et al. (  2017 ), the obtained score was higher for valence than for arousal or liking (the third dimension, instead of dominance)\n\nwith their strategy on acoustic features. In contrast, AbdelWahab and Busso (2018) obtained a lower score for valence than for arousal and dominance by using their proposed DANN method on the same MSP-IMPROV dataset (whole data, all four scenarios). Given this comparison, we conclude that the higher valence score obtained here was an effect of the DNN architecture because of the multitask learning. Our result on a single modality (acoustic network) outperformed the DANN result on MSP-IMPROV, where their highest CCC scores were (0.303, 0.176, 0.476) as compared to our scores of (0.404, 0.605, 0.517) for valence, arousal, and dominance, respectively.\n\nTo find the optimal parameter values for α and β, a linear search was performed on the scale [0.0, 1.0] with a step of 0.1. Using this conventional technique, we found four sets of optimal parameters for the acoustic and text networks. Note that while only the improvised and natural scenarios (MSP-I+N)\n\nwere used to find the optimal text-network parameters for the MSP-IMPROV dataset, the whole dataset was used to find the optimal acoustic-network parameters. Table  5  lists the optimal parameter values for α and β.\n\nTo summarize the single-modality results, average CCC scores from three emotion dimensions can be used to justify which features perform better among others. The results show that HSF2 was the most useful of the acoustic feature sets (in two of four datasets), while the word embedding (WE) with pretrained GloVe embedding was the most useful of the text feature sets. The performance of dimensional emotion recognition in the speaker-independent (LOSO) case was lower than in the speaker-dependent (SD) case, as predicted. Note that both WE weighted by pretrained word vector  (Mikolov et al., 2013) ; GloVe: WE weighted by pretrained global vector  (Pennington et al., 2014) .\n\nFeature  acoustic and text emotion networks used a fixed seed number to achieve the same result for each run; however, the text network resulted in different scores.\n\nHence, standard deviations were given to measure fluctuation in 20 runs.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Results From Svm-Based Fusion",
      "text": "The main proposal of this research is the late-fusion approach combining the results from acoustic and text networks for dimensional emotion recognition.\n\nThis subsection presents the results of the late-fusion approach, including the obtained performances, comparison with the single-modality results, which pairs of acoustic-text results performed better, and our overall findings.\n\nFor each dataset (IEMOCAP-SD, MSPIN-SD, IEMOCAP-LOSO, MSPIN-LOSO), nine combinations of acoustic-text result pairs could be fed to the SVM system. Tables  6, 7 , 8, and 9 list the respective CCC results for these datasets.\n\nGenerally, our proposed two-stage dimensional emotion recognition improved the CCC score from single-modality emotion recognition. The pair of results from HSF2 (acoustic) and Word2Vec (text) gave the highest CCC score on speaker-dependent scenarios.\n\nOn the speaker-independent IEMOCAP dataset (IEMOCAP-LOSO), the result from the pair of HSF2 and GloVe gave the highest CCC score. This result linearly correlated with the single-modality results for that dataset, in which HSF2 obtained the highest CCC score among the acoustic features, and GloVe was the best among the text features. On the four datasets, the results from HSF2 obtained the highest CCC score for two out of four datasets, while GloVe obtained the highest CCC score for all four datasets. Hence, we conclude that the highest result from a single modality, when paired with the highest result from another modality, will achieve the highest performance among possible pairs.\n\nTo evaluate the improvement obtained by SVM-based late fusion, the average CCC scores again can be used as a single metric. The rightmost column in Tables  6, 7 , 8, and 9 shows the average CCC scores obtained from the nine pairs of acoustic and text results on the four different datasets. Comparing these   Aside from the fact that a speaker-independent dataset is usually more difficult than a speaker-dependent dataset, the low score on MSPIN-LOSO was due to its low scores on a single modality. In other words, lower pair performance from a single modality will result in low performance in late fusion. In particular, these low results derive from low CCC scores from the text modality (Table  4 ). The average CCC score for the text modality on the MSPIN-LOSO dataset was less than 0.16, compared to an average score higher than 0.34 for the acoustic modality. All nine pairs in late-fusion approaches improved on the single-modality results because of the two-stage DNN and SVM regression analysis. Thus, out of 36 trials (9 pairs × 4 datasets), our proposed two-stage dimensional emotion recognition outperforms any single modality result (used in a pair).\n\nThe low score on MSPIN for the text modality can be tracked to the origin of the dataset; that is, there may have been a number of sentences semantically identical to the target sentences in the dataset we used. Although we chose sentences only from the improvised dialogues (minus the target sentence) and from the natural interactions (those sentences produced by the experimenters and subjects during the breaks), some of the sentences in this corpus were semantically identical to that of the target sentences in the \"Target-Improvised\" data set. This was confirmed retroactively by manually checking the provided transcription and our automatic transcription. Given the nature of the elicitation task in a dialogue framework, this is not surprising. A similarly low result\n\nfor the text modality on this MSPIN dataset was also shown in  Zhang et al. (2019) . In general, compared to the IEMOCAP dataset, the MSPIN dataset suffers from low accuracy in recognizing the valence category by using acoustic and lexical properties. Interestingly, however, those authors also did not show improvement on the IEMOCAP scripted dataset, another text-based session in which lexical/text features do not contribute significantly.\n\nTo measure the improvement by our proposed two-stage late fusion, we calculated the relative improvement obtained by late fusion from the highest CCC scores for a single modality. For example, the pair of LLD + WE used the results from the LLDs in the acoustic network and the WE in the text network.\n\nWe compared the result for LLD + WE with that for the LLDs, as it had a higher score than the WE did. Figure  6  thus shows the relative improvement for all nine pairs. All of 36 trials showed improvements ranging from 5.11% to 40.32%. Table  10  lists the statistics for the obtained relative improvement.\n\nOur results show higher relative accuracy improvement as compared to those obtained by  Zhang et al. (2019)  for valence prediction, which ranged from 6% to 9%. Nevertheless, their multistage fusion method also showed benefits over the multimodal and single-modality approaches. These findings confirm the benefits of using bimodal/multimodal fusion instead of single-modality processing for valence, arousal, and dominance prediction.  and bimodal acoustic-text results due to differences in the data (deterministic vs. non-deterministic).\n\nTable  11  shows if there is a significant difference between speaker-dependent and speaker-independent results on the same feature set. We set p-value = 0.05 with a two-tail paired t-test between mean scores of speaker-dependent and speaker-independent results. This paired t-test was based on the assumption that there are no outliers (after pre-processing) and two different inputs are fed into the same system. Only one result from text emotion recognition shows no significant difference on IEMOCAP dataset while all results on MSPIN dataset show a significant difference between speaker-dependent and speakerindependent results. This result reveals a tendency for a difference in evaluating speaker-dependent and speaker-independent data. The results from speakerdependent data were different from those of speaker-independent data. In other words, results from speaker-dependent data cannot be used to justify speakerindependent or whole data.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Effect Of Removing Target Sentence From Mspin Dataset",
      "text": "Since the goal of this research is to evaluate the contribution of both acoustic and linguistic information in affective expressions, it is necessary to have sentences in the dataset that are free from any stimuli control. However, the original MSP-IMPROV dataset contains 20 \"target\" sentences; a sentence with the same linguistic content but produced with different emotions. These parts of MSP-IMPROV dataset are irrelevant to this study; hence, we remove it from the influence from target sentences. These results may be explained, as mentioned in section 5.2, that some utterances in the data analyzed in this study also inadvertently included sentences semantically the same as those in the improvised target sentences.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Final Remarks",
      "text": "We tried to perform a benchmark between our results and others on the same datasets, scenarios, and metrics. Unfortunately, to the best of our knowledge, the only reference is the one reported by  Atmaja and Akagi (2020) , which reports an early fusion method on IEMOCAP dataset. We improve the average CCC score from 0.508 to 0.532. This higher result suggests that late fusion is better than early fusion in modeling how humans fuse multimodal information, which is in line with neuropsychological research. This late-fusion approach can be embedded with current speech technology, i.e., ASR, in which the text output can be processed to weigh emotion prediction from acoustic features.\n\nAbdelWahab and Busso (2018) used MSP-Podcast  (Lotfian and Busso, 2019)  as a target corpora, which is not available for the public yet, and IEMOCAP with MSP-IMPROV as a source corpus to implement their DANN for cross-corpus speech emotion recognition. Although the goal is different, we observed similar patterns between theirs and our acoustic-only speech emotion recognition.\n\nFirst, we observed that the order of highest to lowest CCC scores is arousal, dominance, and valence. This pattern is also consistent when IEMOCAP is mixed with MSP-IMPROV as reported by  Parthasarathy and Busso (2017)  (in Table  2 ). Second, we observed that the CCC scores obtained in IEMOCAP are higher than those obtained in MSP-IMPROV; we believe that this lower score in MSP-IMPROV was due to the smaller size of the dataset.\n\nAlong with our SVM architecture, we also explored the parameters C and γ, because both parameters are important for an RBF-kernel-based SVM architecture  (Pedregosa et al., 2011) . Linear search was used in the ranges of Other strategies should also be proposed, such as how to handle the data differently when the linguistically identical sentences elicit different emotions (i.e., the whole MSP-IMPROV dataset). In contrast, the currently evaluated word embeddings treat the same words to have the same representations, even when it conveys different emotions.",
      "page_start": 37,
      "page_end": 38
    },
    {
      "section_name": "Conclusions",
      "text": "In conclusion, we summarize several findings. First, we found a linear correlation between the single-modality and late-fusion methods in dimensional emotion recognition. The best results from each modality, when they were paired, gave the best fusion result. In the same way, the worst results obtained from each network, when they were paired, gave the worst fusion results for bimodal emotion recognition. This finding differs from that reported in Atmaja et al.\n\n(2019), which used an early-fusion approach for categorical emotion recognition.\n\nIn their work, the best pair differs from the best methods in single modalities.\n\nSecond, text features strongly influenced the score of dimensional SER on the valence dimension, while acoustic features strongly influenced arousal and dominance scores. Accordingly, the proposed two-stage processing can take advantage of text features that are commonly used in predicting sentiment (valence) for the dimensional emotion recognition task. The proposed fusion method improves all three emotion dimensions without attenuating the performance of any dimension. That is, the proposed method elevates the scores for valence, arousal, and dominance subsequently from the highest to the lowest gain.\n\nThird, the combination of input pairs does not matter in the proposed fusion method, as indicated by the low deviation in relative improvement across the nine possible input pairs. What does matter is the performance of the input in the DNN stage. If the performance of a feature set in the DNN stage is low (CCC ≤ 0.2), it will also result in low performance when paired with another low-performance input in the SVM stage.\n\nFinally, this bimodal approach can be extended to a multimodal approach.\n\nBoth acoustic and text features can be combined with visual and motion-capture measurements that have advantages in specific emotion dimensions (liking or naturalness). The results can be benchmarked with current results to observe such improvements by adding more modalities. The SVM stage itself can be performed many times to obtain such improvements. These broad research directions are open challenges for researchers in human-computer interaction.",
      "page_start": 39,
      "page_end": 40
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Both methods are evaluated with the same unseen test sets to compare the",
      "page": 14
    },
    {
      "caption": "Figure 1: Proportions of data splitting for each partition of each dataset. In one-stage LSTM",
      "page": 15
    },
    {
      "caption": "Figure 2: illustrates the calculation of",
      "page": 17
    },
    {
      "caption": "Figure 2: Moving frame to calculate the silence feature.",
      "page": 18
    },
    {
      "caption": "Figure 3: shows an overview of the acoustic network. LSTM is chosen because",
      "page": 20
    },
    {
      "caption": "Figure 3: shows HSF2 as the input feature, the same architecture",
      "page": 20
    },
    {
      "caption": "Figure 3: Structure of acoustic network to process acoustic features.",
      "page": 21
    },
    {
      "caption": "Figure 4: for the MSP-I+N dataset, uses the same",
      "page": 22
    },
    {
      "caption": "Figure 4: Structure of text network to process word embeddings/vectors.",
      "page": 23
    },
    {
      "caption": "Figure 5: shows the architecture of this two-stage emotion recognition",
      "page": 24
    },
    {
      "caption": "Figure 5: Proposed two-stage dimensional emotion recognition method using DNNs and an",
      "page": 26
    },
    {
      "caption": "Figure 6: thus shows the relative improvement",
      "page": 34
    },
    {
      "caption": "Figure 6: Relative improvement in average CCC scores from late fusion using an SVM as",
      "page": 35
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "input: (None, 300)\nInput Layer\noutput: (None, 300)\ninput: (None, 300)\nEmbedding\noutput: (None, 300, 300)\ninput: (None, 300, 300)\nLSTM\noutput: (None, 300, 256)\ninput: (None, 300, 256)\nLSTM\noutput: (None, 300, 256)\ninput: (None, 300, 256)\nLSTM\noutput: (None, 256)\ninput: (None, 128)\nDense\noutput: (None, 128)\ninput: (None, 128)\nDropout\noutput: (None, 128)\ninput: (None, 128) input: (None, 128) input: (None, 128)\nDense Dense Dense\noutput: (None, 1) output: (None, 1) output: (None, 1)": "Dense",
          "Column_2": "input:",
          "Column_3": "(None, 128)",
          "Column_4": "Dense",
          "Column_5": "input:",
          "Column_6": "(None, 128)"
        },
        {
          "input: (None, 300)\nInput Layer\noutput: (None, 300)\ninput: (None, 300)\nEmbedding\noutput: (None, 300, 300)\ninput: (None, 300, 300)\nLSTM\noutput: (None, 300, 256)\ninput: (None, 300, 256)\nLSTM\noutput: (None, 300, 256)\ninput: (None, 300, 256)\nLSTM\noutput: (None, 256)\ninput: (None, 128)\nDense\noutput: (None, 128)\ninput: (None, 128)\nDropout\noutput: (None, 128)\ninput: (None, 128) input: (None, 128) input: (None, 128)\nDense Dense Dense\noutput: (None, 1) output: (None, 1) output: (None, 1)": "",
          "Column_2": "output:",
          "Column_3": "(None, 1)",
          "Column_4": "",
          "Column_5": "output:",
          "Column_6": "(None, 1)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input Laye": "",
          "input:\nr\noutput:": "",
          "(None, 300)": "(None, 300)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Embedding": "",
          "input:": "output:",
          "(None, 300)": "(None, 300, 300)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LSTM": "",
          "input:": "output:",
          "(None, 300, 300)": "(None, 300, 256)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LSTM": "",
          "input:": "output:",
          "(None, 300, 256)": "(None, 300, 256)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LSTM": "",
          "input:": "output:",
          "(None, 300, 256)": "(None, 256)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dense": "",
          "input:": "output:",
          "(None, 128)": "(None, 128)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dropout": "",
          "input:": "output:",
          "(None, 128)": "(None, 128)"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dense": "",
          "input:": "output:",
          "(None, 128)": "(None, 1)"
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Domain Adversarial for Acoustic Emotion Recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
      "doi": "10.1109/TASLP.2018.2867099",
      "arxiv": "arXiv:1804.07690"
    },
    {
      "citation_id": "2",
      "title": "Spoken emotion recognition using hierarchical classifiers",
      "authors": [
        "E Albornoz",
        "D Milone",
        "H Rufiner"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language",
      "doi": "10.1016/j.csl.2010.10.001"
    },
    {
      "citation_id": "3",
      "title": "Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Z Aldeneh",
        "S Khorram",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "ICMI 2017 -Proceedings of the 19th ACM International Conference on Multimodal Interaction",
      "doi": "10.1145/3136755.3136760"
    },
    {
      "citation_id": "4",
      "title": "Emotions from text: machine learning for text-based emotion prediction",
      "authors": [
        "C Alm",
        "D Roth",
        "R Sproat"
      ],
      "year": "2005",
      "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing -HLT '05",
      "doi": "10.3115/1220575.1220648",
      "arxiv": "arXiv:arXiv:1011.1669v3"
    },
    {
      "citation_id": "5",
      "title": "Speech Emotion Recognition Based on Speech Segment Using LSTM with Attention Model, in: 2019 IEEE International Conference on Signals and Systems (ICSigSys)",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Based on Speech Segment Using LSTM with Attention Model, in: 2019 IEEE International Conference on Signals and Systems (ICSigSys)",
      "doi": "10.1109/ICSIGSYS.2019.8811080"
    },
    {
      "citation_id": "6",
      "title": "Dimensional speech emotion recognition from speech features and word embeddings by using multitask learning",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "APSIPA Transactions on Signal and Information Processing",
      "doi": "10.1017/ATSIP.2020.14"
    },
    {
      "citation_id": "7",
      "title": "Speech Emotion Recognition Using Speech Feature and Word Embedding",
      "authors": [
        "B Atmaja",
        "K Shirai",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Speech Feature and Word Embedding"
    },
    {
      "citation_id": "8",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "doi": "10.1109/APSIPAASC47483.2019.9023098"
    },
    {
      "citation_id": "9",
      "title": "Whodunnit -Searching for the most important feature types signalling emotionrelated user states in speech",
      "authors": [
        "A Batliner",
        "S Steidl",
        "B Schuller",
        "D Seppi",
        "T Vogt",
        "J Wagner",
        "L Devillers",
        "L Vidrascu",
        "V Aharonson",
        "L Kessous",
        "N Amir"
      ],
      "year": "2011",
      "venue": "Computer Speech and Language",
      "doi": "10.1016/j.csl.2009.12.003"
    },
    {
      "citation_id": "10",
      "title": "Neural foundations of emotional speech processing",
      "authors": [
        "C Berckmoes",
        "G Vingerhoets"
      ],
      "year": "2004",
      "venue": "Neural foundations of emotional speech processing",
      "doi": "10.1111/j.0963-7214.2004.00303.x"
    },
    {
      "citation_id": "11",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "12",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "D Charles",
        "E Paul",
        "P Phillip"
      ],
      "year": "1872",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "13",
      "title": "Multimodal multi-task learning for dimensional and continuous emotion recognition",
      "authors": [
        "S Chen",
        "Q Jin",
        "J Zhao",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "14",
      "title": "Others, 2015. Keras",
      "authors": [
        "F Chollet"
      ],
      "venue": "Others, 2015. Keras"
    },
    {
      "citation_id": "15",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "16",
      "title": "Improving speech emotion dimensions estimation using a three-layer model of human perception",
      "authors": [
        "R Elbarougy",
        "M Akagi"
      ],
      "year": "2014",
      "venue": "Acoustical Science and Technology",
      "doi": "10.1250/ast.35.86"
    },
    {
      "citation_id": "17",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E Andre",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2457417"
    },
    {
      "citation_id": "18",
      "title": "On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "A Graves",
        "B Schuller",
        "E Douglas-Cowie",
        "R Cowie"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces",
      "doi": "10.1007/s12193-009-0032-6"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "20",
      "title": "Proceedings of the 8th International Workshop on Audio/Visual Emotion Challenge, AVEC'18, colocated with the 26th ACM International Conference on Multimedia",
      "authors": [
        "Fabien Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud",
        "E Salah",
        "¸ Ali",
        "H Pantic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 8th International Workshop on Audio/Visual Emotion Challenge, AVEC'18, colocated with the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2017.02.013"
    },
    {
      "citation_id": "22",
      "title": "The World of Emotions Is Not Two-Dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "C Phoebe",
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2017",
      "venue": "Psychological science"
    },
    {
      "citation_id": "23",
      "title": "A dimensional approach to emotion recognition of speech from movies",
      "authors": [
        "T Giannakopoulos",
        "A Pikrakis",
        "S Theodoridis"
      ],
      "year": "2009",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2009.4959521"
    },
    {
      "citation_id": "24",
      "title": "Logic and Conversation",
      "authors": [
        "H Grice"
      ],
      "year": "2002",
      "venue": "Foundations of Cognitive Psychology",
      "doi": "10.7551/mitpress/3080.003.0049"
    },
    {
      "citation_id": "25",
      "title": "Primitives-based evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2007.01.010"
    },
    {
      "citation_id": "26",
      "title": "The unreasonable effectiveness of data",
      "authors": [
        "A Halevy",
        "P Norvig",
        "F Pereira"
      ],
      "year": "2009",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2009.36"
    },
    {
      "citation_id": "27",
      "title": "Multimodal sentiment analysis to explore the structure of emotions",
      "authors": [
        "A Hu",
        "S Flaxman"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "doi": "10.1145/3219819.3219853"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2015.7178872",
      "arxiv": "arXiv:arXiv:1011.1669v3"
    },
    {
      "citation_id": "29",
      "title": "An Emotion Space Model for Recognition of Emotions in Spoken Chinese",
      "authors": [
        "X Jin",
        "Z Wang"
      ],
      "year": "2005",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics",
      "doi": "10.1007/11573548_51"
    },
    {
      "citation_id": "30",
      "title": "Lexicons for Sentiment and Affect Extraction",
      "authors": [
        "D Jurafsky",
        "J Martin"
      ],
      "year": "2017",
      "venue": "Speech and Language Processing, An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"
    },
    {
      "citation_id": "31",
      "title": "Combining semantic and acoustic features for valence and arousal recognition in speech",
      "authors": [
        "S Karadogan",
        "J Larsen"
      ],
      "year": "2012",
      "venue": "2012 3rd International Workshop on Cognitive Information Processing",
      "doi": "10.1109/CIP.2012.6232924"
    },
    {
      "citation_id": "32",
      "title": "Evaluation of Unsupervised Emotion Models to Textual Affect Recognition",
      "authors": [
        "S Kim",
        "A Valitutti",
        "R Calvo"
      ],
      "year": "2010",
      "venue": "Workshop on Computational Approaches to Analysis and Generation ofEmotion in Text"
    },
    {
      "citation_id": "33",
      "title": "Recognition and Simulation of Emotions",
      "authors": [
        "C Kleine-Cosack"
      ],
      "year": "2006",
      "venue": "Recognition and Simulation of Emotions"
    },
    {
      "citation_id": "34",
      "title": "Improving multilingual speech emotion recognition by combining acoustic features in a three-layer model",
      "authors": [
        "X Li",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2019.04.004"
    },
    {
      "citation_id": "35",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2736999"
    },
    {
      "citation_id": "36",
      "title": "Mining valence, arousal, and dominance",
      "authors": [
        "M Mäntylä",
        "B Adams",
        "G Destefanis",
        "D Graziotin",
        "M Ortu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 13th International Workshop on Mining Software Repositories -MSR '16",
      "doi": "10.1145/2901739.2901752",
      "arxiv": "arXiv:1603.04287"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "Efficient Estimation of Word Representations in Vector Space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient Estimation of Word Representations in Vector Space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "39",
      "title": "Sentiment Analysis, in: Emotion Measurement",
      "authors": [
        "S Mohammad"
      ],
      "year": "2016",
      "venue": "Sentiment Analysis, in: Emotion Measurement"
    },
    {
      "citation_id": "41",
      "title": "Word-level emotion recognition using highlevel features",
      "authors": [
        "J Moore",
        "L Tian",
        "C Lai"
      ],
      "year": "2014",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Project DeepSpeech: A TensorFlow implementation of Baidu's DeepSpeech architecture",
      "authors": [
        "Mozilla"
      ],
      "year": "2019",
      "venue": "Project DeepSpeech: A TensorFlow implementation of Baidu's DeepSpeech architecture"
    },
    {
      "citation_id": "43",
      "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
      "doi": "10.21437/Interspeech.2017-1494"
    },
    {
      "citation_id": "44",
      "title": "Scikitlearn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "45",
      "title": "GloVe: Global Vectors for Word Representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Emotion In Speech: Recognition And Application To Call Centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering 710"
    },
    {
      "citation_id": "47",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "48",
      "title": "Deep Recurrent Neural Networks for Emotion Recognition in Speech",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Deep Recurrent Neural Networks for Emotion Recognition in Speech",
      "doi": "10.3390/s17112556"
    },
    {
      "citation_id": "49",
      "title": "Role of Regularization in the Prediction of Valence from Speech",
      "authors": [
        "K Sridhar",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, ISCA, ISCA",
      "doi": "10.21437/Interspeech.2018-2508"
    },
    {
      "citation_id": "50",
      "title": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning",
      "authors": [
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "51",
      "title": "The Effect of Silence Feature in Dimensional Speech Emotion Recognition",
      "authors": [
        "B Tris Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "th Interna-tional Conference on Speech Prosody 2020, ISCA, ISCA",
      "doi": "10.21437/SpeechProsody.2020-6"
    },
    {
      "citation_id": "52",
      "title": "Predicting Arousal and Valence from Waveforms and Spectrograms Using Deep Neural Networks",
      "authors": [
        "Z Yang",
        "J Hirschberg"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, ISCA, ISCA",
      "doi": "10.21437/Interspeech.2018-2397"
    },
    {
      "citation_id": "53",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Z Yang",
        "D Yang",
        "C Dyer",
        "X He",
        "A Smola",
        "E Hovy"
      ],
      "year": "2016",
      "venue": "2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 -Proceedings of the Conference",
      "doi": "10.18653/v1/N16-1174"
    },
    {
      "citation_id": "54",
      "title": "Bimodal Emotion Recognition from Speech and Text",
      "authors": [
        "W Ye",
        "X Fan"
      ],
      "year": "2014",
      "venue": "International Journal of Advanced Computer Science and Applications",
      "doi": "10.14569/ijacsa.2014.050204"
    },
    {
      "citation_id": "55",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text. Spoken Language Technology Workshop",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Multimodal Speech Emotion Recognition Using Audio and Text. Spoken Language Technology Workshop",
      "arxiv": "arXiv:1810.04635"
    },
    {
      "citation_id": "56",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics",
      "doi": "10.18653/v1/D17-1115",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "57",
      "title": "Exploiting Acoustic and Lexical Properties of Phonemes to Recognize Valence from Speech",
      "authors": [
        "B Zhang",
        "S Khorram",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP.2019.8683190"
    }
  ]
}