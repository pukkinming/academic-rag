{
  "paper_id": "2306.13076v1",
  "title": "Çok Kipli Duygu Tanıma İçin Zaman Bazlı Modellerin Karşılaştırılması A Comparison Of Time-Based Models For Multimodal Emotion Recognition",
  "published": "2023-06-22T17:48:18Z",
  "authors": [
    "Ege Kesim",
    "Selahattin Serdar Helli",
    "Sena Nur Cavsak"
  ],
  "keywords": [
    "Automatic audio-video emotion recognition (AVER)",
    "Emotion recognition",
    "Mel-frequency cepstral coefficients",
    "Multimodal fusion",
    "Transformers",
    "Deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Özetçe Duygu tanıma, insan-bilgisayar etkileşim alanında önemli bir araştırma konusu haline gelmiştir. Duyguları anlamak için ses ve videolar üzerine yapılan çalışmalar, temel olarak yüz ifadesini analiz etmeye odaklanmış ve 6 temel duyguyu sınıflandırmıştır. Bu çalışmada, çok kipli duygu tanımada farklı sekans modellerinin performansları karşılaştırılmıştır. Ses ve görüntüler önce çok katmanlı CNN modelleri tarafından işlenmiş ve bu modellerin çıktıları çeşitli sekans modellerine beslenmiştir. Sekans modeli olarak GRU, Transformer, LSTM ve Maksimum Havuzlama kullanılmıştır. Bütün modellerin kesinlik, duyarlılık ve F1 Skor değerleri hesaplanmıştır. Deneylerde çok kipli olan CREMA-D veri seti kullanılmıştır. CREMA-D veri kümesinin karşılaştırma sonucunda, F1 skorda en iyi sonucu gösteren 0.640 ile GRU bazlı mimari, kesinlik metriğinde 0.699 ile LSTM bazlı mimari, duyarlılıkta ise zaman içinde maksimum havuzlama bazlı mimari 0.620 ile en iyi sonucu göstermiştir. Sonuç olarak, karşılaştırılan sekans modellerinin birbirine yakın performans verdiği gözlenmiştir. Anahtar Kelimeler -Otomatik ses-video duygu tanıma (AVER), Duygu tanıma, Mel-frekans kepstrum katsayıları, Çok kipli füzyon, Transformers, Derin öğrenme.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I.",
      "text": "GİRİŞ Goleman duyguyu tanımlarken, insanı harekete geçmeye sevk eden ve yaşamın güçlükleri ile baş etmesini sağlayan hisler olarak tarif eder ve hayatın her alanında büyük etkiye sahip olduğunu belirtir. İnsanların duygusal durumlarını doğru bir şekilde tanıması, insan ilişkilerinde hayati bir rol taşımaktadır. İnsan ilişkilerinde bir kişinin duygusal durumunu karşı tarafa doğru bir şekilde aktarabilmesi, iletilen mesajın daha iyi anlaşılmasına neden olur. Günümüzde duygu tanıma, çeşitli amaçlar için kullanılmaktadır. Okulda  [17, 18] , akıllı kartlarda, eğlence ve sağlık hizmetlerinde  [11] , robot teknolojisinde ve güvenlik kontrolünde  [19]  duygu tanıma uygulanmaktadır. Kurumsal uygulamalarda ise, perakende, medya, insan kaynakları, müşteri iletişimini içeren çağrı merkezlerinde, kişisel asistanlarda ve akademide duygu analitiğinin çok sayıda kullanım senaryosu vardır.\n\nYapay zeka teknolojileri ile duygu tanıma, insan duygularını algılama ve anlama sürecinin simülasyonuna odaklanmaktadır. Şu anda, duygu tanıma hala zorlu bir konudur ve araştırmacıların odağındadır. Sağlam otomatik duygu tanıma yöntemleri geliştirmek için artan bir talep vardır. Bu nedenle ses, video, konuşma veya yüz ifadelerinden insan duygularını tanımaya çalışan araştırmaların sayısında son yıllarda önemli bir artış görülmüştür.\n\nSesler duygusal ifade için önemli bir yöntemdir. Konuşma, duygularla zenginleştirilmiş bir iletişim kanalıdır. Duygu tanımada öznitelik çıkarımı sıklıkla kullanılmaktadır. Bu kullanılan öznitelikler arasında MelFrequency Cepstral katsayıları (MFCC)  [1, 2]  ve Mel-Spectrogram  [3, 4]  gibi klasik öznitelik çıkarımları yada derin öğrenme tabanlı Wav2vec 2.0  [5]  yöntemleri kullanılmaktadır  [6, 7] .\n\nDuygu tanımada görüntüler de kullanılabilmektedir. Bu görüntüler genellikle kişilerin yüz görüntülerinden oluşmaktadır. Genellikle görüntülerden duygu tanıma yöntemi yüz algılama ve sınıflandırma olarak iki aşamadan oluşmaktadır.\n\nGörüntü işleme alanındaki başarılarından dolayı CNN  [8, 9, 10]  modelleri duygu tespitinde sıklıkla kullanılmaktadır.\n\nÇok kipli duygu tanıma, çoklu modalitelerden (ses, video, metin, fizyolojik ve diğerleri) yararlanarak duygu tahmini yapılmasıdır.  [11] 'deki çalışmalarında, yazarlar çok kipli duygu tanıma konusu üzerinde çalışmıştırlar. Bu çalışmada kullanılan modelin görsel ve işitsel verilerden çıkardığı bilgiler arasındaki uzaklık optimize edilerek iki kipten benzer bilgiler çıkarılması sağlanmıştır. Ayrıca, üçlü kayıp fonksiyonu (Triplet Loss) kullanarak model performansı arttırılmıştır. CREMA-D  [12]  veri kümesinde %74'lük bir doğruluk elde ettiklerini raporlamışlardır.\n\n[13]' da yazarlar, kısa video kliplerini kapsayan ve çiftli sesgörsel zaman pencerelerinden oluşan yeni bir çerçeve önermektedirler. Ses ve görüntü için çıkarılan gömüler (embbeding) iki transformer'ın  [15]  kodlayıcıları tarafından işlenmektedir. Bu yeni çift taraflı çerçeve, duygu tanıma için ses-görsel ipuçlarının zamansal sunumunu incelemeyi amaçlamaktadır. CREMA-D veri kümesinde %67.2'lik bir doğruluk elde ettiklerini raporlamışlardır.\n\n[14]'da yazarlar AuxFormer adlı Transformer bazlı yeni bir model yapısı sunmuşlardır. Modelde hem çok kip için hem de her kip için ayrı kayıp fonksiyonları kullanılmaktadır. Tek kipli kayıp fonksiyonlarının eklenmesi ana ağdaki belgelerin kaybolmasını engellemektedir. Yazarlar , CREMA-D veri kümesinde yaptıkları test sonuçlarında makro ve mikro F1 Skoru için sırasıyla %70.4'lük ve % 76.5 performans elde etmişlerdir.\n\nBu çalışmada, derin öğrenme yöntemi ile ses ve görüntüler üzerinden çok kipli duygu tanıma yapılmıştır. Ses ve görüntüler önce çok katmanlı evrişimsel sinir ağ modelleri (CNN) tarafından işlenmiş ve bu modellerin çıktıları GRU , Transformer , LSTM ve zaman boyunca maksimum havuzlama modellerine beslenmiştir. Bu dört farklı sekans modeli karşılaştırılmıştır.\n\nMakalenin devamında, Bölüm II'de kullanılan modeller anlatılmakta, Bölüm III'de veri kümesi ve yapılan deneyler bulunmakta, Bölüm IV'te ise sonuç kısmı ile çalışma tamamlanmaktadır.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Yöntem",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Uzun-Kısa Vadeli Bellek (Lstm)",
      "text": "LSTM, bir tekrarlayan sinir ağı (Recurrent Neural Network-RNN) mimarisidir ve RNN'in karşılaştığı sorunları çözmek için geliştirilmiş bir yapay sinir ağıdır. LSTM ağları gradyan kaybolması sorununun üstesinden gelebilmek için bilgi akışını düzenleyen ve kontrol eden kapılardan oluşturmaktadır.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Geçitli Tekrarlayan Ünite (Gru)",
      "text": "GRU, tekrarlayan sinir ağlarında sık görülen gradyan sorununu çözmek amacıyla önerilmiştir. GRU, LSTM'e göre daha az parametreye sahip olduğundan daha küçük ve seyrek veri kümelerinde daha iyi performans göstermektedir.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Transformer",
      "text": "Transformer, Doğal Dil İşleme (NLP) mimarilerinin temel yapı taşını oluşturmaktadır ve dikkat mekanizmasına (attentionbased) dayanmaktadır. Transformer, birbirine bağlı ve birbirlerini etkileyen uzak veri öğeleri dizisinde ince bağlantıları algılamak için kullanılmaktadır. Bu model paralel olarak çalıştırmaya daha uygun ve uzun metinlerdeki anlamı daha kolay hatırlayabilmektedir.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Maksimum Havuzlama (Max Pooling)",
      "text": "Maksimum havuzlama, belli bir alandaki en büyük değeri alan bir filtre gibi çalışmaktadır. Veri boyutu azaltılarak hem gereken işlem gücü hem de yakalanan gereksiz özellikler azaltılır ve böylece daha önemli özelliklere odaklanılır.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Deneyler",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Veri Kümesi",
      "text": "Bu çalışmada CREMA-D veri kümesini kullanılmıştır. Veri kümesi, 6 temel duyguyu (mutlu, kızgın, üzgün, nötr, iğrenme ve korku) gösteren profesyonel oyuncuların 7442 klibinden oluşur. Her klipte bir oyuncu, verilen bir cümle söylerken duygu kategorilerinden birini sergiler. Kliplerin süresi bir ila beş saniye arasında değişir. Videolardaki farklı cümle sayısı on iki ve toplam oyuncu sayısı 91'dir. Duygu tanımadaki insan performansı veri kümesi üzerinde sadece ses, sadece görüntü, hem ses hem de görüntü kullanılarak ölçülmüştür. Bu durumlar için insan performansı sırasıyla %40,9, %58,9 ve %63,6 olarak ölçülmüştür.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Veri Ön İşleme",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Video",
      "text": "Bu çalışmada, görsel özellikler olarak oyuncuların görüntülerdeki yüz görüntüleri içerecek şekilde görüntüler kırpılmış ve 64x64 boyuta indirilerek kullanılmıştır. Yüzlerin tespit edilmesinde Dlib  [16]  kütüphanesi kullanılmıştır. Ayrıca zaman adımlarının sayısı 50'ye sabitlenmiştir.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ses",
      "text": "Bu çalışmada, sesi temsil etmek için Mel Frekans Cepstral Katsayıları (MFCC) kullanıldı. 22 kHz örnekleme oranında girdi sinyallerini kullanarak çerçeve uzunluğu 2048 ve atlama uzunluğu 512 kullanarak ses kaynaklarından MFCC çıkarıldı. Bu işlemlerde Librosa kütüphanesi kullanıldı. Tüm kliplerin uzunluğu 5 saniye olacak şekilde dolduruldu. Elde edilen öznitelikler 216x40 boyutlu bir matris şeklindedir. Daha sonra, bu öznitelik 40 boyutunda bir kayan pencere ve 20 adımlık bir adım boyutu kullanarak 9 örtüşen zaman adımına bölündü. En sonunda , tek bir klibin nihai ses öznitelikleri, 9x40x40 şeklinde veri elde edildi. Her bir klibin süresi 5 saniye olduğundan, zaman adım sayısı da 9 olarak sabittir. 'deki sonuçlara göre CREMA-D veri kümesinin karşılaştırma sonucunda, F1 skorda en iyi sonucu gösteren 0.640 ile GRU bazlı mimari, kesinlik metriğinde 0.699 ile LSTM bazlı mimari, duyarlılık da ise zaman içinde maksimum havuzlama bazlı mimari 0.620 ile en iyi sonucu göstermiştir. Sonuç olarak, karşılaştırılan modellerin birbirine yakın performans verdiği gözlenmiştir. Duygu tanımada kullanılan modellerin etkinliği kanıtlanmıştır. Gelecekte yeni model yapıları araştırılıp farklı bakış açılarıyla duygu tanıma problemi yeniden ele alınılabilir. Sekans modellerinin yanı sıra öznitelik çıkarımı için farklı yaklaşımlar benimsenebilir. Ayrıca farklı kiplerin model çıktısına olan etkisi gözlemlenebilir. KAYNAKLAR",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sekans Modeller": "",
          "Sonuçlar": "Kesinlik"
        },
        {
          "Sekans Modeller": "Transformer",
          "Sonuçlar": "0.727"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.946"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.506"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.497"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.717"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.733"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.653"
        },
        {
          "Sekans Modeller": "LSTM",
          "Sonuçlar": "0.720"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.966"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.630"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.539"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.655"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.841"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.699"
        },
        {
          "Sekans Modeller": "GRU",
          "Sonuçlar": "0.659"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.897"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.634"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.545"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.655"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.804"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.680"
        },
        {
          "Sekans Modeller": "Maksiumum \nHavuzlama",
          "Sonuçlar": "0.677"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.909"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.533"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.541"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.593"
        },
        {
          "Sekans Modeller": "",
          "Sonuçlar": "0.664"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition using MFCC features and LSTM network",
      "authors": [
        "Harshawardhan Kumbhar",
        "Sheetal Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)"
    },
    {
      "citation_id": "2",
      "title": "2021 3rd international conference on signal processing and communication (ICPSC)",
      "authors": [
        "Harshit Dolka",
        "Arul Xavier",
        "Sujitha Juliet"
      ],
      "year": "2021",
      "venue": "2021 3rd international conference on signal processing and communication (ICPSC)"
    },
    {
      "citation_id": "3",
      "title": "Deep learning techniques for speech emotion recognition: A review",
      "authors": [
        "Sandeep Pandey",
        "Hanumant Kumar",
        "Singh Shekhawat",
        "Sr Mahadeva",
        "Prasanna"
      ],
      "year": "2019",
      "venue": "th International Conference Radioelektronika (RADIOELEKTRONIKA)"
    },
    {
      "citation_id": "4",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "Dongyang Dai"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "7",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "Mariana Makiuchi",
        "Kuniaki Rodrigues",
        "Koichi Uto",
        "Shinoda"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "8",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "Juan Ortega",
        "Patrick Cardinal",
        "Alessandro Koerich"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "10",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "11",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "Esam Ghaleb",
        "Mirela Popa",
        "Stylianos Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "12",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Cao",
        "Houwei"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal attention-mechanism for temporal emotion recognition",
      "authors": [
        "Esam Ghaleb",
        "Jan Niehues",
        "Stylianos Asteriadis"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "14",
      "title": "AuxFormer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "Davis King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "17",
      "title": "Automatic social signal analysis: Facial expression recognition using difference convolution neural network",
      "authors": [
        "Jingying Chen"
      ],
      "year": "2019",
      "venue": "Journal of Parallel and Distributed Computing"
    },
    {
      "citation_id": "18",
      "title": "Classification techniques' performance evaluation for facial expression recognition",
      "authors": [
        "Mayyadah Mahmood"
      ],
      "year": "2021",
      "venue": "Indonesian Journal of Electrical Engineering and Computer Science"
    },
    {
      "citation_id": "19",
      "title": "Research on multi-modal mandarin speech emotion recognition based on SVM",
      "authors": [
        "Chen Caihua"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)"
    }
  ]
}