{
  "paper_id": "2501.13306v2",
  "title": "Osum: Advancing Open Speech Understanding Models With Limited Resources In Academia Audio, Speech And Language Processing Group (Aslp",
  "published": "2025-01-23T01:27:46Z",
  "authors": [
    "Xuelong Geng",
    "Kun Wei",
    "Qijie Shao",
    "Shuiyun Liu",
    "Zhennan Lin",
    "Zhixian Zhao",
    "Guojian Li",
    "Wenjie Tian",
    "Peikun Chen",
    "Yangze Li",
    "Pengcheng Guo",
    "Mingchen Shao",
    "Shuiyuan Wang",
    "Yuang Cao",
    "Chengyou Wang",
    "Tianyi Xu",
    "Yuhang Dai",
    "Xinfa Zhu",
    "Yue Li",
    "Li Zhang",
    "Lei Xie"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions. However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community. Moreover, the lack of transparency in training details creates additional barriers to further innovation. In this study, we present OSUM, an Open Speech Understanding Model designed to explore the potential of training SLUMs under constrained academic resources. The OSUM model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speechto-text chat (STTC). By employing an ASR+X training strategy, OSUM achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks. Beyond delivering strong performance, OSUM emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community. By doing so, we aim to accelerate research and innovation in advanced SULM technologies. * This is the v2 version of the technical report. The experimental results reported herein differ from those in v1 because of adding new data and training in more steps.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence (AGI) in recent years. Given the inherent human preference for speech-based interaction, there has been growing interest in extending LLMs with speech capabilities to develop Speech LLMs. To generate fluent and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age. Moreover, this comprehension ability is also crucial for audio data labeling. Currently, the mainstream multi-label generation approach is to use multiple models to label each task separately, which consumes extremely high computational resources. A labeling model capable of accurately generating multiple labels simultaneously holds broad application prospects.\n\nThe area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements through projects such as Qwen-Audio  (Chu et al., 2023) , Qwen2-Audio  (Chu et al., 2024) , PandGPT  (Su et al., 2023) , and SALMONN  (Tang et al., 2024) . Whisper  (Radford et al., 2023 ) marks a pioneering exploration Figure  1 : Comparison of Qwen2-Audio and our OSUM model. In most tasks, OSUM achieves a better performance than Qwen2-Audio despite using significantly fewer computational resources and training data. The values for each model's task in the radar chart are based on the average results on the public and internal test sets, as shown in Table  4  and Table 5 . of speech understanding independent of LLMs, utilizing an encoder-decoder Transformer  (Vaswani, 2017)  architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation (ST), language identification (LID), and voice activity detection (VAD). Building on Whisper's design, SenseVoice  (An et al., 2024)  and TouchASP  (Song et al., 2024)  expand more tasks like speech emotion recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend human speech. Qwen-Audio integrates Whisper's encoder with the text-based Qwen LLM  (Bai et al., 2023) , enabling the latter to understand speech. Compared to Whisper, Qwen-Audio leverages a more powerful LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of SULMs. Its successor, Qwen2-Audio, further enhances these capabilities by supporting natural language prompts and achieving superior performance across various benchmarks  (Chu et al., 2024) .\n\nAlthough these advanced SULMs have achieved remarkable progress, most of them are developed by industry, leveraging millions of hours of training data and massive GPU resources. For instance, TouchASP and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively. Such large-scale resources are typically beyond the reach of academia institutions. Furthermore, while inference models are often opensourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters configurations are rarely disclosed. These limitations hinder academic community efforts to further optimize and expand SULM research. Recently, a growing movement advocating for open science in Speech LLM research has emerged. This movement emphasizes the importance of releasing comprehensive training frameworks, datasets, and methodological details to promote research and innovation. A notable example is the Open Whisper-style Speech Model (OWSM) series  (Peng et al., 2023) , which replicates Whisper-style training using open-sourced tools and publicly available data, significantly advancing public understanding and research on speech understanding models.\n\nIn this study, we aim to foster broader academic exploration of SULMs with limited resource demands, encouraging wider research community participation. To this end, we introduce OSUM, an open SULM with its data processing pipeline and training details publicly available. The OSUM model integrates a Whisper speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM. It is capable of performing a wide range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC). Notably, SSR is a distinctive feature of our OSUM model and serves as a vital component of speech understanding. It enhances the model's capability by improving contextual comprehension and boosting performance across various downstream speech tasks. Furthermore, it establishes a foundation for enabling more natural and context-aware speech-based interactions. We adopt an ASR+X training strategy to enhance training stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized alongside the primary target task (denoted as \"X\"). For instance, during the training of the SER task, we concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each speech sample. This multi-task training accelerates modality alignment, enabling the LLM to effectively utilize both textual and acoustic modalities. Our OSUM model utilizes only 50,500 hours of training data and achieves comparable or superior performance to other SULMs. The overall performance of OSUM is illustrated in Fig.  1 . The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting inference on both platforms. The goal of this study is to foster transparency and accelerate progress in the field of SULMs by providing accessible tools and resources for the broader research community.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section introduces our proposed OSUM, a model designed for comprehensive speech understanding. Section 2.1 presents its architecture;.Section 2.2 details its multitask training process. Section 2.3 and Section 2.4 provide an overview of the training data and processing pipeline, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "As shown in Figure  2 , our OSUM model comprises a speech encoder, an adaptor, and an LLM. During the training, all of the parameters in the encoder and adaptor are updated, while the LLM is fine-tuned with LoRA  (Hu et al., 2022) . The input of our model consists of a speech and a natural language prompt. Unlink Whisper  (Radford et al., 2023)  and Qwen-audio  (Bai et al., 2023) , which rely on instruct tags, the OSUM employs descriptive text, converting all eight supported tasks as shown in Fig.  2 . Currently, our model supports only text-based responses, but audio output capabilities are under active development. The following sections describe each sub-module in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Encoder",
      "text": "Our OSUM utilizes the Whisper-Medium 1  model as its speech encoder, which consists of 2 one-dimensional convolutional layers with 2 times downsampling, and 24 Transformer layers with 1024 hidden state dimensions and 16-headed self-attention. The encoder has approximately 300 million parameters, which makes it take into account both speech comprehension ability and inference efficiency.\n\nAdaptor The adaptor module features a hybrid architecture combining 3-layer 1D convolutional layers (Conv1D) and 4-layer Transformer. The Conv1D layers use kernel widths of (3, 3, 3) and strides of (1, 2, 2), achieving an overall 4 times downsampling. The Transformer layers have a model dimension of 1,280, an inner dimension of 2,560, and 4 attention heads. This architecture bridges the output of the speech encoder with the input requirements of the LLM, enabling efficient modality alignment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Llm With Lora",
      "text": "The Qwen2-7B-Instruct is selected as our LLM. Qwen2-7B-Instruct 2  is a general-purpose LLM with a parameter scale of 7 billion, specifically designed for multi-task instruction optimization. In our work, we fine-tune the Qwen2-7B-Instruct model using LoRA (Low-Rank Adaptation) technology. The LoRA hyperparameters-α, rank, and dropout ratio are set to 32, 8, and 0.1, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multitask Supervised Training",
      "text": "The training procedure includes two stages. First, we perform multi-task supervised fine-tuning on the original Whisper model without an LLM. Second, we integrate the fine-tuned Whisper encoder with the Qwen2 LLM to create the complete OSUM system, then conduct further supervised training using a larger dataset.\n\nWhisper Fine-tuning The original Whisper model supports a limited scope of speech-understanding tasks, which makes the direct integration of the Whisper with an LLM for multi-task training risky when data and computation resources are constrained. Therefore, we first fine-tune the Whisper via multi-task data to ensure faster convergence of the OSUM model. Furthermore, this stage allows us to verify the reliability of our multi-task data. Specifically, we expand Whisper's instruction tag set to accommodate more tasks. Each forward pass executes only a single task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Osum Training",
      "text": "Training SULMs typically begins with pre-training on an ASR task, which serves as a foundation for incorporating additional speech tasks to enable LLMs to process semantic content from the speech encoder. Given computational constraints, we introduce an ASR+X paradigm for OSUM's multitask training. It concurrently trains ASR and a secondary task \"X\", accelerating training while allowing the \"X\" task to utilize both textual and acoustic features, thereby potentially improving performance The ASR+X paradigm follows a two-step process: first, transcribing speech to text (ASR); then, integrating\n\nthis transcription with acoustic features to execute the target task (X). This is achieved within the LLM's autoregressive framework by adjusting predicted labels, without modifications to the model architectures or loss functions. We implemented the ASR+X paradigm by prompting the LLLM with natural language prompts. ChatGPT 3  is used to generate 5 candidate prompts for each task, one of which is randomly selected during training. Table  1  shows examples of the prompts and ASR+X prediction labels.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Training Data",
      "text": "Our OSUM is designed to perform multi-task training using diverse speech datasets, with the goal of building a unified model capable of comprehensively understanding input speech in conversational scenarios.\n\nThe multi-task training process enables tasks to benefit from shared learning, enhancing overall model performance. Upon completion of training, OSUM can be utilized for speech data annotation or further extended into a conversational Speech LLM. Detailed information about the datasets used for training is provided in Table  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Processing Pipe-Line",
      "text": "The data processing pipeline is crucial for training multi-task SULMs. In this section, we reveal the data processing schemes used for each task in the OSUM project, with the aim of providing a valuable reference for academic research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Asr",
      "text": "The training data include publicly available resources like Wenetspeech  (Zhang et al., 2022) , AISHELL-1  (Bu et al., 2017) , AISHELL-2  (Du et al., 2018) , and LibriSpeech  (Panayotov et al., 2015) , along with our internal ASR dataset, resulting in a total of 24,000 hours. Yes SRWT For the SRWT task, a Gaussian Mixture Model -Hidden Markov Model (GMM-HMM) based conventional ASR model, is used to conduct force alignment and obtain word-level timestampes. This model is trained on the 54,000-hour proprietary ASR dataset. To evaluate its performance, we establish an internal SRWT test set and assess alignment quality using the Average Alignment Score (AAS) metric  (Shi et al., 2022) . The GMM-HMM model achieves an AAS of 7.55, demonstrating its efficacy in generating reliable word-level timestamps.\n\nSSR Given the absence of open-sourced tools for annotating style labels directly from audio data, we leverage two text-based LLMs-Qwen2.5-14B 4  and GLM-4-9B-Chat 5  -to annotate speech transcriptions using carefully designed prompts. To enhance annotation accuracy and reliability, we retain only the intersection of labeling results from both models. This intersection-based approach ensures high-quality annotations for training the SSR task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ved",
      "text": "We have attempted to train a vocal event labeling tool; however, due to the limited availability of training data, its classification performance is suboptimal, especially when vocal events and speech occur within the same audio segment. Therefore, we employ a Voice Conversion (VC) tool to modify the timbre of vocal event audio and insert it randomly into speech audio, creating a dataset of ASR+VED format. We find that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training data with the assistance of VC. The open-source vocal event datasets we use include Audioset  (Gemmeke et al., 2017) ,  ESC-50 (Piczak, 2015) , Vocal Sound  (Gong et al., 2022), and Nonspeech7k (Rashid et al., 2023) , while the ASR data consists solely of AISHELL-2  (Du et al., 2018) . SER Emotion2Vec  (Ma et al., 2024)  is the first universal speech emotion representation model. Without additional fine-tuning, we directly apply the pre-trained Emotion2Vec+ Large model 6  , which is trained on 40,000 hours of emotional speech data, to annotate the audio with emotion labels. Additionally, we leverage the GLM-4-9B-Chat model to generate emotion labels from the textual transcriptions of the speech. By intersecting these annotations, we generate high-quality emotional labels for the entire dataset.\n\nSGC Efforts to train a speaker gender classification model to label web-sourced data yield unsatisfactory performance. Consequently, we discard the pseudo-labeled data and relied solely exclusively on manually labeled datasets for training. For the SGC task, we select KeSpeech  (Tang et al., 2021) , Datatang-Kid (Datatang Tech Co Ltd, 2024), AISHELL-1  (Bu et al., 2017) , AISHELL-2  (Du et al., 2018) , LibriSpeech  (Panayotov et al., 2015) , Kaggle-CommonVoice (Kaggle Community, 2017), and Magicdata-Read OpenSLR (2019) as training dataset, as they include reliable speaker gender labels. In addition, we employ a noisy data augmentation strategy.\n\nSAP Similar to the SGC task, due to the poor performance of the automated labeling model, only manually labeled data is used for training. We use KeSpeech  (Tang et al., 2021) , Datatang-Kid (Datatang Tech Co Ltd, 2024), Magicdata-Read (OpenSLR, 2019), Kaggle-CommonVoice (Kaggle Community, 2017), AISHELL-ASR0060 (AISHELL Tech Co Ltd, 2024), and AISHELL-ASR0018 (AISHELL Tech Co Ltd, 2024) as the training dataset for the SAP task, as these datasets provide reliable speaker age labels. Noisy data augmentation is also utilized in this task.\n\nSTTC For the STTC task, we use three types of data. First, we utilize a human-recorded audio questionanswer dataset  Databacker-Conversation (Databaker Tech Co Ltd, 2024) . Then, we use a text-based dialogue dataset LCCC  (Wang et al., 2020)  and the ChatTTS 7  system with random speaker capabilities to generate the utterances of the questioner in the dialogue, thus obtaining the speech-text pairs for the dialogue task. Finally, we filter suitable response sentences from the Wenetspeech  (Zhang et al., 2022)  dataset using Qwen2.5-7B 8  , guiding the LLM to generate text answers.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "This section begins by presenting our training setup in Section 3.1. Subsequently, to conduct a more comprehensive evaluation of OSUM, we establish a series of complete internal evaluation sets, as detailed in Section 3.2. Finally, we report the performance of OSUM on both public and internal test sets, accompanied by an analysis in Section 3.3.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Setup",
      "text": "The two-stage training process for OSUM is detailed as follows:\n\nWhisper Fine-tuning In the first stage, we fine-tune the Whisper-Medium model on the multi-task datasets described in Table  2 . This stage is conducted on 8 Nvidia A6000 GPUs. A warm-up scheduler is employed to adjust the learning rate, peaking at 5e-5. The multitask Whisper is trained for 150,000 steps, which takes approximately 15.3 days.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Osum Training",
      "text": "In the second stage, we conduct experiments on 24 Huawei Ascend NPUs, using a learning rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days. The test set consists of internally human-recorded audio from ten adult male and female speakers, all reading in Chinese Mandarin, with the insertion of speech events occurring at random positions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "400",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ser",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Test Ser",
      "text": "Test data is selected from five publicly available emotional evaluation sets: IEMOCAP  (Busso et al., 2008) , MER2023  (Lian et al., 2023) , M3ED  (Zhao et al., 2022) , MSP-IMPROV  (Busso et al., 2017) , and ESD  (Zhou et al., 2021) , including both Chinese and English, encompassing eight types of emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "3885",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ssr",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Test Ssr",
      "text": "This test set comprises two components: web-crawled data annotated with style labels and highly expressive data intended for TTS training. It encompasses eight speaking styles, predominantly in Chinese.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "3000",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sgc",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Test Sgc",
      "text": "The internal test set includes data from two age categories, with a female-to-male ratio of 2:3. All recordings are from human-recorded dialogue scenarios. 3000",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sap",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Test Sap",
      "text": "The internal test set includes three age categories: child, adult, and old, with an equal distribution of 1:1:1. All data consists of human-recorded dialogue scene data. 3000",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sttc Test Sttc",
      "text": "This test set is generated from a Chinese text dialogue dataset. Specifically, we synthesize the question text into audio using the Cosyvoice2 TTS model, while the answers remain in text form. The text dialogue dataset used for the test set is derived from the same source as the training set. 200",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Internal Test Sets",
      "text": "Currently, most SULMs evaluate multi-task performance using publicly available English datasets  (Chu et al., 2023 (Chu et al., , 2024;; Radford et al., 2023; Song et al., 2024) . However, as OSUM training incorporates a substantial amount of Chinese data, we have developed a series of internal multi-task test sets tailored for Chinese 10  . These complement the publicly available English test sets, creating a more comprehensive evaluation framework. To support the ASR+X paradigm, we further enhance the test sets with speech transcripts. However, ASR metrics are used solely for internal reference to assess model convergence and will not be publicly reported. Table  3  presents a description of our internal multi-task test sets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Main Results",
      "text": "Table  4  and Table  5  show the experimental results of our OSUM across various tasks. The results reveal that our approach achieves performance that is comparable to, and in many cases superior to, speech understanding models such as Qwen-audio, Qwen2-audio, Whisper, and SenseVoice. Furthermore, in this section, we will highlight the performance disparities between our model and other comparable approaches, while providing a detailed analysis of the challenges SULMs face in these tasks. We hope that these experiences can provide useful references for researchers.   4 , our approach reveals obvious advantages in the ASR task on the Chinese test sets. Notably, the proposed OSUM consistently outperforms other models on the WenetSpeech test-meeting set, three AISHELL-2 sub-test sets, and four internally used SpeechIO test sets. While OSUM does not surpass the top-performing method on the English test set, it rivals performance comparable to SenseVoice-S. These results are achieved, remarkably, with substantially less training data. In addition, we find that OSUM exhibits a surprisingly impressive ability to recognize Chinese-English code-mixed speech, even though such code-mixed datasets are not included during training. To be specific, the MER/CER/WER is 3.89%/2.41%/19.30% on the ASRU code-switching test set  (Shi et al., 2020) . Going forward, we will enhance this function. Overall, these results underscore that our ASR+X task paradigm effectively enhances model convergence in ASR tasks, significantly minimizing the data and computational resources required for SULMs training.\n\nSRWT Table  5  presents the SRWT evaluation results for our proposed OSUM model compared to Whisper-Large-v3, Qwen-Audio, and the GMM-HMM model used for generating annotated data in SRWT tasks. Our OSUM model significantly outperforms Whisper-Large-v3 by relative 36.70% and also surpasses Qwen-Audio.\n\nAdditionally, our OSUM's performance in the SRWT task even slightly surpasses that of the GMM-HMM model, which is widely recognized for its high accuracy in timestamp prediction. These results underscore the effectiveness of OSUM in the SRWT task. Additionally, OSUM's high performance in the SRWT task not only enables it to predict timestamps in an end-to-end manner but, more importantly, simplifies its integration with other tasks, such as speaker diarization.\n\nVED We first evaluate OSUM's performance on the public test sets ESC-50 and VocalSound. However, since the event categories in these two datasets do not completely align with those in OSUM, the comparison to other approaches should only serve as a rough assessment. Specifically, the ESC-50 contains a substantial number of non-vocal audio events, we categorize them as \"other.\" The experimental results on this test set demonstrate that our model successfully classifies these non-vocal audio events as \"other.\" Additionally, on the VocalSound set, we select the categories supported by OSUM and calculate the average accuracy across these categories. This result reveals that our OSUM exhibits a gap compared to Qwen2-audio, primarily due to our training data consisting of concatenated speech and vocal events. In contrast, the VocalSound test set includes only the latter, resulting in a significant mismatch. Nevertheless, our OSUM achieves a norm level, successfully identifying the most independent vocal events. In our internal human-recoded ASR+VED test set, PANNs become unusable due to similar mismatches, particularly because their design treats speech as a standalone event, exacerbating accuracy degradation. Qwen2-audio performs relatively better but also experiences a performance decline in our test set, likely due to overfitting. In contrast, our model demonstrates balanced results in both the public and internal test sets, showcasing enhanced generalization. This indicates that using VC to augment data for vocal events can effectively mitigate overfitting in VED tasks.\n\nSER For the SER task, we extract the categories supported by OSUM from the public datasets MELD and MER2023 for testing, followed by a comprehensive evaluation on our internal test set. In the experiments with the public datasets, OSUM demonstrates superior performance on the MER2023 test set, outperforming several recent public benchmark models. On the MELD dataset, OSUM's performance ranks just below the SenseVoice-L model, likely due to the latter's additional training on a larger-scale speech emotion dataset.\n\nIn addition, while OSUM's result on the internal test set is comparable to that of the EmoBox model, it significantly surpasses other comparative approaches. Furthermore, we observe that among the eight emotions supported, disgust and fear are particularly challenging to recognize, a difficulty partly attributed to the scarcity of training data for these two emotions. In our future work, we plan to enhance the model's performance and generalization capability by utilizing OSUM for labeling, thereby obtaining a larger and more balanced emotion dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ssr",
      "text": "The acoustic-text dual-modal style classification employed by our OSUM significantly outperforms the single-text modality of GLM-4-9B-Chat. It demonstrates a strong ability to distinguish among eight styles: news and science reporting, horror stories, fairy tales, customer service, poetry and prose, audiobooks, spontaneous conversation, and others. Notably, the classification performance for news science communication, audiobooks, fairy tales, and customer service styles is commendable; however, there remains room for improvement in the categorization of poetry and prose, horror stories, and other styles. Moving forward, we leverage OSUM to label additional data, aiming to enhance data quality and optimize the distribution across categories.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sgc",
      "text": "In the SGC task, we evaluate Qwen2-Audio and OSUM. The results demonstrate that OSUM achieves an 100% accuracy on the AISHELL-1 test set. While this result is commendable, we suspect it may indicate some degree of overfitting. Furthermore, on the Kaggle test set, our approach slightly outperforms Qwen2-Audio, yet it falls short on our internal test set. This indicates that Qwen2-Audio has strong robustness in the SGC task, which may be due to the fact that they have used a wider range of data. In the follow-up, we plan to continue to increase the training data for this task. Overall, OSUM exhibits its value in the SGC task.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sttc",
      "text": "In the STTC task, we follow AirBench's evaluation protocol across all test sets. This involves providing the text of audio queries along with the text of two distinct answers, allowing a text-based LLM to assign subjective scores from 1 to 10. The two answers consist of a real response and the answer generated by SULMs. While AirBench employs GPT-4 as the scoring LLM, it is currently inaccessible, so we instead utilize GPT-3.5-Turbo. The test results presented in Table  5  indicate that, on AirBench's official speech sub-test set, our score is lower than that of Qwen2-Audio, suggesting that our model's capabilities in English conversation and audio description lag behind those of Qwen2-Audio. This is primarily because we do not use English conversational data for training; the current score relies entirely on the LLM's performance. However, on our internal Chinese conversational test set, OSUM outperforms Qwen2-Audio, which indicates that our strategy of performing ASR before chat is beneficial. Overall, our OSUM model is comparable to Qwen2-Audio in terms of conversational ability. We will not be content with this achievement. Our future work will mainly focus on the conversation task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Future Works",
      "text": "While OSUM demonstrates commendable performance, our research remains an ongoing endeavor to push the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several key areas for improvement and innovation:\n\n• Expanding OSUM's Functionalities. We plan to enhance OSUM with additional capabilities, such as language and accent identification, to broaden its applicability in multilingual and diverse speech scenarios.\n\n• Enabling Multi-Task Capability. We aim to activate OSUM's ability to perform multiple tasks simultaneously, such as identifying the emotion, age, gender, and speaking style in a single inference pass. Leveraging this multi-task capability, we plan to develop a versatile data labeling tool to streamline audio data processing pipelines.\n\n• Incorporating Full-Duplex Voice Interaction. To improve naturalness and responsiveness, we plan to integrate full-duplex voice interaction capabilities into OSUM. This enhancement will allow OSUM to generate context-aware, natural responses, such as matching the questioner's emotion or mimicking specific speaking styles, like that of a child.\n\n• Open Science Contributions. As part of our commitment to advancing academic research, we will continue to share detailed training methodologies, data pipelines, and model updates. Our aim is to foster collaboration, provide valuable resources for researchers, and democratize access to cutting-edge Speech LLM technologies.\n\nThrough these efforts, we seek to extend OSUM's capabilities, establish new benchmarks for Speech LLMs, and contribute meaningfully to the academic study and practical applications of speech understanding.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Acknowledgment",
      "text": "We appreciate that AISHELL 11  , Databaker 12  , MagicData 13  , and DataTang 14  generously provide some training data. Most of our model components are trained using Huawei 15  Ascend NPUs.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Authors",
      "text": "The ranking is from top to bottom and then from left to right. * indicates equal contribution. † indicates the corresponding author.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of Qwen2-Audio and our OSUM model. In most tasks, OSUM achieves a better",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of the architecture and tasks of OSUM.",
      "page": 3
    },
    {
      "caption": "Figure 1: The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting",
      "page": 3
    },
    {
      "caption": "Figure 2: , our OSUM model comprises a speech encoder, an adaptor, and an LLM. During",
      "page": 4
    },
    {
      "caption": "Figure 2: Currently, our",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Large Language Models (LLMs) have made significant progress in various downstream"
        },
        {
          "Abstract": "tasks, inspiring the development of Speech Understanding Language Models (SULMs) to"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "veloped by the industry, leveraging large-scale datasets and computational resources that"
        },
        {
          "Abstract": "are not readily available to the academic community. Moreover, the lack of transparency"
        },
        {
          "Abstract": "in training details creates additional barriers to further innovation."
        },
        {
          "Abstract": "present OSUM, an Open Speech Understanding Model designed to explore the potential"
        },
        {
          "Abstract": "of training SLUMs under constrained academic resources. The OSUM model combines"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "cluding speech recognition (ASR), speech recognition with timestamps (SRWT), vocal"
        },
        {
          "Abstract": "event detection (VED), speech emotion recognition (SER), speaking style recognition"
        },
        {
          "Abstract": "(SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-"
        },
        {
          "Abstract": "to-text chat (STTC). By employing an ASR+X training strategy, OSUM achieves efficient"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Beyond delivering strong performance, OSUM emphasizes transparency by providing"
        },
        {
          "Abstract": "openly available data preparation and training methodologies, offering valuable insights"
        },
        {
          "Abstract": "and practical guidance for the academic community. By doing so, we aim to accelerate"
        },
        {
          "Abstract": "research and innovation in advanced SULM technologies."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "",
          "Introduction": "Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence"
        },
        {
          "1": "(AGI) in recent years. Given the inherent human preference for speech-based interaction, there has been",
          "Introduction": ""
        },
        {
          "1": "",
          "Introduction": "growing interest in extending LLMs with speech capabilities to develop Speech LLMs. To generate fluent"
        },
        {
          "1": "",
          "Introduction": "and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both"
        },
        {
          "1": "",
          "Introduction": "its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age."
        },
        {
          "1": "",
          "Introduction": "Moreover, this comprehension ability is also crucial for audio data labeling. Currently, the mainstream"
        },
        {
          "1": "",
          "Introduction": "multi-label generation approach is to use multiple models to label each task separately, which consumes"
        },
        {
          "1": "",
          "Introduction": "extremely high computational resources. A labeling model capable of accurately generating multiple labels"
        },
        {
          "1": "simultaneously holds broad application prospects.",
          "Introduction": ""
        },
        {
          "1": "",
          "Introduction": "The area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements"
        },
        {
          "1": "",
          "Introduction": "through projects such as Qwen-Audio(Chu et al., 2023), Qwen2-Audio(Chu et al., 2024), PandGPT(Su et al.,"
        },
        {
          "1": "",
          "Introduction": "2023), and SALMONN (Tang et al., 2024). Whisper (Radford et al., 2023) marks a pioneering exploration"
        },
        {
          "1": "",
          "Introduction": "∗This is the v2 version of the technical report. The experimental results reported herein differ from those in v1 because of adding"
        },
        {
          "1": "new data and training in more steps.",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "performance than Qwen2-Audio despite using significantly fewer computational resources and training data."
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "The values for each model’s task in the radar chart are based on the average results on the public and internal"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "test sets, as shown in Table 4 and Table 5."
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "of speech understanding independent of LLMs, utilizing an encoder-decoder Transformer (Vaswani, 2017)"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "(ST),\nlanguage identification (LID), and voice activity detection (VAD). Building on Whisper’s design,"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "SenseVoice (An et al., 2024) and TouchASP (Song et al., 2024) expand more tasks like speech emotion"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "human speech. Qwen-Audio integrates Whisper’s encoder with the text-based Qwen LLM (Bai et al., 2023),"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "enabling the latter to understand speech. Compared to Whisper, Qwen-Audio leverages a more powerful"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "SULMs.\nIts successor, Qwen2-Audio, further enhances these capabilities by supporting natural language"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "prompts and achieving superior performance across various benchmarks (Chu et al., 2024)."
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "Although these advanced SULMs have achieved remarkable progress, most of\nthem are developed by"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "industry, leveraging millions of hours of training data and massive GPU resources. For instance, TouchASP"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively. Such large-scale resources"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "are typically beyond the reach of academia institutions. Furthermore, while inference models are often open-"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "sourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "configurations are rarely disclosed. These limitations hinder academic community efforts to further optimize"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "and expand SULM research. Recently, a growing movement advocating for open science in Speech LLM"
        },
        {
          "Figure 1: Comparison of Qwen2-Audio and our OSUM model.\nIn most tasks, OSUM achieves a better": "research has emerged. This movement emphasizes the importance of releasing comprehensive training"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "frameworks, datasets, and methodological details to promote research and innovation. A notable example is"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "the Open Whisper-style Speech Model (OWSM) series (Peng et al., 2023), which replicates Whisper-style"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "training using open-sourced tools and publicly available data, significantly advancing public understanding"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "and research on speech understanding models."
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "In this study, we aim to foster broader academic exploration of SULMs with limited resource demands,"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "encouraging wider research community participation. To this end, we introduce OSUM, an open SULM with"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "its data processing pipeline and training details publicly available. The OSUM model integrates a Whisper"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM. It is capable of performing a wide"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "(SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR),"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC). Notably,"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "SSR is a distinctive feature of our OSUM model and serves as a vital component of speech understanding."
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "It enhances the model’s capability by improving contextual comprehension and boosting performance"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "across various downstream speech tasks. Furthermore, it establishes a foundation for enabling more natural"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "and context-aware speech-based interactions. We adopt an ASR+X training strategy to enhance training"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "alongside the primary target task (denoted as “X”). For instance, during the training of the SER task, we"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "speech sample. This multi-task training accelerates modality alignment, enabling the LLM to effectively"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "utilize both textual and acoustic modalities. Our OSUM model utilizes only 50,500 hours of training data"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "and achieves comparable or superior performance to other SULMs. The overall performance of OSUM is"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "illustrated in Fig. 1. The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "inference on both platforms. The goal of this study is to foster transparency and accelerate progress in the"
        },
        {
          "Figure 2: The overview of the architecture and tasks of OSUM.": "field of SULMs by providing accessible tools and resources for the broader research community."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.1\nModel Architecture": "As shown in Figure 2, our OSUM model comprises a speech encoder, an adaptor, and an LLM. During"
        },
        {
          "2.1\nModel Architecture": "the training, all of the parameters in the encoder and adaptor are updated, while the LLM is fine-tuned"
        },
        {
          "2.1\nModel Architecture": "with LoRA (Hu et al., 2022). The input of our model consists of a speech and a natural language prompt."
        },
        {
          "2.1\nModel Architecture": "Unlink Whisper (Radford et al., 2023) and Qwen-audio (Bai et al., 2023), which rely on instruct tags, the"
        },
        {
          "2.1\nModel Architecture": "OSUM employs descriptive text, converting all eight supported tasks as shown in Fig. 2. Currently, our"
        },
        {
          "2.1\nModel Architecture": "model supports only text-based responses, but audio output capabilities are under active development. The"
        },
        {
          "2.1\nModel Architecture": "following sections describe each sub-module in detail."
        },
        {
          "2.1\nModel Architecture": "Our OSUM utilizes the Whisper-Medium 1 model as its speech encoder, which consists"
        },
        {
          "2.1\nModel Architecture": "Speech Encoder"
        },
        {
          "2.1\nModel Architecture": "of 2 one-dimensional convolutional\nlayers with 2 times downsampling, and 24 Transformer layers with"
        },
        {
          "2.1\nModel Architecture": "1024 hidden state dimensions and 16-headed self-attention. The encoder has approximately 300 million"
        },
        {
          "2.1\nModel Architecture": "parameters, which makes it take into account both speech comprehension ability and inference efficiency."
        },
        {
          "2.1\nModel Architecture": "The adaptor module features a hybrid architecture combining 3-layer 1D convolutional layers"
        },
        {
          "2.1\nModel Architecture": "Adaptor"
        },
        {
          "2.1\nModel Architecture": "(Conv1D) and 4-layer Transformer. The Conv1D layers use kernel widths of (3, 3, 3) and strides of (1, 2, 2),"
        },
        {
          "2.1\nModel Architecture": "achieving an overall 4 times downsampling. The Transformer layers have a model dimension of 1,280, an"
        },
        {
          "2.1\nModel Architecture": "inner dimension of 2,560, and 4 attention heads. This architecture bridges the output of the speech encoder"
        },
        {
          "2.1\nModel Architecture": "with the input requirements of the LLM, enabling efficient modality alignment."
        },
        {
          "2.1\nModel Architecture": "The Qwen2-7B-Instruct is selected as our LLM. Qwen2-7B-Instruct 2 is a general-purpose"
        },
        {
          "2.1\nModel Architecture": "LLM with LoRA"
        },
        {
          "2.1\nModel Architecture": "LLM with a parameter scale of 7 billion, specifically designed for multi-task instruction optimization.\nIn"
        },
        {
          "2.1\nModel Architecture": "our work, we fine-tune the Qwen2-7B-Instruct model using LoRA (Low-Rank Adaptation) technology. The"
        },
        {
          "2.1\nModel Architecture": "LoRA hyperparameters-α, rank, and dropout ratio are set to 32, 8, and 0.1, respectively."
        },
        {
          "2.1\nModel Architecture": "2.2 Multitask Supervised Training"
        },
        {
          "2.1\nModel Architecture": "The training procedure includes two stages. First, we perform multi-task supervised fine-tuning on the"
        },
        {
          "2.1\nModel Architecture": "original Whisper model without an LLM. Second, we integrate the fine-tuned Whisper encoder with the"
        },
        {
          "2.1\nModel Architecture": "Qwen2 LLM to create the complete OSUM system, then conduct further supervised training using a larger"
        },
        {
          "2.1\nModel Architecture": "dataset."
        },
        {
          "2.1\nModel Architecture": "The original Whisper model supports a limited scope of speech-understanding tasks,"
        },
        {
          "2.1\nModel Architecture": "Whisper Fine-tuning"
        },
        {
          "2.1\nModel Architecture": "which makes the direct integration of the Whisper with an LLM for multi-task training risky when data"
        },
        {
          "2.1\nModel Architecture": "and computation resources are constrained. Therefore, we first fine-tune the Whisper via multi-task data to"
        },
        {
          "2.1\nModel Architecture": "ensure faster convergence of the OSUM model. Furthermore, this stage allows us to verify the reliability of"
        },
        {
          "2.1\nModel Architecture": "our multi-task data. Specifically, we expand Whisper’s instruction tag set to accommodate more tasks. Each"
        },
        {
          "2.1\nModel Architecture": "forward pass executes only a single task."
        },
        {
          "2.1\nModel Architecture": "Training SULMs typically begins with pre-training on an ASR task, which serves as a"
        },
        {
          "2.1\nModel Architecture": "OSUM Training"
        },
        {
          "2.1\nModel Architecture": "foundation for incorporating additional speech tasks to enable LLMs to process semantic content from the"
        },
        {
          "2.1\nModel Architecture": "speech encoder. Given computational constraints, we introduce an ASR+X paradigm for OSUM’s multi-"
        },
        {
          "2.1\nModel Architecture": "task training.\nIt concurrently trains ASR and a secondary task “X”, accelerating training while allowing"
        },
        {
          "2.1\nModel Architecture": "the “X” task to utilize both textual and acoustic features, thereby potentially improving performance The"
        },
        {
          "2.1\nModel Architecture": "ASR+X paradigm follows a two-step process: first,\ntranscribing speech to text (ASR); then,\nintegrating"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "Prompt"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "转录这段音频中的语音内容为文字。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将以下音频进行转录，同时标记出每个英文"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "单词及对应中文字符的起始与结束时间，时间"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "单位精确到0.1秒，并用<>来表示这些时间范围。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将以下音频进行转录，并在结尾处给出<音频"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "事件>标签，音频事件分为8类：laugh，"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "cough，cry，screaming，sigh，"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "throat clearing，sneeze，other。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将以下音频内容进行转录，并在结尾处给出"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "<情感>标签，情感共分为8类：sad，anger，neutral，"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "happy，surprise，fear，disgust，以及 other。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将以下音频内容进行转录，并在结尾处给出<风格>"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "标签，风格共分为8类：新闻科普，恐怖故事，童话"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "故事，客服，诗歌散文，有声书，日常口语，其他。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将这段音频进行转录，并在文本末尾附加<性别>"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "标签。性别分为两种：female，male。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "请将以下音频内容转录成文字，并在文字的最后"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "加上<年龄>标签，标明是child、adult还是old。"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "首先将语音转化为书面语，随后以<开始回答>"
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": ""
        },
        {
          "Table 1: Example of prompts and ASR+X labels in OSUM multitask training.": "作分隔，最后对语音内容做出答复。"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "Task"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "ASR"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "SRWT"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "VED"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "SER"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "SSR"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "SGC"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "SAP"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": "STTC"
        },
        {
          "Table 2: Details of the multi-task training data for OSUM. The total duration is 50,500 hours. The total training": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "data with the assistance of VC. The open-source vocal event datasets we use include Audioset (Gemmeke"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "et al., 2017), ESC-50 (Piczak, 2015), Vocal Sound (Gong et al., 2022), and Nonspeech7k (Rashid et al., 2023),"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "while the ASR data consists solely of AISHELL-2 (Du et al., 2018)."
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "Emotion2Vec (Ma et al., 2024) is the first universal speech emotion representation model. Without"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "SER"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "additional fine-tuning, we directly apply the pre-trained Emotion2Vec+ Large model 6, which is trained on"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "40,000 hours of emotional speech data, to annotate the audio with emotion labels. Additionally, we leverage"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "the GLM-4-9B-Chat model to generate emotion labels from the textual transcriptions of the speech. By"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "intersecting these annotations, we generate high-quality emotional labels for the entire dataset."
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "Efforts to train a speaker gender classification model to label web-sourced data yield unsatisfactory"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "SGC"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "performance. Consequently, we discard the pseudo-labeled data and relied solely exclusively on manually"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "labeled datasets for training. For the SGC task, we select KeSpeech (Tang et al., 2021), Datatang-Kid (Datatang"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "Tech Co Ltd, 2024), AISHELL-1 (Bu et al., 2017), AISHELL-2 (Du et al., 2018), LibriSpeech (Panayotov et al.,"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "2015), Kaggle-CommonVoice (Kaggle Community, 2017), and Magicdata-Read OpenSLR (2019) as training"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "dataset, as they include reliable speaker gender labels. In addition, we employ a noisy data augmentation"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "strategy."
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "Similar to the SGC task, due to the poor performance of the automated labeling model, only manually"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "SAP"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "labeled data is used for training. We use KeSpeech (Tang et al., 2021), Datatang-Kid (Datatang Tech Co Ltd,"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "2024), Magicdata-Read (OpenSLR, 2019), Kaggle-CommonVoice (Kaggle Community, 2017), AISHELL-"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "ASR0060 (AISHELL Tech Co Ltd, 2024), and AISHELL-ASR0018 (AISHELL Tech Co Ltd, 2024) as the training"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "dataset for the SAP task, as these datasets provide reliable speaker age labels. Noisy data augmentation is"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "also utilized in this task."
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "For the STTC task, we use three types of data. First, we utilize a human-recorded audio question-"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "STTC"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "answer dataset Databacker-Conversation (Databaker Tech Co Ltd, 2024). Then, we use a text-based dialogue"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "dataset LCCC (Wang et al., 2020) and the ChatTTS 7 system with random speaker capabilities to generate the"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "utterances of the questioner in the dialogue, thus obtaining the speech-text pairs for the dialogue task. Finally,"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "we filter suitable response sentences from the Wenetspeech (Zhang et al., 2022) dataset using Qwen2.5-7B 8,"
        },
        {
          "that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training": "guiding the LLM to generate text answers."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "Task"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "ASR"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "SRWT"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "VED"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "SER"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "SSR"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "SGC"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "SAP"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": "STTC"
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        },
        {
          "rate of 5e-5. The process completes a total of 704,000 training steps and consumes 10 days.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3": "Table 4 and Table 5 show the experimental results of our OSUM across various tasks. The results reveal that our",
          "Main Results": ""
        },
        {
          "3.3": "",
          "Main Results": "approach achieves performance that is comparable to, and in many cases superior to, speech understanding"
        },
        {
          "3.3": "",
          "Main Results": "models such as Qwen-audio, Qwen2-audio, Whisper, and SenseVoice. Furthermore, in this section, we will"
        },
        {
          "3.3": "",
          "Main Results": "highlight the performance disparities between our model and other comparable approaches, while providing"
        },
        {
          "3.3": "",
          "Main Results": "a detailed analysis of the challenges SULMs face in these tasks. We hope that these experiences can provide"
        },
        {
          "3.3": "useful references for researchers.",
          "Main Results": ""
        },
        {
          "3.3": "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best",
          "Main Results": ""
        },
        {
          "3.3": "result among the same test set. All internal results are inferred by ourselves.",
          "Main Results": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": "Task"
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": "ASR"
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        },
        {
          "Table 4: Evaluation results of ASR tasks on public and internal test sets. The bold font represents the best": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "model, which is widely recognized for its high accuracy in timestamp prediction. These results underscore"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "the effectiveness of OSUM in the SRWT task. Additionally, OSUM’s high performance in the SRWT task"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "not only enables it to predict timestamps in an end-to-end manner but, more importantly, simplifies its"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "integration with other tasks, such as speaker diarization."
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "VED We first evaluate OSUM’s performance on the public test sets ESC-50 and VocalSound. However,"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "since the event categories in these two datasets do not completely align with those in OSUM, the comparison"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "to other approaches should only serve as a rough assessment. Specifically, the ESC-50 contains a substantial"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "number of non-vocal audio events, we categorize them as \"other.\" The experimental results on this test set"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "demonstrate that our model successfully classifies these non-vocal audio events as \"other.\" Additionally, on"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "the VocalSound set, we select the categories supported by OSUM and calculate the average accuracy across"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "these categories. This result reveals that our OSUM exhibits a gap compared to Qwen2-audio, primarily due"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "to our training data consisting of concatenated speech and vocal events.\nIn contrast, the VocalSound test set"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "includes only the latter, resulting in a significant mismatch. Nevertheless, our OSUM achieves a norm level,"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "successfully identifying the most independent vocal events. In our internal human-recoded ASR+VED test"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "set, PANNs become unusable due to similar mismatches, particularly because their design treats speech as"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "a standalone event, exacerbating accuracy degradation. Qwen2-audio performs relatively better but also"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "experiences a performance decline in our test set, likely due to overfitting. In contrast, our model demonstrates"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "balanced results in both the public and internal test sets, showcasing enhanced generalization. This indicates"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "that using VC to augment data for vocal events can effectively mitigate overfitting in VED tasks."
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "For the SER task, we extract the categories supported by OSUM from the public datasets MELD and"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "SER"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "MER2023 for testing, followed by a comprehensive evaluation on our internal test set. In the experiments"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "with the public datasets, OSUM demonstrates superior performance on the MER2023 test set, outperforming"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "several recent public benchmark models. On the MELD dataset, OSUM’s performance ranks just below the"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "SenseVoice-L model, likely due to the latter’s additional training on a larger-scale speech emotion dataset."
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "In addition, while OSUM’s result on the internal test set is comparable to that of the EmoBox model,\nit"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "significantly surpasses other comparative approaches.\nFurthermore, we observe that among the eight"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "emotions supported, disgust and fear are particularly challenging to recognize, a difficulty partly attributed"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "to the scarcity of training data for these two emotions. In our future work, we plan to enhance the model’s"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "performance and generalization capability by utilizing OSUM for labeling, thereby obtaining a larger and"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "more balanced emotion dataset."
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "The acoustic-text dual-modal style classification employed by our OSUM significantly outperforms"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "SSR"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "the single-text modality of GLM-4-9B-Chat.\nIt demonstrates a strong ability to distinguish among eight"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "styles: news and science reporting, horror stories, fairy tales, customer service, poetry and prose, audiobooks,"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "spontaneous conversation, and others. Notably, the classification performance for news science communica-"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "tion, audiobooks, fairy tales, and customer service styles is commendable; however, there remains room for"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "improvement in the categorization of poetry and prose, horror stories, and other styles. Moving forward, we"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "leverage OSUM to label additional data, aiming to enhance data quality and optimize the distribution across"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "categories."
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "In the SGC task, we evaluate Qwen2-Audio and OSUM. The results demonstrate that OSUM achieves"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "SGC"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "an 100% accuracy on the AISHELL-1 test set. While this result is commendable, we suspect it may indicate"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "some degree of overfitting. Furthermore, on the Kaggle test set, our approach slightly outperforms Qwen2-"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "Audio, yet it falls short on our internal test set. This indicates that Qwen2-Audio has strong robustness in the"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "SGC task, which may be due to the fact that they have used a wider range of data.\nIn the follow-up, we plan"
        },
        {
          "Additionally, our OSUM’s performance in the SRWT task even slightly surpasses that of the GMM-HMM": "to continue to increase the training data for this task. Overall, OSUM exhibits its value in the SGC task."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "Task"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "SRWT"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "VED"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "SER"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "SSR"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "SGC"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "SAP"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": "STTC"
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        },
        {
          "Table 5: Evaluation results of multi-tasking on public and internal test sets. The best results for each test set": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "Kaggle test set and our internal test set. This may stem from their overly detailed age categorization, which"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "hinders the model’s training accuracy. Our model significantly surpasses Qwen2-Audio on the Kaggle test set,"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "achieving an accuracy of 76.52%. Although the classification accuracy slightly declines on our proprietary"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "test set,"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "capabilities on different data."
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": ""
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "STTC"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "the text of audio queries along with the text of two distinct answers, allowing a text-based LLM to assign"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "subjective scores from 1 to 10. The two answers consist of a real response and the answer generated by"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "SULMs. While AirBench employs GPT-4 as the scoring LLM, it is currently inaccessible, so we instead utilize"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "GPT-3.5-Turbo. The test results presented in Table 5 indicate that, on AirBench’s official speech sub-test set,"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "our score is lower than that of Qwen2-Audio, suggesting that our model’s capabilities in English conversation"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "and audio description lag behind those of Qwen2-Audio. This is primarily because we do not use English"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "conversational data for training; the current score relies entirely on the LLM’s performance. However, on our"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "internal Chinese conversational test set, OSUM outperforms Qwen2-Audio, which indicates that our strategy"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "of performing ASR before chat is beneficial. Overall, our OSUM model is comparable to Qwen2-Audio in"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "terms of conversational ability. We will not be content with this achievement. Our future work will mainly"
        },
        {
          "our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the": "focus on the conversation task."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "key areas for improvement and innovation:"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "• Expanding OSUM’s Functionalities. We plan to enhance OSUM with additional capabilities, such"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "as language and accent identification, to broaden its applicability in multilingual and diverse speech"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "scenarios."
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "• Enabling Multi-Task Capability. We aim to activate OSUM’s ability to perform multiple tasks simul-"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "taneously, such as identifying the emotion, age, gender, and speaking style in a single inference pass."
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "Leveraging this multi-task capability, we plan to develop a versatile data labeling tool to streamline"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "audio data processing pipelines."
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "• Incorporating Full-Duplex Voice Interaction. To improve naturalness and responsiveness, we plan to"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "integrate full-duplex voice interaction capabilities into OSUM. This enhancement will allow OSUM to"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "generate context-aware, natural responses, such as matching the questioner’s emotion or mimicking"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "specific speaking styles, like that of a child."
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "• Open Science Contributions. As part of our commitment to advancing academic research, we will"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "continue to share detailed training methodologies, data pipelines, and model updates. Our aim is to"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "foster collaboration, provide valuable resources for researchers, and democratize access to cutting-edge"
        },
        {
          "the boundaries of academic exploration in Speech LLMs. In the coming months, we aim to address several": "Speech LLM technologies."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "corresponding author."
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Xuelong Geng"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Kun Wei"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Qijie Shao"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Shuiyun Liu∗"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Zhennan Lin∗"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Zhixian Zhao∗"
        },
        {
          "The ranking is from top to bottom and then from left to right. ∗ indicates equal contribution.": "• Guojian Li∗"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11https://www.aishelltech.com": "12https://www.data-baker.com"
        },
        {
          "11https://www.aishelltech.com": "13https://www.magicdatatech.com"
        },
        {
          "11https://www.aishelltech.com": "14https://www.datatang.com"
        },
        {
          "11https://www.aishelltech.com": "15https://www.huawei.com"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": ""
        },
        {
          "References": "accessed on 2025-1-9."
        },
        {
          "References": "Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui"
        },
        {
          "References": "Hu, Kai Hu, et al. FunaudioLLM: Voice understanding and generation foundation models for natural"
        },
        {
          "References": "interaction between humans and LLMs. arXiv preprint arXiv:2407.04051, 2024."
        },
        {
          "References": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei"
        },
        {
          "References": "Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023."
        },
        {
          "References": "Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. AISHELL-1: An open-source mandarin speech"
        },
        {
          "References": ""
        },
        {
          "References": "national Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA),"
        },
        {
          "References": "2017."
        },
        {
          "References": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "References": "Chang, Sungbok Lee, and Shrikanth S Narayanan. IEMOCAP: Interactive emotional dyadic motion capture"
        },
        {
          "References": "database. Language Resources and Evaluation (LREC), 42:335–359, 2008."
        },
        {
          "References": "Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mohammed AbdelWahab, Najmeh Sadoughi, and"
        },
        {
          "References": ""
        },
        {
          "References": "IEEE Transactions on Affective Computing, 8:67–80, 2017."
        },
        {
          "References": "Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou."
        },
        {
          "References": ""
        },
        {
          "References": "arXiv preprint arXiv:2311.07919, 2023."
        },
        {
          "References": "Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He,"
        },
        {
          "References": "Junyang Lin, et al. Qwen2-Audio technical report. arXiv preprint arXiv:2407.10759, 2024."
        },
        {
          "References": "Databaker Tech Co Ltd. Data products, 2024. https://www.data-baker.com, Last accessed on 2025-1-9."
        },
        {
          "References": "Datatang Tech Co Ltd. Data products, 2024. https://www.datatang.com/speechRecognition, Last accessed"
        },
        {
          "References": "on 2025-1-9."
        },
        {
          "References": "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. AISHELL-2: Transforming Mandarin ASR research into"
        },
        {
          "References": "industrial scale. arXiv preprint arXiv:1808.10583, 2018."
        },
        {
          "References": "Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore,"
        },
        {
          "References": "Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and human-labeled dataset for audio events."
        },
        {
          "References": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages"
        },
        {
          "References": "776–780, 2017."
        },
        {
          "References": "Yuan Gong, Jin Yu, and James Glass. Vocalsound: A dataset for improving human vocal sounds recognition."
        },
        {
          "References": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages"
        },
        {
          "References": "151–155, 2022."
        },
        {
          "References": "Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu"
        },
        {
          "References": ""
        },
        {
          "References": "on Learning Representations (ICLR), pages 1–26, 2022."
        },
        {
          "References": "Kaggle Community. Datasets, 2017. https://www.kaggle.com/datasets/mozillaorg/common-voice, Last"
        },
        {
          "References": "accessed on 2025-1-9."
        },
        {
          "References": "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming"
        },
        {
          "References": "Zhao, Ye Liu, Bin Liu, Jiangyan Yi, Meng Wang, Erik Cambria, Guoying Zhao, Björn W. Schuller, and"
        },
        {
          "References": "Jianhua Tao. MER 2023: Multi-label learning, modality robustness, and semi-supervised learning."
        },
        {
          "References": "Proceedings of the ACM International Conference on Multimedia (ACM MM), page 9610–9614, 2023."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Self-supervised pre-training for speech emotion representation. In Proceedings of the Findings of the Association"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "for Computational Linguistics (ACL), pages 15747–15760, 2024."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Luz Martinez-Lucas, Mohammed Abdelwahab, and Carlos Busso. The MSP-conversation corpus. Proceedings"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "of the Conference of the International Speech Communication Association (Interspeech), 2020."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "OpenSLR. MAGICDATA mandarin Chinese read speech corpus, 2019. https://openslr.org/68/, Last"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "accessed on 2025-1-9."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the IEEE International Conference on Acoustics, Speech and"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Signal Processing (ICASSP), pages 5206–5210, 2015."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora,"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "William Chen, Roshan Sharma, et al. Reproducing whisper-style training using an open-source toolkit and"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "(ASRU), pages 1–8, 2023."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the Annual ACM"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Conference on Multimedia (ACM MM), pages 1015–1018, 2015."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Annual Meeting of the Association for Computational Linguistics (ACL), pages 527–536, 2019."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "speech recognition via large-scale weak supervision. In Proceedings of the International Conference on Machine"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Learning (ICML), pages 28492–28518, 2023."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Muhammad Mamunur Rashid, Guiqing Li, and Chengrui Du. Nonspeech7k dataset: Classification and"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "IET Signal Processing, page e12233, 2023."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Xian Shi, Qiangze Feng, and Lei Xie. The ASRU 2019 Mandarin-English code-switching speech recognition"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "challenge: Open datasets, tracks, methods and results. arXiv preprint arXiv:2007.05916, 2020."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Xian Shi, Yanni Chen, Shiliang Zhang, and Zhijie Yan. Achieving timestamp prediction while recognizing"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the National Conference on Man-Machine"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Speech Communication (NCMMSC), pages 89–100, 2022."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Xingchen Song, Chengdong Liang, Binbin Zhang, Pengshen Zhang, ZiYu Wang, Youcheng Ma, Menglong"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Xu, Lin Wang, Di Wu, Fuping Pan, et al. TouchASP: Elastic automatic speech perception that everyone can"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "touch. arXiv preprint arXiv:2412.15622, 2024."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. PandaGPT: One model to instruction-"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the Workshop on Taming Large Language Models (TLLM), pages 11–23,"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "2023."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "International Conference on Learning Representations (ICLR), pages 1–23, 2024."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Zhiyuan Tang, Dong Wang, Yanguang Xu, Jianwei Sun, Xiaoning Lei, Shuaijiang Zhao, Cheng Wen, Xingjun"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Tan, Chuandong Xie, Shuran Zhou, et al. Kespeech: An open source speech dataset of Mandarin and"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "In Proceedings of the Conference on Neural Information Processing Systems Datasets and"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Benchmarks Track (Round 2), 2021."
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Shreya G. Upadhyay, Woan-Shiuan Chien, Bo-Hao Su, Lucas Goncalves, Ya-Tse Wu, Ali N. Salman, Carlos"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "Busso, and Chi-Chun Lee. An intelligent infrastructure toward large scale naturalistic affective speech"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "corpora collection. In Proceedings of the International Conference on Affective Computing and Intelligent Interaction"
        },
        {
          "Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, ShiLiang Zhang, and Xie Chen. Emotion2vec:": "(ACII), pages 1–8, 2023."
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "FunaudioLLM: Voice understanding and generation foundation models for natural interaction between humans and LLMs",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Hu"
      ],
      "year": "2024",
      "venue": "FunaudioLLM: Voice understanding and generation foundation models for natural interaction between humans and LLMs",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "2",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "3",
      "title": "AISHELL-1: An open-source mandarin speech corpus and a speech recognition baseline",
      "authors": [
        "Hui Bu",
        "Jiayu Du",
        "Xingyu Na",
        "Bengu Wu",
        "Hao Zheng"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "5",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "7",
      "title": "Qwen2-Audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-Audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "8",
      "title": "AISHELL-2: Transforming Mandarin ASR research into industrial scale",
      "authors": [
        "Jiayu Du",
        "Xingyu Na",
        "Xuechen Liu",
        "Hui Bu"
      ],
      "year": "2018",
      "venue": "AISHELL-2: Transforming Mandarin ASR research into industrial scale",
      "arxiv": "arXiv:1808.10583"
    },
    {
      "citation_id": "9",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "F Jort",
        "Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Vocalsound: A dataset for improving human vocal sounds recognition",
      "authors": [
        "Yuan Gong",
        "Jin Yu",
        "James Glass"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "Edward Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "12",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao",
        "Ye Liu",
        "Bin Liu",
        "Jiangyan Yi",
        "Meng Wang",
        "Erik Cambria",
        "Guoying Zhao",
        "Björn Schuller",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the Findings of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "14",
      "title": "The MSP-conversation corpus",
      "authors": [
        "Luz Martinez-Lucas",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "15",
      "title": "MAGICDATA mandarin Chinese read speech corpus",
      "authors": [
        "Openslr"
      ],
      "year": "2019",
      "venue": "MAGICDATA mandarin Chinese read speech corpus"
    },
    {
      "citation_id": "16",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Reproducing whisper-style training using an open-source toolkit and publicly available data",
      "authors": [
        "Yifan Peng",
        "Jinchuan Tian",
        "Brian Yan",
        "Dan Berrebbi",
        "Xuankai Chang",
        "Xinjian Li",
        "Jiatong Shi",
        "Siddhant Arora",
        "William Chen",
        "Roshan Sharma"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "18",
      "title": "ESC: Dataset for environmental sound classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the Annual ACM Conference on Multimedia (ACM MM)"
    },
    {
      "citation_id": "19",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "20",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "21",
      "title": "Nonspeech7k dataset: Classification and analysis of human non-speech sound",
      "authors": [
        "Guiqing Muhammad Mamunur Rashid",
        "Chengrui Li",
        "Du"
      ],
      "year": "2023",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "The ASRU 2019 Mandarin-English code-switching speech recognition challenge: Open datasets, tracks, methods and results",
      "authors": [
        "Xian Shi",
        "Qiangze Feng",
        "Lei Xie"
      ],
      "year": "2020",
      "venue": "The ASRU 2019 Mandarin-English code-switching speech recognition challenge: Open datasets, tracks, methods and results",
      "arxiv": "arXiv:2007.05916"
    },
    {
      "citation_id": "23",
      "title": "Achieving timestamp prediction while recognizing with non-autoregressive end-to-end ASR model",
      "authors": [
        "Xian Shi",
        "Yanni Chen",
        "Shiliang Zhang",
        "Zhijie Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the National Conference on Man-Machine Speech Communication (NCMMSC)"
    },
    {
      "citation_id": "24",
      "title": "Elastic automatic speech perception that everyone can touch",
      "authors": [
        "Xingchen Song",
        "Chengdong Liang",
        "Binbin Zhang",
        "Pengshen Zhang",
        "Ziyu Wang",
        "Youcheng Ma",
        "Menglong Xu",
        "Lin Wang",
        "Di Wu",
        "Fuping Pan"
      ],
      "year": "2024",
      "venue": "Elastic automatic speech perception that everyone can touch",
      "arxiv": "arXiv:2412.15622"
    },
    {
      "citation_id": "25",
      "title": "PandaGPT: One model to instructionfollow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the Workshop on Taming Large Language Models (TLLM)"
    },
    {
      "citation_id": "26",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Chao Ma Zejun",
        "Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "27",
      "title": "Kespeech: An open source speech dataset of Mandarin and its eight subdialects",
      "authors": [
        "Zhiyuan Tang",
        "Dong Wang",
        "Yanguang Xu",
        "Jianwei Sun",
        "Xiaoning Lei",
        "Shuaijiang Zhao",
        "Cheng Wen",
        "Xingjun Tan",
        "Chuandong Xie",
        "Shuran Zhou"
      ],
      "venue": "Proceedings of the Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "28",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "G Shreya",
        "Woan-Shiuan Upadhyay",
        "Bo-Hao Chien",
        "Lucas Su",
        "Ya-Tse Goncalves",
        "Ali Wu",
        "Carlos Salman",
        "Chi-Chun Busso",
        "Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "30",
      "title": "A large-scale Chinese short-text conversation dataset",
      "authors": [
        "Yida Wang",
        "Pei Ke",
        "Yinhe Zheng",
        "Kaili Huang",
        "Yong Jiang",
        "Xiaoyan Zhu",
        "Minlie Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of the Natural Language Processing and Chinese Computing International Conference (NLPCC)"
    },
    {
      "citation_id": "31",
      "title": "WENETSPEECH: A 10000+ hours multi-domain Mandarin corpus for speech recognition",
      "authors": [
        "Binbin Zhang",
        "Hang Lv",
        "Pengcheng Guo",
        "Qijie Shao",
        "Chao Yang",
        "Lei Xie",
        "Xin Xu",
        "Hui Bu",
        "Xiaoyu Chen",
        "Chenchen Zeng",
        "Di Wu",
        "Zhendong Peng"
      ],
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "33",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}