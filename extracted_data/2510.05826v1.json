{
  "paper_id": "2510.05826v1",
  "title": "Leveraging Vision Transformers For Enhanced Classification Of Emotions Using Ecg Signals",
  "published": "2025-10-07T11:49:57Z",
  "authors": [
    "Pubudu L. Indrasiri",
    "Bipasha Kashyap",
    "Pubudu N. Pathirana"
  ],
  "keywords": [
    "Biomedical signals",
    "vision transformers",
    "ECG",
    "wavelet transform",
    "power spectral density",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Biomedical signals provide insights into various conditions affecting the human body. Beyond diagnostic capabilities, these signals offer a deeper understanding of how specific organs respond to an individual's emotions and feelings. For instance, ECG data can reveal changes in heart rate variability linked to emotional arousal, stress levels, and autonomic nervous system activity. This data offers a window into the physiological basis of our emotional states. Recent advancements in the field diverge from conventional approaches by leveraging the power of advanced transformer architectures, which surpass traditional machine learning and deep learning methods. We begin by assessing the effectiveness of the Vision Transformer (ViT), a forefront model in image classification, for identifying emotions in imaged ECGs. Following this, we present and evaluate an improved version of ViT, integrating both CNN and SE blocks, aiming to bolster performance on imaged ECGs associated with emotion detection. Our method unfolds in two critical phases: first, we apply advanced preprocessing techniques for signal purification and converting signals into interpretable images using continuous wavelet transform and power spectral density analysis; second, we unveil a performance-boosted vision transformer architecture, cleverly enhanced with convolutional neural network components, to adeptly tackle the challenges of emotion recognition. Our methodology's robustness and innovation were thoroughly tested using ECG data from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the YAAD dataset, our approach outperformed existing state-of-the-art methods in classifying seven unique emotional states, as well as in valence and arousal classification. Similarly, in the DREAMER dataset, our method excelled in distinguishing between valence, arousal and dominance, surpassing current leading techniques.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Automatic detection of emotions plays a pivotal role in affective computing, finding successful integration across diverse fields including multimedia applications  [1] , biopsychosocial healthcare systems  [3] , and human-computer interaction (HCI)  [4] . Advancements in wearable technology have significantly boosted research into multisensory data acquisition and analysis for emotion detection [5],  [6] . Multisensory or multimodal data, gathered using various sensors across different modalities, encompass a wide range of inputs such as images of facial expressions, vocal and speech patterns, and physiological signals.\n\nIn the landscape of emotion recognition, biosignal-based methods  [2] ,  [3] ,  [4] ,  [5]  are highly accurate and are not susceptible to being masked, unlike other methods such as facial emotion recognition and speech analysis  [6] . Advances in Human-Computer Interaction (HCI) technologies have led to the creation of sophisticated multimodal databases for emotion recognition  [7] ,  [8] ,  [9] . These databases encompass a wide array of physiological signals, aiming to construct a detailed emotional profile that includes affect (the experience of feeling or emotion), valence (the positive or negative quality of an emotion) and arousal (the level of alertness or excitement). Central to these collections are signals such as electroencephalography (EEG), facial electromyography (EMG), electrocardiography (ECG), and galvanic skin response (GSR). Each modality contributes unique dimensions to emotion recognition, enabling more nuanced and precise interpretations of affective states. Table ?? delineates the salient features of several prominent datasets in this domain, providing a comparative overview of their composition and the physiological signals they encompass. These databases are typically generated under controlled laboratory conditions, where participants' emotions are induced through the viewing of emotionally charged video content.\n\nNotwithstanding the increased accuracy, the deployment of multiple sensors has occasionally resulted in user discomfort or dissatisfaction  [11] . This underscores the necessity of balancing technical precision with practical usability in the design of emotion recognition systems. Within the spectrum of biosignal sensors, the electrocardiogram (ECG) emerges as a predominant choice  [11] ,  [12] . Its ubiquity is grounded in the reliability of ECG signals, which are notably robust against noise and their proven correlation with emotional state.\n\nConventional emotion recognition machine learning techniques, such as Gaussian Naive Bayes, Support Vector Machines, k-Nearest Neighbors, and Random Forests [5],  [7] ,  [13] ,  [14] , rely on expert-driven manual selection of temporal and spectral features. While these methods of feature extraction are intricate, they are hindered by suboptimal predictive accuracy  [15] . Addressing these challenges, emotion recognition has evolved, with deep learning models now at the forefront  [16] ,  [17] ,  [18] , harnessing physiological signals for enhanced performance. The predominant deep learning strategies encompass unimodal and multimodal tasks, with the 1D-CNN  [18]  and hybrid 2D-CNN-LSTM  [19]  frameworks being particularly prevalent. In these CNN-based methods, physiological signals are transformed into visual representations through spectrograms  [20]  and scalograms  [16]  through wavelet transforms before CNN processing.\n\nHowever, conventional convolutional neural network (CNN) methodologies exhibit inherent limitations, particularly in processing complex data with long-range dependencies. This shortcoming is crucial in the context of emotion classification using ECG data, where spatial relationships across the data significantly influence the identification of emotional states. In contrast, Vision Transformers (ViTs)  [21]  effectively capture these long-range dependencies through their self-attention mechanism, allowing for a comprehensive assessment of the entire input space, a critical feature for interpreting ECG images. ViTs dynamically focus on salient features across the dataset, irrespective of their spatial location, adapting effectively to the nuanced demands of emotion recognition from biomedical signals. These architectures have demonstrated utility across multimodal inputs including text, visuals, audio, and physiological data [22],  [23] ,  [24] ,  [25] ,  [26] , and have been extended to general time-series analysis  [27] . Notably, the study by Arjun et al.  [28]  adapted the Vision Transformer for EEG signal interpretation, employing continuous wavelet transform to create image-based signal inputs, demonstrating the versatility and effectiveness of ViTs in signal processing. The integration of ViTs into emotion recognition represents a transformative step towards more accurate and responsive healthcare diagnostics, potentially enhancing patient monitoring and treatment strategies.\n\nIn this study, we present a groundbreaking framework that significantly advances emotion detection from ECG data by leveraging an optimized Vision Transformer architecture. The proposed approach involves transforming ECG signals into a composite three-channel image through",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "In this section, we explore the corpus of related research encompassing multimodal emotion detection, ECG-centric approaches to emotion discernment, and the application of deep learning methodologies within the realm of emotion detection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Emotion Detection",
      "text": "A novel ensemble learning method integrating EEG, ECG, and GSR signals achieved an impressive 94.5% accuracy on the AMIGOS dataset, demonstrating the potential of ensemble approaches in this domain  [2] . Comprehensive reviews provide overviews of emotion classification techniques using ECG and GSR signals, delineating the evolution and effectiveness of these methods  [3] ,  [16] . Practical applications using SVM classifiers on ECG and GSR data have shown varying degrees of success; studies with the MAH-NOB database reported accuracies around 46% for Arousal and 45.5% for Valence  [4] , while another study using the ASCERTAIN database reported slightly higher accuracies  [5] . Additionally, innovative approaches employing deep learning and multimodal models to utilize EEG alongside peripheral physiological signals mark a significant shift towards more sophisticated, accurate, and reliable emotion detection systems  [18] ,  [29] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ecg Based Emotion Detection",
      "text": "Sayed Ismail et al.  [30] . converted ECG data from the DREAMER database into images and obtained an accuracy of 63% for Valence and an accuracy of 58% for Arousal. They further obtained an accuracy of 79% for Valence and an accuracy of 69% for Arousal for numerical ECG data using the SVM classifier, proving that ECG numerical data give better classification accuracy than ECG images. The study  [31]  used a virtual reality headset to allow subjects to view 360-degree video stimuli. They recorded ECG signals from 20 participants using the Empatica E4 wristband. Inter-subject classification achieved 46.7% accuracy for SVM, 42.9% for KNN, and 43.3% for Random Forest. A valence and arousal accuracy of 62.3% was obtained for ECG signals from the DREAMER for emotion classification  [8] . Miranda-Correa et al.  [7]  obtained classification accuracies of 59.7% for Valence and 58.4% for Arousal using ECG data. The study  [17]  developed a deep convolutional neural network with attention mechanisms, achieving improved emotion recognition accuracies using ECG data: 96.5% on the WESAD dataset, 83.6% for arousal and 84.2% for valence on the DREAMER dataset, and 68.0% for arousal and 64.5% for valence on the ASCERTAIN dataset. These results demonstrate the model's effectiveness across multiple datasets. In the study  [32] , Extra Trees and Multi-Layer Perceptron (MLP) algorithms were assessed for ECG-based emotion recognition. On the DREAMER dataset, it excelled in valence prediction (74.6%) and MLP in arousal prediction (74.6%). The study's  [33]  self-supervised model for ECG-based emotion recognition achieved accuracies of 79.6% and 78.3% for arousal and valence in AMIGOS, 77.1% and 74.9% in DREAMER, 95.0% in WESAD, and 92.6%, 93.8%, and 90.2% for arousal, valence, and stress in SWELL, demonstrating robust performance across multiple datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Deep Learning For Emotion Detection",
      "text": "A notable strategy, as detailed by  [18] , involves deploying a 1D Convolutional Neural Network (CNN) for feature extraction and subsequently using a fully connected network (FCN) for emotion classification. An innovative variation by Harper and Southern  [32]  integrates a long-shortterm memory (LSTM) network with a 1D-CNN for a combined approach. In a different tactic, Siddharth et al.  [33]  transform signals into images via spectrograms  [34] , employing a 2D-CNN for extracting features, and an extreme learning machine  [35]  for the classification phase, showcasing the versatility of deep learning in advancing emotion recognition research. The study  [16]  explores emotion classification with CWT features and various CNN models, achieving high accuracy up to 99.19%. The study  [17]  presents a new deep convolutional neural network incorporating attentional mechanisms for ECG emotion recognition.\n\nIn terms of transformer approaches, the study  [34]  presents a self-supervised learning framework using transformers for effective fusion of multimodal data in wearable emotion recognition. The study  [35]  introduces a Transformer-based fusion mechanism for self-supervised multimodal emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Motivations And Contributions",
      "text": "The",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Materials And Methods",
      "text": "Our proposed framework encompasses three distinct phases: 1) Signal preprocessing, 2) Conversion of signals into images, and 3) Application of the images to a performance-enhanced Vision Transformer model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset Descriptions",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Signal Preprocessing",
      "text": "Given s[n] as the raw ECG signal, where n represents the discrete time index, and f s as the sampling frequency, which in this scenario is f s = 128 Hz.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baseline Removel",
      "text": "Initially, the study  [9]  highlighted that the stimulus initiation occurs after the initial five-second interval. Thus, the baseline period in the samples is calculated by Baseline samples = BW × f s , where BW is the baseline duration in seconds, in this scenario 5 seconds. Then, the baseline is calculated as the average value of the signal over the baseline window. If we let b be the baseline, then it can be calculated as:\n\nFinally, the signal with the baseline removed, s br [n], is then calculated by subtracting the baseline from the original signal for each sample, expressed as s br [n] = s[n] -b, and pass to the filtering process.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ecg Filtering",
      "text": "The baseline removed signal (s br ), undergoes a pre-filtering process using a second-order bandpass Butterworth filter. This filter, with cutoff frequencies set at 0.5 Hz and 15 Hz, is applied to mitigate the impact of environmental noise and muscle movements. This ensures the purity of the signal and enhances its suitability for subsequent analysis.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ecg Signal Segmentation",
      "text": "In our study, segmentation involved isolating a full cycle of the ECG signal from the overall waveform.\n\nTo accomplish this, we utilized the PeakUtils Python library to identify the R-peaks within the filtered ECG signal. The parameter 'thres=0.5' specifies the relative threshold for detecting peaks in the signal. A peak is identified if its amplitude is at least 50% of the maximum amplitude observed in the signal after filtering. These peaks served as reference points for segmentation. For each detected R-peak, we segmented the signal by extracting 100 samples to the left and 100 samples to the right of the peak, resulting in segments of a fixed size of 200 samples each. This method ensured consistent segmentation across the ECG dataset for analysis. All the signal processing steps are visualized in Fig.  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ecg Image Encoding",
      "text": "Given our objective to leverage Vision Transformers for analysis, it is imperative to transform the signal data into a visual format. We opted for the wavelet transform approach due to its dual capacity to encapsulate information pertinent to both time and frequency domains. This choice aligns with the intrinsic architecture of Vision Transformers, which necessitates input in an image format, thereby enabling a comprehensive analysis that integrates temporal dynamics with frequency characteristics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Continuous Wavelet Transform",
      "text": "CWT is a powerful tool for time-frequency analysis. Unlike Fourier Transform, which only provides frequency information, CWT maintains both time and frequency information. This makes CWT particularly suited for analyzing signals where the frequency components vary over time, as is often the case with ECG and GSR signals.\n\nFor our analysis, we employed the complex Morlet wavelet, also known as the Gabor wavelet, with 50 band-pass filter banks. This wavelet is renowned for its equal variance in both time and frequency domains, offering a balanced analysis framework. This selection was made to take advantage of the Morlet wavelet's capacity for precise time-frequency localization, essential for capturing the nuanced dynamics of ECG and GSR signals.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Power Spectral Density",
      "text": "The PSD is a common way to analyze the frequency content of a signal, providing insights into the power distribution across various frequency bands. This transformation is particularly useful in understanding the underlying physiological processes and detecting abnormalities in ECG signals.\n\nWelch's method (scipy.signal.welch) divides the signal into overlapping segments, applies a window to each segment, computes the periodogram for each segment, and then averages these periodograms to estimate the PSD. Welch's method can be applied to the entire signal without prior segmentation by the user, as the method itself handles the segmentation internally.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rgb Image Formation",
      "text": "In terms of a single participant, we meticulously applied the wavelet transform to each segmented portion of the signal, thereby producing multiple 2D representations for the individual. Subsequent to this, both the Continuous Wavelet Transform (CWT) and the Power Spectral Density (PSD) analyses were conducted on the entirety of the filtered signal, each yielding distinct 2D visual outputs. These resultant images were then ingeniously amalgamated to form a composite RGB image. This methodological innovation enables a multifaceted visual representation that encapsulates both the time-frequency characteristics and the energy distribution across frequencies of the signal, offering an unparalleled depth of analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Diving Deep Into Vision Transformers",
      "text": "Vision Transformers (ViTs) have been instrumental in advancing the field of computer vision, harnessing the power of self-attention mechanisms, a concept derived from the domain of natural language processing. The essence of ViTs lies in the Multi-Head Self-Attention (MHSA) module, which is particularly effective at capturing longrange dependencies in visual data. Consider an input X ∈ R H×W ×C , where H, W , and C symbolize the height, width, and feature dimension of the input, respectively. This input undergoes a reshaping process, leading to the formulation of the query (Q), key (K), and value (V) matrices as:\n\nHere, W q ∈ R C×C , W k ∈ R C×C , and W v ∈ R C×C represent the learnable weight matrices associated with linear transformations for Q, K, and V, respectively. Assuming a simplistic scenario where the input and output dimensions are equal, the MHSA operation is then depicted as:\n\nIn this equation, √ d is a scaling factor for normalization, and the Softmax function is applied to each row. The product QK T calculates the pairwise similarity score for each token, with the output token being a weighted combination of all tokens, influenced by these scores. Post MHSA, a residual connection is introduced to facilitate the optimization process:\n\nIn equation (3), W p ∈ R C×C is a trainable matrix used for feature projection. The final step involves the application of a Multi-Layer Perceptron (MLP) to enhance the representation:\n\nwhere Y signifies the output of a transformer block.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Proposed Vision Transformer Architecture",
      "text": "In Fig.  1 , we unveil a refined architectural design for the Vision Transformer (ViT), significantly augmenting its performance metrics. This innovative methodology draws inspiration from the groundbreaking ResNet framework, which revolutionized neural network design through the integration of skip connections. To this end, in the advanced architecture delineated in Fig.  1 , termed the ECG Signal Vision Transformer (ES-ViT), we introduce a novel mechanism for preserving the integrity of the original input image throughout the network's processing layers. This is accomplished by strategically positioning a convolutional block in tandem with the primary ViT framework. The convolutional block is ingeniously designed to process the entirety of the input image, subsequently generating a comprehensive embedding. This embedding is meticulously merged with the output from each encoder layer within the Transformer, ensuring that the network retains a holistic representation of the original image following the conclusion of each encoder phase. Initially, the convolutional block processes the input image X to produce a dense representation or embedding E, capturing global contextual information. This embedding process can be succinctly described by the equation E = Conv(X), where Conv(•) denotes the convolutional operation applied to the input image X.\n\nThe resulting output embedding E retains the spatial dimensions of the input with dimensions H × W × C, while potentially altering the channel dimension C to align with the Transformer's input specifications.\n\nTo augment this architecture further, we have integrated the Squeeze-and-Excitation (SE) block, a cutting-edge component known for its ability to enhance performance by recalibrating channelwise feature responses. The SE block is seamlessly incorporated into the convolutional block, where it fine-tunes the embedding of the whole image before the concatenation process. Following the initial embedding, the Squeeze-and-Excitation (SE) block refines this embedding to produce E ′ , an enhanced representation emphasizing critical features while attenuating less relevant ones. This enhancement process can be mathematically described as\n\nwhere F sq (•) represents the squeeze operation that aggregates the embedding features across spatial dimensions to produce a channel-wise descriptor. F ex (•) denotes the excitation operation, applying a self-gating mechanism to recalibrate the channelwise features based on the global information compressed by the squeeze operation.\n\nEach Transformer encoder layer receives an augmented input T ′ i that combines the Transformer's current layer output T i with the enhanced embedding E ′ , facilitating the incorporation of global image context at every layer. This process can be formally described by the equation\n\n, where T i is the output of the i th Transformer encoder layer, E ′ is the enhanced global embedding from the SE block, and ⊕ symbolizes an operation such as concatenation, which in this context is used to integrate E ′ with T i . The choice of integration method ⊕-here specified as concatenation-depends on the architectural design and how the global context is best preserved and utilized within the Transformer layers.\n\nWith E ′ integrated, the attention mechanism in each Transformer encoder layer is adapted to leverage the enhanced global context:\n\nHere, W q , W k , and W v are learnable weight matrices for queries, keys, and values, respectively, within the attention mechanism. d k represents the dimensionality of the key vectors, providing a normalization factor. This adaptation ensures that the attention mechanism dynamically weighs the input features, taking into account both local and global contextual cues.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Final Output Projection",
      "text": "After processing through the attention mechanism, the output A i is projected and combined with the initial embedding E ′ to ensure that each layer contributes to preserving the global context:\n\nwhere MLP(•) represents a Multi-Layer Perceptron applied to the attention mechanism's output, further refining the representation before it is passed to the subsequent layer or used as the final output.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimental Setup",
      "text": "In the investigation of the ECG Signal Vision Transformer (ES-ViT) architecture's efficacy relative to the conventional Vision Transformer (ViT) framework across two distinct ECG datasets, a comprehensive analysis was conducted. This evaluation encompassed comparisons among the Base and Large configurations of both architectures, specifically the B/16, B/32, L/16, and L/32 variants. Leveraging the principles of transfer learning, the ViT components of both the proposed and the original architectures were equipped with pre-trained ImageNet dataset weights, ensuring a robust foundational knowledge base. The novel segments of the ES-ViT model were subjected to a randomized weight initialization, which underwent optimization during the subsequent fine-tuning stages. The adaptation of each model variant to the specificities of the datasets was achieved by tailoring the classifier layer to reflect the dataset's class diversity, employing a holistic end-to-end training regimen for refinement.\n\nIn addition to the direct comparison between the proposed ES-ViT and the original ViT architectures, this study extended its analysis to include evaluations against widely recognized architectures such as ResNet50 and MobileNet, which also benefited from ImageNet pre-trained weights. This multifaceted assessment strategy underscores a comprehensive effort to ascertain the relative performance enhancements offered by the ES-ViT architecture within the realm of imaged ECG analysis, setting a new benchmark in the application of advanced neural network architectures for emotion detection using ECG signals. Our proposed architecture is implemented using PyTorch on an NVIDIA 3070 Ti GPU. To train both networks (signal transformation recognition and emotion recognition), the adam optimizer is used with a learning rate of 0.001 and batch size of 64. The signal transformation recognition network is trained for 30 epochs, while the emotion recognition network is trained for 100 epochs, as steady states are reached with a different number of epochs.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "In our study, we conducted a detailed evaluation of both the novel and established Vision Transformer (ViT) models, specifically the B/16, B/32, L/16, and L/32 configurations as in Table  2  through rigorously designed supervised classification experiments. These experiments were strategically crafted to gauge performance across two distinct electrocardiogram (ECG)-based emotion recognition datasets, each presenting unique classification challenges. The YAAD dataset involves a tripartite classification of emotions, arousal, and valence accuracy, while the DREAMER dataset similarly categorizes arousal, valence, and dominance accuracy. To ensure a thorough evaluation, we assessed all models using a comprehensive suite of metrics: Accuracy, Recall (Sensitivity), Precision, and F1-score, thereby providing a holistic view of each model's capabilities in handling nuanced emotional recognition tasks. The classification performance achieved by the proposed vision transformer models and traditional vision transformer models on YAAD and DREAMER datasets is depicted in Table  3  and 4 respectively.\n\nAccording to the classification results of the YAAD dataset as in Table  3 , all the proposed VIT model variants outperform their respective default VIT variant in terms of most of the matrices. In the emotion category, the ES-VIT-L/32 model stands out with the highest accuracy (75.4%) and F1-score (77.6%), which signifies its robust capability to balance true positive detection with the precision of the classification. This model also achieves the highest recall (77.5%), illustrating its effectiveness in identifying most true positives without a significant number of false negatives. The precision leader in this category is ES-VIT-L/16 (75.7%), indicating a superior ability to minimize false positives in its predictions. For arousal, the ES-VIT-B/32 model shows the highest overall accuracy (77.2%) and the best F1-score (78.8%), demonstrating exceptional consistency and precision in its predictions. This model, alongside the ES-VIT-L/32-which displays the highest precision (78.6%) and recall (76.9%) in the category-demonstrates that larger and enhanced models are particularly adept at handling the complexities involved in recognizing arousal states. Valence detection is best performed by ES-VIT-L/32, which not only achieves the highest accuracy (78.9%) but also scores highly on the F1-score (78.8%), suggesting an exemplary balance between recall and precision. The same model, along with ES-VIT-L/16-which has the highest recall (78.3%) and F1-score (79.8%)-illustrates the superior performance of large models in accurately and consistently categorizing valence, a critical aspect of emotional recognition.\n\nThe proposed VIT model variants also demonstrate superior performance on the DREAMER dataset, as shown in Table  4 . The ES-ViT models, particularly the larger configuration (L/32), demonstrate superior performance across all three emotional dimensions. For instance, the ES-ViT-L/32 model stands out with the highest accuracy in arousal (85.6%) and valence (86.8%), and nearly the highest in dominance (83.1%), underscoring its robustness in complex emotional state recognition tasks. This model also achieves remarkable precision in arousal (84.2%) and consistently high F1-scores, indicating of its excellent balance between recall and precision-essential for reducing false positives and negatives in practical applications. In contrast, the standard ViT models generally exhibit lower performance metrics, highlighting the optimizations in the ES-ViT models that contribute to their improved effectiveness. For example, the ViT-B/16 and ViT-B/32 models show a notable drop in performance in dominance, with accuracy scores of 77.2% and 79.2%, respectively, which could impact their reliability in applications where understanding dominance cues is critical. The enhanced recall in arousal for the   Our experiments utilized the YAAD and DREAMER datasets, renowned benchmarks in the field of emotion detection. The ES-ViT model consistently outperformed established CNN models (ResNet50, MobileNet, VGG-16) and recent state-of-the-art techniques across multiple evaluation metrics, including accuracy, precision, recall, and F1-score. On the YAAD dataset, the ES-ViT-L/32 variant demonstrated exceptional capability in classifying emotion, arousal, and valence, achieving the highest accuracy and F1-scores. On the DREAMER dataset, the ES-ViT-L/32 model excelled in distinguishing arousal, valence, and dominance, surpassing models like CNN-CABM, MLP, Extra Tree, and Self-Supervised models, and achieving the highest metrics. These results highlight the model's robust performance in detecting subtle emotional cues from ECG signals.\n\nThe superior performance of the ES-ViT model has significant implications for the advancement of emotion detection technology. The integration of ViTs with CNN and SE blocks marks a transformative step in emotion recognition, offering a scalable and highly accurate approach to interpreting physiological signals. This advancement is critical for applications in personalized healthcare, mental health monitoring, and adaptive human-computer interactions, potentially enhancing patient monitoring systems, therapeutic interventions, and interactive technologies. Furthermore, our study paves the way for",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: 4.4 ECG Image Encoding",
      "page": 5
    },
    {
      "caption": "Figure 1: Proposed architecture for ECG data classification using vision transformers.",
      "page": 6
    },
    {
      "caption": "Figure 2: Signal Processing steps.",
      "page": 6
    },
    {
      "caption": "Figure 1: , we unveil a refined architectural design",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "†These authors contributed equally to this work.": "Abstract"
        },
        {
          "†These authors contributed equally to this work.": "Biomedical signals provide insights into various conditions affecting the human body. Beyond diag-"
        },
        {
          "†These authors contributed equally to this work.": "nostic capabilities,\nthese signals offer a deeper understanding of how specific organs\nrespond to an"
        },
        {
          "†These authors contributed equally to this work.": "individual’s emotions and feelings. For instance, ECG data can reveal changes in heart rate variability"
        },
        {
          "†These authors contributed equally to this work.": "linked to emotional arousal, stress levels, and autonomic nervous system activity. This data offers a"
        },
        {
          "†These authors contributed equally to this work.": "window into the physiological basis of our emotional states. Recent advancements in the field diverge"
        },
        {
          "†These authors contributed equally to this work.": "from conventional approaches by leveraging the power of advanced transformer architectures, which"
        },
        {
          "†These authors contributed equally to this work.": "surpass traditional machine learning and deep learning methods. We begin by assessing the effective-"
        },
        {
          "†These authors contributed equally to this work.": "ness of the Vision Transformer (ViT), a forefront model in image classification, for identifying emotions"
        },
        {
          "†These authors contributed equally to this work.": "in imaged ECGs. Following this, we present and evaluate an improved version of ViT, integrating both"
        },
        {
          "†These authors contributed equally to this work.": "CNN and SE blocks, aiming to bolster performance on imaged ECGs associated with emotion detec-"
        },
        {
          "†These authors contributed equally to this work.": "tion. Our method unfolds in two critical phases: first, we apply advanced preprocessing techniques for"
        },
        {
          "†These authors contributed equally to this work.": "signal purification and converting signals\ninto interpretable images using continuous wavelet\ntrans-"
        },
        {
          "†These authors contributed equally to this work.": "form and power spectral density analysis; second, we unveil a performance-boosted vision transformer"
        },
        {
          "†These authors contributed equally to this work.": "architecture, cleverly enhanced with convolutional neural network components, to adeptly tackle the"
        },
        {
          "†These authors contributed equally to this work.": "challenges of\nemotion recognition. Our methodology’s\nrobustness and innovation were\nthoroughly"
        },
        {
          "†These authors contributed equally to this work.": "tested using ECG data from the YAAD and DREAMER datasets, leading to remarkable outcomes. For"
        },
        {
          "†These authors contributed equally to this work.": "the YAAD dataset, our approach outperformed existing state-of-the-art methods in classifying seven"
        },
        {
          "†These authors contributed equally to this work.": "unique emotional states, as well as in valence and arousal classification. Similarly,\nin the DREAMER"
        },
        {
          "†These authors contributed equally to this work.": "dataset, our method excelled in distinguishing between valence, arousal and dominance,\nsurpassing"
        },
        {
          "†These authors contributed equally to this work.": "current leading techniques."
        },
        {
          "†These authors contributed equally to this work.": "Keywords: Biomedical signals, vision transformers, ECG, wavelet transform, power spectral density,"
        },
        {
          "†These authors contributed equally to this work.": "emotion recognition"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data acquisition and analysis\nfor emotion detec-": "tion\n[5],\n[6]. Multisensory\nor multimodal\ndata,",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "dered\nby\nsuboptimal\npredictive\naccuracy\n[15]."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "gathered\nusing\nvarious\nsensors\nacross\ndifferent",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "Addressing these challenges, emotion recognition"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "modalities, encompass a wide range of inputs such",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "has evolved, with deep learning models now at the"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "as\nimages of\nfacial expressions, vocal and speech",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "forefront\n[16],[17],\n[18],\nharnessing\nphysiological"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "patterns, and physiological signals.",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "signals\nfor enhanced performance. The predomi-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "In the landscape of emotion recognition, bio-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "nant deep learning strategies encompass unimodal"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "signal-based methods\n[2],\n[3],\n[4],\n[5]\nare highly",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "and multimodal tasks, with the 1D-CNN [18] and"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "accurate and are not susceptible to being masked,",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "hybrid 2D-CNN-LSTM [19] frameworks being par-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "unlike\nother methods\nsuch\nas\nfacial\nemotion",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "ticularly prevalent. In these CNN-based methods,"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "recognition and speech analysis\n[6]. Advances\nin",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "physiological\nsignals are\ntransformed into visual"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Human-Computer Interaction (HCI) technologies",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "representations\nthrough\nspectrograms\n[20]\nand"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "have\nled\nto\nthe\ncreation\nof\nsophisticated mul-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "scalograms [16] through wavelet transforms before"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "timodal\ndatabases\nfor\nemotion\nrecognition\n[7],",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "CNN processing."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "[8],\n[9]. These databases encompass a wide array",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "However,\nconventional\nconvolutional\nneural"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "of\nphysiological\nsignals,\naiming\nto\nconstruct\na",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "network\n(CNN) methodologies\nexhibit\ninherent"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "detailed emotional profile that includes affect (the",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "limitations,\nparticularly\nin\nprocessing\ncomplex"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "experience\nof\nfeeling\nor\nemotion),\nvalence\n(the",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "data with long-range dependencies. This\nshort-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "positive or negative quality of an emotion) and",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "coming is crucial\nin the context of emotion clas-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "arousal\n(the\nlevel\nof\nalertness\nor\nexcitement).",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "sification\nusing ECG data, where\nspatial\nrela-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Central\nto\nthese\ncollections\nare\nsignals\nsuch as",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "tionships\nacross\nthe data\nsignificantly\ninfluence"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "electroencephalography (EEG), facial electromyo-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "the\nidentification\nof\nemotional\nstates.\nIn\ncon-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "graphy (EMG), electrocardiography (ECG), and",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "trast, Vision Transformers\n(ViTs)\n[21] effectively"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "galvanic skin response (GSR). Each modality con-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "capture\nthese\nlong-range\ndependencies\nthrough"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "tributes unique dimensions\nto\nemotion recogni-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "their self-attention mechanism, allowing for a com-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "tion,\nenabling more nuanced and precise\ninter-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "prehensive assessment of\nthe\nentire\ninput\nspace,"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "pretations of affective states. Table ?? delineates",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "a\ncritical\nfeature\nfor\ninterpreting ECG images."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "the salient features of several prominent datasets",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "ViTs dynamically focus on salient features across"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "in this domain, providing a comparative overview",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "the\ndataset,\nirrespective\nof\ntheir\nspatial\nloca-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "of their composition and the physiological signals",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "tion, adapting effectively to the nuanced demands"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "they\nencompass. These\ndatabases\nare\ntypically",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "of\nemotion recognition from biomedical\nsignals."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "generated under controlled laboratory conditions,",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "These\narchitectures\nhave\ndemonstrated\nutility"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "where participants’ emotions are induced through",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "across multimodal\ninputs\nincluding text, visuals,"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "the viewing of emotionally charged video content.",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "audio, and physiological data [22],\n[23],\n[24],\n[25],"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Notwithstanding\nthe\nincreased accuracy,\nthe",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "[26], and have been extended to general time-series"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "deployment of multiple\nsensors has occasionally",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "analysis\n[27]. Notably,\nthe study by Arjun et al."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "resulted in user discomfort or dissatisfaction [11].",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "[28] adapted the Vision Transformer for EEG sig-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "This underscores the necessity of balancing techni-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "nal\ninterpretation, employing continuous wavelet"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "cal precision with practical usability in the design",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "transform to\ncreate\nimage-based\nsignal\ninputs,"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "of emotion recognition systems. Within the spec-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "demonstrating the versatility and effectiveness of"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "trum of biosignal\nsensors,\nthe\nelectrocardiogram",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "ViTs in signal processing. The integration of ViTs"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "(ECG) emerges as a predominant choice [11],\n[12].",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "into\nemotion\nrecognition\nrepresents\na\ntransfor-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Its ubiquity is grounded in the reliability of ECG",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "mative\nstep towards more accurate and respon-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "signals, which are notably robust against noise and",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "sive healthcare diagnostics, potentially enhancing"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "their proven correlation with emotional state.",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "patient monitoring and treatment strategies."
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Conventional\nemotion\nrecognition machine",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "In\nthis\nstudy, we\npresent\na\ngroundbreaking"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "learning\ntechniques,\nsuch\nas\nGaussian\nNaive",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "framework\nthat\nsignificantly\nadvances\nemotion"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Bayes,\nSupport\nVector\nMachines,\nk-Nearest",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "detection from ECG data by leveraging an opti-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "Neighbors, and Random Forests [5],\n[7],\n[13],[14],",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "mized Vision Transformer architecture. The pro-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "rely on expert-driven manual selection of tempo-",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "posed approach involves\ntransforming ECG sig-"
        },
        {
          "data acquisition and analysis\nfor emotion detec-": "ral\nand spectral\nfeatures. While\nthese methods",
          "of\nfeature\nextraction are\nintricate,\nthey are hin-": "nals into a composite three-channel image through"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: SummaryofEmotionRecognitionDatasets",
      "data": [
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "Dataset\nPartic. Modalities\nVideos"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "tion"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "AMIGOS [7]\n40\nEEG, ECG, GSR, Video,"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "Facial Exp."
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "DEAP [10]\n32\nEEG, ECG, GSR, EMG,\n40 (60 sec)"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "Video"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "DREAMER [8]\n23\nEEG, ECG"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "MAHNOB-HCI [4]\n27\nEEG,\nPeripheral\nSig-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "nals, Video"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "YAAD [9]\n25\nECG, GSR\n21 (39 sec)"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "Continuous Wavelet Transform (CWT) and Power"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "Spectral Density\n(PSD)\nand\nthe model\nbuilds"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "on the ViT architecture and introduces a CNN"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "block which integrates with squeeze and excita-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "tion blocks that are used to create an embedding"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "of\nthe full\ninput\nimage, which is\nthen iteratively"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "fed to each Transformer encoder layer by concate-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "nating the image embedding to the output of each"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "transformer\nencoder\nlayer. Rigorously\nvalidated"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "against\nthe ECG component of\nthe YAAD and"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "DREAMER datasets, our methodology not only"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "pioneers\nthe use of Vision Transformers\nfor uni-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "modal physiological\nsignal analysis but also sets"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "a new benchmark in accuracy, surpassing existing"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "state-of-the-art methods."
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "2 Related Works"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "In this\nsection, we explore the corpus of\nrelated"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "research encompassing multimodal emotion detec-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "tion,\nECG-centric\napproaches\nto\nemotion\ndis-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "cernment,\nand the\napplication of deep learning"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "methodologies within the realm of emotion detec-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "tion."
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "2.1 Multimodal Emotion Detection"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": ""
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "A\nnovel\nensemble\nlearning method\nintegrat-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "ing EEG, ECG,\nand GSR signals\nachieved\nan"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "impressive\n94.5% accuracy\non\nthe\nAMIGOS"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "dataset,\ndemonstrating\nthe\npotential\nof\nensem-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "ble approaches in this domain [2]. Comprehensive"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "reviews provide overviews of emotion classification"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "techniques using ECG and GSR signals, delin-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "eating\nthe\nevolution\nand\neffectiveness\nof\nthese"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "methods\n[3],\n[16]. Practical\napplications\nusing"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "SVM classifiers on ECG and GSR data have shown"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "varying degrees of success; studies with the MAH-"
        },
        {
          "Table 1\nSummary of Emotion Recognition Datasets": "NOB database reported accuracies around 46% for"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "Multi-Layer Perceptron (MLP)\nalgorithms were",
          "emerging as a potent tool outperforming Convolu-": "tional Neural Networks (CNNs) in specific scenar-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "assessed for ECG-based emotion recognition. On",
          "emerging as a potent tool outperforming Convolu-": "ios. This shift highlights a promising yet underex-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "the DREAMER dataset, it excelled in valence pre-",
          "emerging as a potent tool outperforming Convolu-": "plored avenue for ECG-based emotion detection,"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "diction (74.6%)\nand MLP in arousal prediction",
          "emerging as a potent tool outperforming Convolu-": "where the unique capabilities of ViTs have not yet"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "(74.6%). The study’s [33] self-supervised model for",
          "emerging as a potent tool outperforming Convolu-": "been applied. Given the sparse research focusing"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "ECG-based emotion recognition achieved accura-",
          "emerging as a potent tool outperforming Convolu-": "solely on ECG signals for emotion recognition. In"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "cies of 79.6% and 78.3% for arousal and valence",
          "emerging as a potent tool outperforming Convolu-": "this regard, our main contributions are as follows:"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "in AMIGOS,\n77.1% and 74.9% in DREAMER,",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "• We\npropose\na\nperformance-enhanced Vision"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "95.0% in WESAD, and 92.6%, 93.8%, and 90.2%",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "Transformer\narchitecture\ntailored\nfor\nECG-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "for\narousal,\nvalence,\nand\nstress\nin\nSWELL,",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "based\nemotion\ndetection,\nleveraging\nspatial-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "demonstrating robust performance across multiple",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "temporal ECG signal characteristics."
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "datasets.",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "• A novel technique for generating three-channel"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "images\nfrom\nECG\nsignals\nis\nintroduced,"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "2.3 Deep Learning for Emotion",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "enabling the application of ViTs\nfor\nimproved"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "detection",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "feature extraction."
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "• The proposed model\nis\nvalidated against\nthe"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "A notable\nstrategy, as detailed by [18],\ninvolves",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "YAAD and DREAMER datasets, demonstrat-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "deploying\na\n1D Convolutional Neural Network",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "ing superior performance over existing methods"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "(CNN)\nfor\nfeature\nextraction and subsequently",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "and establishing a new benchmark in the field."
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "using a fully connected network (FCN)\nfor emo-",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "tion\nclassification. An\ninnovative\nvariation\nby",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "Harper and Southern [32]\nintegrates a long-short-",
          "emerging as a potent tool outperforming Convolu-": "4 Materials and Methods"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "term memory (LSTM) network with a 1D-CNN",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "for a combined approach. In a different tactic, Sid-",
          "emerging as a potent tool outperforming Convolu-": "Our proposed framework encompasses\nthree dis-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "dharth et al.\n[33]\ntransform signals\ninto images",
          "emerging as a potent tool outperforming Convolu-": "tinct phases: 1) Signal preprocessing, 2) Conver-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "via spectrograms\n[34],\nemploying a 2D-CNN for",
          "emerging as a potent tool outperforming Convolu-": "sion of\nsignals\ninto\nimages,\nand 3) Application"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "extracting\nfeatures,\nand\nan\nextreme\nlearning",
          "emerging as a potent tool outperforming Convolu-": "of\nthe images\nto a performance-enhanced Vision"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "machine\n[35]\nfor\nthe\nclassification phase,\nshow-",
          "emerging as a potent tool outperforming Convolu-": "Transformer model."
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "casing the versatility of deep learning in advanc-",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "ing emotion recognition research. The study [16]",
          "emerging as a potent tool outperforming Convolu-": "4.1 Dataset Descriptions"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "explores emotion classification with CWT features",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "4.1.1 YAAD"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "and various CNN models,\nachieving high accu-",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "racy\nup\nto\n99.19%. The\nstudy\n[17]\npresents\na",
          "emerging as a potent tool outperforming Convolu-": "The YAAD dataset, presented by Dar et al.\n[9],"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "new deep convolutional neural network incorpo-",
          "emerging as a potent tool outperforming Convolu-": "contains different biosignals of subjects exposed to"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "rating attentional mechanisms\nfor ECG emotion",
          "emerging as a potent tool outperforming Convolu-": "stimulus of seven different emotions through video"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "recognition.",
          "emerging as a potent tool outperforming Convolu-": "visualization. The YAAD dataset is composed of"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "In terms of transformer approaches, the study",
          "emerging as a potent tool outperforming Convolu-": "two subsets: a single-modal subset which contains"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "[34] presents a self-supervised learning framework",
          "emerging as a potent tool outperforming Convolu-": "ECG signals from 13 subjects up to three rounds"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "using\ntransformers\nfor\neffective\nfusion\nof mul-",
          "emerging as a potent tool outperforming Convolu-": "for some of\nthem, resulting in 154 single-channel"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "timodal\ndata\nin wearable\nemotion\nrecognition.",
          "emerging as a potent tool outperforming Convolu-": "samples;\na multi-modal\nsubset which\ncontains"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "The\nstudy\n[35]\nintroduces\na Transformer-based",
          "emerging as a potent tool outperforming Convolu-": "3\nrounds\nof\nboth ECG and GSR signals\nfrom"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "fusion mechanism for\nself-supervised multimodal",
          "emerging as a potent tool outperforming Convolu-": "another 12 different subjects, resulting in 252 two-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "emotion recognition.",
          "emerging as a potent tool outperforming Convolu-": "channel samples. ECG signals were acquired at a"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "sampling frequency of 128 Hz and have a dura-"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "3 Motivations and",
          "emerging as a potent tool outperforming Convolu-": "tion of 39 s. On the contrary, GSR samples have"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "a\nsampling\nfrequency\nof\n256 Hz\nand the\nsame"
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "Contributions",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "",
          "emerging as a potent tool outperforming Convolu-": "duration."
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "The field of emotion detection from biosignals\nis",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "increasingly gravitating towards computer vision",
          "emerging as a potent tool outperforming Convolu-": ""
        },
        {
          "datasets.\nIn\nthe\nstudy\n[32],\nExtra Trees\nand": "techniques,\nwith\nVision\nTransformers\n(ViTs)",
          "emerging as a potent tool outperforming Convolu-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1.2 DREAMER": "The DREAMER data set is a multimodal emotion",
          "4.3 ECG Signal Segmentation": "In our study, segmentation involved isolating a full"
        },
        {
          "4.1.2 DREAMER": "data set developed by Katsigiannis and Ramzan",
          "4.3 ECG Signal Segmentation": "cycle of the ECG signal from the overall waveform."
        },
        {
          "4.1.2 DREAMER": "[46]. The DREAMER data set\nconsists of EEG",
          "4.3 ECG Signal Segmentation": "To\naccomplish\nthis, we\nutilized\nthe PeakUtils"
        },
        {
          "4.1.2 DREAMER": "and ECG signals from 23 subjects (14 males and",
          "4.3 ECG Signal Segmentation": "Python library to identify the R-peaks within the"
        },
        {
          "4.1.2 DREAMER": "9 females). The participants watched 18 film clips",
          "4.3 ECG Signal Segmentation": "filtered ECG signal. The parameter\n’thres=0.5’"
        },
        {
          "4.1.2 DREAMER": "to elicit nine different\nemotions. After watching",
          "4.3 ECG Signal Segmentation": "specifies the relative threshold for detecting peaks"
        },
        {
          "4.1.2 DREAMER": "a\nclip,\nthe\nself-assessment manikins were\nused",
          "4.3 ECG Signal Segmentation": "in the\nsignal. A peak is\nidentified if\nits\nampli-"
        },
        {
          "4.1.2 DREAMER": "to\nacquire\nassessments\nof\nvalence,\narousal,\nand",
          "4.3 ECG Signal Segmentation": "tude is at\nleast 50% of\nthe maximum amplitude"
        },
        {
          "4.1.2 DREAMER": "dominance.",
          "4.3 ECG Signal Segmentation": "observed in the signal after filtering. These peaks"
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "served as\nreference points\nfor\nsegmentation. For"
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "each detected R-peak, we segmented the signal by"
        },
        {
          "4.1.2 DREAMER": "4.2 Signal Preprocessing",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "extracting 100 samples\nto the left and 100 sam-"
        },
        {
          "4.1.2 DREAMER": "Given s[n] as\nthe raw ECG signal, where n rep-",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "ples to the right of the peak, resulting in segments"
        },
        {
          "4.1.2 DREAMER": "resents\nthe\ndiscrete\ntime\nindex,\nand\nas\nthe\nfs",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "of a fixed size of 200 samples each. This method"
        },
        {
          "4.1.2 DREAMER": "sampling frequency, which in this scenario is fs =",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "ensured consistent\nsegmentation across\nthe ECG"
        },
        {
          "4.1.2 DREAMER": "128 Hz.",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "dataset for analysis. All the signal processing steps"
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "are visualized in Fig. 2."
        },
        {
          "4.1.2 DREAMER": "4.2.1 Baseline Removel",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "Initially,\nthe study [9] highlighted that\nthe stim-",
          "4.3 ECG Signal Segmentation": "4.4 ECG Image Encoding"
        },
        {
          "4.1.2 DREAMER": "ulus\ninitiation occurs after\nthe initial five-second",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "Given our objective to leverage Vision Transform-"
        },
        {
          "4.1.2 DREAMER": "interval. Thus, the baseline period in the samples",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "ers for analysis,\nit is imperative to transform the"
        },
        {
          "4.1.2 DREAMER": "is calculated by Baselinesamples = BW × fs, where",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "signal data into a visual\nformat. We opted for the"
        },
        {
          "4.1.2 DREAMER": "BW is the baseline duration in seconds, in this sce-",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "wavelet transform approach due to its dual capac-"
        },
        {
          "4.1.2 DREAMER": "nario 5 seconds. Then,\nthe baseline is calculated",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "ity to encapsulate information pertinent\nto both"
        },
        {
          "4.1.2 DREAMER": "as the average value of the signal over the baseline",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "time and frequency domains. This\nchoice aligns"
        },
        {
          "4.1.2 DREAMER": "window. If we let b be the baseline, then it can be",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "with the\nintrinsic\narchitecture\nof Vision Trans-"
        },
        {
          "4.1.2 DREAMER": "calculated as:",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "formers, which\nnecessitates\ninput\nin\nan\nimage"
        },
        {
          "4.1.2 DREAMER": "BWsamples−1",
          "4.3 ECG Signal Segmentation": "format, thereby enabling a comprehensive analysis"
        },
        {
          "4.1.2 DREAMER": "1",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "that integrates temporal dynamics with frequency"
        },
        {
          "4.1.2 DREAMER": "(cid:88) n\nb =\ns[n]\n(1)",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "BWsamples",
          "4.3 ECG Signal Segmentation": "characteristics."
        },
        {
          "4.1.2 DREAMER": "=0",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "Finally,\nthe\nsignal with\nthe\nbaseline\nremoved,",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "4.5 Continuous Wavelet Transform"
        },
        {
          "4.1.2 DREAMER": "is then calculated by subtracting the base-\nsbr[n],",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "CWT is a powerful tool\nfor time-frequency analy-"
        },
        {
          "4.1.2 DREAMER": "line\nfrom the\noriginal\nsignal\nfor\neach\nsample,",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "sis. Unlike Fourier Transform, which only provides"
        },
        {
          "4.1.2 DREAMER": "expressed as\nto the\nsbr[n] = s[n] − b, and pass",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "frequency information, CWT maintains both time"
        },
        {
          "4.1.2 DREAMER": "filtering process.",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "and frequency information. This makes CWT par-"
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "ticularly\nsuited for\nanalyzing\nsignals where\nthe"
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "frequency components vary over time, as is often"
        },
        {
          "4.1.2 DREAMER": "4.2.2 ECG Filtering",
          "4.3 ECG Signal Segmentation": ""
        },
        {
          "4.1.2 DREAMER": "",
          "4.3 ECG Signal Segmentation": "the case with ECG and GSR signals."
        },
        {
          "4.1.2 DREAMER": "The baseline\nremoved signal\na\n(sbr), undergoes",
          "4.3 ECG Signal Segmentation": "For\nour\nanalysis, we\nemployed the\ncomplex"
        },
        {
          "4.1.2 DREAMER": "pre-filtering process using\na\nsecond-order band-",
          "4.3 ECG Signal Segmentation": "Morlet wavelet, also known as the Gabor wavelet,"
        },
        {
          "4.1.2 DREAMER": "pass Butterworth filter. This\nfilter, with cutoff",
          "4.3 ECG Signal Segmentation": "with 50 band-pass filter banks. This wavelet\nis"
        },
        {
          "4.1.2 DREAMER": "frequencies set at 0.5 Hz and 15 Hz,\nis applied to",
          "4.3 ECG Signal Segmentation": "renowned\nfor\nits\nequal\nvariance\nin\nboth\ntime"
        },
        {
          "4.1.2 DREAMER": "mitigate\nthe\nimpact of\nenvironmental noise and",
          "4.3 ECG Signal Segmentation": "and frequency domains, offering a balanced anal-"
        },
        {
          "4.1.2 DREAMER": "muscle movements. This ensures the purity of the",
          "4.3 ECG Signal Segmentation": "ysis\nframework. This\nselection was made to take"
        },
        {
          "4.1.2 DREAMER": "signal and enhances its suitability for subsequent",
          "4.3 ECG Signal Segmentation": "advantage\nof\nthe Morlet wavelet’s\ncapacity\nfor"
        },
        {
          "4.1.2 DREAMER": "analysis.",
          "4.3 ECG Signal Segmentation": "precise\ntime-frequency localization,\nessential\nfor"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "bands. This\ntransformation\nis\nparticularly\nuse-"
        },
        {
          "Fig. 1": "RAW\nSIGNAL",
          "Proposed architecture for ECG data classification using vision transformers.": "ful\nin understanding the underlying physiological"
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "processes\nand\ndetecting\nabnormalities\nin ECG"
        },
        {
          "Fig. 1": "FILTERED\nSIGNAL",
          "Proposed architecture for ECG data classification using vision transformers.": "signals."
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "Welch’s method\n(scipy.signal.welch)\ndivides"
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "the\nsignal\ninto\noverlapping\nsegments,\napplies\na"
        },
        {
          "Fig. 1": "SEGMENTED\nSIGNAL",
          "Proposed architecture for ECG data classification using vision transformers.": "100\n100\nwindow to\neach\nsegment,\ncomputes\nthe\nperi-"
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "odogram for\neach\nsegment,\nand\nthen\naverages"
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "these periodograms to estimate the PSD. Welch’s"
        },
        {
          "Fig. 1": "Fig. 2",
          "Proposed architecture for ECG data classification using vision transformers.": "Signal Processing steps."
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "method can be applied to the entire signal with-"
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "out prior segmentation by the user, as the method"
        },
        {
          "Fig. 1": "capturing the nuanced dynamics of ECG and GSR",
          "Proposed architecture for ECG data classification using vision transformers.": ""
        },
        {
          "Fig. 1": "",
          "Proposed architecture for ECG data classification using vision transformers.": "itself handles the segmentation internally."
        },
        {
          "Fig. 1": "signals.",
          "Proposed architecture for ECG data classification using vision transformers.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2D representations for the individual. Subsequent": "to this, both the Continuous Wavelet Transform"
        },
        {
          "2D representations for the individual. Subsequent": "(CWT)\nand the Power Spectral Density (PSD)"
        },
        {
          "2D representations for the individual. Subsequent": "analyses were\nconducted on the\nentirety of\nthe"
        },
        {
          "2D representations for the individual. Subsequent": "filtered\nsignal,\neach\nyielding\ndistinct\n2D visual"
        },
        {
          "2D representations for the individual. Subsequent": "outputs. These resultant\nimages were then inge-"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "niously amalgamated to form a composite RGB"
        },
        {
          "2D representations for the individual. Subsequent": "image. This methodological\ninnovation enables a"
        },
        {
          "2D representations for the individual. Subsequent": "multifaceted visual\nrepresentation that\nencapsu-"
        },
        {
          "2D representations for the individual. Subsequent": "lates both the time-frequency characteristics and"
        },
        {
          "2D representations for the individual. Subsequent": "the energy distribution across\nfrequencies of\nthe"
        },
        {
          "2D representations for the individual. Subsequent": "signal, offering an unparalleled depth of analysis."
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "4.8 Diving Deep into Vision"
        },
        {
          "2D representations for the individual. Subsequent": "Transformers"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "Vision Transformers (ViTs) have been instrumen-"
        },
        {
          "2D representations for the individual. Subsequent": "tal\nin advancing the field of computer vision, har-"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "nessing\nthe power\nof\nself-attention mechanisms,"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "a\nconcept\nderived\nfrom the\ndomain\nof\nnatural"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "language processing. The essence of ViTs\nlies\nin"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "the Multi-Head Self-Attention (MHSA) module,"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "which is particularly effective at capturing long-"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "range dependencies\nin visual data. Consider\nan"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "input X ∈ RH×W ×C, where H, W , and C sym-"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "bolize\nthe height, width, and feature dimension"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "of the input, respectively. This input undergoes a"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "reshaping process,\nleading to the\nformulation of"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "the query (Q), key (K), and value (V) matrices as:"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "X ∈ RH×W ×C → X ∈ R(H×W )×C,"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "(1)\nQ = XWq,\nK = XWk,\nV = XWv,"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "Here, Wq ∈ RC×C, Wk ∈ RC×C, and Wv ∈"
        },
        {
          "2D representations for the individual. Subsequent": "RC×C\nrepresent\nthe\nlearnable weight matrices"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "associated with linear\ntransformations\nfor Q, K,"
        },
        {
          "2D representations for the individual. Subsequent": "and V,\nrespectively. Assuming\na\nsimplistic\nsce-"
        },
        {
          "2D representations for the individual. Subsequent": "nario where the input and output dimensions are"
        },
        {
          "2D representations for the individual. Subsequent": "equal, the MHSA operation is then depicted as:"
        },
        {
          "2D representations for the individual. Subsequent": ""
        },
        {
          "2D representations for the individual. Subsequent": "(cid:19)\n(cid:18) QK T"
        },
        {
          "2D representations for the individual. Subsequent": "√"
        },
        {
          "2D representations for the individual. Subsequent": "A = Softmax\nV,\n(2)"
        },
        {
          "2D representations for the individual. Subsequent": "d"
        },
        {
          "2D representations for the individual. Subsequent": "√"
        },
        {
          "2D representations for the individual. Subsequent": "In this equation,\nd is a scaling factor for nor-"
        },
        {
          "2D representations for the individual. Subsequent": "malization, and the Softmax function is applied"
        },
        {
          "2D representations for the individual. Subsequent": "to\neach row. The product QK T\ncalculates\nthe"
        },
        {
          "2D representations for the individual. Subsequent": "pairwise similarity score for each token, with the"
        },
        {
          "2D representations for the individual. Subsequent": "output token being a weighted combination of all"
        },
        {
          "2D representations for the individual. Subsequent": "tokens,\ninfluenced by these scores. Post MHSA, a"
        },
        {
          "2D representations for the individual. Subsequent": "residual connection is introduced to facilitate the"
        },
        {
          "2D representations for the individual. Subsequent": "optimization process:"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To augment this architecture further, we have": "integrated the Squeeze-and-Excitation (SE) block,",
          "dimensionality of the key vectors, providing a nor-": "malization factor. This\nadaptation ensures\nthat"
        },
        {
          "To augment this architecture further, we have": "a\ncutting-edge\ncomponent known for\nits\nability",
          "dimensionality of the key vectors, providing a nor-": "the attention mechanism dynamically weighs the"
        },
        {
          "To augment this architecture further, we have": "to enhance performance by recalibrating channel-",
          "dimensionality of the key vectors, providing a nor-": "input features, taking into account both local and"
        },
        {
          "To augment this architecture further, we have": "wise\nfeature\nresponses. The\nSE block\nis\nseam-",
          "dimensionality of the key vectors, providing a nor-": "global contextual cues."
        },
        {
          "To augment this architecture further, we have": "lessly incorporated into the\nconvolutional block,",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "where\nit fine-tunes\nthe\nembedding of\nthe whole",
          "dimensionality of the key vectors, providing a nor-": "4.9.1 Final Output Projection"
        },
        {
          "To augment this architecture further, we have": "image before the concatenation process. Following",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "After\nprocessing\nthrough\nthe\nattention mecha-"
        },
        {
          "To augment this architecture further, we have": "the initial embedding, the Squeeze-and-Excitation",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "nism,\nis projected and combined\nthe output Ai"
        },
        {
          "To augment this architecture further, we have": "(SE)\nblock\nrefines\nthis\nembedding\nto\nproduce",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "with the initial embedding E′ to ensure that each"
        },
        {
          "To augment this architecture further, we have": "E′, an enhanced representation emphasizing crit-",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "layer contributes to preserving the global context:"
        },
        {
          "To augment this architecture further, we have": "ical\nfeatures while attenuating less relevant ones.",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "This\nenhancement\nprocess\ncan\nbe mathemati-",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "(4)\nYi = MLP(Ai) + E′,"
        },
        {
          "To augment this architecture further, we have": "cally described as E′ = SE(E) = Fex(Fsq(E)),",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "where MLP(·)\nrepresents a Multi-Layer Per-"
        },
        {
          "To augment this architecture further, we have": "where Fsq(·) represents the squeeze operation that",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "ceptron applied to the attention mechanism’s out-"
        },
        {
          "To augment this architecture further, we have": "aggregates\nthe embedding features across\nspatial",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "put, further refining the representation before it is"
        },
        {
          "To augment this architecture further, we have": "dimensions to produce a channel-wise descriptor.",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "passed to the subsequent layer or used as the final"
        },
        {
          "To augment this architecture further, we have": "Fex(·) denotes the excitation operation, applying a",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "output."
        },
        {
          "To augment this architecture further, we have": "self-gating mechanism to recalibrate the channel-",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "wise\nfeatures\nbased\non\nthe\nglobal\ninformation",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "compressed by the squeeze operation.",
          "dimensionality of the key vectors, providing a nor-": "5 Experimental Setup"
        },
        {
          "To augment this architecture further, we have": "Each Transformer\nencoder\nlayer\nreceives\nan",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "augmented\ninput\nT ′\nthat\ncombines\nthe Trans-",
          "dimensionality of the key vectors, providing a nor-": "In\nthe\ninvestigation\nof\nthe ECG Signal Vision"
        },
        {
          "To augment this architecture further, we have": "i",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "Transformer (ES-ViT) architecture’s efficacy rela-"
        },
        {
          "To augment this architecture further, we have": "former’s current layer output Ti with the enhanced",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "embedding E′,\nfacilitating\nthe\nincorporation\nof",
          "dimensionality of the key vectors, providing a nor-": "tive to the conventional Vision Transformer (ViT)"
        },
        {
          "To augment this architecture further, we have": "global\nimage\ncontext\nat\nevery\nlayer. This\npro-",
          "dimensionality of the key vectors, providing a nor-": "framework\nacross\ntwo\ndistinct\nECG datasets,"
        },
        {
          "To augment this architecture further, we have": "cess\ncan be\nformally described by the\nequation",
          "dimensionality of the key vectors, providing a nor-": "a\ncomprehensive\nanalysis was\nconducted. This"
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "evaluation encompassed comparisons among the"
        },
        {
          "To augment this architecture further, we have": "is\nthe\noutput\ni = Concat(Ti ⊕ E′), where Ti",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "of\nthe\nith Transformer\nencoder\nlayer, E′\nis\nthe",
          "dimensionality of the key vectors, providing a nor-": "Base and Large\nconfigurations of both architec-"
        },
        {
          "To augment this architecture further, we have": "enhanced global\nembedding from the SE block,",
          "dimensionality of the key vectors, providing a nor-": "tures, specifically the B/16, B/32, L/16, and L/32"
        },
        {
          "To augment this architecture further, we have": "and ⊕ symbolizes an operation such as concatena-",
          "dimensionality of the key vectors, providing a nor-": "variants.\nLeveraging\nthe\nprinciples\nof\ntransfer"
        },
        {
          "To augment this architecture further, we have": "tion, which in this context is used to integrate E′",
          "dimensionality of the key vectors, providing a nor-": "learning,\nthe ViT components\nof both the pro-"
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "posed and the original architectures were equipped"
        },
        {
          "To augment this architecture further, we have": "with Ti. The choice of integration method ⊕—here",
          "dimensionality of the key vectors, providing a nor-": ""
        },
        {
          "To augment this architecture further, we have": "specified as concatenation—depends on the archi-",
          "dimensionality of the key vectors, providing a nor-": "with pre-trained ImageNet dataset weights, ensur-"
        },
        {
          "To augment this architecture further, we have": "tectural\ndesign\nand\nhow the\nglobal\ncontext\nis",
          "dimensionality of the key vectors, providing a nor-": "ing\na\nrobust\nfoundational knowledge base. The"
        },
        {
          "To augment this architecture further, we have": "best preserved and utilized within the Transformer",
          "dimensionality of the key vectors, providing a nor-": "novel\nsegments of\nthe ES-ViT model were\nsub-"
        },
        {
          "To augment this architecture further, we have": "layers.",
          "dimensionality of the key vectors, providing a nor-": "jected to a randomized weight initialization, which"
        },
        {
          "To augment this architecture further, we have": "With E′\nintegrated, the attention mechanism",
          "dimensionality of the key vectors, providing a nor-": "underwent\noptimization\nduring\nthe\nsubsequent"
        },
        {
          "To augment this architecture further, we have": "in each Transformer encoder\nlayer\nis adapted to",
          "dimensionality of the key vectors, providing a nor-": "fine-tuning stages. The adaptation of each model"
        },
        {
          "To augment this architecture further, we have": "leverage the enhanced global context:",
          "dimensionality of the key vectors, providing a nor-": "variant\nto\nthe\nspecificities\nof\nthe\ndatasets was"
        },
        {
          "To augment this architecture further, we have": "",
          "dimensionality of the key vectors, providing a nor-": "achieved by tailoring the classifier layer to reflect"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: The ES-ViT mod-",
      "data": [
        {
          "the\nrelative\nperformance\nenhancements\noffered": "by the ES-ViT architecture within the\nrealm of",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "true positives without a significant number of false"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "imaged ECG analysis, setting a new benchmark in",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "negatives. The precision leader in this category is"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "the application of advanced neural network archi-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "ES-VIT-L/16 (75.7%),\nindicating a superior abil-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "tectures for emotion detection using ECG signals.",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "ity to minimize false positives\nin its predictions."
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "Our proposed architecture\nis\nimplemented using",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "For arousal,\nthe ES-VIT-B/32 model\nshows\nthe"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "PyTorch on an NVIDIA 3070 Ti GPU. To train",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "highest\noverall\naccuracy\n(77.2%)\nand\nthe\nbest"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "both networks (signal transformation recognition",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "F1-score (78.8%), demonstrating exceptional con-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "and emotion recognition),\nthe adam optimizer\nis",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "sistency\nand\nprecision\nin\nits\npredictions. This"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "used with a learning rate of 0.001 and batch size",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "model,\nalongside\nthe ES-VIT-L/32—which\ndis-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "of 64. The signal transformation recognition net-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "plays\nthe\nhighest\nprecision\n(78.6%)\nand\nrecall"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "work is\ntrained for 30 epochs, while the emotion",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "(76.9%) in the category—demonstrates that larger"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "recognition network is trained for 100 epochs, as",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "and enhanced models\nare particularly\nadept\nat"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "steady states are reached with a different number",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "handling the complexities involved in recognizing"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "of epochs.",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "arousal states. Valence detection is best performed"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "by ES-VIT-L/32, which\nnot\nonly\nachieves\nthe"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "6 Results",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "highest accuracy (78.9%) but also scores highly"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "on\nthe F1-score\n(78.8%),\nsuggesting\nan\nexem-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "In\nour\nstudy, we\nconducted\na\ndetailed\nevalua-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "plary balance between recall and precision. The"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "tion\nof\nboth\nthe\nnovel\nand\nestablished Vision",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "same model,\nalong with\nES-VIT-L/16—which"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "Transformer (ViT) models, specifically the B/16,",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "has\nthe\nhighest\nrecall\n(78.3%)\nand\nF1-score"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "B/32, L/16, and L/32 configurations as in Table 2",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "(79.8%)—illustrates\nthe\nsuperior performance of"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "through rigorously designed supervised classifica-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "large models\nin accurately and consistently cat-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "tion experiments. These experiments were strate-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "egorizing valence, a critical aspect of\nemotional"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "gically crafted to gauge performance across\ntwo",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "recognition."
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "distinct electrocardiogram (ECG)-based emotion",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "The proposed VIT model variants also demon-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "recognition datasets, each presenting unique clas-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "strate\nsuperior performance on the DREAMER"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "sification challenges. The YAAD dataset involves",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "dataset, as\nshown in Table 4. The ES-ViT mod-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "a tripartite classification of emotions, arousal, and",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "els, particularly the\nlarger\nconfiguration (L/32),"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "valence accuracy, while\nthe DREAMER dataset",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "demonstrate superior performance across all three"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "similarly categorizes arousal, valence, and domi-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "emotional dimensions. For instance, the ES-ViT-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "nance accuracy. To ensure a thorough evaluation,",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "L/32 model stands out with the highest accuracy"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "we\nassessed\nall models\nusing\na\ncomprehensive",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "in arousal (85.6%) and valence (86.8%), and nearly"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "suite\nof metrics: Accuracy, Recall\n(Sensitivity),",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "the highest\nin dominance\n(83.1%), underscoring"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "Precision, and F1-score, thereby providing a holis-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "its\nrobustness\nin complex emotional\nstate recog-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "tic view of each model’s capabilities\nin handling",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "nition\ntasks. This model\nalso\nachieves\nremark-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "nuanced emotional\nrecognition tasks. The\nclas-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "able precision in arousal (84.2%) and consistently"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "sification performance achieved by the proposed",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "high F1-scores,\nindicating\nof\nits\nexcellent\nbal-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "vision transformer models and traditional vision",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "ance between recall\nand precision—essential\nfor"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "transformer models\non YAAD and DREAMER",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "reducing false positives and negatives in practical"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "datasets is depicted in Table 3 and 4 respectively.",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "applications. In contrast, the standard ViT mod-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "According to the\nclassification results of\nthe",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "els generally exhibit\nlower performance metrics,"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "YAAD dataset\nas\nin Table\n3,\nall\nthe proposed",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "highlighting the optimizations in the ES-ViT mod-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "VIT model variants outperform their\nrespective",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "els that contribute to their improved effectiveness."
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "default VIT variant\nin\nterms\nof most\nof\nthe",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "For example, the ViT-B/16 and ViT-B/32 models"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "matrices.\nIn the\nemotion category,\nthe ES-VIT-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "show a\nnotable\ndrop\nin\nperformance\nin\ndomi-"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "L/32 model stands out with the highest accuracy",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "nance, with accuracy scores of 77.2% and 79.2%,"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "(75.4%) and F1-score (77.6%), which signifies its",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "respectively, which could impact their reliability in"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "robust capability to balance true positive detec-",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "applications where understanding dominance cues"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "tion with the precision of\nthe classification. This",
          "illustrating\nits\neffectiveness\nin\nidentifying most": "is critical. The enhanced recall\nin arousal\nfor the"
        },
        {
          "the\nrelative\nperformance\nenhancements\noffered": "model\nalso\nachieves\nthe highest\nrecall\n(77.5%),",
          "illustrating\nits\neffectiveness\nin\nidentifying most": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: SpecificationsandnumberofparametersforVisionTransformerconfigurations.",
      "data": [
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "Model"
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": ""
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "ViT-B/16"
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "ES-ViT-B/16"
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "ViT-B/32"
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "ViT-L/16"
        },
        {
          "Specifications and number of parameters for Vision Transformer configurations.": "ViT-L/32"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: SpecificationsandnumberofparametersforVisionTransformerconfigurations.",
      "data": [
        {
          "Emotion": "",
          "Arousal": "",
          "Valence": ""
        },
        {
          "Emotion": "Prec.",
          "Arousal": "Prec.",
          "Valence": "Prec."
        },
        {
          "Emotion": "73.7",
          "Arousal": "78.6",
          "Valence": "65.4"
        },
        {
          "Emotion": "73.3",
          "Arousal": "73.7",
          "Valence": "69.8"
        },
        {
          "Emotion": "75.7",
          "Arousal": "78.7",
          "Valence": "75.6"
        },
        {
          "Emotion": "75.1",
          "Arousal": "78.6",
          "Valence": "77.6"
        },
        {
          "Emotion": "70.6",
          "Arousal": "71.2",
          "Valence": "64.1"
        },
        {
          "Emotion": "72.3",
          "Arousal": "73.5",
          "Valence": "67.9"
        },
        {
          "Emotion": "70.3",
          "Arousal": "72.8",
          "Valence": "72.5"
        },
        {
          "Emotion": "72.1",
          "Arousal": "72.5",
          "Valence": "72.5"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: SpecificationsandnumberofparametersforVisionTransformerconfigurations.",
      "data": [
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "ViT-L/32\n72.4\n72.1\n74.5\n75.1\n73.9",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "72.5\n75.6\n76.1\n72.4\n72.5\n73.1\n75.2"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "ES-ViT-L/16 model (85.2%)—the highest among",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "establishing\nit\nas\nthe\nsuperior model\nfor ECG-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "all\nthe models—suggests a particular\nsensitivity",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "based emotion detection. We have\nselected this"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "to\ncorrectly identifying\ntrue positives,\na\ncrucial",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "model\nfor detailed comparative analysis."
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "capability in scenarios where missing an emotional",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "The comparison results for the YAAD dataset"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "cue could have significant\nrepercussions,\nsuch as",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "reveal that our model demonstrates superior accu-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "in mental health assessments. Furthermore,\nthe",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "racy, precision,\nrecall, and F1-score across Emo-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "consistently high scores in valence for the ES-ViT-",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "tion, Arousal, and Valence dimensions when com-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "L/32 model, with the highest F1-score of 85.6%,",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "pared to established CNN models like ResNet50,"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "reflect its adeptness at balancing the precision and",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "MobileNet, and VGG-16. Our model achieves the"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "recall\nin emotionally nuanced environments, mak-",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "highest\nscores,\nindicating its\nrobust performance"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "ing\nit\nespecially\nsuitable\nfor\ncontexts\nrequiring",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "in emotion detection using ECG signals."
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "fine-grained emotion detection,\nlike personalized",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "For\nthe DREAMER dataset,\nthe ES-ViT/32"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "interaction systems or therapeutic settings.",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "model excels in the Arousal, Valence, and Domi-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "nance categories, achieving higher scores in accu-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "racy,\nprecision,\nrecall,\nand\nF1-score\ncompared"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "6.1 Comparison to Established",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "to\nother models\nincluding ResNet, MobileNet,"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "Models",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "and VGG-16.\nIt\nalso\noutperforms\nspecific mod-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "This\nsection\npresents\na\ncomprehensive\nper-",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "els cited in recent\nstudies,\nsuch as CNN-CABM,"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "formance\ncomparison\nof\nour\noptimal\nmodel,",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "MLP, Extra Tree,\nand\nSelf-Supervised models."
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "Enhanced\nCardiovascular\nVision\nTransformer",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "The\nresults demonstrate\nthe model’s\nrobustness"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "(ES-ViT/32),\nagainst\nestablished CNN models",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "and superior performance in ECG-based emotion"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "and prior studies on the YAAD and DREAMER",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "detection."
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "datasets. According to the results, the ES-ViT/32",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "These\nevaluations\nhighlight\nthe ES-ViT/32"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "model\noutperforms\nothers\nacross most metrics,",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": ""
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "model’s ability to accurately and efficiently dis-"
        },
        {
          "ViT-L/16\n71.2\n70.3\n72.3\n73.4\n73.1": "",
          "72.8\n74.5\n74.1\n72.4\n72.5\n72.7\n73.5": "cern emotional states from ECG data, making it a"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: PerformanceComparisonofDifferentViTVariantsonArousal,Valence,andDominanceontheDREAMER",
      "data": [
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ViT-L/32\n83.1\n81.7\n83.8\n83.1\n83.2",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "83.1\n83.8\n82.5\n80.4\n79.4\n79.9\n79.9"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "significant advancement in the field. The detailed",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "feature representation, addressing the limitations"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "comparison underscores\nits potential\nfor applica-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "of conventional CNN-based methods."
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "tions in healthcare and affective computing, where",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "Our\nexperiments\nutilized\nthe\nYAAD\nand"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "precise emotion recognition is crucial. The signifi-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "DREAMER datasets,\nrenowned\nbenchmarks\nin"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "cant advancements our model offers over existing",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "the field of emotion detection. The ES-ViT model"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "approaches reinforce its efficiency and accuracy in",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "consistently outperformed established CNN mod-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "discerning emotional states from ECG data.",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "els\n(ResNet50, MobileNet, VGG-16)\nand recent"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "state-of-the-art techniques across multiple evalua-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "tion metrics,\nincluding accuracy, precision, recall,"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "7 Conclusion",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": ""
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "and F1-score. On the YAAD dataset, the ES-ViT-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "In this\ncomprehensive\nstudy, we\nintroduced the",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "L/32\nvariant demonstrated exceptional\ncapabil-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "Enhanced ECG Signal Vision Transformer\n(ES-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "ity in classifying emotion, arousal, and valence,"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ViT), a groundbreaking model\nfor emotion detec-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "achieving the highest accuracy and F1-scores. On"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "tion using ECG signals. Our approach represents",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "the DREAMER dataset, the ES-ViT-L/32 model"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "a significant advancement over\ntraditional meth-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "excelled\nin\ndistinguishing\narousal,\nvalence,\nand"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ods\nby\ncombining\nsophisticated\nsignal\nprocess-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "dominance,\nsurpassing models\nlike CNN-CABM,"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ing techniques with state-of-the-art deep learning",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "MLP, Extra Tree, and Self-Supervised models, and"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "architectures\nto improve the accuracy and relia-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "achieving the highest metrics. These results high-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "bility of\nemotion recognition. The methodology",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "light the model’s robust performance in detecting"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "comprised\ntwo\ncritical\nphases:\nadvanced\nsignal",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "subtle emotional cues from ECG signals."
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "preprocessing and image conversion,\nfollowed by",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "The\nsuperior\nperformance\nof\nthe\nES-ViT"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "the application of an enhanced Vision Transformer",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "model has significant implications for the advance-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "architecture. We meticulously\npreprocessed\nthe",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "ment of emotion detection technology. The inte-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ECG signals\nto\nensure purity\nand transformed",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "gration of ViTs with CNN and SE blocks marks a"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "them into interpretable images using Continuous",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "transformative step in emotion recognition, offer-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "Wavelet Transform (CWT)\nand Power Spectral",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "ing\na\nscalable\nand highly accurate\napproach to"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "Density (PSD) analysis. This dual approach cap-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "interpreting physiological\nsignals. This advance-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "tures both temporal and frequency domain infor-",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "ment\nis\ncritical\nfor\napplications\nin\npersonal-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "mation,\nproviding\na\nrich\nrepresentation\nof\nthe",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "ized healthcare, mental health monitoring,\nand"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "ECG data. The ES-ViT model, which integrates",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "adaptive\nhuman-computer\ninteractions,\npoten-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "convolutional neural network (CNN) components",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "tially enhancing patient monitoring systems, ther-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "and squeeze-and-excitation (SE) blocks\ninto the",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "apeutic\ninterventions,\nand\ninteractive\ntechnolo-"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "Vision Transformer\n(ViT)\nframework,\neffectively",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": "gies. Furthermore, our\nstudy paves\nthe way for"
        },
        {
          "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1": "captures\nlong-range dependencies\nand enhances",
          "84.3\n82.6\n82.9\n78.5\n79.9\n80.3\n80.1": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: PerformanceComparisonofDifferentViTVariantsonArousal,Valence,andDominanceontheDREAMER",
      "data": [
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Dataset."
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Arousal"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Models"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Acc.\nPrec.\nRec.\nF1\nAcc."
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ES-ViT-B/16\n82.1\n83.4\n82.3\n82.4\n82.9"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ES-ViT-B/32\n84.3\n84.1\n83.4\n84.6\n83.1"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ES-ViT-L/16\n84.1\n83.1\n85.2\n84.7\n84.1"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ES-ViT-L/32\n85.6\n84.2\n84.8\n83.9\n86.8"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ViT-B/16\n81.1\n81.4\n83.2\n81.7\n80.2"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ViT-B/32\n82.1\n81.6\n82.3\n80.2\n81.1"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ViT-L/16\n82.3\n81.2\n81.5\n80.9\n82.1"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ViT-L/32\n83.1\n81.7\n83.8\n83.1\n83.2"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "significant advancement in the field. The detailed"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "comparison underscores\nits potential\nfor applica-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "tions in healthcare and affective computing, where"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "precise emotion recognition is crucial. The signifi-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "cant advancements our model offers over existing"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "approaches reinforce its efficiency and accuracy in"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "discerning emotional states from ECG data."
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": ""
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": ""
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "7 Conclusion"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": ""
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "In this\ncomprehensive\nstudy, we\nintroduced the"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Enhanced ECG Signal Vision Transformer\n(ES-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ViT), a groundbreaking model\nfor emotion detec-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "tion using ECG signals. Our approach represents"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "a significant advancement over\ntraditional meth-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ods\nby\ncombining\nsophisticated\nsignal\nprocess-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ing techniques with state-of-the-art deep learning"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "architectures\nto improve the accuracy and relia-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "bility of\nemotion recognition. The methodology"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "comprised\ntwo\ncritical\nphases:\nadvanced\nsignal"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "preprocessing and image conversion,\nfollowed by"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "the application of an enhanced Vision Transformer"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "architecture. We meticulously\npreprocessed\nthe"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ECG signals\nto\nensure purity\nand transformed"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "them into interpretable images using Continuous"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Wavelet Transform (CWT)\nand Power Spectral"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Density (PSD) analysis. This dual approach cap-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "tures both temporal and frequency domain infor-"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "mation,\nproviding\na\nrich\nrepresentation\nof\nthe"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "ECG data. The ES-ViT model, which integrates"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "convolutional neural network (CNN) components"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "and squeeze-and-excitation (SE) blocks\ninto the"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "Vision Transformer\n(ViT)\nframework,\neffectively"
        },
        {
          "Table 4 Performance Comparison of Different ViT Variants on Arousal, Valence, and Dominance on the DREAMER": "captures\nlong-range dependencies\nand enhances"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: PerformanceEvaluationofOurBestModelComparedwithState-of-the-ArtonEmotion,Arousal,andValence",
      "data": [
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "Using the YAAD Dataset."
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": ""
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "Models"
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": ""
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "Ours"
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "ResNet-50"
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "MobileNet"
        },
        {
          "Table 5 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Emotion, Arousal, and Valence": "VGG-16"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 5: PerformanceEvaluationofOurBestModelComparedwithState-of-the-ArtonEmotion,Arousal,andValence",
      "data": [
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "Dominance Using the DREAMER Dataset."
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": ""
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "Models"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": ""
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "Ours"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "ResNet"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "MobileNet"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "VGG-16"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "CNN-CABM [17]"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "MLP [32]"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "Extra Tree [32]"
        },
        {
          "Table 6 Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and": "Self-Supervised [33]"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recognition and implicit tagging. IEEE trans-": "actions\non affective\ncomputing 3(1),\n42–55",
          "autonomic nervous system. Heart 55(3), 253–": "258 (1986)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "(2011)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[13] Hsu, Y.-L., Wang,\nJ.-S.,\nChiang, W.-C.,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[5] Subramanian, R., Wache, J., Abadi, M.K.,",
          "autonomic nervous system. Heart 55(3), 253–": "Hung, C.-H.: Automatic\necg-based emotion"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Vieriu, R.L., Winkler, S., Sebe, N.: Ascertain:",
          "autonomic nervous system. Heart 55(3), 253–": "recognition in music\nlistening.\nIEEE Trans-"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Emotion\nand\npersonality\nrecognition\nusing",
          "autonomic nervous system. Heart 55(3), 253–": "actions on Affective Computing 11(1), 85–99"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "commercial\nsensors.\nIEEE Transactions\non",
          "autonomic nervous system. Heart 55(3), 253–": "(2017)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Affective Computing 9(2), 147–160 (2016)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[14] Shu, L., Yu, Y., Chen, W., Hua, H., Li, Q.,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[6] Kim,\nJ., Andr´e,\nE.:\nEmotion\nrecognition",
          "autonomic nervous system. Heart 55(3), 253–": "Jin, J., Xu, X.: Wearable emotion recognition"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "based on physiological changes\nin music lis-",
          "autonomic nervous system. Heart 55(3), 253–": "using heart rate data from a smart bracelet."
        },
        {
          "recognition and implicit tagging. IEEE trans-": "tening. IEEE transactions on pattern analysis",
          "autonomic nervous system. Heart 55(3), 253–": "Sensors 20(3), 718 (2020)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "and machine\nintelligence 30(12), 2067–2083",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[15] Dissanayake, T., Rajapaksha, Y., Ragel, R.,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "(2008)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "Nawinne,\nI.: An ensemble learning approach"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[7] Miranda-Correa, J.A., Abadi, M.K., Sebe, N.,",
          "autonomic nervous system. Heart 55(3), 253–": "for\nelectrocardiogram sensor\nbased\nhuman"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Patras, I.: Amigos: A dataset for affect, per-",
          "autonomic nervous system. Heart 55(3), 253–": "emotion\nrecognition.\nSensors\n19(20),\n4495"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "sonality\nand mood research on individuals",
          "autonomic nervous system. Heart 55(3), 253–": "(2019)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "and groups.\nIEEE transactions on affective",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[16] Dessai, A., Virani, H.: Emotion classification"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "computing 12(2), 479–493 (2018)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "based on cwt\nof\necg\nand gsr\nsignals using"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[8] Katsigiannis,\nS., Ramzan, N.: Dreamer: A",
          "autonomic nervous system. Heart 55(3), 253–": "various cnn models. Electronics 12(13), 2795"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "database for emotion recognition through eeg",
          "autonomic nervous system. Heart 55(3), 253–": "(2023)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "and ecg signals from wireless low-cost off-the-",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[17] Fan, T., Qiu, S., Wang, Z., Zhao, H., Jiang,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "shelf devices. IEEE journal of biomedical and",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "J., Wang, Y., Xu,\nJ.,\nSun, T.,\nJiang, N.:"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "health informatics 22(1), 98–107 (2017)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "A new deep\nconvolutional\nneural\nnetwork"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[9] Dar,\nM.N.,\nRahim,\nA.,\nAkram,\nM.U.,",
          "autonomic nervous system. Heart 55(3), 253–": "incorporating attentional mechanisms for ecg"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Khawaja,\nS.G., Rahim, A.: Yaad:\nyoung",
          "autonomic nervous system. Heart 55(3), 253–": "emotion recognition. Computers\nin Biology"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "adult’s affective data using wearable ecg and",
          "autonomic nervous system. Heart 55(3), 253–": "and Medicine 159, 106938 (2023)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "gsr sensors.\nIn: 2022 2nd International Con-",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[18] Santamaria-Granados, L., Munoz-Organero,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "ference on Digital Futures and Transforma-",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "M., Ramirez-Gonzalez, G., Abdulhay,\nE.,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "tive Technologies (ICoDT2), pp. 1–7 (2022).",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "Arunkumar, N.: Using\ndeep\nconvolutional"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "IEEE",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "neural network for\nemotion detection on a"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[10] Koelstra, S., Muhl, C., Soleymani, M., Lee,",
          "autonomic nervous system. Heart 55(3), 253–": "physiological signals dataset (amigos).\nIEEE"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "J.-S., Yazdani, A., Ebrahimi, T., Pun, T.,",
          "autonomic nervous system. Heart 55(3), 253–": "Access 7, 57–67 (2018)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Nijholt, A., Patras,\nI.: Deap: A database for",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[19] Dar, M.N., Akram, M.U., Khawaja,\nS.G.,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "emotion analysis; using physiological signals.",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "Pujari, A.N.: Cnn and lstm-based emotion"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "IEEE transactions\non\naffective\ncomputing",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "charting using physiological\nsignals. Sensors"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "3(1), 18–31 (2011)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "20(16), 4551 (2020)"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[11] Nikolova, D.,\nPetkova,\nP., Manolova, A.,",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[20] Rahim, A., Sagheer, A., Nadeem, K., Dar,"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Georgieva, P.: Ecg-based\nemotion\nrecogni-",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "M.N., Rahim, A., Akram, U.: Emotion chart-"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "tion: Overview of methods and applications.",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "ing using real-time monitoring of physiologi-"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "ANNA’18; Advances in Neural Networks and",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "cal signals. In: 2019 International Conference"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "Applications 2018, 1–5 (2018)",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "on Robotics\nand Automation\nin\nIndustry"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "[12] Bexton, R., Vallin, H., Camm, A.: Diurnal",
          "autonomic nervous system. Heart 55(3), 253–": "(ICRAI), pp. 1–5 (2019). IEEE"
        },
        {
          "recognition and implicit tagging. IEEE trans-": "variation of\nthe qt\ninterval–influence of\nthe",
          "autonomic nervous system. Heart 55(3), 253–": ""
        },
        {
          "recognition and implicit tagging. IEEE trans-": "",
          "autonomic nervous system. Heart 55(3), 253–": "[21] Dosovitskiy, A., Beyer, L., Kolesnikov, A.,"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Dehghani, M., Minderer, M., Heigold, G.,",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "5723–5726 (2021). IEEE"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Gelly, S.,\net\nal.: An image\nis worth 16x16",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[29] Zhao, Y., Cao, X., Lin,\nJ., Yu, D., Cao,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "words: Transformers\nfor\nimage\nrecognition",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "X.: Multimodal\nemotion recognition model"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "at\nscale.\narXiv\npreprint\narXiv:2010.11929",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "using\nphysiological\nsignals.\narXiv\ne-prints,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "(2020)",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "1911 (2019)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[22] Tsai, Y.-H.H., Bai, S., Liang, P.P., Kolter,",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[30]\nIsmail, S.N.M.S., Aziz, N.A.A., Ibrahim, S.Z.,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "J.Z., Morency, L.-P., Salakhutdinov, R.: Mul-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Nawawi,\nS.W., Alelyani,\nS., Mohana, M.,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "timodal\ntransformer\nfor\nunaligned multi-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Chun, L.C.: Evaluation of electrocardiogram:"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "modal\nlanguage\nsequences.\nIn: Proceedings",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Numerical vs.\nimage data for emotion recog-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "of the Conference. Association for Computa-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "nition system. F1000Research 10 (2021)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "tional Linguistics. Meeting, vol. 2019, p. 6558",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "(2019). NIH Public Access",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[31] Bulagang, A.F., Mountstephens, J., Teo, J.:"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Multiclass\nemotion\nprediction\nusing\nheart"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[23] Wu, Z., Zhang, X., Zhi-Xuan, T., Zaki, J.,",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "rate and virtual reality stimuli. Journal of Big"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Ong, D.C.: Attending\nto\nemotional\nnarra-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Data 8, 1–12 (2021)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "tives.\nIn: 2019 8th International Conference",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "on Affective Computing and Intelligent Inter-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[32] Khan, C.M.T., Ab Aziz, N.A., Raja,\nJ.E.,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "action (ACII), pp. 648–654 (2019). IEEE",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Nawawi,\nS.W.B., Rani,\nP.:\nEvaluation\nof"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "machine\nlearning\nalgorithms\nfor\nemotions"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[24] Huang, J., Tao, J., Liu, B., Lian, Z., Niu, M.:",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "recognition using electrocardiogram. Emerg-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Multimodal transformer fusion for continuous",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "ing Science Journal 7(1), 147–161 (2022)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "emotion recognition.\nIn:\nICASSP 2020-2020",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "IEEE International Conference on Acoustics,",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[33] Sarkar, P., Etemad, A.: Self-supervised ecg"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Speech and Signal Processing (ICASSP), pp.",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "representation learning for emotion recogni-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "3507–3511 (2020). IEEE",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "tion.\nIEEE Transactions on Affective Com-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "puting 13(3), 1541–1554 (2020)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[25] Cai, C., He, Y., Sun, L., Lian, Z., Liu, B., Tao,",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "J., Xu, M., Wang, K.: Multimodal sentiment",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[34] Wu, Y., Daoudi, M., Amad, A.: Transformer-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "analysis based on recurrent neural network",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "based self-supervised multimodal representa-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "and multimodal attention. In: Proceedings of",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "tion learning for wearable\nemotion recogni-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "the 2nd on Multimodal Sentiment Analysis",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "tion.\nIEEE Transactions on Affective Com-"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Challenge, pp. 61–67 (2021)",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "puting (2023)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[26] Chien, W.-S., Chou, H.-C., Lee, C.-C.: Self-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "[35] Siriwardhana,\nS.,\nKaluarachchi,\nT.,"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "assessed emotion classification from acoustic",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Billinghurst,\nM.,\nNanayakkara,\nS.:"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "and physiological features within small-group",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "Multimodal\nemotion\nrecognition\nwith"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "conversation.\nIn: Companion Publication of",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "transformer-based\nself\nsupervised\nfeature"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "the 2021 International Conference on Multi-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": "fusion. Ieee Access 8, 176274–176285 (2020)"
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "modal Interaction, pp. 230–239 (2021)",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[27] Wu, N., Green, B., Ben, X., O’Banion, S.:",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Deep transformer models for time series fore-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "casting: The influenza prevalence case. arXiv",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "preprint arXiv:2001.08317 (2020)",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "[28] Arjun, A., Rajpoot, A.S., Panicker, M.R.:",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Introducing attention mechanism for eeg sig-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "nals: Emotion recognition with vision trans-",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "formers.\nIn: 2021 43rd Annual\nInternational",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        },
        {
          "Weissenborn, D., Zhai, X., Unterthiner, T.,": "Conference\nof\nthe\nIEEE\nEngineering\nin",
          "Medicine & Biology\nSociety\n(EMBC),\npp.": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and Dominance Using the DREAMER Dataset. References",
      "venue": "Performance Evaluation of Our Best Model Compared with State-of-the-Art on Arousal, Valence, and Dominance Using the DREAMER Dataset. References"
    },
    {
      "citation_id": "2",
      "title": "Facefetch: A user emotion driven multimedia content recommendation system based on facial expression recognition",
      "authors": [
        "M Mariappan",
        "M Suk",
        "B Prabhakaran"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Symposium on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "An ensemble learning method for emotion charting using multimodal physiological signals",
      "authors": [
        "A Awan",
        "S Usman",
        "S Khalid",
        "A Anwar",
        "R Alroobaea",
        "S Hussain",
        "J Almotiri",
        "S Ullah",
        "M Akram"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "A review of recent approaches for emotion classification using electrocardiography and electrodermography signals",
      "authors": [
        "A Bulagang",
        "N Weng",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Informatics in Medicine Unlocked"
    },
    {
      "citation_id": "5",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "9",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-theshelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "10",
      "title": "Yaad: young adult's affective data using wearable ecg and gsr sensors",
      "authors": [
        "M Dar",
        "A Rahim",
        "M Akram",
        "S Khawaja",
        "A Rahim"
      ],
      "year": "2022",
      "venue": "2022 2nd International Conference on Digital Futures and Transformative Technologies"
    },
    {
      "citation_id": "11",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "12",
      "title": "Ecg-based emotion recognition: Overview of methods and applications. ANNA'18; Advances in Neural Networks and Applications",
      "authors": [
        "D Nikolova",
        "P Petkova",
        "A Manolova",
        "P Georgieva"
      ],
      "year": "2018",
      "venue": "Ecg-based emotion recognition: Overview of methods and applications. ANNA'18; Advances in Neural Networks and Applications"
    },
    {
      "citation_id": "13",
      "title": "Diurnal variation of the qt interval-influence of the autonomic nervous system",
      "authors": [
        "R Bexton",
        "H Vallin",
        "A Camm"
      ],
      "year": "1986",
      "venue": "Heart"
    },
    {
      "citation_id": "14",
      "title": "Automatic ecg-based emotion recognition in music listening",
      "authors": [
        "Y.-L Hsu",
        "J.-S Wang",
        "W.-C Chiang",
        "C.-H Hung"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Wearable emotion recognition using heart rate data from a smart bracelet",
      "authors": [
        "L Shu",
        "Y Yu",
        "W Chen",
        "H Hua",
        "Q Li",
        "J Jin",
        "X Xu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "An ensemble learning approach for electrocardiogram sensor based human emotion recognition",
      "authors": [
        "T Dissanayake",
        "Y Rajapaksha",
        "R Ragel",
        "I Nawinne"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Emotion classification based on cwt of ecg and gsr signals using various cnn models",
      "authors": [
        "A Dessai",
        "H Virani"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "18",
      "title": "A new deep convolutional neural network incorporating attentional mechanisms for ecg emotion recognition",
      "authors": [
        "T Fan",
        "S Qiu",
        "Z Wang",
        "H Zhao",
        "J Jiang",
        "Y Wang",
        "J Xu",
        "T Sun",
        "N Jiang"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "19",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-Gonzalez",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Cnn and lstm-based emotion charting using physiological signals",
      "authors": [
        "M Dar",
        "M Akram",
        "S Khawaja",
        "A Pujari"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Emotion charting using real-time monitoring of physiological signals",
      "authors": [
        "A Rahim",
        "A Sagheer",
        "K Nadeem",
        "M Dar",
        "A Rahim",
        "U Akram"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics and Automation in Industry (ICRAI)"
    },
    {
      "citation_id": "22",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "23",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "24",
      "title": "Attending to emotional narratives",
      "authors": [
        "Z Wu",
        "X Zhang",
        "T Zhi-Xuan",
        "J Zaki",
        "D Ong"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "25",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Multimodal sentiment analysis based on recurrent neural network and multimodal attention",
      "authors": [
        "C Cai",
        "Y He",
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao",
        "M Xu",
        "K Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "27",
      "title": "Selfassessed emotion classification from acoustic and physiological features within small-group conversation",
      "authors": [
        "W.-S Chien",
        "H.-C Chou",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "authors": [
        "N Wu",
        "B Green",
        "X Ben",
        "S O'banion"
      ],
      "year": "2020",
      "venue": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "arxiv": "arXiv:2001.08317"
    },
    {
      "citation_id": "29",
      "title": "Introducing attention mechanism for eeg signals: Emotion recognition with vision transformers",
      "authors": [
        "A Arjun",
        "A Rajpoot",
        "M Panicker"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition model using physiological signals",
      "authors": [
        "Y Zhao",
        "X Cao",
        "J Lin",
        "D Yu",
        "X Cao"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition model using physiological signals"
    },
    {
      "citation_id": "31",
      "title": "Evaluation of electrocardiogram: Numerical vs. image data for emotion recognition system",
      "authors": [
        "S Ismail",
        "N Aziz",
        "S Ibrahim",
        "S Nawawi",
        "S Alelyani",
        "M Mohana",
        "L Chun"
      ],
      "year": "2021",
      "venue": "F1000Research"
    },
    {
      "citation_id": "32",
      "title": "Multiclass emotion prediction using heart rate and virtual reality stimuli",
      "authors": [
        "A Bulagang",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2021",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "33",
      "title": "Evaluation of machine learning algorithms for emotions recognition using electrocardiogram",
      "authors": [
        "C Khan",
        "N Ab Aziz",
        "J Raja",
        "S Nawawi",
        "P Rani"
      ],
      "year": "2022",
      "venue": "Emerging Science Journal"
    },
    {
      "citation_id": "34",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Transformerbased self-supervised multimodal representation learning for wearable emotion recognition",
      "authors": [
        "Y Wu",
        "M Daoudi",
        "A Amad"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Ieee Access"
    }
  ]
}