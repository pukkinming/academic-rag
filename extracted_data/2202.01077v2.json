{
  "paper_id": "2202.01077v2",
  "title": "Experimental Investigation Of Trust In Anthropomorphic Agents As Task Partners",
  "published": "2022-02-02T15:04:51Z",
  "authors": [
    "Akihiro Maehigashi",
    "Takahiro Tsumura",
    "Seiji Yamada"
  ],
  "keywords": [
    "trust",
    "anthropomorphism",
    "AI agent",
    "human",
    "social robot"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study investigated whether human trust in a social robot with anthropomorphic physicality is similar to that in an AI agent or in a human in order to clarify how anthropomorphic physicality influences human trust in an agent. We conducted an online experiment using two types of cognitive tasks, calculation and emotion recognition tasks, where participants answered after referring to the answers of an AI agent, a human, or a social robot. During the experiment, the participants rated their trust levels in their partners. As a result, trust in the social robot was basically neither similar to that in the AI agent nor in the human and instead settled between them. The results showed a possibility that manipulating anthropomorphic features would help assist human users in appropriately calibrating trust in an agent.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "AI has been entering all aspects of life. Successful cooperation between a human user and an AI agent requires the user to appropriately adjust their use of the agent to maximize their task performance  [11, 17] . In the field of human factors, trust in an autonomous AI agent, such as automation, has been known to be a fundamental parameter in deciding the level of use of the agent  [11, 16] .\n\nProper use of an AI agent is achieved through proper trust calibration, where trust in the agent is appropriately calibrated to its actual reliability  [11, 17] . However, people tend to over-trust and misuse the agent (inappropriate utilization of the agent) because they generally have a positive bias toward the agent, assuming that the agent performs perfectly without error. However, when people find that an AI agent has made task errors, they tend to undertrust and disuse the agent (inappropriate underutilization of the agent) because the assumption that the agent performs perfectly collapses  [5] . These poor trust calibrations would eventually lower task performance  [11, 16] .\n\nSuch issues of trust calibration have been investigated and discussed in the HAI community as well  [2, 10, 14, 18] . In particular, several studies experimentally indicated that human trust in an autonomous agent and in a human differ  [10, 18] . Moreover, the over-trust and under-trust seen toward AI agents are suppressed toward humans  [5, 12] . These previous studies are considered to show that over-trust and under-trust toward an AI agent might be suppressed toward a social robot as in the case of humans since people tend to behave socially toward a social robot with anthropomorphic physicality that increases the sense of anthropomorphism  [6] .\n\nIn order to clarify how anthropomorphic physicality influences human trust in an agent, this study investigated whether human trust in a social robot with anthropomorphic physicality is similar to that in an AI agent or a human. The hypotheses in this study are as follows.\n\nH1: Trust in a social robot is similar to that in a human. H2: Trust in a social robot is similar to that in an AI agent.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiment 2.1 Experimental Design And Participants",
      "text": "The experiment had a two-factor between-participants design. The factors were the task (calculation and emotion recognition) and the partner (AI, human, and robot). A priori G*Power analysis revealed that 26 participants in each condition were needed at least for a medium effect size (ùëì = 0.25) with a power of 0.80 and alpha of 0.05  [7]  in this experimental design. On the basis of this analysis and in consideration of the possibility that some participants would act or perform irregularly, a total of 258 participants (190 male, 68 female) were recruited through a cloud-sourcing service provided by Yahoo! Japan. Their ages ranged from 21 to 76 years old (ùëÄ = 47.24; ùëÜùê∑ = 10.49). They were randomly assigned to one of six conditions. As a result, in the calculation task, there were 44 participants in the AI, 42 in the human, and 45 in the robot conditions. Also, in the emotion recognition task, there were 45 participants in the AI, 41 in the human, and 41 in the robot conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Procedure",
      "text": "They were randomly assigned to one of six conditions, and the task partner was introduced with one of the pictures in Figure  1  depending on the experimental condition. In the AI and robot conditions, the AI agent and social robot were explained to have computational functions in the calculation task or emotional recognition functions in the emotion recognition task and would work with the participants in real time. In the human condition, the partner was introduced as an experimental collaborator who had previously answered identical problems that would be given in the experiment.\n\nAfter that, the participants first performed 10 calculation or emotion recognition problems by themselves without a partner. After that, they performed 36 calculation or emotion recognition problems with one of the partners. In the calculation task, participants mentally calculated two-digit addition problems with carry up and subtraction problems with carry down. In the emotion recognition task, participants chose which of five emotions (anger, disgust, surprise, sadness, and fear) was expressed in pictures of human facial expressions using AffectNet  [15] .\n\nThe task procedures are shown in Figure  2 . The procedure was as follows. (1) A cross was displayed at the center of the screen for 0.5 seconds, (2) a picture of a facial expression in a two-digit addition or subtraction problem in the calculation task and the emotion recognition task was presented for 5 seconds, (3) the task partner took 3 seconds to answer the problem, (4) the partner's answer was displayed, and (5) the participant's answer was entered with a numeric keypad by the participants. While the task partner was answering problems and while their answer was displayed, one of the pictures in Figure  1  was displayed depending on the experimental condition.\n\nMoreover, in this experiment, the partner's accuracy was manipulated to change. Each task contained 36 problems, and they were divided into 3 trials with 12 problems for each trial. The first and third trials were correct trials where the partner gave all correct answers. The second trial was an error trial where the partner gave all incorrect answers.\n\nRegarding the measurement of trust, we asked participants to rate their trust levels during the task as in previous studies on human-automation interaction  [3, 4, 13] . Participants were asked \"how much do you trust your partner?\" and were required to rate their trust levels in their partners on a 7-point scale (1: Extremely untrustable -7: Extremely trustable). The trust level was measured before the start of each task and after each of four problems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "First, to confirm the analysis of the data, on the basis of the a priori G*Power analysis, we selected the data of the first 26 participants in each condition to avoid Type I and II errors in the following statistical analyses. Second, we searched for irregular data related to the accuracy rate, that is, the rate at which the participants answered correctly, without and with the partner in 2SD above or below the mean in each condition, and we eliminated the irregular data of the participants in each condition. We repeated the first and second procedure until 26 participants were secured for each condition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Accuracy Rate",
      "text": "As a task analysis, we conducted a 2 (task: calculation and emotion recognition) √ó 3 (partner: AI, human, and robot) √ó 2 (task situation: with and without partner) ANOVA on the accuracy rate in each task (Figure  3 ).\n\nAs a result, there was a significant interaction between the task and the task situation factors (ùêπ (1, 150) = 26.32, ùëù < 0.001, ùúÇ 2 ùëù = 0.15). A significant simple main effect was found on the task situation factor, showing that the accuracy rate was higher with the partner than without it in the emotion recognition task (ùêπ (1, 75) = 27.80, ùëù < 0.001, ùúÇ 2 ùëù = 0.27). There was no other significant interactions. Furthermore, there were significant main effects on the task factor (ùêπ (  1  This result indicated that the calculation task was easier than the emotion recognition task. The participants could show high accuracy even by themselves without a partner in the calculation task, although the participants could take advantage of performing with the partner in the emotion recognition task to increase the accuracy of their answers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Trust Rating",
      "text": "As an analysis, we conducted a 2 (task: calculation and emotion recognition) √ó 3 (partner: AI, human, robot) ANOVA on the dependent variables in each trial. From the overall results, there were significant differences in the trust ratings among the partner conditions before the tasks and in the first correct trial (Figure  4a, b ).\n\nFirst, regarding the trust rating before the tasks, there was a significant interaction (ùêπ (2, 150) = 4.21, ùëù < 0.05, ùúÇ 2 ùëù = 0.05) and a significant simple main effect on the partner condition in the calculation task (ùêπ (2, 150) = 5.59, ùëù < 0.01, ùúÇ 2 ùëù = 0.07), showing that the trust rating in the AI condition was higher than that in the human condition (ùë° (150) = 3.38, ùëù < 0.001, ùëü = 0.27).\n\nMoreover, regarding the trust rating in the first correct trial, there was a significant main effect on the partner factor (ùêπ (2, 150) = 3.80, ùëù < 0.05, ùúÇ 2 ùëù = 0.05), and the results of multiple comparisons showed that the trust rating in the AI condition was higher than that in the human condition (ùë° (150) = 2.73, ùëù < 0.01, ùëü = 0.22).\n\nFurthermore, there was a significant difference in the decline in the trust ratings among the partner conditions in the error trial. To compare the declines in trust ratings due to partner error, we calculated the difference value between the last trust rating in the first correct trial and the first rating in the error trial for each participant and conducted an analysis on the difference value (Figure  4c ). As a result, there was a significant main effect on the partner factor (ùêπ (2, 150) = 3.07, ùëù < 0.05, ùúÇ 2 ùëù = 0.04), and the results of multiple comparisons showed that the difference value for the AI condition was lower than that for the human condition (ùë° (150) = 2.47, ùëù < 0.05, ùëü = 0.20).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "This study investigated whether human trust in a social robot with anthropomorphic physicality is similar to that in an AI agent or in a human in order to clarify how anthropomorphic physicality influences human trust in an agent. The results showed that the participants in this study formed trust in the social robot that was neither similar to the AI nor human and settled between them before and during the tasks. Therefore, H1 and 2 were not supported. However, the results showed that manipulating anthropomorphic features influenced trust in an agent.\n\nThe results of this study are considered to be supportive of the human perception of agency in mind perception theory. Mind perception theory indicates that people perceive mind along with dimensions of experience (the capacity to feel and to sense) and agency (the capacity to do, to plan, and to exert self-control)  [8, 9] . In regard to the agency dimension, robots with anthropomorphic physicality were perceived to have higher agency than those without it  [1] ; however, they were not perceived to have as much agency as humans  [8] . Because a social robot has anthropomorphic physicality, people might perceive the robot differently from an AI agent and a human acting as a task partner and form trust in the robot differently from that in the AI agent and the human.\n\nAs a conclusion, the results showed a possibility that additional anthropomorphic features would increase anthropomorphism, and therefore, anthropomorphic physicality of an agent might be effective for suppressing over-trust and under-trust. In future work, we need to investigate how trust developed based on anthropomorphism is properly calibrated to an agent's reliability and how it could be controlled.",
      "page_start": 1,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Task partner.",
      "page": 1
    },
    {
      "caption": "Figure 2: Procedure of calculation and emotion recognition tasks in robot condition.",
      "page": 2
    },
    {
      "caption": "Figure 2: The procedure was",
      "page": 2
    },
    {
      "caption": "Figure 1: was displayed depending on the",
      "page": 2
    },
    {
      "caption": "Figure 3: Accuracy rate.",
      "page": 3
    },
    {
      "caption": "Figure 4: Trust ratings and difference values. Bars show standard errors. ‚àó‚àóùëù< 0.01, ‚àó‚àó‚àóùëù< 0.001.",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Robots with Display Figure 4: Trust ratings and difference values. Bars show standard errors. * * ùëù < 0.01, * * * ùëù < 0.001. Screens: A Robot with a More Humanlike Face Display Is Perceived To Have More Mind and a Better Personality",
      "authors": [
        "Elizabeth Broadbent",
        "Vinayak Kumar",
        "Xingyan Li",
        "John Sollers 3rd",
        "Rebecca Stafford",
        "Bruce Macdonald",
        "Daniel Wegner"
      ],
      "year": "2013",
      "venue": "PloS One",
      "doi": "10.1371/journal.pone.0072589"
    },
    {
      "citation_id": "2",
      "title": "A Markovian Method for Predicting Trust Behavior in Human-Agent Interaction",
      "authors": [
        "V Pynadath",
        "Wang David",
        "Kamireddy Ning",
        "Sreekar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 7th International Conference on Human-Agent Interaction (HAI '19)",
      "doi": "10.1145/3349537.3351905"
    },
    {
      "citation_id": "3",
      "title": "The Effects of Errors on System Trust, Self-Confidence, and the Allocation of Control in Route Planning",
      "authors": [
        "Cees Peter De Vries",
        "Don Midden",
        "Bouwhuis"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/S1071-5819(03)00039-9"
    },
    {
      "citation_id": "4",
      "title": "The Role of Trust in Automation Reliance",
      "authors": [
        "Mary Dzindolet",
        "Scott Peterson",
        "Regina Pomranky",
        "Linda Pierce"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/S1071-5819(03)00038-7"
    },
    {
      "citation_id": "5",
      "title": "The Perceived Utility of Human and Automated Aids in a Visual Detection Task",
      "authors": [
        "Mary Dzindolet",
        "Linda Pierce",
        "Hall Beck",
        "Lloyd Dawe"
      ],
      "year": "2002",
      "venue": "Human Factors",
      "doi": "10.1518/0018720024494856"
    },
    {
      "citation_id": "6",
      "title": "On Seeing Human: A Three-Factor Theory of Anthropomorphism",
      "authors": [
        "Nicholas Epley",
        "Adam Waytz",
        "John Cacioppo"
      ],
      "year": "2007",
      "venue": "Psychological Review",
      "doi": "10.1037/0033-295X.114.4.864"
    },
    {
      "citation_id": "7",
      "title": "G*Power 3: A Flexible Statistical Power Analysis Program for the Social",
      "authors": [
        "Franz Faul",
        "Edgar Erdfelder",
        "Albert-Georg Lang",
        "Axel Buchner"
      ],
      "year": "2007",
      "venue": "Behavioral, and Biomedical Sciences. Behavior Research Methods",
      "doi": "10.3758/bf03193146"
    },
    {
      "citation_id": "8",
      "title": "Dimensions of Mind Perception",
      "authors": [
        "Heather Gray",
        "Kurt Gray",
        "Daniel Wegner"
      ],
      "year": "2007",
      "venue": "Science",
      "doi": "10.1126/science.1134475"
    },
    {
      "citation_id": "9",
      "title": "Feeling Robots and Human Zombies: Mind Perception and the Uncanny Valley",
      "authors": [
        "Kurt Gray",
        "Daniel Wegner"
      ],
      "year": "2012",
      "venue": "Cognition",
      "doi": "10.1016/j.cognition.2012.06.007"
    },
    {
      "citation_id": "10",
      "title": "Personal Influences on Dynamic Trust Formation in Human-Agent Interaction",
      "authors": [
        "Huang Hsiao",
        "- Ying",
        "Bashir Masooda"
      ],
      "year": "2017",
      "venue": "Proceedings of the 5th International Conference on Human-Agent Interaction (HAI '17)",
      "doi": "10.1145/3125739.3125749"
    },
    {
      "citation_id": "11",
      "title": "Trust in Automation: Designing for Appropriate Reliance",
      "authors": [
        "John Lee",
        "Katrina See"
      ],
      "year": "2004",
      "venue": "Human Factors",
      "doi": "10.1518/hfes.46.1.50_30392"
    },
    {
      "citation_id": "12",
      "title": "The Dynamics of Trust: Comparing Humans to Automation",
      "authors": [
        "Stephan Lewandowsky",
        "Michael Mundy",
        "Gerard Tan"
      ],
      "year": "2000",
      "venue": "Journal of Experimental Psychology Applied",
      "doi": "10.1037/1076-898X.6.2.104"
    },
    {
      "citation_id": "13",
      "title": "Automation Failures on Tasks Easily Performed by Operators Undermine Trust in Automated Aids",
      "authors": [
        "Poornima Madhavan",
        "Douglas Wiegmann",
        "Frank Lacson"
      ],
      "year": "2006",
      "venue": "Human Factors",
      "doi": "10.1518/001872006777724408"
    },
    {
      "citation_id": "14",
      "title": "Comparing Human Trust Attitudes Towards Human and Agent Teammates",
      "authors": [
        "Haf√§≈õzo√§ ¬ßlu",
        "Feyza Merve",
        "Sen Sandip"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th International Conference on Human-Agent Interaction (HAI '20)",
      "doi": "10.1145/3406499.3415082"
    },
    {
      "citation_id": "15",
      "title": "TAffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "16",
      "title": "Humans and Automation: Use, Misuse, Disuse, Abuse",
      "authors": [
        "Raja Parasuraman",
        "Victor Riley"
      ],
      "year": "1997",
      "venue": "Human Factors",
      "doi": "10.1518/001872097778543886"
    },
    {
      "citation_id": "17",
      "title": "Automated Diagnostic Aids: The Effects of Aid Reliability on Users' Trust and Reliance",
      "authors": [
        "Douglas Wiegmann",
        "Aaron Rich",
        "Hui Zhang"
      ],
      "year": "2001",
      "venue": "Theoretical Issues in Ergonomics Science",
      "doi": "10.1080/14639220110110306"
    },
    {
      "citation_id": "18",
      "title": "Investigating the Effects of (Empty) Promises on Human-Automation Interaction and Trust Repair",
      "authors": [
        "Albayram Yusuf",
        "Jensen Theodore",
        "Khan Mohammad Maifi Hasan",
        "Fahim Md",
        "Abdullah Al",
        "Ross Buck",
        "Coman Emil"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th International Conference on Human-Agent Interaction (HAI '20)",
      "doi": "10.1145/3406499.3415064"
    }
  ]
}