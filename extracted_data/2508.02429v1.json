{
  "paper_id": "2508.02429v1",
  "title": "Multimodal Large Language Models For End-To-End Affective Computing: Benchmarking And Boosting With Generative Knowledge Prompting",
  "published": "2025-08-04T13:49:03Z",
  "authors": [
    "Miaosen Luo",
    "Jiesen Long",
    "Zequn Li",
    "Yunying Yang",
    "Yuncheng Jiang",
    "Sijie Mai"
  ],
  "keywords": [
    "Multimodal Affective Computing",
    "Multimodal Large Language Models",
    "End-to-End Learning",
    "Generative Knowledge Prompting"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Affective Computing (MAC) aims to recognize and interpret human emotions by integrating information from diverse modalities such as text, video, and audio. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly reshaped the landscape of MAC by offering a unified framework for processing and aligning cross-modal information. However, practical challenges remain, including performance variability across complex MAC tasks and insufficient understanding of how architectural designs and data characteristics impact affective analysis. To address these gaps, we conduct a systematic benchmark evaluation of state-of-the-art open-source MLLMs capable of concurrently processing audio, visual, and textual modalities across multiple established MAC datasets. Our evaluation not only compares the performance of these MLLMs but also provides actionable insights into model optimization by analyzing the influence of model architectures and dataset properties. Furthermore, we propose a novel hybrid strategy that combines generative knowledge prompting with supervised fine-tuning to enhance MLLMs' affective computing capabilities. Experimental results demonstrate that this integrated approach significantly improves performance across various MAC tasks, offering a promising avenue for future research and development in this field. Our code is released on https://github.com/LuoMSen/MLLM-MAC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Multimodal Affective Computing (MAC) aims to recognize, perceive, infer, and interpret human emotions through the integration of information from multiple modalities, including text, video, and audio  [1] . Human emotional expressions are inherently complex and multimodal in nature  [2] , a characteristic that makes unimodal approaches particularly vulnerable to ambiguity, noise interference, and information loss  [3] . By fusing data across diverse modalities, multimodal affective computing effectively mitigates this limitation, enabling a more comprehensive and robust understanding of affective states-an approach that aligns with the holistic manner in which humans perceive emotions  [4] . In prior studies  [5] , the majority of works employ non-end-to-end approaches, where Fig.  1 . Schematic diagram of the end-to-end affective computing process based on the multimodal large language model (MLLM). In this process, the original video and audio data are directly fed into the MLLM, while the conversational text is embedded into the prompt, thereby outputting the corresponding affective computing results.\n\nThe recent revolutionary advancements in Large Language Models (LLMs) have dramatically reshaped the landscape of natural language processing and beyond  [6] . Their extraordinary ability to comprehend, reason, and generate human-like text originates from extensive pre-training on massive corpora. Importantly, this progress has rapidly expanded to Multimodal Large Language Models (MLLMs), such as GPT-4V  [7] , LLaVA  [8] , Gemini  [9] , and Qwen-VL  [10] . MLLMs inherit the robust linguistic and reasoning capacities of LLMs while integrating the capability to process and align information across diverse modalities (images, audio, video) within a unified, end-to-end framework  [4] . This offers a paradigmshifting prospect for affective computing. As depicted in Figure  1 , MLLMs can concurrently receive raw audio, video, and text data as input, implicitly acquire complex crossmodal interactions through supervised fine-tuning, and conduct end-to-end affective computing by utilizing their powerful contextual learning and instruction-following abilities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2508.02429V1 [Cs.Ai] 4 Aug 2025",
      "text": "Despite the immense potential of MLLMs in MAC, significant challenges persist in their practical application. Current MLLMs demonstrate substantial performance variability across complex MAC tasks, which can be attributed to differences in architectural designs, pre-training objectives, data scales, and inherent capabilities  [11] . However, the precise relationship between these design choices and their specific impacts on MAC performance remains insufficiently explored and understood. Existing benchmarks have primarily focused on assessing unimodal LLMs or dual-modal MLLMs (e.g., text+vision, text+audio) in relatively simple tasks such as sentiment analysis  [12] -  [14] . A critical void exists: the lack of a systematic, comprehensive evaluation of state-of-the-art MLLMs capable of processing all relevant modalities (text, audio, visual) on established MAC datasets. Such a benchmark is essential to identify which models excel at specific aspects of affective understanding, thereby guiding model selection and future development.\n\nFurthermore, the performance of MLLMs demonstrates a marked sensitivity to the framing of tasks within prompts  [15] . Although supervised fine-tuning (SFT) has been empirically validated to boost their task-specific efficacy, the potential of advanced prompt engineering strategies to unlock and optimize their inherent affective computing capabilities remains substantially underexplored in current research.\n\nTo address these critical gaps, we carry out a thorough benchmark assessment of open-source MLLMs capable of processing audio, visual, and textual modalities concurrently. Our evaluation spans multiple well-established MAC datasets, including CMU-MOSI  [16] , CMU-MOSEI  [17] , CH-SIMS  [17] , CH-SIMS v2  [18] , MELD  [19] , and UR-FUNNY v2  [20] . This evaluation not only compares MLLMs against each other but also juxtaposes their performance with traditional machine learning methods to quantify the advancements and identify remaining challenges. Additionally, we perform an indepth analysis to elucidate how model architecture characteristics (e.g., modality alignment mechanisms, fusion strategies, model size) and dataset properties (e.g., modality dominance, domain) influence performance in affective analysis.\n\nTo enhance the performance of MLLMs in MAC, we propose a simple but effective strategy that integrates generative knowledge prompting  [21]  with SFT. Specifically, we first leverage the zero-shot capability of MLLMs to extract descriptions from both audio and video modalities. Subsequently, we design knowledge-guided prompts to effectively incorporate these extracted cues into the model input, followed by SFT on the augmented input. Experimental results validate that this strategy outperforms standalone SFT methods, achieving significant improvements in MLLM performance across affective computing tasks.\n\nThe main contributions are summarized as follows:\n\n• We conduct the first systematic evaluation of state-of-theart MLLMs capable of simultaneous processing of audio, visual, and textual modalities. • We reveal the mechanisms by which model architectural designs and dataset characteristics influence MLLMs' performance in affective analysis tasks, providing actionable insights for model optimization.\n\n• We propose a hybrid strategy that integrates generative knowledge prompting with SFT. Experimental results demonstrate that this approach significantly enhances MLLMs' performance in affective computing tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Multimodal Affective Computing",
      "text": "MAC seeks to recognize and analyze human emotions by integrating information from multiple modalities. Traditional methods often rely on early fusion  [22] ,  [23] , late fusion  [24] ,  [25] , or attention-based strategies  [26] ,  [27] . Although these techniques outperform unimodal methods, they still fail to adequately capture the complex cross-modal interplay of affective cues  [3] . This limitation has spurred a recent shift towards MLLMs  [28] ,  [29] . MLLMs leverage their unified semantic space and emergent reasoning ability to more effectively detect subtle interactions between modalities and understand emotions within specific contexts  [30] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multimodal Large Language Models",
      "text": "MLLMs are built upon LLMs and integrate multimodal encoders  [31] -  [33]  through projection or cross-attention to enable unified multimodal processing. In recent years, the open-source community has yielded powerful MLLMs, ranging from vision-language models with advanced visual reasoning capabilities (e.g., BLIP-2  [34] , LLaVA  [8] ) to audiolanguage models exhibiting robust audio comprehension (e.g., SALMONN  [35] , Qwen-Audio  [36] ). More recently, the development of MLLMs is progressing towards comprehensive omnimodal models that unify multiple modalities within a single framework  [37] -  [39] . For instance, Qwen2.5-Omni  [40]  perceives diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. These omnimodal models not only capture complex relationships between text, vision, and sound but also demonstrate enhanced robustness in real-world scenarios. In this study, we selected multiple opensource MLLMs that support joint modeling of text, video, and audio for benchmarking purposes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Prompting Strategy",
      "text": "Effective prompting strategies are crucial for enhancing MLLMs' affective reasoning capabilities. Recent research demonstrates their potential for MAC. For instance, methods like Multi-Views Prompt Learning  [41]  effectively capture the emotional cues involved in different levels of semantic information, while Set-of-Vision-Text Prompting (SoVTP)  [42]  preserves holistic scene context by overlaying spatial annotations on full-scene inputs and integrating auxiliary cues like body posture, environment, and social dynamics. Additionally, combining prompts with acoustic analysis or Chainof-Thought (CoT) reasoning has shown promise in emotion recognition in conversation tasks  [29] .\n\nHowever, existing work has primarily explored bimodal scenarios in MAC tasks. To address this gap in trimodal tasks, we propose a strategy combining generative knowledge prompting across text, audio, and video modalities. III. BENCHMARK",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "In this study, we employ six datasets, encompassing multimodal sentiment analysis (MSA) datasets (CMU-MOSI  [16] , CMU-MOSEI  [43] , CH-SIMS  [17] , and CH-SIMS v2  [18] ), multimodal emotion recognition (MER) dataset (MELD  [19] ), and multimodal humor detection (MHD) dataset (UR-FUNNY v2  [20] ). Here, We present a concise overview of these datasets below, with detailed statistics summarized in the Appendix.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "1) Cmu-Mosi And Cmu-Mosei:",
      "text": "The CMU-MOSI dataset  [16]  consists of 93 YouTube videos, which are divided into 2,199 clips, with each clip annotated with sentiment scores on a 7-point scale ranging from strong negative (-3) to strong positive (+3). Likewise, the CMU-MOSEI dataset  [43]  encompasses 23,453 video clips derived from various online platforms and adheres to the same sentiment score labeling scheme.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "2) Ch-Sims And Ch-Sims V2:",
      "text": "The CH-SIMS dataset  [17]  contains 2,281 refined video segments sourced from movies, TV series, and variety shows, with sentiment annotations ranging from negative (-1) to positive (+1) for each clip. The CH-SIMS v2 dataset  [18]  extends this corpus to 4,402 supervised segments and 10,161 unsupervised segments (totaling 14,563 clips), collected from 11 diverse scenarios like vlogs, interviews, and talk shows, emphasizing richer non-verbal behaviors while retaining the original annotation methodology.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Meld:",
      "text": "The MELD dataset  [19]  is a multimodal corpus specifically designed for emotion recognition in conversational contexts. This dataset is constructed based on dialogues from the television series \"Friends\", comprising over 1,400 conversational sequences containing 13,000 speaker utterances. Each utterance is annotated with one of the seven basic emotional categories (anger, disgust, sadness, joy, neutral, surprise, fear) as well as sentiment polarity labels (positive, negative, neutral).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "4) Ur-Funny V2:",
      "text": "The UR-FUNNY v2 dataset  [20]  is a diverse multimodal resource for humor detection in natural language processing. Compared with the original UR-FUNNY dataset, it removes noisy and overlapping instances from the original dataset. In terms of content composition, UR-FUNNY v2 incorporates a greater number of contextual sentences compared to its predecessor, which enriches the contextual information available for analysis.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Multimodal Large Language Models",
      "text": "To achieve end-to-end affective computing, the evaluated MLLMs must support the collaborative input of audio, video, and text. Additionally, the models must be open-source to enable effective SFT. Based on the above requirements, this study selects HumanOmni  [38] , Qwen2.5Omni  [40] , VideoLLaMA2-AV  [44] , Ola  [39] , MiniCPM-o 2.6  [37] , Emotion-LLaMA  [45] , and PandaGPT  [46]  as the experimental models. Their basic information is summarized in Table  I , and detailed characteristics can be found in Appendix A.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Method Overview 1) Supervised Fine-Tuning:",
      "text": "To enhance the adaptability of MLLMs for MAC tasks like MSA, MER, and MHD, the employment of SFT to adjust model parameters is adopted. As a task-specific optimization paradigm built on pre-trained models, SFT leverages labeled datasets-comprising input samples and their corresponding target outputs-to further refine model parameters, thereby enabling the model to achieve better alignment with the characteristics and requirements of specific downstream tasks.\n\nFurthermore, to mitigate computational overhead, we incorporated Low-Rank Adaptation (LoRA) technology  [47] . Instead of directly modifying all model parameters, LoRA implements fine-tuning by injecting low-rank matrices into the model's weight matrices. Specifically, in LoRA-based finetuning, two low-rank matrices A and B are introduced. A rank-r matrix ∆W = A × B is then constructed from these two matrices and added to the original weight matrix W . The formula is as follows:\n\nHere, W is the original weight matrix of the pre-trained model, which is typically kept fixed during fine-tuning. A and B are the low-rank matrices that need to be trained, and the number of parameters in these matrices is significantly smaller than that of the original weight matrix W .\n\n2) Prompt Strategy: To enhance the performance of MLLMs in MAC, we propose an innovative strategy that synergistically integrates generative knowledge prompting with SFT. As illustrated in Figure  2 , our approach commences by leveraging the zero-shot capabilities of MLLMs to extract salient descriptions pertaining to affective computing directly from raw video and audio inputs. This initial",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ouput Supervised Fine-Tuning",
      "text": "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to be communicating with someone off-camera. Throughout the video, the firefighter's expressions change from concern to surprise and then to a more determined look as they seem to be conveying a message or information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Description:",
      "text": "The audio clip features a series of emotional vocalizations, predominantly in the form of shouting or crying. The voice is characterized by a high level of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise, likely representing an explosion or a similar catastrophic event. The sound is characterized by a deep rumbling and a chaotic quality, which adds to the overall sense of urgency and fear conveyed by the vocalizations. step is followed by a comprehensive knowledge aggregation phase, wherein we systematically consolidate multimodal data streams-including original audiovisual data, their corresponding generated descriptions, and textual dialogue content-into a unified input framework for the MLLM. Subsequent SFT is designed to align the model's output distribution with the specific requirements of MAC tasks, thereby ensuring optimal adaptation to the nuances of affective analysis. By focusing on getting emotion-related semantic features from unstructured multimedia data, this approach helps the MLLM pay more refined attention to affective cues.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We conducted experiments using multiple MAC datasets and several open-source MLLMs. Due to space constraints, detailed information on evaluation metrics and model details is provided in the Appendix.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Evaluation Baselines",
      "text": "In this study, we employed the MLLMs, previously introduced, as the baseline models and further conducted a comparative analysis with state-of-the-art (SOTA) multimodal machine learning (MML) methods.\n\n1) MLLM: As shown in Table  I , the MLLMs we selected include Qwen2.5Omni, HumanOmni, Ola, VideoLLaMA2-AV, MiniCPM-o, PandaGPT, and Emotion-LLaMA. Among them, Qwen2.5Omni, HumanOmni, Ola, and MiniCPM-o are based on the same large language model (LLM), namely Qwen2.5 (7B); VideoLLaMA2-AV is based on Qwen2 (7B), PandaGPT is based on Vicuna (7B), and Emotion-LLaMA is based on LLaMA2 (7B).\n\n2) MML: For comparative analysis, we selected the SOTA methods for each dataset. Specifically, or the CMU-MOSI and CMU-MOSEI datasets, we chose MOAC  [5] , C-MIB  [48] , MGT  [49] , and KAN-MCP  [50]  as baseline methods. For the CH-SIMS dataset, we used HGTFM  [51]  as the primary comparison benchmark. For the CH-SIMS v2 dataset, we selected HGTFM and KAN-MCP as comparison benchmarks. For the UR-FUNNY v2 dataset, we adopted SemanticMAC  [52]  as the reference method. For the MELD dataset, we used SemanticMAC and MGT as comparison baselines.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Supervised Fine-Tuning Details",
      "text": "To adapt MLLMs to the task of MAC, we conducted supervised fine-tuning on MLLMs across six different datasets. Specifically, Qwen2.5Omni and MiniCPM-o models were fine-tuned using the LLaMA-Factory framework  [53] , while the other models were fine-tuned using the code from their respective open-source repositories. During the fine-tuning process, we incorporated the FlashAttention-2  [54]  to optimize the attention module of transformers, effectively reducing memory consumption and computational time. Additionally, to further reduce computational costs, we employed BF16 precision and utilized the DeepSpeed library to achieve distributed training.\n\nIn terms of hyperparameter settings, the training epoch for Emotion-LLaMA was selected from 10,20,30,40, while that for PandaGPT was chosen within the range of 1 to 10. For the remaining models, the training epoch was selected from Regarding model training strategies, VideoLLaMA2-AV, Ola, and HumanOmni adopted a two-stage training and finetuning approach. In the first stage, the LLM parameters were frozen, focusing on training the audio and visual encoders and projectors to enable the model to efficiently extract and understand audio and visual information. In the second stage, the parameters of the audio and visual encoders and projectors were frozen, and the LLM was fine-tuned using the efficient LoRA fine-tuning technique to achieve a deep integration of visual, audio features, and language information, thereby further enhancing the model's performance in multimodal affective computing tasks.\n\nIn contrast, Qwen2.5Omni, MiniCPM-o, PandaGPT, and Emotion-LLaMA employed a single training strategy based on LoRA fine-tuning, directly optimizing the language model to adapt to specific task requirements.  II , MLLMs demonstrate exceptional performance on the CMU-MOSI dataset. This outstanding performance can be attributed to the dominant role of the text modality in this dataset  [55] -MLLMs can fully leverage their robust language understanding and generation capabilities by fine-tuning the language model module, thereby achieving significant performance improvements in relevant tasks and ultimately yielding excellent results on the CMU-MOSI dataset. Specifically, in the testing phase of this dataset, except for Emotion-LLaMA, all other MLLMs significantly outperform MOAC across most evaluation metrics. Among them, Qwen2.5Omni leads by 5.3% in the Acc7 metric, and HumanOmni is 2.3% higher in the Acc2 metric, with particularly notable advantages.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Main Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Results On Msa: As Shown In Table",
      "text": "However, the performance of MLLMs on the CMU-MOSEI dataset shows a divergent trend. In terms of the Acc7 metric, only HumanOmni and VideoLLaMA2-AV outperform MOAC, with improvements of 4.3% and 3.6% respectively, while the remaining MLLMs lag behind this MML model in multiple metrics. Regarding the Acc2 metric, all MLLMs perform worse than MOAC. An analysis of the training data distribution and model output results of the CMU-MOSEI dataset reveals that the dataset has a significant issue of sample  distribution imbalance-21.7% of the samples are labeled as 0. This imbalance directly causes the fine-tuned MLLMs to tend to output 0 labels during prediction, ultimately leading to poor performance in metrics such as Acc2. This phenomenon profoundly highlights the significant impact of dataset sample distribution on the sentiment analysis performance of MLLMs, suggesting that data balance is a key factor to be prioritized in model optimization.\n\nAs shown in Table  III , in CH-SIMS and CH-SIMS v2, all MLLMs achieve excellent performance except for PandaGPT and Emotion-LLaMA, which perform relatively poorly. In the CH-SIMS dataset, HumanOmni performs the best; compared with HGTFM, its Acc5 is improved by 8.1%, Acc2 by 4.6%, and F1 by 4.7%. In CH-SIMS v2, Qwen2.5Omni is the optimal model, with Acc5 improved by 3.7%, Acc2 and F1 both improved by 4.0% compared with HGTFM. The above results indicate that MLLMs exhibit more prominent performance advantages in datasets where the contributions of various modalities are more balanced  [17] , which further verifies their strong ability in fusing and processing multimodal information. Especially in data environments with good modal synergy, they can better exert their architectural advantages.\n\n2) Results on MER and MHD: In the task of MER, MLLMs all demonstrate excellent performance on the MELD dataset. As shown in Table  IV , compared with SemanticMAC, the HumanOmni model achieves a 6.7% improvement in w-Acc and a 5.2% improvement in w-F1.\n\nIn the task of MHD, the experimental results on the UR-FUNNY v2 dataset are presented in Table  IV . The Ola, MiniCPM-o, and HumanOmni models perform better than Se-manticMAC, while the performance of the remaining MLLMs is inferior to this benchmark model. Among these betterperforming models, the Ola model stands out with its w-Acc being 4.8% higher than that of SemanticMAC. Notably, although the Ola model shows average performance on multiple datasets, it exhibits excellent performance on the UR-FUNNY v2 dataset. This phenomenon indicates that different MLLMs have significant differences in their adaptability to specific datasets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Enhancing Mllm With Prompt Engineering",
      "text": "To enhance the MAC capability of MLLMs, we optimized the high-performing HumanOmni model by adopting a combined strategy of generative knowledge prompting and supervised fine-tuning. The results are presented in Table  II , III and IV. This strategy outperformed the original simple fine-tuning on all datasets, with particularly significant improvements in multi-class accuracy (Acc). Specifically, the Acc5 on CH-SIMS increased by 7.0%, and the Acc7 on CMU-MOSI rose by 3.1%. This indicates that supplementing the model with descriptive knowledge of audio and video can strengthen its understanding of the deep correlations between multimodal emotional features, thereby improving classification accuracy in complex scenarios.\n\nHowever, in CMU-MOSEI and CH-SIMS v2, the improvement from this strategy was marginal. This may be because the emotional features in these two datasets are relatively distinct, and simple SFT alone enables the model to sufficiently learn the core discriminative information. In such cases, the additional descriptive information fails to provide effective gains and may even slightly interfere with the model's judgments due to information redundancy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Case Study",
      "text": "The workflow of our prompting strategy in affective computing is illustrated in Figure  2 . Firstly, raw video and audio data are input into the model, which performs precise analysis to identify key emotional clues and generate descriptions. For the video, the model captures visual information such as \"a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to be communicating with someone off-camera\", while also meticulously recording the emotional changes: \"the firefighter's expressions change from concern to surprise and then to a more determined\". In the audio analysis, the model identifies sound elements like 'shouting', 'crying', and 'continuous, noisy, and loud noises', and matches them with emotional information such as 'pain', 'alarm', 'fear', and 'sense of urgency'.\n\nAfter the initial extraction of key clue descriptions, we reinput the raw video, audio data, generated descriptions, and dialogue texts into the model for in-depth reasoning. By fully integrating this multimodal information, the model ultimately outputs an emotional score of -1 (representing extremely negative emotion). Through the extraction of multimodal emotional description information, the model can more accurately grasp the emotional context in videos and audios, and the final experimental results verify the effectiveness of this prompting strategy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Analysis Of Input Impact",
      "text": "To investigate the contribution mechanisms of different modalities in MLLMs, this study selected the HumanOmni model, which demonstrates excellent performance across multiple datasets, to conduct unimodal analysis experiment.\n\nAs shown in Figure  3 , on the CH-SIMS dataset, the text modality exhibits a common advantage-all MLLMs achieve superior performance, indicating that the current mechanisms for processing textual information in models possess crossmodel universal effectiveness. In terms of the audio modality, the prediction performance of VideoLLaMA2-AV is significantly lower than the average level, revealing that this model may have design limitations in aspects such as audio feature encoding, the mapping of acoustic information to the semantic space, or cross-modal alignment mechanisms, making it difficult to effectively capture key information in the audio modality. In sharp contrast, HumanOmni, Qwen2.5Omni, and MiniCPM-o perform prominently in the audio modality, suggesting that these three models possess more robust modality modeling capabilities in the audio signal processing pipeline. Regarding the visual modality, Qwen2.5Omni outperforms other comparative models by a significant margin. This result indicates that the model has notable technical advantages in the visual feature extraction stage, and its visual encoder and modality fusion mechanism can better adapt to the characteristics of visual tasks in the CH-SIMS dataset, thereby more accurately capturing key visual information in video frames and converting it into effective semantic representations.\n\nAs shown in Figure  4 , on the CMU-MOSI dataset, where the text modality dominates, the text modality also shows a consistent advantage, with all MLLMs maintaining excellent performance. In the audio modality, the test results of Hu-manOmni, Qwen2.5Omni, and MiniCPM-o are significantly better than those of other MLLMs. In-depth analysis reveals that all three models employ Whisper as the audio encoder, and this encoder has undergone sufficient training for speechto-text tasks during the pre-training phase. This technical characteristic enables its performance in the standalone audio modality to be comparable to that of the text modality. The above results confirm a key conclusion: the degree of adaptation between the pre-training tasks of the audio encoder and downstream sentiment analysis tasks directly affects the performance of the model.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this paper, we have systematically evaluated state-ofthe-art MLLMs capable of simultaneous processing of audio, visual, and textual modalities. Our comprehensive benchmark assessment across multiple MAC datasets has revealed how model architectural designs and dataset characteristics influence MLLMs' performance in affective analysis tasks. We have also proposed a hybrid strategy that integrates generative knowledge prompting with supervised fine-tuning, which has significantly enhanced MLLMs' performance in affective computing tasks. These findings offer valuable insights for model optimization and highlight the potential of advanced prompt engineering strategies in unlocking the full capabilities of MLLMs for affective computing. Future work can further explore the optimization of MLLMs in more complex and diverse MAC scenarios, as well as the development of more sophisticated prompting strategies to continue pushing the boundaries of affective computing technology. 3) Ola: The Ola  [39]  is an omnimodal language model capable of processing text, images, videos, and audio inputs, achieving competitive performance in image, video, and audio understanding tasks. Its core architecture is built upon Qwen2.5, incorporating advanced visual and audio encoding capabilities. The visual encoder of Ola employs OryxViT  [62] , which is initialized from SigLIP-400M and preserves the original aspect ratio of images or video frames for arbitraryresolution visual input processing. Ola introduces a Local-Global Attention Pooling layer to reduce the token length of visual features while minimizing information loss. For audio encoding, Ola adopts a dual-encoder approach, utilizing Whisper-v3 as the speech encoder and BEATs  [33]  as the music encoder. By concatenating the embedding features of speech and music encoders across the channel dimension, Ola achieves comprehensive audio feature extraction.\n\n4) VideoLLaMA2-AV: The VideoLLaMA2 is a Video Large Language Model (Video-LLM) designed to enhance spatialtemporal modeling and audio understanding in video and audio-related tasks. Built upon its predecessor, VideoLLaMA2 introduces a tailored Spatial-Temporal Convolution (STC) connector to effectively capture the intricate spatial and temporal dynamics of video data.\n\nVideoLLaMA2 adopts a dual-branch framework comprising a Vision-Language Branch and an Audio-Language Branch. The language decoders are initialized with Qwen2  [63] . The Vision-Language Branch utilizes the CLIP (ViT-L/14) model  [31]  as its vision backbone, processing video frames individually. The Audio-Language Branch employs BEATs, a cuttingedge audio encoder, to extract audio features, which are then aligned with the dimensions of the large language model through a multilayer perceptron (MLP) block.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "5) Minicpm-O:",
      "text": "The MiniCPM-o  [37]  is an open-source multimodal large language model (MLLM) developed by OpenBMB, capable of processing image, text, audio, and video inputs and generating high-quality text and speech outputs in an end-to-end manner. The model is based on SigLip-400M, Whisper-medium-300M, and Qwen2.5-7B-Instruct with a total of 8B parameters.\n\n6) PandaGPT: The PandaGPT  [46]  is a groundbreaking multimodal model capable of processing six modalities, including image/video, text, audio, depth, thermal, and inertial measurement units, while generating text responses. Its core architecture combines the multimodal encoders from Image-Bind  [64]  and the LLM from Vicuna, creating a system for vision-and audio-grounded instruction following tasks. 7) Emotion-LLaMA: The Emotion-LLaMA  [45]  is a multimodal large language model designed for accurate emotion recognition and reasoning. The model integrates audio, visual, and textual inputs through emotion-specific encoders and employs instruction tuning on the MERR dataset  [45]  to enhance emotional recognition and reasoning capabilities.\n\nThe audio encoder employs HuBERT  [65] , while the visual encoder uses a combination of MAE (Masked Autoencoders)  [66] , VideoMAE (Masked Autoencoders for video)  [32] , and EVA (Efficient Vision Analysis)  [67]  to capture facial details, dynamics, and context. The multimodal features are aligned into a shared space using a modified LLaMA language model  [68] , which processes these inputs through a structured prompt template.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Statistics Of Datasets",
      "text": "In this study, we employ six datasets, encompassing multimodal sentiment analysis (MSA) datasets (CMU-MOSI  [16] , CMU-MOSEI  [43] , CH-SIMS  [17] , and CH-SIMS v2  [18] ), multimodal emotion recognition (MER) dataset (MELD  [19] ), and multimodal humor detection (MHD) dataset (UR-FUNNY v2  [20] ). Here, We present a concise overview of these datasets below, with detailed statistics summarized in the table VI.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Hyperparameter Setting",
      "text": "In terms of hyperparameter settings, the training epoch for Emotion-LLaMA was selected from 10,20,30,40, while that for PandaGPT was chosen within the range of 1 to 10. For the remaining models, the training epoch was selected from 1,2,3. The learning rate of the models was adjusted within the range of 1e-6 to 1e-3. For the LoRA module, the rank and α parameters were set to  8, 16, 64, 128, 256 and 16, 32, 128, 256, 512 , respectively. Please refer to Table V for detailed information on the hyperparameter settings employed in our experiments.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic diagram of the end-to-end affective computing process",
      "page": 1
    },
    {
      "caption": "Figure 1: , MLLMs can concurrently receive raw audio, video,",
      "page": 1
    },
    {
      "caption": "Figure 2: , our approach com-",
      "page": 3
    },
    {
      "caption": "Figure 2: Enhancing MLLM Performance in MAC via knowledge generation and supervised fine-tuning.",
      "page": 4
    },
    {
      "caption": "Figure 3: Performance comparison of MLLMs in unimodal settings on the CH-",
      "page": 6
    },
    {
      "caption": "Figure 4: Performance comparison of MLLMs in unimodal settings on the",
      "page": 6
    },
    {
      "caption": "Figure 2: Firstly, raw video and audio",
      "page": 7
    },
    {
      "caption": "Figure 3: , on the CH-SIMS dataset, the text",
      "page": 7
    },
    {
      "caption": "Figure 4: , on the CMU-MOSI dataset, where",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "†sijiemai@m.scnu.edu.cn": "Abstract—Multimodal Affective Computing\n(MAC)\naims\nto"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "recognize\nand interpret human emotions by\nintegrating\ninfor-"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "mation from diverse modalities\nsuch as\ntext, video, and audio."
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "Recent\nadvancements\nin Multimodal Large Language Models"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "(MLLMs)\nhave\nsignificantly\nreshaped\nthe\nlandscape\nof MAC"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "by\noffering\na\nunified\nframework\nfor\nprocessing\nand\naligning"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "cross-modal\ninformation. However, practical challenges remain,"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "including\nperformance\nvariability\nacross\ncomplex MAC tasks"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "and\ninsufficient\nunderstanding\nof\nhow architectural\ndesigns"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "and data\ncharacteristics\nimpact\naffective\nanalysis. To\naddress"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "these\ngaps, we\nconduct\na systematic benchmark evaluation of"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "state-of-the-art\nopen-source MLLMs\ncapable\nof\nconcurrently"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "processing audio, visual, and textual modalities across multiple"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "established MAC datasets. Our\nevaluation not\nonly\ncompares"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "the performance of\nthese MLLMs but also provides actionable"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "insights\ninto model\noptimization\nby\nanalyzing\nthe\ninfluence"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "of model\narchitectures\nand\ndataset\nproperties.\nFurthermore,"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "we propose\na novel hybrid strategy\nthat\ncombines\ngenerative"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "knowledge\nprompting with\nsupervised\nfine-tuning\nto\nenhance"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "MLLMs’ affective computing capabilities. Experimental results"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "demonstrate that this integrated approach significantly improves"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "performance\nacross\nvarious MAC tasks,\noffering\na promising"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "avenue\nfor\nfuture\nresearch and development\nin this field. Our"
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "code is released on https://github.com/LuoMSen/MLLM-MAC."
        },
        {
          "†sijiemai@m.scnu.edu.cn": ""
        },
        {
          "†sijiemai@m.scnu.edu.cn": "Index Terms—Multimodal Affective Computing, Multimodal"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "Large\nLanguage Models,\nEnd-to-End\nLearning,\nGenerative"
        },
        {
          "†sijiemai@m.scnu.edu.cn": "Knowledge Prompting."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nificant\nimprovements in MLLM performance across affective": "computing tasks.",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "annotations on full-scene inputs and integrating auxiliary cues"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "The main contributions are summarized as follows:",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "like body posture, environment, and social dynamics. Addi-"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "• We conduct the first systematic evaluation of state-of-the-",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "tionally, combining prompts with acoustic analysis or Chain-"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "art MLLMs capable of simultaneous processing of audio,",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "of-Thought\n(CoT)\nreasoning has\nshown promise"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "visual, and textual modalities.",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "recognition in conversation tasks [29]."
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "• We reveal\nthe mechanisms by which model architectural",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "However,\nexisting work\nhas\nprimarily\nexplored"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "designs\nand\ndataset\ncharacteristics\ninfluence MLLMs’",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "scenarios\nin MAC tasks. To\naddress\nthis\ngap\nin"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "performance in affective analysis tasks, providing action-",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "tasks, we propose a strategy combining generative knowledge"
        },
        {
          "nificant\nimprovements in MLLM performance across affective": "able insights for model optimization.",
          "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying": "prompting across text, audio, and video modalities."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "• We propose a hybrid strategy that\nintegrates generative"
        },
        {
          "2": "knowledge\nprompting with\nSFT. Experimental\nresults"
        },
        {
          "2": "demonstrate\nthat\nthis\napproach\nsignificantly\nenhances"
        },
        {
          "2": "MLLMs’ performance in affective computing tasks."
        },
        {
          "2": ""
        },
        {
          "2": "II. RELATED WORK"
        },
        {
          "2": "A. Multimodal Affective Computing"
        },
        {
          "2": ""
        },
        {
          "2": "MAC seeks\nto recognize and analyze human emotions by"
        },
        {
          "2": ""
        },
        {
          "2": "integrating information from multiple modalities. Traditional"
        },
        {
          "2": ""
        },
        {
          "2": "methods\noften\nrely\non\nearly\nfusion\n[22],\n[23],\nlate\nfusion"
        },
        {
          "2": ""
        },
        {
          "2": "[24],\n[25], or attention-based strategies\n[26],\n[27]. Although"
        },
        {
          "2": ""
        },
        {
          "2": "these techniques outperform unimodal methods,\nthey still\nfail"
        },
        {
          "2": ""
        },
        {
          "2": "to adequately capture\nthe\ncomplex cross-modal\ninterplay of"
        },
        {
          "2": ""
        },
        {
          "2": "affective cues\n[3]. This\nlimitation has\nspurred a recent\nshift"
        },
        {
          "2": ""
        },
        {
          "2": "towards MLLMs\n[28],\n[29]. MLLMs\nleverage\ntheir\nunified"
        },
        {
          "2": ""
        },
        {
          "2": "semantic\nspace\nand emergent\nreasoning ability to more\nef-"
        },
        {
          "2": ""
        },
        {
          "2": "fectively\ndetect\nsubtle\ninteractions\nbetween modalities\nand"
        },
        {
          "2": ""
        },
        {
          "2": "understand emotions within specific contexts [30]."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "B. Multimodal Large Language Models"
        },
        {
          "2": ""
        },
        {
          "2": "MLLMs\nare\nbuilt\nupon LLMs\nand\nintegrate multimodal"
        },
        {
          "2": ""
        },
        {
          "2": "encoders\n[31]–[33]\nthrough\nprojection\nor\ncross-attention\nto"
        },
        {
          "2": ""
        },
        {
          "2": "enable\nunified multimodal\nprocessing.\nIn\nrecent\nyears,\nthe"
        },
        {
          "2": ""
        },
        {
          "2": "open-source community has yielded powerful MLLMs,\nrang-"
        },
        {
          "2": ""
        },
        {
          "2": "ing from vision-language models with advanced visual\nrea-"
        },
        {
          "2": ""
        },
        {
          "2": "soning capabilities\n(e.g., BLIP-2 [34], LLaVA [8])\nto audio-"
        },
        {
          "2": ""
        },
        {
          "2": "language models exhibiting robust audio comprehension (e.g.,"
        },
        {
          "2": ""
        },
        {
          "2": "SALMONN [35], Qwen-Audio [36]). More recently,\nthe de-"
        },
        {
          "2": ""
        },
        {
          "2": "velopment of MLLMs is progressing towards comprehensive"
        },
        {
          "2": ""
        },
        {
          "2": "omnimodal models\nthat\nunify multiple modalities within\na"
        },
        {
          "2": ""
        },
        {
          "2": "single framework [37]–[39]. For instance, Qwen2.5-Omni [40]"
        },
        {
          "2": ""
        },
        {
          "2": "perceives\ndiverse modalities,\nincluding\ntext,\nimages,\naudio,"
        },
        {
          "2": ""
        },
        {
          "2": "and video, while simultaneously generating text and natural"
        },
        {
          "2": ""
        },
        {
          "2": "speech responses\nin a\nstreaming manner. These omnimodal"
        },
        {
          "2": ""
        },
        {
          "2": "models not only capture complex relationships between text,"
        },
        {
          "2": ""
        },
        {
          "2": "vision, and sound but also demonstrate enhanced robustness in"
        },
        {
          "2": ""
        },
        {
          "2": "real-world scenarios. In this study, we selected multiple open-"
        },
        {
          "2": ""
        },
        {
          "2": "source MLLMs that support\njoint modeling of text, video, and"
        },
        {
          "2": ""
        },
        {
          "2": "audio for benchmarking purposes."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "C. Prompting Strategy"
        },
        {
          "2": ""
        },
        {
          "2": "Effective\nprompting\nstrategies\nare\ncrucial\nfor\nenhancing"
        },
        {
          "2": "MLLMs’\naffective\nreasoning\ncapabilities. Recent\nresearch"
        },
        {
          "2": "demonstrates their potential\nfor MAC. For\ninstance, methods"
        },
        {
          "2": "like Multi-Views Prompt Learning\n[41]\neffectively\ncapture"
        },
        {
          "2": "the\nemotional\ncues\ninvolved\nin\ndifferent\nlevels\nof\nseman-"
        },
        {
          "2": "tic information, while Set-of-Vision-Text Prompting (SoVTP)"
        },
        {
          "2": "[42]\npreserves\nholistic\nscene\ncontext\nby\noverlaying\nspatial"
        },
        {
          "2": "annotations on full-scene inputs and integrating auxiliary cues"
        },
        {
          "2": "like body posture, environment, and social dynamics. Addi-"
        },
        {
          "2": "tionally, combining prompts with acoustic analysis or Chain-"
        },
        {
          "2": "of-Thought\n(CoT)\nreasoning has\nshown promise\nin emotion"
        },
        {
          "2": "recognition in conversation tasks [29]."
        },
        {
          "2": "However,\nexisting work\nhas\nprimarily\nexplored\nbimodal"
        },
        {
          "2": "scenarios\nin MAC tasks. To\naddress\nthis\ngap\nin\ntrimodal"
        },
        {
          "2": "tasks, we propose a strategy combining generative knowledge"
        },
        {
          "2": "prompting across text, audio, and video modalities."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "COMPARISON OF MLLMS AND THEIR COMPONENTS."
        },
        {
          "TABLE I": "Visual Encoder"
        },
        {
          "TABLE I": "Qwen2.5-VL"
        },
        {
          "TABLE I": "SigLIP"
        },
        {
          "TABLE I": "SigLIP"
        },
        {
          "TABLE I": "CLIP"
        },
        {
          "TABLE I": "SigLip"
        },
        {
          "TABLE I": "ImageBind"
        },
        {
          "TABLE I": "MAE,VideoMAE,EVA"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MiniCPM-o\nSigLip": "PandaGPT\nImageBind",
          "Whisper-medium\nQwen2.5 (7B)": "ImageBind\nVicuna (7B)"
        },
        {
          "MiniCPM-o\nSigLip": "Emotion-LLaMA\nMAE,VideoMAE,EVA",
          "Whisper-medium\nQwen2.5 (7B)": "HuBERT\nllama2 (7B)"
        },
        {
          "MiniCPM-o\nSigLip": "III. BENCHMARK",
          "Whisper-medium\nQwen2.5 (7B)": "B. Multimodal Large Language Models"
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "To achieve\nend-to-end affective\ncomputing,\nthe\nevaluated"
        },
        {
          "MiniCPM-o\nSigLip": "A. Datasets",
          "Whisper-medium\nQwen2.5 (7B)": "MLLMs must support\nthe collaborative input of audio, video,"
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "and\ntext. Additionally,\nthe models must\nbe\nopen-source\nto"
        },
        {
          "MiniCPM-o\nSigLip": "In this study, we employ six datasets, encompassing multi-",
          "Whisper-medium\nQwen2.5 (7B)": "enable\neffective\nSFT.\nBased\non\nthe\nabove\nrequirements,"
        },
        {
          "MiniCPM-o\nSigLip": "modal sentiment analysis (MSA) datasets (CMU-MOSI\n[16],",
          "Whisper-medium\nQwen2.5 (7B)": "this\nstudy\nselects HumanOmni\n[38], Qwen2.5Omni\n[40],"
        },
        {
          "MiniCPM-o\nSigLip": "CMU-MOSEI\n[43], CH-SIMS [17], and CH-SIMS v2 [18]),",
          "Whisper-medium\nQwen2.5 (7B)": "VideoLLaMA2-AV [44], Ola\n[39], MiniCPM-o\n2.6\n[37],"
        },
        {
          "MiniCPM-o\nSigLip": "multimodal emotion recognition (MER) dataset (MELD [19]),",
          "Whisper-medium\nQwen2.5 (7B)": "Emotion-LLaMA [45], and PandaGPT [46] as the experimen-"
        },
        {
          "MiniCPM-o\nSigLip": "and multimodal humor detection (MHD) dataset (UR-FUNNY",
          "Whisper-medium\nQwen2.5 (7B)": "tal models. Their basic information is summarized in Table I,"
        },
        {
          "MiniCPM-o\nSigLip": "v2 [20]). Here, We present a concise overview of these datasets",
          "Whisper-medium\nQwen2.5 (7B)": "and detailed characteristics can be found in Appendix A."
        },
        {
          "MiniCPM-o\nSigLip": "below, with detailed statistics summarized in the Appendix.",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "1) CMU-MOSI\nand\nCMU-MOSEI:\nThe\nCMU-MOSI",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "C. Method Overview"
        },
        {
          "MiniCPM-o\nSigLip": "dataset [16] consists of 93 YouTube videos, which are divided",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "1)\nSupervised Fine-tuning: To enhance the adaptability of"
        },
        {
          "MiniCPM-o\nSigLip": "into\n2,199\nclips, with\neach\nclip\nannotated with\nsentiment",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "MLLMs\nfor MAC tasks\nlike MSA, MER,\nand MHD,\nthe"
        },
        {
          "MiniCPM-o\nSigLip": "scores on a 7-point scale ranging from strong negative (-3) to",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "employment of SFT to adjust model parameters\nis adopted."
        },
        {
          "MiniCPM-o\nSigLip": "strong positive (+3). Likewise,\nthe CMU-MOSEI dataset [43]",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "As a task-specific optimization paradigm built on pre-trained"
        },
        {
          "MiniCPM-o\nSigLip": "encompasses 23,453 video clips derived from various online",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "models,\nSFT leverages\nlabeled\ndatasets—comprising\ninput"
        },
        {
          "MiniCPM-o\nSigLip": "platforms\nand adheres\nto the\nsame\nsentiment\nscore\nlabeling",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "samples and their corresponding target outputs—to further re-"
        },
        {
          "MiniCPM-o\nSigLip": "scheme.",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "fine model parameters,\nthereby enabling the model\nto achieve"
        },
        {
          "MiniCPM-o\nSigLip": "2) CH-SIMS and CH-SIMS v2: The CH-SIMS dataset [17]",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "better alignment with the characteristics and requirements of"
        },
        {
          "MiniCPM-o\nSigLip": "contains 2,281 refined video segments\nsourced from movies,",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "specific downstream tasks."
        },
        {
          "MiniCPM-o\nSigLip": "TV series,\nand\nvariety\nshows, with\nsentiment\nannotations",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "Furthermore,\nto mitigate\ncomputational\noverhead, we\nin-"
        },
        {
          "MiniCPM-o\nSigLip": "ranging from negative (-1)\nto positive (+1)\nfor each clip. The",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "corporated Low-Rank Adaptation\n(LoRA)\ntechnology\n[47]."
        },
        {
          "MiniCPM-o\nSigLip": "CH-SIMS v2 dataset\n[18] extends this corpus to 4,402 super-",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "Instead\nof\ndirectly modifying\nall model\nparameters, LoRA"
        },
        {
          "MiniCPM-o\nSigLip": "vised segments\nand 10,161 unsupervised segments\n(totaling",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "implements fine-tuning by injecting low-rank matrices into the"
        },
        {
          "MiniCPM-o\nSigLip": "14,563 clips), collected from 11 diverse scenarios like vlogs,",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "model’s weight matrices. Specifically,\nin LoRA-based fine-"
        },
        {
          "MiniCPM-o\nSigLip": "interviews,\nand\ntalk\nshows,\nemphasizing\nricher\nnon-verbal",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "tuning,\ntwo low-rank matrices A and B are\nintroduced. A"
        },
        {
          "MiniCPM-o\nSigLip": "behaviors while retaining the original annotation methodology.",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "rank-r matrix ∆W = A × B is\nthen constructed from these"
        },
        {
          "MiniCPM-o\nSigLip": "3) MELD: The MELD dataset [19] is a multimodal corpus",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "two matrices and added to the original weight matrix W . The"
        },
        {
          "MiniCPM-o\nSigLip": "specifically designed for emotion recognition in conversational",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "formula is as follows:"
        },
        {
          "MiniCPM-o\nSigLip": "contexts. This dataset\nis constructed based on dialogues from",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "the\ntelevision series\n”Friends”,\ncomprising over 1,400 con-",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "(1)\nWnew = W + A × B"
        },
        {
          "MiniCPM-o\nSigLip": "versational\nsequences\ncontaining 13,000 speaker utterances.",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "Each\nutterance\nis\nannotated with\none\nof\nthe\nseven\nbasic",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "Here, W is\nthe original weight matrix of\nthe pre-trained"
        },
        {
          "MiniCPM-o\nSigLip": "emotional\ncategories\n(anger,\ndisgust,\nsadness,\njoy,\nneutral,",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "model, which is typically kept fixed during fine-tuning. A and"
        },
        {
          "MiniCPM-o\nSigLip": "surprise,\nfear) as well as\nsentiment polarity labels\n(positive,",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "B are the low-rank matrices that need to be trained, and the"
        },
        {
          "MiniCPM-o\nSigLip": "negative, neutral).",
          "Whisper-medium\nQwen2.5 (7B)": ""
        },
        {
          "MiniCPM-o\nSigLip": "",
          "Whisper-medium\nQwen2.5 (7B)": "number of parameters in these matrices is significantly smaller"
        },
        {
          "MiniCPM-o\nSigLip": "4) UR-FUNNY v2: The UR-FUNNY v2 dataset\n[20]\nis a",
          "Whisper-medium\nQwen2.5 (7B)": "than that of\nthe original weight matrix W ."
        },
        {
          "MiniCPM-o\nSigLip": "diverse multimodal\nresource\nfor humor detection in natural",
          "Whisper-medium\nQwen2.5 (7B)": "2) Prompt\nStrategy:\nTo\nenhance\nthe\nperformance\nof"
        },
        {
          "MiniCPM-o\nSigLip": "language processing. Compared with the original UR-FUNNY",
          "Whisper-medium\nQwen2.5 (7B)": "MLLMs\nin MAC, we\npropose\nan\ninnovative\nstrategy\nthat"
        },
        {
          "MiniCPM-o\nSigLip": "dataset,\nit\nremoves noisy and overlapping instances from the",
          "Whisper-medium\nQwen2.5 (7B)": "synergistically\nintegrates\ngenerative\nknowledge\nprompting"
        },
        {
          "MiniCPM-o\nSigLip": "original dataset. In terms of content composition, UR-FUNNY",
          "Whisper-medium\nQwen2.5 (7B)": "with\nSFT. As\nillustrated\nin\nFigure\n2,\nour\napproach\ncom-"
        },
        {
          "MiniCPM-o\nSigLip": "v2\nincorporates\na\ngreater\nnumber\nof\ncontextual\nsentences",
          "Whisper-medium\nQwen2.5 (7B)": "mences by leveraging the\nzero-shot\ncapabilities of MLLMs"
        },
        {
          "MiniCPM-o\nSigLip": "compared to its predecessor, which enriches\nthe\ncontextual",
          "Whisper-medium\nQwen2.5 (7B)": "to extract salient descriptions pertaining to affective comput-"
        },
        {
          "MiniCPM-o\nSigLip": "information available for analysis.",
          "Whisper-medium\nQwen2.5 (7B)": "ing\ndirectly\nfrom raw video\nand\naudio\ninputs. This\ninitial"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt: Zero-shot prompt.": "",
          "Knowledge Integration": "$  Video Raw Data",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "Ouput",
          "Knowledge Integration": "",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "",
          "Knowledge Integration": "#  Audio Raw Data",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "",
          "Knowledge Integration": "",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "%   Video Description",
          "Knowledge Integration": "%  Video Description",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "%  Audio Description",
          "Knowledge Integration": "%  Audio Description",
          "Supervised Fine-tuning": ""
        },
        {
          "Prompt: Zero-shot prompt.": "",
          "Knowledge Integration": "%  Conversation Content",
          "Supervised Fine-tuning": "!"
        },
        {
          "Prompt: Zero-shot prompt.": "",
          "Knowledge Integration": "",
          "Supervised Fine-tuning": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "be communicating with someone off-camera. Throughout the video, the firefighter's expressions change from concern to surprise and then to a more determined look as"
        },
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "they seem to be conveying a message or information."
        },
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "Audio Description: The audio clip features a series of emotional vocalizations, predominantly in the form of shouting or crying. The voice is characterized by a high level"
        },
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,"
        },
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "likely representing an explosion or a similar catastrophic event. The sound is characterized by a deep rumbling and a chaotic quality, which adds to the overall sense of"
        },
        {
          "Video Description: The video features a firefighter in full gear, including a helmet with a light attached to it. The firefighter is seen holding a smartphone and appears to": "urgency and fear conveyed by the vocalizations."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "likely representing an explosion or a similar catastrophic event. The sound is characterized by a deep rumbling and a chaotic quality, which adds to the overall sense of"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "urgency and fear conveyed by the vocalizations."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "Fig. 2.\nEnhancing MLLM Performance in MAC via knowledge generation and supervised fine-tuning."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "step\nis\nfollowed\nby\na\ncomprehensive\nknowledge\naggrega-\nis based on Vicuna (7B), and Emotion-LLaMA is based on"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "tion phase, wherein we systematically consolidate multimodal\nLLaMA2 (7B)."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "2) MML: For comparative analysis, we selected the SOTA\ndata streams—including original audiovisual data,\ntheir cor-"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "methods for each dataset. Specifically, or the CMU-MOSI and\nresponding generated descriptions, and textual dialogue con-"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "CMU-MOSEI datasets, we\nchose MOAC [5], C-MIB [48],\ntent—into a unified input\nframework for\nthe MLLM. Subse-"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "MGT [49],\nand KAN-MCP [50]\nas\nbaseline methods. For\nquent SFT is designed to align the model’s output distribution"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "the CH-SIMS dataset, we used HGTFM [51] as\nthe primary\nwith the specific requirements of MAC tasks, thereby ensuring"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "comparison\nbenchmark.\nFor\nthe CH-SIMS\nv2\ndataset, we\noptimal\nadaptation to the nuances of\naffective\nanalysis. By"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "selected HGTFM and KAN-MCP as comparison benchmarks.\nfocusing on getting emotion-related semantic\nfeatures\nfrom"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "For\nthe UR-FUNNY v2 dataset, we\nadopted SemanticMAC\nunstructured multimedia data,\nthis approach helps the MLLM"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "[52] as the reference method. For the MELD dataset, we used\npay more refined attention to affective cues."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "SemanticMAC and MGT as comparison baselines."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "IV. EXPERIMENTS"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "B.\nSupervised Fine-tuning Details"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "We\nconducted experiments\nusing multiple MAC datasets"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "To\nadapt MLLMs\nto\nthe\ntask\nof MAC, we\nconducted"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "and several open-source MLLMs. Due\nto space\nconstraints,"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "supervised fine-tuning on MLLMs across six different datasets."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "detailed information on evaluation metrics and model details"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "Specifically, Qwen2.5Omni\nand MiniCPM-o models were"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "is provided in the Appendix."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "fine-tuned using the LLaMA-Factory framework [53], while"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "the other models were fine-tuned using the code from their"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "A. Evaluation Baselines"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "respective\nopen-source\nrepositories. During\nthe\nfine-tuning"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "In\nthis\nstudy, we\nemployed\nthe MLLMs,\npreviously\nin-\nprocess, we incorporated the FlashAttention-2 [54] to optimize"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "troduced,\nas\nthe\nbaseline models\nand\nfurther\nconducted\na\nthe\nattention module\nof\ntransformers,\neffectively\nreducing"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "comparative analysis with state-of-the-art (SOTA) multimodal\nmemory consumption and computational time. Additionally, to"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "machine learning (MML) methods.\nfurther reduce computational costs, we employed BF16 preci-"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "1) MLLM: As shown in Table I,\nthe MLLMs we selected\nsion and utilized the DeepSpeed library to achieve distributed"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "include Qwen2.5Omni, HumanOmni, Ola, VideoLLaMA2-AV,\ntraining."
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "In terms of hyperparameter settings,\nthe training epoch for\nMiniCPM-o, PandaGPT, and Emotion-LLaMA. Among them,"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "Emotion-LLaMA was\nselected from 10,20,30,40, while\nthat\nQwen2.5Omni, HumanOmni, Ola, and MiniCPM-o are based"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "for PandaGPT was chosen within the range of 1 to 10. For\non the same large language model\n(LLM), namely Qwen2.5"
        },
        {
          "of intensity and urgency, with variations in pitch and volume that convey a sense of distress or alarm. The background includes a continuous, loud, and chaotic noise,": "the remaining models,\nthe training epoch was\nselected from\n(7B); VideoLLaMA2-AV is based on Qwen2 (7B), PandaGPT"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "MAE↓"
        },
        {
          "TABLE II": "0.728"
        },
        {
          "TABLE II": "0.659"
        },
        {
          "TABLE II": "0.615"
        },
        {
          "TABLE II": "0.605"
        },
        {
          "TABLE II": "0.536"
        },
        {
          "TABLE II": "0.800"
        },
        {
          "TABLE II": "0.636"
        },
        {
          "TABLE II": "0.620"
        },
        {
          "TABLE II": "0.571"
        },
        {
          "TABLE II": "0.523"
        },
        {
          "TABLE II": "0.549"
        },
        {
          "TABLE II": "0.510"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "1,2,3. The\nlearning rate of\nthe models was\nadjusted within",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "V. RESULTS AND DISCUSSION"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "the\nrange of 1e-6 to 1e-3. For\nthe LoRA module,\nthe\nrank",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "A. Main Results"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "and α parameters were set\nto 8, 16, 64, 128, 256 and 16, 32,",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "1) Results on MSA: As shown in Table II, MLLMs demon-"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "128, 256, 512, respectively. During fine-tuning, we monitored",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "strate\nexceptional\nperformance\non\nthe CMU-MOSI\ndataset."
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "the model’s accuracy on the validation set to select the optimal",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "This outstanding performance can be attributed to the domi-"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "inference checkpoint. All experiments were conducted on four",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "nant role of the text modality in this dataset [55]—MLLMs can"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "NVIDIA RTX 4090 48G GPUs.",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "fully leverage their\nrobust\nlanguage understanding and gener-"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "ation capabilities by fine-tuning the language model module,"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "Regarding model\ntraining\nstrategies, VideoLLaMA2-AV,",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "thereby\nachieving\nsignificant\nperformance\nimprovements\nin"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "Ola, and HumanOmni adopted a two-stage training and fine-",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "relevant\ntasks\nand\nultimately\nyielding\nexcellent\nresults\non"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "tuning approach.\nIn the first stage,\nthe LLM parameters were",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "the CMU-MOSI dataset. Specifically,\nin the testing phase of"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "frozen,\nfocusing\non\ntraining\nthe\naudio\nand\nvisual\nencoders",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "this dataset,\nexcept\nfor Emotion-LLaMA,\nall other MLLMs"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "and projectors\nto enable the model\nto efficiently extract and",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "significantly outperform MOAC across most evaluation met-"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "understand audio and visual\ninformation. In the second stage,",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "rics. Among them, Qwen2.5Omni\nleads by 5.3% in the Acc7"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "the parameters of the audio and visual encoders and projectors",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "metric, and HumanOmni\nis 2.3% higher\nin the Acc2 metric,"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "were frozen, and the LLM was fine-tuned using the efficient",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "with particularly notable advantages."
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "LoRA fine-tuning\ntechnique\nto\nachieve\na\ndeep\nintegration",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "However,\nthe performance of MLLMs on the CMU-MOSEI"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "of visual, audio features, and language information,\nthereby",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "dataset shows a divergent\ntrend.\nIn terms of\nthe Acc7 metric,"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "further\nenhancing\nthe model’s\nperformance\nin multimodal",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "only HumanOmni and VideoLLaMA2-AV outperform MOAC,"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "affective computing tasks.",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": ""
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "with improvements of 4.3% and 3.6% respectively, while the"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "remaining MLLMs\nlag behind this MML model\nin multiple"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "In\ncontrast, Qwen2.5Omni, MiniCPM-o, PandaGPT,\nand",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "metrics. Regarding\nthe Acc2 metric,\nall MLLMs\nperform"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "Emotion-LLaMA employed a\nsingle\ntraining strategy based",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "worse\nthan MOAC. An analysis of\nthe\ntraining data distri-"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "on LoRA fine-tuning, directly optimizing the language model",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "bution and model output\nresults of\nthe CMU-MOSEI dataset"
        },
        {
          "59.1\n86.0\n86.3\n0.294\nHumanOmni(Optimized)": "to adapt\nto specific task requirements.",
          "0.770\n63.2\n86.1\n86.0\n0.249\n0.804": "reveals\nthat\nthe\ndataset\nhas\na\nsignificant\nissue\nof\nsample"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "UR-FUNNY v2"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "w-Recall ↑"
        },
        {
          "TABLE IV": "75.6"
        },
        {
          "TABLE IV": "-"
        },
        {
          "TABLE IV": "74.7"
        },
        {
          "TABLE IV": "72.3"
        },
        {
          "TABLE IV": "75.6"
        },
        {
          "TABLE IV": "80.9"
        },
        {
          "TABLE IV": "71.4"
        },
        {
          "TABLE IV": "61.2"
        },
        {
          "TABLE IV": "78.3"
        },
        {
          "TABLE IV": "79.9"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HumanOmni(Optimized)\n79.9": "HumanoMni\nQwen2.5Omni\nMiniCPM-o\nOla\nVideoLLaMA2-AV\nPandaGPT\nEmoEon-LLaMA",
          "69.0\n67.2\n79.9\n79.9\n79.9": "above\nresults\nindicate\nthat MLLMs\nexhibit more prominent"
        },
        {
          "HumanOmni(Optimized)\n79.9": "90\n85.1",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "80.7\n80.5\n79.9\n79.9\n79.9\n79.6\n78.6\n78.3\n77.9\n76.8",
          "69.0\n67.2\n79.9\n79.9\n79.9": "performance advantages in datasets where the contributions of"
        },
        {
          "HumanOmni(Optimized)\n79.9": "76.1",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "73.5\n73.1",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "70.5",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "69.4\n69.4\n69.4\n69.4\n69.4\n67.5",
          "69.0\n67.2\n79.9\n79.9\n79.9": "various modalities are more balanced [17], which further ver-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "ifies their strong ability in fusing and processing multimodal"
        },
        {
          "HumanOmni(Optimized)\n79.9": "49.5",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "Acc2 (%)\n45",
          "69.0\n67.2\n79.9\n79.9\n79.9": "information. Especially in data environments with good modal"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "synergy,\nthey can better exert\ntheir architectural advantages."
        },
        {
          "HumanOmni(Optimized)\n79.9": "22.5",
          "69.0\n67.2\n79.9\n79.9\n79.9": "2) Results on MER and MHD:\nIn the task of MER, MLLMs"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "all demonstrate excellent performance on the MELD dataset."
        },
        {
          "HumanOmni(Optimized)\n79.9": "0",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "T\nA\nV",
          "69.0\n67.2\n79.9\n79.9\n79.9": "As\nshown\nin Table\nIV,\ncompared with SemanticMAC,\nthe"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "HumanOmni model achieves a 6.7% improvement\nin w-Acc"
        },
        {
          "HumanOmni(Optimized)\n79.9": "Fig. 3.\nPerformance comparison of MLLMs in unimodal settings on the CH-",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "and a 5.2% improvement\nin w-F1."
        },
        {
          "HumanOmni(Optimized)\n79.9": "SIMS dataset.",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "In the task of MHD,\nthe experimental\nresults on the UR-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "FUNNY v2\ndataset\nare\npresented\nin Table\nIV. The Ola,"
        },
        {
          "HumanOmni(Optimized)\n79.9": "HumanoMni\nQwen2.5Omni\nMiniCPM-o\nOla\nVideoLLaMA2-AV\nPandaGPT\nEmoEon-LLaMA",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "100",
          "69.0\n67.2\n79.9\n79.9\n79.9": "MiniCPM-o, and HumanOmni models perform better than Se-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "90.6\n89.8\n89.5\n89.2\n89.2\n89",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "87.8",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "86.4\n86.1\n85.6",
          "69.0\n67.2\n79.9\n79.9\n79.9": "manticMAC, while the performance of the remaining MLLMs"
        },
        {
          "HumanOmni(Optimized)\n79.9": "75",
          "69.0\n67.2\n79.9\n79.9\n79.9": "is\ninferior\nto\nthis\nbenchmark model. Among\nthese\nbetter-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "57.7\n57.7\n56\n55.1",
          "69.0\n67.2\n79.9\n79.9\n79.9": "performing models,\nthe Ola model stands out with its w-Acc"
        },
        {
          "HumanOmni(Optimized)\n79.9": "52.2\n52.2",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "Acc2 (%)\n46.7\n50\n45.5",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "44.1\n42.3\n42.2",
          "69.0\n67.2\n79.9\n79.9\n79.9": "being 4.8% higher\nthan that of SemanticMAC. Notably,\nal-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "though the Ola model shows average performance on multiple"
        },
        {
          "HumanOmni(Optimized)\n79.9": "25",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "datasets,\nit exhibits excellent performance on the UR-FUNNY"
        },
        {
          "HumanOmni(Optimized)\n79.9": "0",
          "69.0\n67.2\n79.9\n79.9\n79.9": "v2 dataset. This phenomenon indicates that different MLLMs"
        },
        {
          "HumanOmni(Optimized)\n79.9": "T\nA\nV",
          "69.0\n67.2\n79.9\n79.9\n79.9": "have\nsignificant\ndifferences\nin\ntheir\nadaptability\nto\nspecific"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "datasets."
        },
        {
          "HumanOmni(Optimized)\n79.9": "Fig. 4.\nPerformance\ncomparison of MLLMs\nin unimodal\nsettings on the",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "CMU-MOSI dataset.",
          "69.0\n67.2\n79.9\n79.9\n79.9": ""
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "B. Enhancing MLLM with Prompt Engineering"
        },
        {
          "HumanOmni(Optimized)\n79.9": "",
          "69.0\n67.2\n79.9\n79.9\n79.9": "To enhance the MAC capability of MLLMs, we optimized"
        },
        {
          "HumanOmni(Optimized)\n79.9": "distribution imbalance—21.7% of\nthe samples are labeled as",
          "69.0\n67.2\n79.9\n79.9\n79.9": "the high-performing HumanOmni model by adopting a com-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "0. This\nimbalance directly causes\nthe fine-tuned MLLMs\nto",
          "69.0\n67.2\n79.9\n79.9\n79.9": "bined strategy of generative knowledge prompting and super-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "tend to output 0 labels during prediction, ultimately leading to",
          "69.0\n67.2\n79.9\n79.9\n79.9": "vised fine-tuning. The results are presented in Table II, III and"
        },
        {
          "HumanOmni(Optimized)\n79.9": "poor performance in metrics such as Acc2. This phenomenon",
          "69.0\n67.2\n79.9\n79.9\n79.9": "IV. This strategy outperformed the original simple fine-tuning"
        },
        {
          "HumanOmni(Optimized)\n79.9": "profoundly highlights the significant\nimpact of dataset sample",
          "69.0\n67.2\n79.9\n79.9\n79.9": "on all datasets, with particularly significant\nimprovements\nin"
        },
        {
          "HumanOmni(Optimized)\n79.9": "distribution on the sentiment analysis performance of MLLMs,",
          "69.0\n67.2\n79.9\n79.9\n79.9": "multi-class\naccuracy\n(Acc). Specifically,\nthe Acc5\non CH-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "suggesting that data balance is a key factor\nto be prioritized",
          "69.0\n67.2\n79.9\n79.9\n79.9": "SIMS increased by 7.0%, and the Acc7 on CMU-MOSI\nrose"
        },
        {
          "HumanOmni(Optimized)\n79.9": "in model optimization.",
          "69.0\n67.2\n79.9\n79.9\n79.9": "by 3.1%. This\nindicates\nthat\nsupplementing the model with"
        },
        {
          "HumanOmni(Optimized)\n79.9": "As shown in Table III,\nin CH-SIMS and CH-SIMS v2, all",
          "69.0\n67.2\n79.9\n79.9\n79.9": "descriptive knowledge of audio and video can strengthen its"
        },
        {
          "HumanOmni(Optimized)\n79.9": "MLLMs achieve excellent performance except\nfor PandaGPT",
          "69.0\n67.2\n79.9\n79.9\n79.9": "understanding of\nthe deep correlations between multimodal"
        },
        {
          "HumanOmni(Optimized)\n79.9": "and Emotion-LLaMA, which perform relatively poorly. In the",
          "69.0\n67.2\n79.9\n79.9\n79.9": "emotional\nfeatures,\nthereby improving classification accuracy"
        },
        {
          "HumanOmni(Optimized)\n79.9": "CH-SIMS dataset, HumanOmni performs the best; compared",
          "69.0\n67.2\n79.9\n79.9\n79.9": "in complex scenarios."
        },
        {
          "HumanOmni(Optimized)\n79.9": "with HGTFM,\nits Acc5 is improved by 8.1%, Acc2 by 4.6%,",
          "69.0\n67.2\n79.9\n79.9\n79.9": "However,\nin CMU-MOSEI and CH-SIMS v2,\nthe improve-"
        },
        {
          "HumanOmni(Optimized)\n79.9": "and\nF1\nby\n4.7%.\nIn CH-SIMS\nv2, Qwen2.5Omni\nis\nthe",
          "69.0\n67.2\n79.9\n79.9\n79.9": "ment from this strategy was marginal. This may be because the"
        },
        {
          "HumanOmni(Optimized)\n79.9": "optimal model, with Acc5\nimproved\nby\n3.7%, Acc2\nand",
          "69.0\n67.2\n79.9\n79.9\n79.9": "emotional features in these two datasets are relatively distinct,"
        },
        {
          "HumanOmni(Optimized)\n79.9": "F1\nboth\nimproved\nby\n4.0% compared with HGTFM. The",
          "69.0\n67.2\n79.9\n79.9\n79.9": "and simple SFT alone enables the model\nto sufficiently learn"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "boundaries of affective computing technology."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "REFERENCES"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[1]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of affective"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "computing: From unimodal analysis to multimodal fusion,” Information"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "fusion, vol. 37, pp. 98–125, 2017."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[2] Y. Wang, W. Song, W. Tao, A. Liotta, D. Yang, X. Li, S. Gao, Y. Sun,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "W. Ge, W. Zhang et al., “A systematic review on affective computing:"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "Emotion models, databases, and recent advances,” Information Fusion,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "vol. 83, pp. 19–52, 2022."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[3]\nS. R. Sanku and B. Sandhya,\n“An effective data\nfusion methodology"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "for multi-modal emotion recognition: A survey,” International Journal,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "vol. 12, no. 7, 2024."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[4] M. Li, K. Chen, Z. Bi, M. Liu, B. Peng, Q. Niu,\nJ. Liu,\nJ. Wang,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "S. Zhang, X. Pan et al., “Surveying the mllm landscape: A meta-review"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "of current surveys,” arXiv preprint arXiv:2409.18991, 2024."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[5]\nS. Mai, Y. Zeng, and H. Hu, “Learning by comparing: Boosting multi-"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "modal affective computing through ordinal\nlearning,” in Proceedings of"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "the ACM on Web Conference 2025, 2025, pp. 2120–2134."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[6] M. Gao, X. Hu, X. Yin, J. Ruan, X. Pu, and X. Wan, “Llm-based nlg"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "evaluation: Current\nstatus and challenges,” Computational Linguistics,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "pp. 1–27, 2025."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[7]\nZ. Yang, L. Li, K. Lin,\nJ. Wang, C.-C. Lin, Z. Liu,\nand L. Wang,"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "“The dawn of lmms: Preliminary explorations with gpt-4v (ision),” arXiv"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "preprint arXiv:2309.17421, vol. 9, no. 1, p. 1, 2023."
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "[8] H. Liu, C. Li, Q. Wu,\nand Y.\nJ. Lee,\n“Visual\ninstruction\ntuning,”"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": ""
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "Advances in neural\ninformation processing systems, vol. 36, pp. 34 892–"
        },
        {
          "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe": "34 916, 2023."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "accurately capturing key visual\ninformation in video frames"
        },
        {
          "7": "and converting it\ninto effective semantic representations."
        },
        {
          "7": "As\nshown in Figure 4, on the CMU-MOSI dataset, where"
        },
        {
          "7": "the text modality dominates,\nthe text modality also shows a"
        },
        {
          "7": "consistent advantage, with all MLLMs maintaining excellent"
        },
        {
          "7": "performance.\nIn the\naudio modality,\nthe\ntest\nresults of Hu-"
        },
        {
          "7": ""
        },
        {
          "7": "manOmni, Qwen2.5Omni,\nand MiniCPM-o are\nsignificantly"
        },
        {
          "7": ""
        },
        {
          "7": "better\nthan those of other MLLMs.\nIn-depth analysis\nreveals"
        },
        {
          "7": ""
        },
        {
          "7": "that all\nthree models employ Whisper as\nthe audio encoder,"
        },
        {
          "7": ""
        },
        {
          "7": "and this encoder has undergone sufficient\ntraining for speech-"
        },
        {
          "7": ""
        },
        {
          "7": "to-text\ntasks\nduring\nthe\npre-training\nphase. This\ntechnical"
        },
        {
          "7": ""
        },
        {
          "7": "characteristic enables its performance in the standalone audio"
        },
        {
          "7": ""
        },
        {
          "7": "modality\nto\nbe\ncomparable\nto\nthat\nof\nthe\ntext modality."
        },
        {
          "7": ""
        },
        {
          "7": "The\nabove\nresults\nconfirm a key conclusion:\nthe degree of"
        },
        {
          "7": ""
        },
        {
          "7": "adaptation between the pre-training tasks of the audio encoder"
        },
        {
          "7": ""
        },
        {
          "7": "and downstream sentiment analysis\ntasks directly affects\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "performance of\nthe model."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "VI. CONCLUSIONS"
        },
        {
          "7": ""
        },
        {
          "7": "In\nthis\npaper, we\nhave\nsystematically\nevaluated\nstate-of-"
        },
        {
          "7": "the-art MLLMs capable of simultaneous processing of audio,"
        },
        {
          "7": "visual, and textual modalities. Our comprehensive benchmark"
        },
        {
          "7": "assessment\nacross multiple MAC datasets has\nrevealed how"
        },
        {
          "7": "model architectural designs and dataset characteristics\ninflu-"
        },
        {
          "7": "ence MLLMs’\nperformance\nin\naffective\nanalysis\ntasks. We"
        },
        {
          "7": "have also proposed a hybrid strategy that\nintegrates genera-"
        },
        {
          "7": "tive knowledge prompting with supervised fine-tuning, which"
        },
        {
          "7": "has significantly enhanced MLLMs’ performance in affective"
        },
        {
          "7": "computing\ntasks. These findings\noffer\nvaluable\ninsights\nfor"
        },
        {
          "7": "model optimization and highlight\nthe potential of\nadvanced"
        },
        {
          "7": "prompt engineering strategies in unlocking the full capabilities"
        },
        {
          "7": "of MLLMs for affective computing. Future work can further"
        },
        {
          "7": ""
        },
        {
          "7": "explore\nthe\noptimization\nof MLLMs\nin more\ncomplex\nand"
        },
        {
          "7": ""
        },
        {
          "7": "diverse MAC scenarios, as well as\nthe development of more"
        },
        {
          "7": ""
        },
        {
          "7": "sophisticated\nprompting\nstrategies\nto\ncontinue\npushing\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "boundaries of affective computing technology."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "REFERENCES"
        },
        {
          "7": ""
        },
        {
          "7": "[1]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of affective"
        },
        {
          "7": "computing: From unimodal analysis to multimodal fusion,” Information"
        },
        {
          "7": ""
        },
        {
          "7": "fusion, vol. 37, pp. 98–125, 2017."
        },
        {
          "7": ""
        },
        {
          "7": "[2] Y. Wang, W. Song, W. Tao, A. Liotta, D. Yang, X. Li, S. Gao, Y. Sun,"
        },
        {
          "7": "W. Ge, W. Zhang et al., “A systematic review on affective computing:"
        },
        {
          "7": "Emotion models, databases, and recent advances,” Information Fusion,"
        },
        {
          "7": ""
        },
        {
          "7": "vol. 83, pp. 19–52, 2022."
        },
        {
          "7": ""
        },
        {
          "7": "[3]\nS. R. Sanku and B. Sandhya,\n“An effective data\nfusion methodology"
        },
        {
          "7": "for multi-modal emotion recognition: A survey,” International Journal,"
        },
        {
          "7": "vol. 12, no. 7, 2024."
        },
        {
          "7": ""
        },
        {
          "7": "[4] M. Li, K. Chen, Z. Bi, M. Liu, B. Peng, Q. Niu,\nJ. Liu,\nJ. Wang,"
        },
        {
          "7": ""
        },
        {
          "7": "S. Zhang, X. Pan et al., “Surveying the mllm landscape: A meta-review"
        },
        {
          "7": "of current surveys,” arXiv preprint arXiv:2409.18991, 2024."
        },
        {
          "7": "[5]\nS. Mai, Y. Zeng, and H. Hu, “Learning by comparing: Boosting multi-"
        },
        {
          "7": ""
        },
        {
          "7": "modal affective computing through ordinal\nlearning,” in Proceedings of"
        },
        {
          "7": ""
        },
        {
          "7": "the ACM on Web Conference 2025, 2025, pp. 2120–2134."
        },
        {
          "7": "[6] M. Gao, X. Hu, X. Yin, J. Ruan, X. Pu, and X. Wan, “Llm-based nlg"
        },
        {
          "7": "evaluation: Current\nstatus and challenges,” Computational Linguistics,"
        },
        {
          "7": ""
        },
        {
          "7": "pp. 1–27, 2025."
        },
        {
          "7": ""
        },
        {
          "7": "[7]\nZ. Yang, L. Li, K. Lin,\nJ. Wang, C.-C. Lin, Z. Liu,\nand L. Wang,"
        },
        {
          "7": "“The dawn of lmms: Preliminary explorations with gpt-4v (ision),” arXiv"
        },
        {
          "7": "preprint arXiv:2309.17421, vol. 9, no. 1, p. 1, 2023."
        },
        {
          "7": ""
        },
        {
          "7": "[8] H. Liu, C. Li, Q. Wu,\nand Y.\nJ. Lee,\n“Visual\ninstruction\ntuning,”"
        },
        {
          "7": ""
        },
        {
          "7": "Advances in neural\ninformation processing systems, vol. 36, pp. 34 892–"
        },
        {
          "7": "34 916, 2023."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "[31] A. Radford,\nJ. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "8": "G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable"
        },
        {
          "8": "International\nvisual models\nfrom natural\nlanguage\nsupervision,”\nin"
        },
        {
          "8": "conference on machine learning.\nPmLR, 2021, pp. 8748–8763."
        },
        {
          "8": "[32]\nZ. Tong, Y. Song, J. Wang, and L. Wang, “Videomae: Masked autoen-"
        },
        {
          "8": "coders are data-efficient\nlearners for self-supervised video pre-training,”"
        },
        {
          "8": "Advances in neural\ninformation processing systems, vol. 35, pp. 10 078–"
        },
        {
          "8": "10 093, 2022."
        },
        {
          "8": "[33]\nS. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and F. Wei,"
        },
        {
          "8": "arXiv\npreprint\n“Beats: Audio\npre-training with\nacoustic\ntokenizers,”"
        },
        {
          "8": "arXiv:2212.09058, 2022."
        },
        {
          "8": "[34]\nJ. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-"
        },
        {
          "8": "image\npre-training with\nfrozen\nimage\nencoders\nand\nlarge\nlanguage"
        },
        {
          "8": "conference on machine\nmodels,”\nin International\nlearning.\nPMLR,"
        },
        {
          "8": "2023, pp. 19 730–19 742."
        },
        {
          "8": "[35] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma,"
        },
        {
          "8": "and C. Zhang,\n“Salmonn: Towards generic hearing abilities\nfor\nlarge"
        },
        {
          "8": "language models,” arXiv preprint arXiv:2310.13289, 2023."
        },
        {
          "8": ""
        },
        {
          "8": "[36] Y. Chu, J. Xu, X. Zhou, Q. Yang, S. Zhang, Z. Yan, C. Zhou, and J. Zhou,"
        },
        {
          "8": ""
        },
        {
          "8": "“Qwen-audio: Advancing\nuniversal\naudio\nunderstanding\nvia\nunified"
        },
        {
          "8": ""
        },
        {
          "8": "large-scale audio-language models,” arXiv preprint arXiv:2311.07919,"
        },
        {
          "8": ""
        },
        {
          "8": "2023."
        },
        {
          "8": ""
        },
        {
          "8": "[37] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao,"
        },
        {
          "8": ""
        },
        {
          "8": "Z. He et al., “Minicpm-v: A gpt-4v level mllm on your phone,” arXiv"
        },
        {
          "8": ""
        },
        {
          "8": "preprint arXiv:2408.01800, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[38]\nJ.\nZhao, Q. Yang, Y.\nPeng, D. Bai,\nS. Yao, B.\nSun, X. Chen,"
        },
        {
          "8": ""
        },
        {
          "8": "et\nS.\nFu, X. Wei, L. Bo\nal.,\n“Humanomni: A large\nvision-speech"
        },
        {
          "8": ""
        },
        {
          "8": "language model for human-centric video understanding,” arXiv preprint"
        },
        {
          "8": ""
        },
        {
          "8": "arXiv:2501.15111, 2025."
        },
        {
          "8": ""
        },
        {
          "8": "[39]\nZ. Liu, Y. Dong,\nJ. Wang, Z. Liu, W. Hu,\nJ. Lu,\nand Y. Rao,\n“Ola:"
        },
        {
          "8": ""
        },
        {
          "8": "Pushing the frontiers of omni-modal\nlanguage model with progressive"
        },
        {
          "8": ""
        },
        {
          "8": "modality alignment,” arXiv e-prints, pp. arXiv–2502, 2025."
        },
        {
          "8": ""
        },
        {
          "8": "[40]\nJ. Xu, Z. Guo,\nJ. He, H. Hu, T. He,\nS. Bai, K. Chen,\nJ. Wang,"
        },
        {
          "8": ""
        },
        {
          "8": "Y\n. Fan, K. Dang et al., “Qwen2. 5-omni technical report,” arXiv preprint"
        },
        {
          "8": ""
        },
        {
          "8": "arXiv:2503.20215, 2025."
        },
        {
          "8": ""
        },
        {
          "8": "[41] Q. Xu, Y. Wei, S. Yuan,\nJ. Wu, L. Wang,\nand C. Wu,\n“Learning"
        },
        {
          "8": ""
        },
        {
          "8": "emotional\nprompt\nfeatures with multiple\nviews\nfor\nvisual\nemotion"
        },
        {
          "8": ""
        },
        {
          "8": "analysis,” Information Fusion, vol. 108, p. 102366, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[42]\nZ. Wang, Q. Zhang, P. Zhang, W. Niu, K. Zhang, R. Sankaranarayana,"
        },
        {
          "8": ""
        },
        {
          "8": "S. Caldwell, and T. Gedeon, “Visual and textual prompts\nin vllms\nfor"
        },
        {
          "8": ""
        },
        {
          "8": "IEEE Transactions\non Circuits\nand\nenhancing\nemotion\nrecognition,”"
        },
        {
          "8": ""
        },
        {
          "8": "Systems for Video Technology, 2025."
        },
        {
          "8": ""
        },
        {
          "8": "[43] A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria,\nand L.-P. Morency,"
        },
        {
          "8": ""
        },
        {
          "8": "“Multimodal\nlanguage\nanalysis\nin\nthe wild: Cmu-mosei\ndataset\nand"
        },
        {
          "8": ""
        },
        {
          "8": "the 56th Annual\ninterpretable dynamic fusion graph,” in Proceedings of"
        },
        {
          "8": ""
        },
        {
          "8": "Meeting of\nthe Association for Computational Linguistics\n(Volume 1:"
        },
        {
          "8": ""
        },
        {
          "8": "Long Papers), 2018, pp. 2236–2246."
        },
        {
          "8": ""
        },
        {
          "8": "[44]\nZ. Cheng,\nS. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu,"
        },
        {
          "8": ""
        },
        {
          "8": "W. Zhang, Z. Luo, D. Zhao et al., “Videollama 2: Advancing spatial-"
        },
        {
          "8": ""
        },
        {
          "8": "arXiv\ntemporal modeling\nand\naudio\nunderstanding\nin\nvideo-llms,”"
        },
        {
          "8": ""
        },
        {
          "8": "preprint arXiv:2406.07476, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[45]\nZ. Cheng, Z.-Q. Cheng, J.-Y. He, K. Wang, Y. Lin, Z. Lian, X. Peng,"
        },
        {
          "8": ""
        },
        {
          "8": "and A. Hauptmann, “Emotion-llama: Multimodal emotion recognition"
        },
        {
          "8": ""
        },
        {
          "8": "and reasoning with instruction tuning,” Advances in Neural Information"
        },
        {
          "8": ""
        },
        {
          "8": "Processing Systems, vol. 37, pp. 110 805–110 853, 2024."
        },
        {
          "8": ""
        },
        {
          "8": "[46] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, “Pandagpt: One model"
        },
        {
          "8": ""
        },
        {
          "8": "to instruction-follow them all,” arXiv preprint arXiv:2305.16355, 2023."
        },
        {
          "8": ""
        },
        {
          "8": "[47]\nE.\nJ. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,"
        },
        {
          "8": ""
        },
        {
          "8": "W. Chen et al., “Lora: Low-rank adaptation of\nlarge language models.”"
        },
        {
          "8": ""
        },
        {
          "8": "ICLR, vol. 1, no. 2, p. 3, 2022."
        },
        {
          "8": "[48]\nS. Mai, Y. Zeng,\nand H. Hu,\n“Multimodal\ninformation\nbottleneck:"
        },
        {
          "8": "Learning minimal sufficient unimodal and multimodal\nrepresentations,”"
        },
        {
          "8": "IEEE Transactions on Multimedia, vol. 25, pp. 4121–4134, 2022."
        },
        {
          "8": "[49]\nS. Mai, Y. Zeng, A. Xiong, and H. Hu, “Injecting multimodal\ninforma-"
        },
        {
          "8": "tion into pre-trained language model for multimodal sentiment analysis,”"
        },
        {
          "8": "IEEE Transactions on Affective Computing, 2025."
        },
        {
          "8": "[50] M.\nLuo, Y.\nJiang,\nand\nS. Mai,\n“Towards\nexplainable\nfusion\nand"
        },
        {
          "8": "arXiv\npreprint\nbalanced\nlearning\nin multimodal\nsentiment\nanalysis,”"
        },
        {
          "8": "arXiv:2504.12151, 2025."
        },
        {
          "8": "[51] C. Yang, Z. Liang, D. Yan, Z. Hu, and T. Wu, “Hgtfm: Hierarchical"
        },
        {
          "8": "gating-driven transformer fusion model for robust multimodal sentiment"
        },
        {
          "8": "analysis,” IEEE Access, 2025."
        },
        {
          "8": "[52] R.\nLin,\nY\n.\nZeng,\nS. Mai,\nand\nH.\nHu,\n“End-to-end\nsemantic-"
        },
        {
          "8": "arXiv\npreprint\ncentric\nvideo-based multimodal\naffective\ncomputing,”"
        },
        {
          "8": "arXiv:2408.07694, 2024."
        },
        {
          "8": "[53] Y. Zheng, R. Zhang,\nJ. Zhang, Y. Ye, Z. Luo, Z. Feng,\nand Y. Ma,"
        },
        {
          "8": "“Llamafactory: Unified efficient fine-tuning of 100+ language models,”"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "strength and direction of the relationship between the model’s"
        },
        {
          "9": ""
        },
        {
          "9": "predictions and human annotations."
        },
        {
          "9": ""
        },
        {
          "9": "2) CH-SIMS and CH-SIMS v2 datasets.: We use the fol-"
        },
        {
          "9": ""
        },
        {
          "9": "lowing metrics:\n(1) Acc5:\nthe accuracy of dividing emotional"
        },
        {
          "9": ""
        },
        {
          "9": "scores\ninto five\ndiscrete\ncategories\n(predictions\nrounded\nto"
        },
        {
          "9": ""
        },
        {
          "9": "nearest\ninteger in [-1, 1]); (2) Acc3:\nthe accuracy in categoriz-"
        },
        {
          "9": ""
        },
        {
          "9": "ing emotions into three types (positive, neutral, and negative);"
        },
        {
          "9": ""
        },
        {
          "9": "(3) Acc2, F1 score, MAE, and Corr:\ntheir meanings are the"
        },
        {
          "9": ""
        },
        {
          "9": "same as those of\nthe CMU-MOSI and CMU-MOSEI dataset."
        },
        {
          "9": ""
        },
        {
          "9": "3) MELD dataset: We use\nthe\nfollowing metrics:\n(1) w-"
        },
        {
          "9": ""
        },
        {
          "9": "Acc:\nthe weighted accuracy;\n(2) w-F1:\nthe weighted average"
        },
        {
          "9": ""
        },
        {
          "9": "F1 score."
        },
        {
          "9": "4) UR-FUNNY v2 dataset: We use the following metrics:"
        },
        {
          "9": ""
        },
        {
          "9": "(1) w-Precision:\nthe weighted\nprecision;\n(2) w-Recall:\nthe"
        },
        {
          "9": ""
        },
        {
          "9": "weighted recall;\n(3) w-Acc;\n(4) w-F1."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "B. Multimodal Large Language Models"
        },
        {
          "9": ""
        },
        {
          "9": "1) Qwen2.5Omni: The Qwen2.5-Omni\n[40]\nis an end-to-"
        },
        {
          "9": ""
        },
        {
          "9": "end multimodal model\ncapable\nof\nprocessing\na\nvariety\nof"
        },
        {
          "9": ""
        },
        {
          "9": "modalities,\nincluding\ntext,\nimage,\naudio,\nand\nvideo, while"
        },
        {
          "9": ""
        },
        {
          "9": "simultaneously generating text and natural\nspeech responses."
        },
        {
          "9": ""
        },
        {
          "9": "Its\ncore\narchitecture\nfollows\nthe Thinker-Talker design. The"
        },
        {
          "9": ""
        },
        {
          "9": "Thinker is tasked with processing and interpreting text, audio,"
        },
        {
          "9": ""
        },
        {
          "9": "and video inputs\nto generate high-level\nrepresentations\nand"
        },
        {
          "9": ""
        },
        {
          "9": "corresponding\ntext. The Talker\nthen\nstreams\nspeech\ntokens"
        },
        {
          "9": ""
        },
        {
          "9": "based on the high-level representations created by the Thinker."
        },
        {
          "9": ""
        },
        {
          "9": "This architecture enables Qwen2.5-Omni\nto achieve efficient"
        },
        {
          "9": ""
        },
        {
          "9": "pre-filling,\nreal-time multimodal understanding,\nand concur-"
        },
        {
          "9": ""
        },
        {
          "9": "rent generation of\ntext and speech signals."
        },
        {
          "9": ""
        },
        {
          "9": "The base\nlanguage model of Qwen2.5-Omni\nis\na Trans-"
        },
        {
          "9": ""
        },
        {
          "9": "former\ndecoder,\ninitialized\nfrom Qwen2.5\n[56].\nIts\naudio"
        },
        {
          "9": ""
        },
        {
          "9": "encoder\nis\nbased\non Whisper-large-v3\n[57],\nand\nthe\nvideo"
        },
        {
          "9": ""
        },
        {
          "9": "encoder\ninherits\nfrom Qwen2.5-VL [58]\nand employs\na Vi-"
        },
        {
          "9": ""
        },
        {
          "9": "sion Transformer\n(ViT)\n[59]\nbased\narchitecture. Addition-"
        },
        {
          "9": ""
        },
        {
          "9": "ally, Qwen2.5-Omni\nintroduces TMRoPE (Time-aligned Mul-"
        },
        {
          "9": ""
        },
        {
          "9": "timodal RoPE),\na\nnovel\npositional\nencoding\nalgorithm. By"
        },
        {
          "9": ""
        },
        {
          "9": "decomposing\nthe\noriginal\nrotary\nembedding\ninto\ntemporal,"
        },
        {
          "9": ""
        },
        {
          "9": "height,\nand width components\nand applying them to differ-"
        },
        {
          "9": ""
        },
        {
          "9": "ent modalities\nrespectively, TMRoPE effectively\naligns\nthe"
        },
        {
          "9": ""
        },
        {
          "9": "temporal\ninformation of audio and video,\nthereby enhancing"
        },
        {
          "9": ""
        },
        {
          "9": "multimodal\nintegration."
        },
        {
          "9": "2) HumanOmni: The HumanOmni\n[38]\nis a large vision-"
        },
        {
          "9": ""
        },
        {
          "9": "speech language model designed to focus on human-centric"
        },
        {
          "9": ""
        },
        {
          "9": "video\nunderstanding.\nIts\nkey\ninnovation\nlies\nin\nthe\nability"
        },
        {
          "9": "to simultaneously process visual\nand speech information in"
        },
        {
          "9": "human-centric scenes. The model comprises three specialized"
        },
        {
          "9": "branches\nfor\nunderstanding\nface-related,\nbody-related,\nand"
        },
        {
          "9": "interaction-related scenes. An instruction-driven fusion module"
        },
        {
          "9": "dynamically adjusts the fusion weights of features from these"
        },
        {
          "9": "branches based on user\ninstructions,\nenhancing the model’s"
        },
        {
          "9": "flexibility and adaptability."
        },
        {
          "9": "HumanOmni employs SigLIP [60] as visual encoders and"
        },
        {
          "9": "Qwen2.5 [56] as base large language model. For audio pro-"
        },
        {
          "9": "cessing,\nit\nuses\nthe\naudio\npreprocessor\nand\nencoder\nfrom"
        },
        {
          "9": "Whisper-large-v3 [57],\nleveraging MLP2xGeLU [61]\nto map"
        },
        {
          "9": "audio features into the text domain,\nthus integrating them with"
        },
        {
          "9": "visual and textual\nfeatures."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "Learning Rate"
        },
        {
          "TABLE V": "2e-5"
        },
        {
          "TABLE V": "1e-4"
        },
        {
          "TABLE V": "2e-5"
        },
        {
          "TABLE V": "2e-5"
        },
        {
          "TABLE V": "1e-4"
        },
        {
          "TABLE V": "1e-6"
        },
        {
          "TABLE V": "5e-4"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "7) Emotion-LLaMA: The Emotion-LLaMA [45]\nis a mul-"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "timodal\nlarge language model designed for accurate emotion"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "recognition and reasoning. The model\nintegrates audio, visual,"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "and textual\ninputs through emotion-specific encoders and em-"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "ploys instruction tuning on the MERR dataset [45] to enhance"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "emotional\nrecognition and reasoning capabilities."
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "The audio encoder employs HuBERT [65], while the visual"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "encoder uses a combination of MAE (Masked Autoencoders)"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "[66], VideoMAE (Masked Autoencoders for video)\n[32], and"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "EVA (Efficient Vision Analysis) [67] to capture facial details,"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "dynamics, and context. The multimodal\nfeatures are aligned"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "into a shared space using a modified LLaMA language model"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "[68], which processes these inputs through a structured prompt"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "template."
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "C.\nStatistics of Datasets"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "In this study, we employ six datasets, encompassing multi-"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "modal sentiment analysis (MSA) datasets (CMU-MOSI\n[16],"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "CMU-MOSEI\n[43], CH-SIMS [17], and CH-SIMS v2 [18]),"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "multimodal emotion recognition (MER) dataset (MELD [19]),"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "and multimodal humor detection (MHD) dataset (UR-FUNNY"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "v2 [20]). Here, We present a concise overview of these datasets"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "below, with detailed statistics summarized in the table VI."
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": ""
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "D. Hyperparameter Setting"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "In terms of hyperparameter settings,\nthe training epoch for"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "Emotion-LLaMA was\nselected from 10,20,30,40, while\nthat"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "for PandaGPT was chosen within the range of 1 to 10. For"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "the remaining models,\nthe training epoch was\nselected from"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "1,2,3. The learning rate of the models was adjusted within the"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "range of 1e-6 to 1e-3. For\nthe LoRA module,\nthe rank and α"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "parameters were set\nto 8, 16, 64, 128, 256 and 16, 32, 128,"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "256, 512,\nrespectively. Please\nrefer\nto Table V for detailed"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "information on the hyperparameter\nsettings employed in our"
        },
        {
          "UR-FUNNY v2\nMHD\n7614\n980\n994": "experiments."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PandaGPT\n5 to 10": "3) Ola:\nThe Ola\n[39]\nis\nan omnimodal\nlanguage model",
          "5e-4\n32\n32": "TABLE VI"
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "STATISTICS OF DATASETS IN THE BENCHMARK."
        },
        {
          "PandaGPT\n5 to 10": "capable of processing text,\nimages, videos, and audio inputs,",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "achieving competitive performance in image, video, and au-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "Dataset\nType\n# Train\n# Valid\n# Test"
        },
        {
          "PandaGPT\n5 to 10": "dio\nunderstanding\ntasks.\nIts\ncore\narchitecture\nis\nbuilt\nupon",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "CMU-MOSI\nMSA\n1281\n229\n685"
        },
        {
          "PandaGPT\n5 to 10": "Qwen2.5,\nincorporating advanced visual and audio encoding",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "CMU-MOSEI\nMSA\n16326\n1871\n4659"
        },
        {
          "PandaGPT\n5 to 10": "capabilities. The\nvisual\nencoder\nof Ola\nemploys OryxViT",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "CH-SIMS\nMSA\n1368\n456\n457"
        },
        {
          "PandaGPT\n5 to 10": "[62], which is initialized from SigLIP-400M and preserves the",
          "5e-4\n32\n32": "CH-SIMS v2\nMSA\n2722\n647\n1034"
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "MELD\nMER\n9989\n1109\n2610"
        },
        {
          "PandaGPT\n5 to 10": "original aspect\nratio of\nimages or video frames for arbitrary-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "UR-FUNNY v2\nMHD\n7614\n980\n994"
        },
        {
          "PandaGPT\n5 to 10": "resolution\nvisual\ninput\nprocessing. Ola\nintroduces\na Local-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "Global Attention Pooling\nlayer\nto\nreduce\nthe\ntoken\nlength",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "of\nvisual\nfeatures while minimizing\ninformation\nloss.\nFor",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "7) Emotion-LLaMA: The Emotion-LLaMA [45]\nis a mul-"
        },
        {
          "PandaGPT\n5 to 10": "audio encoding, Ola adopts a dual-encoder approach, utilizing",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "timodal\nlarge language model designed for accurate emotion"
        },
        {
          "PandaGPT\n5 to 10": "Whisper-v3\nas\nthe\nspeech\nencoder\nand BEATs\n[33]\nas\nthe",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "recognition and reasoning. The model\nintegrates audio, visual,"
        },
        {
          "PandaGPT\n5 to 10": "music encoder. By concatenating the embedding features of",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "and textual\ninputs through emotion-specific encoders and em-"
        },
        {
          "PandaGPT\n5 to 10": "speech and music encoders across the channel dimension, Ola",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "ploys instruction tuning on the MERR dataset [45] to enhance"
        },
        {
          "PandaGPT\n5 to 10": "achieves comprehensive audio feature extraction.",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "emotional\nrecognition and reasoning capabilities."
        },
        {
          "PandaGPT\n5 to 10": "4) VideoLLaMA2-AV: The VideoLLaMA2 is a Video Large",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "The audio encoder employs HuBERT [65], while the visual"
        },
        {
          "PandaGPT\n5 to 10": "Language Model\n(Video-LLM) designed to enhance spatial-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "encoder uses a combination of MAE (Masked Autoencoders)"
        },
        {
          "PandaGPT\n5 to 10": "temporal modeling\nand\naudio\nunderstanding\nin\nvideo\nand",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "[66], VideoMAE (Masked Autoencoders for video)\n[32], and"
        },
        {
          "PandaGPT\n5 to 10": "audio-related tasks. Built upon its predecessor, VideoLLaMA2",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "EVA (Efficient Vision Analysis) [67] to capture facial details,"
        },
        {
          "PandaGPT\n5 to 10": "introduces a tailored Spatial-Temporal Convolution (STC) con-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "dynamics, and context. The multimodal\nfeatures are aligned"
        },
        {
          "PandaGPT\n5 to 10": "nector\nto effectively capture the intricate spatial and temporal",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "into a shared space using a modified LLaMA language model"
        },
        {
          "PandaGPT\n5 to 10": "dynamics of video data.",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "[68], which processes these inputs through a structured prompt"
        },
        {
          "PandaGPT\n5 to 10": "VideoLLaMA2 adopts a dual-branch framework comprising",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "template."
        },
        {
          "PandaGPT\n5 to 10": "a Vision-Language Branch and an Audio-Language Branch.",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "The language decoders are initialized with Qwen2 [63]. The",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "C.\nStatistics of Datasets"
        },
        {
          "PandaGPT\n5 to 10": "Vision-Language Branch utilizes the CLIP (ViT-L/14) model",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "In this study, we employ six datasets, encompassing multi-"
        },
        {
          "PandaGPT\n5 to 10": "[31] as its vision backbone, processing video frames individu-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "modal sentiment analysis (MSA) datasets (CMU-MOSI\n[16],"
        },
        {
          "PandaGPT\n5 to 10": "ally. The Audio-Language Branch employs BEATs, a cutting-",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "CMU-MOSEI\n[43], CH-SIMS [17], and CH-SIMS v2 [18]),"
        },
        {
          "PandaGPT\n5 to 10": "edge audio encoder,\nto extract audio features, which are then",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "multimodal emotion recognition (MER) dataset (MELD [19]),"
        },
        {
          "PandaGPT\n5 to 10": "aligned with\nthe\ndimensions\nof\nthe\nlarge\nlanguage model",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "and multimodal humor detection (MHD) dataset (UR-FUNNY"
        },
        {
          "PandaGPT\n5 to 10": "through a multilayer perceptron (MLP) block.",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "v2 [20]). Here, We present a concise overview of these datasets"
        },
        {
          "PandaGPT\n5 to 10": "5) MiniCPM-o:\nThe MiniCPM-o [37]\nis\nan open-source",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "",
          "5e-4\n32\n32": "below, with detailed statistics summarized in the table VI."
        },
        {
          "PandaGPT\n5 to 10": "multimodal\nlarge\nlanguage model\n(MLLM)\ndeveloped\nby",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "OpenBMB,\ncapable\nof\nprocessing\nimage,\ntext,\naudio,\nand",
          "5e-4\n32\n32": ""
        },
        {
          "PandaGPT\n5 to 10": "video\ninputs\nand\ngenerating\nhigh-quality\ntext\nand\nspeech",
          "5e-4\n32\n32": "D. Hyperparameter Setting"
        },
        {
          "PandaGPT\n5 to 10": "outputs\nin\nan\nend-to-end manner.\nThe model\nis\nbased",
          "5e-4\n32\n32": "In terms of hyperparameter settings,\nthe training epoch for"
        },
        {
          "PandaGPT\n5 to 10": "on SigLip-400M, Whisper-medium-300M, and Qwen2.5-7B-",
          "5e-4\n32\n32": "Emotion-LLaMA was\nselected from 10,20,30,40, while\nthat"
        },
        {
          "PandaGPT\n5 to 10": "Instruct with a total of 8B parameters.",
          "5e-4\n32\n32": "for PandaGPT was chosen within the range of 1 to 10. For"
        },
        {
          "PandaGPT\n5 to 10": "6) PandaGPT:\nThe PandaGPT [46]\nis\na\ngroundbreaking",
          "5e-4\n32\n32": "the remaining models,\nthe training epoch was\nselected from"
        },
        {
          "PandaGPT\n5 to 10": "multimodal model\ncapable of processing six modalities,\nin-",
          "5e-4\n32\n32": "1,2,3. The learning rate of the models was adjusted within the"
        },
        {
          "PandaGPT\n5 to 10": "cluding image/video,\ntext, audio, depth,\nthermal, and inertial",
          "5e-4\n32\n32": "range of 1e-6 to 1e-3. For\nthe LoRA module,\nthe rank and α"
        },
        {
          "PandaGPT\n5 to 10": "measurement units, while generating text\nresponses.\nIts core",
          "5e-4\n32\n32": "parameters were set\nto 8, 16, 64, 128, 256 and 16, 32, 128,"
        },
        {
          "PandaGPT\n5 to 10": "architecture combines\nthe multimodal encoders\nfrom Image-",
          "5e-4\n32\n32": "256, 512,\nrespectively. Please\nrefer\nto Table V for detailed"
        },
        {
          "PandaGPT\n5 to 10": "Bind [64] and the LLM from Vicuna, creating a system for",
          "5e-4\n32\n32": "information on the hyperparameter\nsettings employed in our"
        },
        {
          "PandaGPT\n5 to 10": "vision- and audio-grounded instruction following tasks.",
          "5e-4\n32\n32": "experiments."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "2",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "An effective data fusion methodology for multi-modal emotion recognition: A survey",
      "authors": [
        "S Sanku",
        "B Sandhya"
      ],
      "venue": "International Journal"
    },
    {
      "citation_id": "4",
      "title": "Surveying the mllm landscape: A meta-review of current surveys",
      "authors": [
        "M Li",
        "K Chen",
        "Z Bi",
        "M Liu",
        "B Peng",
        "Q Niu",
        "J Liu",
        "J Wang",
        "S Zhang",
        "X Pan"
      ],
      "year": "2024",
      "venue": "Surveying the mllm landscape: A meta-review of current surveys",
      "arxiv": "arXiv:2409.18991"
    },
    {
      "citation_id": "5",
      "title": "Learning by comparing: Boosting multimodal affective computing through ordinal learning",
      "authors": [
        "S Mai",
        "Y Zeng",
        "H Hu"
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Web Conference 2025"
    },
    {
      "citation_id": "6",
      "title": "Llm-based nlg evaluation: Current status and challenges",
      "authors": [
        "M Gao",
        "X Hu",
        "X Yin",
        "J Ruan",
        "X Pu",
        "X Wan"
      ],
      "year": "2025",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Z Yang",
        "L Li",
        "K Lin",
        "J Wang",
        "C.-C Lin",
        "Z Liu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "arxiv": "arXiv:2309.17421"
    },
    {
      "citation_id": "8",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai",
        "A Hauth",
        "K Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "10",
      "title": "Qwen-vl: A versatile vision-language model for understanding, localization",
      "authors": [
        "J Bai",
        "S Bai",
        "S Yang",
        "S Wang",
        "S Tan",
        "P Wang",
        "J Lin",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Text Reading, and Beyond"
    },
    {
      "citation_id": "11",
      "title": "A survey on multimodal large language models",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2024",
      "venue": "National Science Review"
    },
    {
      "citation_id": "12",
      "title": "Emotionqueen: A benchmark for evaluating empathy of large language models",
      "authors": [
        "Y Chen",
        "H Wang",
        "S Yan",
        "S Liu",
        "Y Li",
        "Y Zhao",
        "Y Xiao"
      ],
      "year": "2024",
      "venue": "Emotionqueen: A benchmark for evaluating empathy of large language models",
      "arxiv": "arXiv:2409.13359"
    },
    {
      "citation_id": "13",
      "title": "Eemo-bench: A benchmark for multi-modal large language models on image evoked emotion assessment",
      "authors": [
        "L Gao",
        "Z Jia",
        "Y Zeng",
        "W Sun",
        "Y Zhang",
        "W Zhou",
        "G Zhai",
        "X Min"
      ],
      "year": "2025",
      "venue": "Eemo-bench: A benchmark for multi-modal large language models on image evoked emotion assessment",
      "arxiv": "arXiv:2504.16405"
    },
    {
      "citation_id": "14",
      "title": "Can large language models help multimodal language analysis? mmla: A comprehensive benchmark",
      "authors": [
        "H Zhang",
        "Z Li",
        "Y Zhu",
        "H Xu",
        "P Wang",
        "H Zhu",
        "J Zhou",
        "J Zhang"
      ],
      "year": "2025",
      "venue": "Can large language models help multimodal language analysis? mmla: A comprehensive benchmark",
      "arxiv": "arXiv:2504.16427"
    },
    {
      "citation_id": "15",
      "title": "The future of mllm prompting is adaptive: A comprehensive experimental evaluation of prompt engineering methods for robust multimodal performance",
      "authors": [
        "A Mohanty",
        "V Parthasarathy",
        "A Shahid"
      ],
      "year": "2025",
      "venue": "The future of mllm prompting is adaptive: A comprehensive experimental evaluation of prompt engineering methods for robust multimodal performance",
      "arxiv": "arXiv:2504.10179"
    },
    {
      "citation_id": "16",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "17",
      "title": "Chsims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "18",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 international conference on multimodal interaction"
    },
    {
      "citation_id": "19",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "20",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "M Hasan",
        "W Rahman",
        "A Zadeh",
        "J Zhong",
        "M Tanveer",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Ur-funny: A multimodal language dataset for understanding humor",
      "arxiv": "arXiv:1904.06618"
    },
    {
      "citation_id": "21",
      "title": "Generated knowledge prompting for commonsense reasoning",
      "authors": [
        "J Liu",
        "A Liu",
        "X Lu",
        "S Welleck",
        "P West",
        "R Bras",
        "Y Choi",
        "H Hajishirzi"
      ],
      "year": "2021",
      "venue": "Generated knowledge prompting for commonsense reasoning",
      "arxiv": "arXiv:2110.08387"
    },
    {
      "citation_id": "22",
      "title": "Self-attentive feature-level fusion for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Gorantla",
        "S Poria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on multimedia information processing and retrieval (MIPR)"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition using feature-level fusion of facial expressions and body gestures",
      "authors": [
        "T Keshari",
        "S Palaniswamy"
      ],
      "year": "2019",
      "venue": "2019 international conference on communication and electronics systems (ICCES)"
    },
    {
      "citation_id": "24",
      "title": "Decision-level fusion method for emotion recognition using multimodal emotion recognition information",
      "authors": [
        "K.-S Song",
        "Y.-H Nho",
        "J.-H Seo",
        "D.-S Kwon"
      ],
      "year": "2018",
      "venue": "2018 15th international conference on ubiquitous robots (UR)"
    },
    {
      "citation_id": "25",
      "title": "Deep learning-based late fusion of multimodal information for emotion classification of music video",
      "authors": [
        "Y Pandeya",
        "J Lee"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "26",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W De Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon"
      ],
      "year": "2022",
      "venue": "Proceedings"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "authors": [
        "Q Yang",
        "D Bai",
        "Y.-X Peng",
        "X Wei"
      ],
      "year": "2025",
      "venue": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "arxiv": "arXiv:2501.09502"
    },
    {
      "citation_id": "29",
      "title": "Omnivox: Zero-shot emotion recognition with omni-llms",
      "authors": [
        "J Murzaku",
        "O Rambow"
      ],
      "year": "2025",
      "venue": "Omnivox: Zero-shot emotion recognition with omni-llms",
      "arxiv": "arXiv:2503.21480"
    },
    {
      "citation_id": "30",
      "title": "Mellm: Exploring llm-powered micro-expression understanding enhanced by subtle motion perception",
      "authors": [
        "Z Zhang",
        "S Zhao",
        "S Liu",
        "S Yin",
        "X Mao",
        "T Xu",
        "E Chen"
      ],
      "year": "2025",
      "venue": "Mellm: Exploring llm-powered micro-expression understanding enhanced by subtle motion perception",
      "arxiv": "arXiv:2505.07007"
    },
    {
      "citation_id": "31",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Z Tong",
        "Y Song",
        "J Wang",
        "L Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Beats: Audio pre-training with acoustic tokenizers",
      "authors": [
        "S Chen",
        "Y Wu",
        "C Wang",
        "S Liu",
        "D Tompkins",
        "Z Chen",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Beats: Audio pre-training with acoustic tokenizers",
      "arxiv": "arXiv:2212.09058"
    },
    {
      "citation_id": "34",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang",
        "W Yu",
        "G Sun",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "C Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "36",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Y Chu",
        "J Xu",
        "X Zhou",
        "Q Yang",
        "S Zhang",
        "Z Yan",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "37",
      "title": "Minicpm-v: A gpt-4v level mllm on your phone",
      "authors": [
        "Y Yao",
        "T Yu",
        "A Zhang",
        "C Wang",
        "J Cui",
        "H Zhu",
        "T Cai",
        "H Li",
        "W Zhao",
        "Z He"
      ],
      "year": "2024",
      "venue": "Minicpm-v: A gpt-4v level mllm on your phone",
      "arxiv": "arXiv:2408.01800"
    },
    {
      "citation_id": "38",
      "title": "Humanomni: A large vision-speech language model for human-centric video understanding",
      "authors": [
        "J Zhao",
        "Q Yang",
        "Y Peng",
        "D Bai",
        "S Yao",
        "B Sun",
        "X Chen",
        "S Fu",
        "X Wei",
        "L Bo"
      ],
      "year": "2025",
      "venue": "Humanomni: A large vision-speech language model for human-centric video understanding",
      "arxiv": "arXiv:2501.15111"
    },
    {
      "citation_id": "39",
      "title": "Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment",
      "authors": [
        "Z Liu",
        "Y Dong",
        "J Wang",
        "Z Liu",
        "W Hu",
        "J Lu",
        "Y Rao"
      ],
      "year": "2025",
      "venue": "Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment"
    },
    {
      "citation_id": "40",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "J Xu",
        "Z Guo",
        "J He",
        "H Hu",
        "T He",
        "S Bai",
        "K Chen",
        "J Wang",
        "Y Fan",
        "K Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "41",
      "title": "Learning emotional prompt features with multiple views for visual emotion analysis",
      "authors": [
        "Q Xu",
        "Y Wei",
        "S Yuan",
        "J Wu",
        "L Wang",
        "C Wu"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "42",
      "title": "Visual and textual prompts in vllms for enhancing emotion recognition",
      "authors": [
        "Z Wang",
        "Q Zhang",
        "P Zhang",
        "W Niu",
        "K Zhang",
        "R Sankaranarayana",
        "S Caldwell",
        "T Gedeon"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "43",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "44",
      "title": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "authors": [
        "Z Cheng",
        "S Leng",
        "H Zhang",
        "Y Xin",
        "X Li",
        "G Chen",
        "Y Zhu",
        "W Zhang",
        "Z Luo",
        "D Zhao"
      ],
      "year": "2024",
      "venue": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "45",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Z Cheng",
        "Z.-Q Cheng",
        "J.-Y He",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai"
      ],
      "year": "2023",
      "venue": "Pandagpt: One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "47",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "48",
      "title": "Multimodal information bottleneck: Learning minimal sufficient unimodal and multimodal representations",
      "authors": [
        "S Mai",
        "Y Zeng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Injecting multimodal information into pre-trained language model for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "A Xiong",
        "H Hu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Towards explainable fusion and balanced learning in multimodal sentiment analysis",
      "authors": [
        "M Luo",
        "Y Jiang",
        "S Mai"
      ],
      "year": "2025",
      "venue": "Towards explainable fusion and balanced learning in multimodal sentiment analysis",
      "arxiv": "arXiv:2504.12151"
    },
    {
      "citation_id": "51",
      "title": "Hgtfm: Hierarchical gating-driven transformer fusion model for robust multimodal sentiment analysis",
      "authors": [
        "C Yang",
        "Z Liang",
        "D Yan",
        "Z Hu",
        "T Wu"
      ],
      "year": "2025",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "52",
      "title": "End-to-end semanticcentric video-based multimodal affective computing",
      "authors": [
        "R Lin",
        "Y Zeng",
        "S Mai",
        "H Hu"
      ],
      "year": "2024",
      "venue": "End-to-end semanticcentric video-based multimodal affective computing",
      "arxiv": "arXiv:2408.07694"
    },
    {
      "citation_id": "53",
      "title": "Llamafactory: Unified efficient fine-tuning of 100+ language models",
      "authors": [
        "Y Zheng",
        "R Zhang",
        "J Zhang",
        "Y Ye",
        "Z Luo",
        "Z Feng",
        "Y Ma"
      ],
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "54",
      "title": "",
      "authors": [
        "Thailand Bangkok"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "55",
      "title": "Flashattention-2: Faster attention with better parallelism and work partitioning",
      "authors": [
        "T Dao"
      ],
      "year": "2023",
      "venue": "Flashattention-2: Faster attention with better parallelism and work partitioning",
      "arxiv": "arXiv:2307.08691"
    },
    {
      "citation_id": "56",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "57",
      "title": "Qwen2. 5-coder technical report",
      "authors": [
        "B Hui",
        "J Yang",
        "Z Cui",
        "J Yang",
        "D Liu",
        "L Zhang",
        "T Liu",
        "J Zhang",
        "B Yu",
        "K Lu"
      ],
      "year": "2024",
      "venue": "Qwen2. 5-coder technical report",
      "arxiv": "arXiv:2409.12186"
    },
    {
      "citation_id": "58",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "59",
      "title": "Qwen2. 5-vl technical report",
      "authors": [
        "S Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge",
        "S Song",
        "K Dang",
        "P Wang",
        "S Wang",
        "J Tang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-vl technical report",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "60",
      "title": "A survey on vision transformer",
      "authors": [
        "K Han",
        "Y Wang",
        "H Chen",
        "X Chen",
        "J Guo",
        "Z Liu",
        "Y Tang",
        "A Xiao",
        "C Xu",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "61",
      "title": "Sigmoid loss for language image pre-training",
      "authors": [
        "X Zhai",
        "B Mustafa",
        "A Kolesnikov",
        "L Beyer"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "62",
      "title": "Llava-onevision: Easy visual task transfer",
      "authors": [
        "B Li",
        "Y Zhang",
        "D Guo",
        "R Zhang",
        "F Li",
        "H Zhang",
        "K Zhang",
        "P Zhang",
        "Y Li",
        "Z Liu"
      ],
      "year": "2024",
      "venue": "Llava-onevision: Easy visual task transfer",
      "arxiv": "arXiv:2408.03326"
    },
    {
      "citation_id": "63",
      "title": "Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution",
      "authors": [
        "Z Liu",
        "Y Dong",
        "Z Liu",
        "W Hu",
        "J Lu",
        "Y Rao"
      ],
      "year": "2024",
      "venue": "Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution",
      "arxiv": "arXiv:2409.12961"
    },
    {
      "citation_id": "64",
      "title": "Qwen2 technical report",
      "authors": [
        "Q Team"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report",
      "arxiv": "arXiv:2407.10671"
    },
    {
      "citation_id": "65",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar",
        "A El-Nouby",
        "Z Liu",
        "M Singh",
        "K Alwala",
        "A Joulin",
        "I Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "66",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "67",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "68",
      "title": "Eva: Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Y Fang",
        "W Wang",
        "B Xie",
        "Q Sun",
        "L Wu",
        "X Wang",
        "T Huang",
        "X Wang",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "69",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    }
  ]
}