{
  "paper_id": "2509.22168v1",
  "title": "Teaching Ai To Feel: A Collaborative, Full-Body Exploration Of Emotive Communication",
  "published": "2025-09-26T10:28:56Z",
  "authors": [
    "Esen K. Tütüncü",
    "Lissette Lemus",
    "Kris Pilcher",
    "Holger Sprengel",
    "Jordi Sabater-Mir"
  ],
  "keywords": [
    "Affective Computing",
    "Full-Body Emotion Recognition",
    "Participatory AI",
    "Real-Time Feedback Systems",
    "Embodied Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Commonaiverse is an interactive installation exploring human emotions through full-body motion tracking and real-time AI feedback. Participants engage in three phases: Teaching, Exploration and the Cosmos Phase, collaboratively expressing and interpreting emotions with the system. The installation integrates MoveNet for precise motion tracking and a multi-recommender AI system to analyze emotional states dynamically, responding with adaptive audiovisual outputs. By shifting from top-down emotion classification to participant-driven, culturally diverse definitions, we highlight",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction: A Collective Embodiment Of Emotion",
      "text": "Artificial intelligence (AI) systems are increasingly tasked with interpreting human emotions, with much of the field historically rooted in exploratory but reductionist frameworks such as Ekman's Facial Action Coding System  [9] . This influential work proposed that universal categories of emotions could be tied to specific facial expressions, offering a seemingly intuitive pathway for decoding human affect. The appeal of facial expression analysis lies in its simplicity: the face, as a visible and universally present interface of emotion, provides a natural focal point for researchers aiming to systematize emotional understanding. Studies built upon Ekman's framework have driven significant advancements in emotion recognition technologies, enabling applications in diverse domains such as advertising, health monitoring, and workplace productivity  [20] . Despite its influence, this paradigm has faced growing criticism for oversimplifying the multifaceted nature of emotions. The assumption that emotions are biologically hardwired and universally expressed has been challenged by scholars such as Barrett and Russell, who argue that emotions are constructed phenomena shaped by context, personal experience, and cultural norms  [3, 24] . These critiques expose the limitations of face-centric models, which often fail to capture the complexity, fluidity, and socially situated nature of emotional expression. Affective computing, since its inception, has largely been dominated by facial and vocal analysis, employing methods such as Support Vector Machines (SVMs), Hidden Markov Models (HMMs), and, more recently, Convolutional Neural Networks (CNNs)  [28] . While these techniques have demonstrated success in analyzing facial images, vocal tones, and textual sentiment, they often rely on static datasets that fail to account for the dynamic, context-dependent nature of emotional communication. Moreover, the emphasis on facial expressions risks perpetuating a narrow and fragmented understanding of emotions, particularly in social and embodied contexts  [21] .\n\nAnother critical limitation of existing systems-particularly in multimedia research contexts-lies in their neglect of full-body movements and gestures. While facial expressions and vocal tones play a significant role in emotional communication, research has shown that body language often conveys affective states more effectively than the face alone  [2, 25] . Gestures, postures, and movement patterns provide rich, multimodal cues that can enhance emotional interpretation, yet these remain underrepresented in current AI frameworks  [19] . Although multimodal approaches integrating facial, vocal, and textual data have shown promise  [22] , the gap in incorporating dynamic, whole-body movements remains a significant barrier to achieving a holistic understanding of emotion in interactive multimedia environments.\n\nBeyond the limitations of modality, a critical shortcoming of current emotion recognition systems is their inability to account for the nuanced cultural and social frameworks that shape emotional expression. Emotions are deeply influenced by interpersonal interactions and societal norms, which vary widely across different communities and contexts. For instance, the way joy or sadness is expressed can diverge significantly across cultures, reflecting variations in norms, traditions, and values  [10] . Studies such as those by Gendron et al. have shown that interpretations of facial expressions differ depending on cultural perspectives, emphasizing the need for AI systems to move beyond assumptions of universal emotional standards  [13] . Addressing these gaps requires AI models that can adapt dynamically to diverse emotional landscapes, integrating localized and context-sensitive insights to ensure inclusivity and relevance.\n\nCompounding these issues is the reliance on predefined emotion categories, which oversimplifies the fluid and often overlapping nature of human affect. Emotions exist on a spectrum and can be experienced simultaneously, making their classification into discrete categories inherently reductive. As mentioned before, Barrett and Russell advocate for a constructionist view of emotions, emphasizing their emergent and context-dependent nature  [3] . Similarly, Sherry Turkle critiques the growing trend of quantifying emotions into binary or discrete values, warning that such approaches risk prioritizing convenience over depth and fostering superficial connections  [27] .\n\nEthical considerations further complicate the deployment of AI emotion recognition systems. Crawford and Calo caution that reductionist models often fail to address biases inherent in their datasets, reinforcing systemic inequalities and overlooking privacy concerns  [7] . The emphasis on facial expressions, for example, has been shown to carry racial biases, further marginalizing underrepresented groups. These limitations underscore the need for AI systems that move beyond static datasets, predefined labels, and narrow modalities to engage with emotions as embodied, dynamic, and culturally situated phenomena.\n\nIn light of these challenges, the question arises: How can AI move beyond its current limitations to engage with emotions in ways that are more inclusive, adaptive, and reflective of their embodied nature? To explore this question, we present Commonaiverse, an interactive multimedia installation that enables individuals to express and interpret human emotions through full-body movements, exploring the dynamic relationship between humans and AI in realtime. By emphasizing participatory and multisensory engagement, Commonaiverse seeks to reframe how emotions are understood, not as fixed categories but as living, evolving expressions grounded in social and cultural contexts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Designing The Commonaiverse",
      "text": "The Commonaiverse installation was conceived as a space to explore and express emotions through full-body movements, encouraging participants to reconnect with the physicality of their emotions while acknowledging the interconnectedness of human experience. In designing this system, we sought to challenge the increasingly isolated and digitized modes of expression in modern life by fostering collaboration and mutual engagement. The installation deliberately requires at least two participants, emphasizing the inherently social and interdependent nature of emotional communication.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conceptual Framework",
      "text": "The design of Commonaiverse is based in the understanding that emotions emerge through interactions,both with others and the environment, and are deeply tied to physical movement. By emphasizing full-body gestures, postures, and dynamic movements, the installation fosters a multisensory exploration of emotions, bridging the gap between abstract data representations and lived, embodied experiences. This contrasts sharply with the disembodied focus of traditional AI emotion recognition systems, which often reduce affective states to facial or vocal features alone. To ensure participants feel comfortable expressing themselves, the space was designed as an enclosed, private environment, shielding individuals from external judgment or distractions. This enclosed design, illustrated in Figures  2  and 3 , encourages authentic, uninhibited participation by providing a safe setting for emotional exploration. The room itself, with its integrated visual and spatial elements, supports the fluid exchange of energy and interaction between participants, reinforcing the relational nature of emotions while creating a contained yet immersive narrative environment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Early Iterations",
      "text": "Initial sketches and prototypes focused on creating a reactive environment using LED light strips with low-resolution grids placed along the walls (see Figure  4 ). These early designs mapped participants' movements onto the grid, using shifting colors and patterns to represent their emotional states in real time. However, observations during testing revealed that these visualizations often distracted participants, drawing their attention to the walls rather than fostering connection with their co-participants or encouraging self-expression.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Refinement Of The Interactive Space",
      "text": "Recognizing this limitation, we shifted the focus of the design to prioritize the participants' bodily interactions and the AI's responses rather than emphasizing external visual feedback. The final design minimized reliance on wall-based visuals and instead introduced dynamic, abstract audiovisual outputs projected into the space and the sheer fabric walls via three lasers mounted on the ceiling, which can be seen in Figure  5 . This decision allowed the participants to remain present in the shared experience rather than disengaging to observe the system's reactions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interaction Flow",
      "text": "The interaction within Commonaiverse unfolds in three sequential phases, highlighted in Figure  1 , with the entire experience lasting between 15-20 minutes. Each phase varies in duration, typically lasting 5-7 minutes, and is designed to deepen the participants' engagement with the system and one another:\n\n• Teaching Phase: Participants begin by demonstrating specific emotional states, such as sadness, joy, or anger, using full-body movements. For instance, a participant might slowly crouch and hunch their shoulders to express sadness or leap with outstretched arms to convey joy. These movements are captured and analyzed in real-time. The AI system learns from these movements, creating a data map that correlates body gestures with emotional labels. This phase emphasizes active teaching, where participants co-create the emotional lexicon with the AI rather than working from a pre-existing dataset. • Exploration Phase: In this phase, participants are invited to perform unscripted, free-form movements. The agent attempts to interpret their emotions based on the patterns it has learned during the teaching phase. This introduces a dynamic feedback loop, where participants can evaluate and respond to the AI's interpretations. The system's responses are displayed as abstract audiovisual outputs, evolving in real time to reflect the emotional interplay between participants and the AI. • Emotional Cosmos Phase: The culmination of the interaction is represented as a digital \"cosmos\" that visualizes the emotional exchanges that occurred during the session. This visualization is displayed on an external panel, also made accessible via a unique QR code generated for each session to retrieve afterwards. This phase provides participants with a lasting representation of their engagement, highlighting the relational and evolving nature of emotions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Implementation 3.1 Body Tracking",
      "text": "For the body tracking we used MoveNet, a state-of-the-art human pose estimation model, for real-time tracking of participants' fullbody movements. MoveNet was selected due to its demonstrated robustness and high accuracy compared to other models in both controlled and real-world environments, as highlighted in recent evaluations  [14, 18] . Its lightweight architecture allows for efficient operation on edge devices, ensuring smooth interaction without latency, which is critical for maintaining participant immersion. MoveNet operates by estimating 17 key body points, including major joints and body landmarks, with high temporal precision. The system is optimized to handle variations in body orientation, lighting conditions, and occlusions, which are common in interactive installations. Data streams from MoveNet were processed through custom Python scripts, using the TensorFlow Lite framework for compatibility. These scripts were responsible for configuring the model, interpreting keypoint data, and transmitting pose estimations to the multimedia components of the installation via Open Sound Control (OSC).\n\nTo enhance the temporal resolution and ensure robustness against erratic movements, post-processing techniques were applied. These included smoothing algorithms to interpolate noisy frames and dynamic calibration to adapt to variations in participant height and posture. Additionally, a confidence threshold was established for each keypoint to filter out inaccuracies, enabling the system to focus on meaningful gestures and movements.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Ai Pipeline",
      "text": "The AI architecture operates through a structured multi-recommender system  [12] , analyzing emotional expression in real-time across distinct phases of interaction. The multi-recommender approach relies on specialized entities (recommenders) that analyze data from different perspectives, such as movement amplitude, speed, frequency, and interpersonal proximity, to interpret emotions. Each recommender focuses on specific interaction aspects, providing its own assessment. These inputs from the recommenders are then aggregated to establish a final consensus.This framework ensures that participants' movements and emotional states are accurately captured, contextualized, and adapted over the course of the installation.\n\nThe system is structured into three primary phases, each building upon the preceding one to deepen engagement and refine emotional interpretation: • Preparation Phase: The system initializes by detecting predefined emotions-happiness, relaxation, anger, and sadness-using behavioral metrics such as movement amplitude, speed, and proximity of participants. MoveNet provides the foundational motion data, which is analyzed to establish a baseline emotional state. This phase ensures the system is primed to adapt to dynamic interactions. • Detection Phase: During live interactions, the system iteratively detects and refines emotional states using continuous data streams from MoveNet alongside contextual inputs. The AI dynamically adjusts its predictions to reflect emerging movement patterns and behaviors. By integrating these insights, the system generates audiovisual outputs that align with participants' evolving emotional expressions, ensuring the experience remains responsive and immersive. • Evaluation and Adaptation Phase: Detected emotional states are monitored over time to assess trends or inconsistencies. If shifts in behavior occur, the system reevaluates its predictions and updates its emotional mapping. This phase not only ensures alignment with contextual factors but also provides participants with feedback reflective of their sustained interactions.\n\nThe multi-recommender system enables granular analysis across these phases, with specific modules tailored to the installation:\n\n(1) Behavioral Data Recommender (REC1): REC1 extracts movement features such as speed, amplitude, and participant proximity. These metrics form the foundation for identifying emotional states based on physical patterns, providing a direct link between body language and affective states. (3) Longitudinal Emotion Recommender (REC3): REC3 tracks emotional trends across time, analyzing how states evolve during a session. This module adds a temporal layer to the AI's insights, allowing the system to adapt to sustained or shifting emotional expressions dynamically.\n\nWhile the Commonaiverse installation emphasizes REC1, REC2, and REC3, the architecture also includes provisions for future expansions with additional modules. These modules address challenges associated with traditional systems that rely on pre-made datasets, instead utilizing participant-driven data generated during the experience to ensure adaptability and context-aware emotional interpretations. Specifically:\n\n(1) Facial Expression Recommender (REC4): Processes facial cues dynamically, avoiding reliance on static datasets by integrating real-time participant input. This approach prioritizes context-sensitive interpretations that better reflect the nuances of live interactions. The modularity of the multi-recommender system allows for scalability, ensuring that future expansions can integrate diverse data types for richer emotional analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interactive Real-Time Feedback",
      "text": "The interactive feedback system in Commonaiverse was developed to integrate participants' body movements and emotional states into the installation's audiovisual outputs in real time. Using TouchDesigner [16], the system processed continuous data streams received via OSC from the motion tracking and AI emotion recognition components. These data streams included body position, movement velocity, and inferred emotional states, which were used to manipulate laser visuals projected on reflective fabric walls and update the main display interface visible to participants. Within TouchDesigner, incoming data was mapped to control parameters of the laser projections, which consisted of geometric patterns and color schemes that dynamically adjusted based on the participants' interactions. For instance, increased motion intensity or speed influenced the complexity and fluidity of the laser visuals, while shifts in emotional states altered the colors and overall aesthetic tone of the projections. In the meanwhile, the main display provided a real-time visualization of participants' session data, including movement metrics such as quantity, speed, and range of motion, as well as emotion-specific values such as detected levels of \"happiness, \" \"relaxation, \" or \"sadness. \"\n\nThis real-time system was designed to provide participants with immediate feedback on their interactions while maintaining consistency between physical movements, interpretation of emotions, and projected outputs. By linking motion data and emotional states directly to the visual and data interfaces, the system created an interaction loop that captured and reflected participants' contributions in both artistic and measurable ways. The use of TouchDesigner allowed for flexible, real-time adjustments, ensuring the feedback stayed responsive to the evolving dynamics of the interaction.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Audio",
      "text": "The soundscape of the Commonaiverse installation was designed to dynamically interact with participants' movements and emotional expressions, inspired by established correlations between musical features and affective states  [11, 15] . This approach was rooted in existing literature on the psychophysiological and cognitive effects of musical elements-such as tempo, harmony, rhythm, and dynamics-on emotional perception.\n\nTo achieve a responsive and immersive auditory environment, the Max/MSP  [1]  patch was integrated with TouchDesigner via Open Sound Control (OSC). Six speakers were strategically placed around the installation to create a spatial sound environment, enabling both an overarching auditory realm that unified the experience and individual soundscapes for each participant's interaction. The received data streams were mapped to modulate specific musical parameters dynamically:\n\n• Tempo Adaptation: Real-time movement velocity data influenced the musical tempo, with higher movement speeds triggering faster tempos and lower speeds generating slower tempos. This adaptation aligned the auditory environment with participants' physical energy levels. • Harmonic Shifts: The AI's analysis of participants' perceived emotional states guided transitions between major and minor tonalities, reflecting changes in the overall emotional atmosphere. For instance, a detected transition from joy to melancholy would result in a shift from a major to a minor key. • Rhythmic Complexity: Movement patterns informed rhythmic variations, such as the introduction of syncopation or changes in beat regularity. Periods of high participant activity introduced more complex rhythmic patterns, while slower or steadier movements maintained a regular beat. • Dynamic Range: The amplitude and intensity of participant movements were mapped to musical volume and dynamics. Bigger gestures resulted in louder, more intense soundscapes, while subdued movements created softer, more introspective auditory experiences.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Web3 Integration",
      "text": "The information received throughout each session was conceptualized as a constellation in the cosmos of shared experiences. Thus, the Cosmos Phase of the installation served as a culmination of the interactions between participants and the AI, translating their collective movements and emotional states into a visually compelling and data-rich display. Upon completion, an on-screen visualization represented this data in an abstract, dynamic 3D form, symbolizing the emotional and physical interplay within the interactive space. Each session's data, including emotion levels, time factors, and movement patterns, was sent to a WebGL application. This realtime generator created unique crystal-like installations, where the size, creation time, rotation, and spatial relationships of each crystal piece were defined by the session's data. These geometric structures and color-coded visuals were designed to reflect the emotional states detected by the AI, providing an artistic yet data-driven summary of the shared experience.\n\nParticipants could revisit this visualization through a QR code generated at the end of their session. The QR code encoded all the required information, enabling users to access and recreate their unique crystal installation via the WebGL application. This dual representation-combining the 3D visualization and session data metrics such as movement quantity, speed, and proximity-highlighted the collaborative and embodied nature of the interaction. More importantly, it offered participants a tangible way to reclaim ownership of their interaction data, creating a sense of continuity and personal connection beyond the installation.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Critical Reflections",
      "text": "Emotion recognition technologies are deeply embedded in the sociopolitical fabric of contemporary society. Their deployment in contexts such as workplace productivity tracking, consumer surveillance, and policing often reduces emotions to simplistic, monetizable metrics. This reductionist approach pervades many multimedia data pipelines, commodifying human affect and turning nuanced emotional states into data points for algorithmic optimization and profit generation. Commonaiverse resists this commodification by shifting focus from extraction to collaboration, inviting participants to co-create an embodied emotional lexicon with the AI.\n\nThis critique is particularly relevant given the biases inherent in existing emotion recognition systems. Many such systems are trained on datasets that encode cultural, racial, and gender biases, often privileging Western norms while marginalizing non-Western or minority expressions  [4, 6] . For instance, facial emotion recognition algorithms frequently misclassify expressions from individuals of non-Caucasian backgrounds  [23] . By requiring participants to  teach the AI through embodied interactions, Commonaiverse decentralizes these normative assumptions, emphasizing diversity and rejecting universal emotional standards. This process aligns with critical calls to decolonize AI systems by prioritizing localized and situated knowledge  [17] .\n\nWhile the multi-recommender system provides nuanced emotional detection through behavioral, contextual, and longitudinal analyses, its reliance on movement data introduces potential biases. For instance, privileging certain physical capabilities over others underscores the importance of inclusive design. Similarly, while the teaching phase enables participants to co-create an emotional lexicon, its adaptability across cultural contexts requires further exploration. In a broader multimedia sense, this highlights how model-building should account for diverse body types and movement repertoires to avoid reinforcing new forms of exclusion. Incorporating culturally specific movement patterns could enhance its universality and applicability.\n\nThe installation also critiques the pervasive surveillance logic that underpins many emotion recognition systems. When applied in policing or workplaces, these systems often function as tools for control, monitoring compliance or productivity without explicit consent  [29] . Commonaiverse counters this paradigm by embedding transparency and agency into its design. The Cosmos Phase, which allows participants to view and own their data, exemplifies a participatory ethos. This approach contrasts sharply with the extractive practices of commercial AI systems, presenting data as a resource for reflection and connection rather than exploitation.\n\nMoreover, Commonaiverse interrogates the economic and political motivations driving the adoption of emotion recognition technologies. As emotions become commodities within surveillance capitalism, the human capacity for affective expression risks being dehumanized and instrumentalized  [5] . By reframing emotion recognition as an artistic and communal act, Commonaiverse reclaims affective expression as a domain for creativity and shared meaning-making. It invites participants to critically reflect on how their emotions are interpreted, valued, and potentially exploited within broader sociotechnical systems.\n\nFinally, Commonaiverse contributes to speculative and critical HCI work that challenges conventional AI design paradigms. It aligns with research advocating for more ethical, inclusive, and situated technological approaches  [8, 26] . By positioning itself as both critique and alternative, the project fosters a dialogue about the politics of emotion in digital spaces. In doing so, it also prompts multimedia researchers to consider relationality, agency, and cultural specificity in designing the next generation of affective systems, rather than defaulting to reductive generalizations.\n\nDespite its achievements, the project faces certain limitations. Real-time data processing occasionally misses subtle or overlapping emotional states, and performance can degrade under noisy or crowded conditions. Additionally, its reliance on visual and auditory feedback limits the depth of emotional interaction for some users. Addressing these limitations through iterative development will ensure that Commonaiverse remains inclusive and robust.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Future Directions",
      "text": "One of the immediate opportunities for Commonaiverse lies in its portability. Since the installation is not site-specific, adapting it as a traveling piece could provide a unique opportunity to gather data on diverse forms of bodily expressiveness across various cultures and environments. This traveling aspect would allow us to observe how different cultural contexts influence emotional communication and physical engagement, contributing to a richer understanding of embodied emotional expression and more inclusive AI modeling in multimedia contexts.\n\nFuture iterations could also benefit from moving beyond audiovisual stimuli to integrate additional sensory modalities, such as tactile feedback. Incorporating haptic elements would enhance the multisensory experience, allowing participants not only to see and hear but also to feel their interactions. This heightened level of sensory integration could deepen participants' connection to the installation-and to each other-within immersive multimedia environments.\n\nMoreover, Commonaiverse has the potential to inspire further applications in the realm of emotional interaction and embodiment. Using body tracking and emotional expression as a basis for therapeutic interventions, particularly in art therapy contexts, could foster emotional regulation and self-exploration. The expressive nature of full-body movement, combined with AI's responsive feedback, provides fertile ground for emotional and psychological research within interactive multimedia frameworks.\n\nFinally, integrating Web3 technologies for decentralized data storage could give participants ownership over their interaction data, fostering continuity and personal connection beyond the experience. This approach could also address broader concerns about data privacy and consent, aligning with the project's ethos of collaboration and agency.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "With Commonaiverse, our aim was to explore how AI can engage with human emotions in a way that feels embodied, dynamic, and collaborative. By integrating full-body movements, real-time motion tracking, and interactive multimedia feedback, we created a space where participants could express and interpret emotions together with the AI. Through its three-phase structure-Teaching, Exploration, and the Cosmos Phase-the installation bridges human expression and machine interpretation, emphasizing the social and relational aspects of emotional communication.\n\nThroughout this process, we encountered and tackled challenges in modeling complex emotional states, balancing abstract artistic representations with accessibility, and generating collaboration in public, interactive settings. These challenges highlighted the importance of iterative design and reinforced our belief in the need for inclusivity and adaptability in designing emotion-focused technologies, moving away from static datasets and predefined categories.\n\nUltimately, Commonaiverse represents a vision of how AI can become an active participant in emotional exploration, not merely an observer. We hope this work inspires further experimentation at the intersection of technology, emotion, and embodiment, showing that AI can be a collaborator in understanding-and enhancing-human connection through the lens of multimedia innovation.\n\nBarcelona team: Jordi Sabater, Joan Jené, Cristian Cozar and Lissette Lemus. We would like to thank Emmanuel Martinez for the web development, Carlos Reche for the audio design, and Marc Galvez for technical support. The project was produced by ESPRONCEDA: Institute of Art & Culture with the support of the Department of Cultura of Catalunya and commissioned by Alejandro Martin with the executive production of Dr. Holger Sprengel. The early sketches, 3D models and the interaction visualizations were done by Solimán López. The image in Figure  5  was taken by Aleix Plademunt, Figure  6  by Marti E. Berenguer and Figure  7  by Vitor Schietti.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The interaction flow: 1) The AI agent detects the humans inside the room. 2) Assessment of both individual and",
      "page": 1
    },
    {
      "caption": "Figure 2: Early model for the space, where the walls serve as",
      "page": 3
    },
    {
      "caption": "Figure 3: 3D Model where the ceiling was also thought to be",
      "page": 3
    },
    {
      "caption": "Figure 4: ). These early designs mapped par-",
      "page": 3
    },
    {
      "caption": "Figure 4: Although entertaining, having the ledwalls meant",
      "page": 3
    },
    {
      "caption": "Figure 5: This decision allowed the participants to",
      "page": 4
    },
    {
      "caption": "Figure 5: Final configuration of the room, with lasers pro-",
      "page": 4
    },
    {
      "caption": "Figure 1: , with the entire experience lasting",
      "page": 4
    },
    {
      "caption": "Figure 6: Body Tracking visualization during the session, the",
      "page": 5
    },
    {
      "caption": "Figure 7: Participants starting the training phase of the Com-",
      "page": 6
    },
    {
      "caption": "Figure 8: Visual Feedback given the users while starting the process",
      "page": 7
    },
    {
      "caption": "Figure 9: Audio Design of the Commonaiverse",
      "page": 7
    },
    {
      "caption": "Figure 10: The Final State of the interaction, where people",
      "page": 8
    },
    {
      "caption": "Figure 5: was taken by Aleix Plademunt, Figure",
      "page": 9
    },
    {
      "caption": "Figure 7: by Vitor Schietti.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Neu",
          "Column_2": "tral"
        }
      ],
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Max/MSP",
      "year": "2025",
      "venue": "Max/MSP"
    },
    {
      "citation_id": "2",
      "title": "Evidence for distinct contributions of form and motion information to the recognition of emotions from body gestures",
      "authors": [
        "Mary Anthony P Atkinson",
        "Winand Tunstall",
        "Dittrich"
      ],
      "year": "2007",
      "venue": "Cognition"
    },
    {
      "citation_id": "3",
      "title": "The psychological construction of emotion",
      "authors": [
        "Lisa Feldman",
        "James Russell"
      ],
      "year": "2014",
      "venue": "The psychological construction of emotion"
    },
    {
      "citation_id": "4",
      "title": "Race after Technology: Abolitionist Tools for the New Jim Code. Cambridge and Medford",
      "authors": [
        "Benjamin"
      ],
      "year": "2019",
      "venue": "Race after Technology: Abolitionist Tools for the New Jim Code. Cambridge and Medford"
    },
    {
      "citation_id": "5",
      "title": "The costs of connection: How data are colonizing human life and appropriating it for capitalism",
      "authors": [
        "Nick Couldry",
        "Ulises Mejias"
      ],
      "year": "2020",
      "venue": "The costs of connection: How data are colonizing human life and appropriating it for capitalism"
    },
    {
      "citation_id": "6",
      "title": "The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence",
      "year": "2021",
      "venue": "The Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "There is a blind spot in AI research",
      "authors": [
        "Kate Crawford",
        "Ryan Calo"
      ],
      "year": "2016",
      "venue": "Nature"
    },
    {
      "citation_id": "8",
      "title": "Divining a digital future: Mess and mythology in ubiquitous computing",
      "authors": [
        "Paul Dourish",
        "Genevieve Bell"
      ],
      "year": "2011",
      "venue": "Divining a digital future: Mess and mythology in ubiquitous computing"
    },
    {
      "citation_id": "9",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "10",
      "title": "On the universality and cultural specificity of emotion recognition: a meta-analysis",
      "authors": [
        "Hillary Anger",
        "Nalini Ambady"
      ],
      "year": "2002",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "11",
      "title": "Emotional expression in music",
      "authors": [
        "Alf Gabrielsson",
        "Patrik Juslin"
      ],
      "year": "2003",
      "venue": "Emotional expression in music"
    },
    {
      "citation_id": "12",
      "title": "Recommenders for Improved Lesson Planning in Formal Education",
      "authors": [
        "Pere Garcia-Calvés",
        "Jordi Sabater-Mir",
        "Pablo Aramendía",
        "Jordi Corominas",
        "Bryan Ruchat"
      ],
      "year": "2023",
      "venue": "Recommenders for Improved Lesson Planning in Formal Education",
      "doi": "10.3233/FAIA230689"
    },
    {
      "citation_id": "13",
      "title": "Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture",
      "authors": [
        "Maria Gendron",
        "Debi Roberson",
        "Jacoba Marietta Van Der Vyver",
        "Lisa Barrett"
      ],
      "year": "2014",
      "venue": "Emotion"
    },
    {
      "citation_id": "14",
      "title": "MoveEnet: Online high-frequency human pose estimation with an event camera",
      "authors": [
        "Gaurvi Goyal",
        "Franco Di Pietro",
        "Nicolo Carissimi",
        "Arren Glover",
        "Chiara Bartolozzi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Feelings and perceptions of happiness and sadness induced by music: similarities, differences, and mixed emotions",
      "authors": [
        "G Patrick",
        "Glenn Hunter",
        "Ulrich Schellenberg",
        "Schimmack"
      ],
      "year": "2010",
      "venue": "Psychology of Aesthetics, Creativity, and the Arts"
    },
    {
      "citation_id": "16",
      "title": "Postcolonial computing: a lens on design and development",
      "authors": [
        "Lilly Irani",
        "Janet Vertesi",
        "Paul Dourish",
        "Kavita Philip",
        "Rebecca Grinter"
      ],
      "year": "2010",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "17",
      "title": "Comparative analysis of OpenPose, PoseNet, and MoveNet models for pose estimation in mobile devices",
      "authors": [
        "Beomjun Jo",
        "Seongki Kim"
      ],
      "year": "2022",
      "venue": "Traitement du Signal"
    },
    {
      "citation_id": "18",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "Andrea Kleinsmith",
        "Nadia Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Emotional AI: The rise of empathic media",
      "authors": [
        "Andrew Mcstay"
      ],
      "year": "2018",
      "venue": "Emotional AI: The rise of empathic media"
    },
    {
      "citation_id": "20",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "21",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Erik Cambria",
        "Alexander Gelbukh",
        "Amir Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "22",
      "title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products",
      "authors": [
        "Deborah Inioluwa",
        "Joy Raji",
        "Buolamwini"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "23",
      "title": "Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies",
      "authors": [
        "Russell James"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "24",
      "title": "Human facial expressions as adaptations: Evolutionary questions in facial expression research",
      "authors": [
        "L Karen",
        "Jeffrey Schmidt",
        "Cohn"
      ],
      "year": "2001",
      "venue": "American Journal of Physical Anthropology: The Official Publication of the American Association of Physical Anthropologists"
    },
    {
      "citation_id": "25",
      "title": "Human-machine reconfigurations: Plans and situated actions",
      "authors": [
        "Lucille Alice"
      ],
      "year": "2007",
      "venue": "Human-machine reconfigurations: Plans and situated actions"
    },
    {
      "citation_id": "26",
      "title": "Alone together: Why we expect more from technology and less from each other",
      "authors": [
        "Sherry Turkle"
      ],
      "year": "2017",
      "venue": "Alone together: Why we expect more from technology and less from each other"
    },
    {
      "citation_id": "27",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "28",
      "title": "The age of surveillance capitalism",
      "authors": [
        "Shoshana Zuboff"
      ],
      "year": "2023",
      "venue": "Social theory re-wired. Routledge"
    }
  ]
}