{
  "paper_id": "2205.01030v1",
  "title": "Gmss: Graph-Based Multi-Task Self-Supervised Learning For Eeg Emotion Recognition",
  "published": "2022-04-12T03:37:21Z",
  "authors": [
    "Yang Li",
    "Ji Chen",
    "Fu Li",
    "Boxun Fu",
    "Hao Wu",
    "Youshuo Ji",
    "Yijin Zhou",
    "Yi Niu",
    "Guangming Shi",
    "Wenming Zheng"
  ],
  "keywords": [
    "EEG emotion recognition",
    "multi-task learning",
    "self-supervised learning",
    "graph neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Previous electroencephalogram (EEG) emotion recognition relies on single-task learning, which may lead to overfitting and learned emotion features lacking generalization. In this paper, a graph-based multi-task self-supervised learning model (GMSS) for EEG emotion recognition is proposed. GMSS has the ability to learn more general representations by integrating multiple self-supervised tasks, including spatial and frequency jigsaw puzzle tasks, and contrastive learning tasks. By learning from multiple tasks simultaneously, GMSS can find a representation that captures all of the tasks thereby decreasing the chance of overfitting on the original task, i.e., emotion recognition task. In particular, the spatial jigsaw puzzle task aims to capture the intrinsic spatial relationships of different brain regions. Considering the importance of frequency information in EEG emotional signals, the goal of the frequency jigsaw puzzle task is to explore the crucial frequency bands for EEG emotion recognition. To further regularize the learned features and encourage the network to learn inherent representations, contrastive learning task is adopted in this work by mapping the transformed data into a common feature space. The performance of the proposed GMSS is compared with several popular unsupervised and supervised methods. Experiments on SEED, SEED-IV, and MPED datasets show that the proposed model has remarkable advantages in learning more discriminative and general features for EEG emotional signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTION is close to everyone and plays an important role in our daily lives  [1] . It is a complex and comprehensive psychological and physiological state that can be characterized by behavioral and physiological signals  [2] . Neuroscience research indicates that physiological signals are closer to the source of emotion than behavioral signals  [3] . As a physiological signal, EEG has the advantage of being difficult to disguise and hide compared with behavioral signals, such as facial expressions and voice  [4] . Moreover, EEG signals significantly benefited from the technological developments in non-invasive EEG recording methods, and are widely used in the research on emotion recognition  [5] [6] . In recent years, emotion recognition has become a research hotspot in human-computer interaction and affective computing  [7] .\n\nA wide variety of methods has been proposed to effectively analyze EEG emotional signals over the past decades. Traditional machine learning methods typically adopt a two-stage model to implement emotional recognition. For example, Lin et al.  [8]  extracted power spectrum density, differential asymmetry power, and rational asymmetry power as features of EEG signals, and then classified them using a support vector machine to study the relationship between emotion and EEG signals. Jenke et al.  [9]  studied and compared the effects of EEG emotion features extracted from the time domain, the frequency domain, and the time-frequency domain on EEG emotion signal recognition. However, traditional machine learning methods rely on handcrafted features and expert experience  [10] . With the spectacular success of deep learning methods in the field of computer vision and language recognition, many researchers have considered deep learning models for EEG emotion signals for their ability to automatically extract complex features  [11]    [12] . For instance, some researchers utilized convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to handle emotion recognition  [13]    [14] .\n\nRecently, the topological structure of EEG signals has been increasingly studied in EEG emotion recognition owing to the superior performance of graph neural networks (GNNs) in irregular data structures  [15] . Wang et al.  [16]  proposed a multichannel EEG emotion recognition method based on phase locking value (PLV) graph convolutional neural networks (P-GCNN) to extract the spatio-temporal characteristics and the inherent information in functional connections.\n\nBased on the literature, most EEG-based emotion recognition methods basically face three challenges:  (1)  how to generalize the emotion recognition model, and correctly classify new data;  (2)  how to make full use of EEG characteristics to capture more discriminative data representation for emotion recognition; and (3) how to solve the problem of emotional noise labels. Regarding the first challenge, EEG displays a highly heterogeneous and nonstationary behavior because emotional signals usually consist of many neural process  [17] . The enormous data distribution shift leads to a lack of generalization for data from different subject or new situation of the current subject  [18] . Thus, some researchers have adopted the domain adaptation (DA) method to improve generalization. For example, Li et al.  [18]  proposed a bi-hemisphere domain adversarial neural network (BiDANN) that contains three domain discriminators to assist with the learning of discriminative emotional features, narrowing the distribution gap between training and testing data, and improving the generality of the recognition model. However, most DA-based methods achieve generality by training the model on labeled training data and unlabeled testing data, which is not suitable for real applications. Thus, it is meaningful and applicable to explore other methods that learn general data representations without test data. For the second challenge, handcrafted features such as power spectrum density, statistical measure, and discrete wavelet transform are frequently used for generic EEG signal classification tasks. However, these features are not specially designed for EEG emotion signal  [18] . This issue has also been discussed in recent deeplearning literature on EEG emotion recognition. For example, Zheng et al.  [19]  employed a deep belief network (DBN) to directly model the EEG emotion signal. Even though these handcrafted and deep features have been able to extract certain emotion discriminative information, they do not sufficiently exploit specific emotion-related information in EEG emotion recognition tasks. Thus, it is necessary to utilize the characteristics of EEG signals to extract highlevel features. Regarding the third challenge, the emotion labels in the collected EEG data may be noisy and inconsistent as participants may not always produce the expected emotions when watching emotions stimulate stimuli  [20] . Consequently, it is challenging and meaningful to explore how to solve the problem of emotional noise labels that are often ignored in EEG emotion recognition research.\n\nTo address the above three major issues in EEG emotion recognition tasks, in this paper we propose GMSS, which can learn general EEG emotion representation and improve EEG emotion recognition ability by solving three self-supervised pretext tasks. To improve generality, GMSS adopts multiple EEG emotion-related tasks that share learned knowledge to generate more general features and avoid overfitting  [21] . GMSS consists of two graph-based jigsaw puzzle tasks and a contrastive learning task, making it capable to study the impact of emotional expression on spatial and frequency information. The spatial jigsaw puzzle task enables the predefined distant electrodes to become neighbor electrodes and more emotion-related spatial information is learned in return. Meanwhile, the frequency jigsaw puzzle task explores crucial frequency bands for EEG emotion recognition. Utilizing the augmented samples of the above jigsaw puzzle tasks, the contrastive learning task further standardizes the feature space and enhances the generalization ability of the model. These self-supervised pretext tasks, which are based on the intrinsic attributes of EEG emotion data, allow GMSS to deal with EEG noise labels without semantic labeling. In this study, both unsupervised and supervised approaches of GMSS were evaluated. The experimental results show that GMSS achieves state-of-the-art (SOTA) performance on three public datasets.\n\nIn summary, the contributions of this work can be outlined as follows:\n\n• To the best of our knowledge, this is the first work that adopts multi-task learning to improve model generalization capability and avoid overfitting in EEG emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "Through the pretext tasks of jigsaw puzzles and contrastive learning, GMSS learns more discriminative features and alleviates the problem of emotional noise labels, which further improves EEG emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "The experimental results, based on both unsupervised and supervised learning approaches, demonstrate that GMSS can achieve SOTA performance on three benchmark datasets.\n\nThe rest of this paper is organized as follows: Section II provides an overview of previous studies on EEG emotion recognition, graph neural networks, multi-task learning, and self-supervised learning. Section III specifies the GMSS method and its application to EEG emotion recognition. In section IV the proposed method is evaluated for EEG emotion recognition through extensive experiments. Finally, section V concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "In this section, related works on EEG-based emotion recognition, graph neural networks, multi-task learning, and selfsupervised learning are introduced.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "The general process of EEG emotion recognition includes feature extraction and classification. Traditional machine learning-based methods typically adopt the statistical measure, discrete wavelet transform, or power spectrum density  [8]  as features and then classify the extracted features using SVM, LDA, or LR  [22] . However, deep learning based methods generally extract features by designing feature extraction neural networks followed by linear layers to achieve classification. Many deep learning methods such as CNN, RNN and GNN have been introduced to effectively distinguish different emotional states in EEG emotional signals. Li et al.  [23]  proposed a hierarchical spatial-temporal neural network (R2G-STNN) based on a bidirectional long shortterm memory (BiLSTM) network to capture the intrinsic spatial relationships of EEG electrodes within the brain region and between brain regions for EEG emotion recognition. Song et al.  [24]  proposed a multichannel EEG emotion recognition method based on a novel dynamic graph convolutional neural network (DGCNN) to dynamically learn the intrinsic relationship between different EEG channels to assist with features classification. Zhong et al.  [20]  proposed a regularized graph neural network (RGNN) with two regularizers to deal with cross-subject EEG variations and the noise label problem, and achieved promising results. Li et al.  [25]  proposed a bi-hemispheric discrepancy model (BiHDM) to learn discrepancy information between two hemispheres to improve EEG emotion recognition ability.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Graph Neural Network",
      "text": "The traditional convolutional neural network is excellent for dealing with Euclidean data. However, GNN is suitable for handling non-Euclidean data and has shown great promise in the field of social networks, recommendation systems, and knowledge maps  [26]  [27]  [28] . The GNN fall into two categories, spectral-based and spatial-based. The spectralbased method specifies graphic convolution by introducing a filter from the perspective of graphic signal processing, where the graphic convolution operation is viewed as noise removal from the graphic signal. The spatial-based method is based on the recurrent neural network theory and defines the graph convolution through information propagation  [29] . Defferrard et al.  [30]  argued that the original spectrum convolution suffers from the disadvantages of a large number of parameters and high complexity, and proposed a fast localized convolution algorithm using a recursive formulation of the K-order Chebyshev polynomials to approximate the filters. Kipf et al.  [15]  proposed a graph convolutional network (GCN) with a faster localized graph convolutional operation, which is the first-order approximation of Chebyshev polynomials, that is, K = 1. Veli et al.  [31]  proposed a graph attention network (GAT), which stacking layers in nodes that are able to attend over their neighborhoods' features, specifying different weights to different nodes in a neighborhood, without requiring costly matrix operation or depending on knowing the graph structure upfront.\n\nBianchi et al.  [32]  proposed a graph convolutional layer that provides a flexible frequency response, which is more robust to noise, and better captures the global graph structure. Bouritsas et al.  [33]  proposed a graph substructure network that is more expressive than Weisfeiler-Leman graph isomorphism test, which allows the model retains multiple attractive properties of standard GNNs, while being able to eliminate even hard instances of graph isomorphism. Ciano et al.  [34]  proposed a mixed inductive-transductive GNN model, study its properties and introduce an experimental strategy that help to understand and distinguish the role of inductive and transductive learning. Tiezzi et al.  [35]  proposed an approach to learning in GNNs based on constrained optimization in the Lagrangian framework. Learning both the transition function and the node states is the outcome of a joint process, in which the state convergence procedure is implicitly expressed by a constraint satisfaction mechanism, avoiding iterative epoch-wise procedures and the network unfolding.\n\nHowever, in EEG emotion recognition, some GNN-based methods  [24]  only consider second-order or third-order neighbors to avoid over-smoothing, which may result in the loss of valuable information between distant nodes. Thus, the spatial jigsaw puzzle was applied to challenge this problem.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Multi-task learning is an effective machine learning method and has shown its advantages in many fields, including computer vision  [36]    [37] , natural language processing  [38] , and speech recognition  [39] . Ruder et al.  [21]  introduced two commonly used multi-task learning methods in deep learning, clarifying the working principle of multi-task learning as well as pointing out that properly designed pretext tasks can encourage the model to learn a more general representation while decreasing the risk of overfitting. Compared with a single task, multi-task learning combines multiple related tasks and utilizes all the data from each task so that the knowledge on each task is shared. Additional information on the associated tasks is also obtained in multitask learning models, resulting in significant improvements in the learning ability, generalization capability, and robustness of the model  [40] . However, considering the different significance of each task, the weight of each task should be dynamic. Sener et al.  [41]  regarded multi-task learning as a multi-objective optimization problem and proved that optimizing the upper bound of the multi-objective loss can obtain the Pareto optimal solution. Kendall et al.  [37]  proposed a principled approach to multi-task deep learning that weighs multiple loss functions by considering the homoscedastic uncertainty of each task to avoid the cost of manual tuning. Benefiting from these advantages, in this work, multi-task learning framework is adopted to learn more generalization features and reduce the risk of overfitting.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Self-Supervised Learning",
      "text": "Self-supervised learning is a popular method for learning intrinsic information using unlabeled data  [42] . Generally, self-supervised learning applies the attributes of data to generate pseudo labels as opposed to human-annotated labels to train the network. Based on the different data attributes used in the design, there are four categories of pretext tasks: generation-based, context-based, free semantic label-based, and cross-modal-based  [42] . In the visual feature learning field, context-based pretext tasks mainly employs spatial structure, temporal structure, and context similarity for the design. Many studies learn the general features of images by predicting the relative position of the patches to solve jigsaw puzzle tasks, thereby solving the problem of image classification  [43]  [44]  [45] . Gidaris et al.  [46]  applied a 2D rotation to the image to construct the pretext task and then predicted the rotation angle to enable the model to learn the position, type, and posture of objects in the image. Carno et al.  [47]  used a clustering method to generate pseudo labels for images and combined learning neural network parameters and result features to obtain more abundant semantic information. Mathilde et al.  [48]  proposed a method for unsupervised learning of visual features by contrasting cluster assignments (SwAV), which takes advantage of contrastive methods without requiring to compute pairwise comparisons. He et al.  [49]  suggested that momentum contrast (MoCo) would significantly narrow the gap between unsupervised representation learning and supervised representation learning. The performance of the contrastive SimCLR framework proposed by Chen et al.  [50]  on ImageNet surpasses that of supervised learning based models. Xinlei et al.  [51]  proposed a simple Siamese (SimSiam) network that achieved the best results without negative samples, large batches, and momentum encoders. In addition, the contrastive learning method was applied in the field of video processing, and achieved excellent performance at the time it was proposed  [52]",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "I N P U T D A T A F E A T U R E E X T R A C T O R M U L T It A S K H E A D S",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Labels",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D A T A T R A N S F O R M",
      "text": "Fig.  1 . Framework of GMSS. In the unsupervised training mode, for the upstream task, the original graph data are not used to train the network. For the downstream task, the feature extractor is frozen and only the pink part with a substituted one linear layer is executed. In the supervised training mode, all the parts are executed simultaneously.\n\nfield of EEG emotion recognition, Xie et al.  [54]  proposed an innovative solution which contains six different transformations to learn high-level EEG representation (SSL-EEG). Mohsenvand et al.  [55]  present a framework for learning representations from EEG signals via contrastive learning which recombines channels from multi-channel recordings and trains a channel-wised feature extractor to learn EEG emotion representation (SeqCLR). Inspired by self-supervised learning, in this work, two jigsaw puzzle tasks and a contrastive learning task were designed to assist with the learning of general EEG emotional features while circumventing the problem of EEG emotion noise labels. Further, DeepCluster is a method based on clustering, and SwAV use a swapped prediction mechanism to predict the cluster assignment of a view from the representation of another view, SSL-EEG learn the EEG representations from complex signal transformation, while MoCo, SimCLR, SimSiam and SeqCLR are methods based on maximizing the similarity between positive pairs. Compared with these methods above, our GMSS is a multi-task framework that incorporates multiple emotion-related tasks that utilizes all the data from each task so that the knowledge on each task is shared. This will helpful to obtain the additional information on the associated tasks that results in improving the learning ability, generalization capability, and robustness of the model. Another difference is that our self-supervised model concentrates on the characteristics of EEG emotion signal. For example, the jigsaw puzzle learning will force our model focus on the important brain regions and frequency bands of EEG signal, which are very important for emotion expression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph-Based Multi-Task Self-Supervised Learning For Eeg Emotion Recognition",
      "text": "The goal of GMSS is to capture general and discriminative EEG emotion features using multi-task self-supervised learning, as illustrate in Fig.  1 . Three self-supervised tasks are designed to achieve this goal under unsupervised and supervised modes. These tasks share a common feature extractor. There are four task heads, i.e., Spatial Head H s (•), Frequency Head H f (•), Projection Head H p (•), Classification Head H c (•). H s and H f are employed for spatial puzzle and frequency puzzle respectively. H p is adopted to project the learned representation into feature space. H c is used for emotion recognition. Each head consists of three fully connected layers. Fig.  2 . Spatial jigsaw puzzle. The 62 electrodes are divided into 10 blocks according to the location of brain regions. The placement of these channels is relocated while keeping the original connection based on the topology of the scalp. The spatial jigsaw puzzle task is to identify which of the 128 classes the channels reorganized by blocks belong.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multiple Self-Supervised Tasks",
      "text": "To learn more generalized and discriminative features and alleviate the noise problem of EEG emotion labels, multiple self-supervised learning tasks are considered, including the spatial jigsaw puzzle task, the frequency jigsaw puzzle task, and the contrastive learning task. Each of these three pretext tasks is described in depth.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spatial Jigsaw Puzzle",
      "text": "The spatial jigsaw puzzle aims to capture the spatial patterns of EEG electrodes in different brain regions. Due to the different effects of brain regions on emotion expression, the spatial jigsaw puzzle task is defined as a series of brain region permutations  [56]  [57] [58]  [59] . As shown in Table  1 , the original EEG data X ∈ R n×d are partitioned into 10 blocks according to the location of the brain regions, denoted as\n\nThen, all brain region permutations can be obtained:\n\nwhere Xi and y i represent the i-th permutation and its serial number, respectively. There are 10! = 3628800 permutations in total. The goal is to distinguish which permutation the spatial transformed data corresponds to. However, it is quite challenging to distinguish these massive permutations for self-supervised pretext tasks. Therefore, we develop a R k (•) operator. R k (•) selects the k permutations with maximum Hamming distance from the full permutation of Eq. (  1 ) and randomly transformed the input data to one of the k permutations. We define a unique pseudo label for each of these k permutations, generating k different kinds of pseudo labels in total, with a range from 1 to k. Each input data is randomly transformed into one of the k permutations and the corresponding unique pseudo labels are obtained. k is set to 128. The overall permutation is displayed in Fig.  2 , and is formulated as follows:\n\nwhere X s is the generated EEG data with pseudo label y s ∈ Z 128 + .\n\nTo recognize these spatial jigsaw puzzles, a classification head H s (•) is applied, and cross entropy is adopted as the loss function. Formally, the loss of spatial jigsaw puzzle tasks can be expressed as L s :\n\nwhere F(•) is the shared feature extractor, ȳs i is the one-hot encoding of the corresponding pseudo label y s i , and N is the number of training samples.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Frequency Jigsaw Puzzle",
      "text": "The frequency jigsaw puzzle task is designed to learn the inner relationship between frequency bands, explore the crucial frequency bands for EEG emotion recognition and improve the discrimination ability of the model. In general, as illustrated in Fig.  3 , the energy features of the EEG data are extracted from five emotion expression-related frequency bands, including δ (1-3 Hz), θ (4-7 Hz), α (8-13 Hz), β (14-30 Hz), γ (31-50 Hz). Similar to the spatial jigsaw puzzle, the original EEG data X are divided into five blocks according to different frequency bands, denoted as (x 1 , x 2 , • • • , x 5 ), where x j ∈ R n×1 . The goal is to identify the corresponding permutation of the frequency transformed data. All frequency bands permutations can be obtained:\n\nwhere X j and y j represent the j-th permutation and its serial number, respectively. In the frequency jigsaw puzzle, the operator R k (•) is applied to generate transformed data with pseudo label, and k = 120:\n\nwhere X f is the generated EEG data with pseudo label y f ∈ Z 120 + .\n\nTo recognize these frequency jigsaw puzzles, a classification head H f (•) is applied and cross entropy is adopted as the loss function. Formally, the loss in the frequency jigsaw puzzle task can be expressed as follows:\n\nwhere F(•) is the shared feature extractor and ȳf j is the onehot encoding of the corresponding pseudo label y f j .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Maximize Agreement",
      "text": "Fig.  4 . Contrastive learning. The original data applied with spatial transformation and frequency transformation to generate the pairs data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Contrastive Learning",
      "text": "To further regularize feature learning and encourage the network to learn inherent representations, contrastive learning is adopted to map the transformed data into a common feature space. The purpose is to maximize the agreement between the different augmented data of the same EEG emotion data, as shown in Fig.  4 . To ensure that positive pairs move closer and negative pairs move far away in feature space, a data augmentation operation Q(•) is defined to consider the spatial and frequency transformations of the same original EEG emotion data. For each original EEG emotion data\n\nAs a result, each augmented data has (M -1) positive pairs and (N -1) × M negative pairs. In total, N × M augmented data are obtained by:\n\nwhere X nm ∈ R n×d is the m-th transformation of the n-th EEG sample. Similar to SimCLR  [50] , a projection head H p (•) is applied to map the EEG emotion data onto the feature space, that is, Z nm = H p (F(X nm )). The similarity of two data points is quantitatively described by the dot product, which normalizes u and v through the 2 -norm. i.e., sim(u, v) = u T v/ u v . Then, the loss of all positive pairs n of sample X n is calculated as follows:\n\nwhere (Z ni , Z nj ) are positive pairs, and (Z no , Z tw ) are negative pairs. τ is the temperature parameter and is set to 0.5. Furthermore, the arithmetic average of the loss of all positive pairs' n of all samples is calculated for backpropagation as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Mode For Eeg Emotion Recognition",
      "text": "Two modes of training are provided: unsupervised and supervised. The feature extractor F(•) in both modes are the same. When training the feature extractor, the distinction is in the presence or absence of ground-truth emotion labels.\n\nIn the unsupervised mode, instead of using ground-truth emotion labels, the feature extractor F(•) is trained only on the self-supervised tasks mentioned above. Then, the frozen feature extractor F(•) is transferred to the downstream task and the performance is verified using a linear classifier. In the supervised mode, a joint training strategy is adopted.\n\nThe network is simultaneously trained on self-supervised tasks and supervised tasks. To avoid manually tuning the weights of the different loss functions, the total loss function is defined by considering the homoscedastic uncertainty of each task  [37] . In particular, the training loss L is calculated as follows:\n\nwhere L c is the cross entropy loss of supervised EEG emotion classification task; σ Ls , σ L f , σ Lp and σ Lc are the observation noise scalars of the corresponding tasks  [37] . ψ is the mode-selection operator. The observation noise scalar σ is a principled approach to multi-task deep learning which weighs multiple loss functions by the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings, which can balance these weightings optimally, resulting in superior performance. These scalars can be calculated as learnable parameters which change constantly during the model training process and the initial values are 1.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Feature Extractor Of Gmss",
      "text": "As shown in Fig.  5 , an undirected graph G(V, E) is employed to model the EEG data. Meanwhile, the adjacency matrix A of the EEG data was obtained. In G(V, E), V denotes the set of nodes where |V| = n; each node has d dimensions. As a result, nodes can be represented by the feature matrix X ∈ R n×d . E denotes a set of edges between nodes. (v i , v j ) is the edge between nodes v i and node v j , that is, (v i , v j ) ∈ E. The adjacency matrix A ∈ R n×n contains the topological information of the undirected graph, that is, the EEG data. D is the degree matrix of the vertices, and L = D -A is the combinatorial Laplacian matrix. In this study, n denotes the channel number of EEG data; d denotes the number of frequency bands and d = 5. The energy feature is extracted five bands, namely, δ (1-3 Hz), θ (4-7 Hz), α (8-13 Hz), β (14-30 Hz), γ (31-50 Hz).\n\nIn the GMSS model, Chebyshev polynomials are employed instead of the convolution kernel of SCNN  [60]  in the spectral domain, so that there are only k parameters in the convolution kernel, and feature decomposition is not required, reducing the computational load. Thus, the feature extractor F(•) of the GMSS can be formulated as:\n\nwhere σ(•) is the activation function; X is the input EEG emotion data; β k refers to the learning parameters in network training; and T k (•) is the Chebyshev polynomial of order K. Additionally, L = 2L/λ max -I, where λ max is the maximum eigenvalue of Laplace matrix L. In this study, we set K = 2 to avoid over-smoothing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "In this section, experiments were conduct on the following three datasets to evaluate the performance of our model: SEED  [19] , SEED-IV  [64] , and MPED  [63] . All three datasets were collected while subjects watched emotional video clips in a quiet, comfortable, and non-interfering environment. All three datasets were generated by recording EEG signals through the ESI NeuroScan system using 62 electrode channels positioned according to the 10-20 system  [56] . These three datasets are introduced next along with the experimental results.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Dataset",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Seed.",
      "text": "In the SEED dataset, there are a total of 15 subjects. There are three sessions associated with each subject. In each session, there are a total of 15 film clips to induce happy, neutral, and sad emotions, and there are 5 film clips for each emotion. That is, there are 15 trials per session, and each trial has 185-238 samples, resulting in approximately 3400 samples per session. SEED-IV. In the SEED-IV dataset, similar to SEED, there are 15 subjects, and three sessions for each subject. The difference is that each session includes four kinds of emotions: happy, neutral, sad, and fear. Each emotion has 6 different film clips. As a result, there are 24 trials, and each trial has 12-64 samples for each session. Consequently, each session has approximately 830 samples. MPED. In the MPED dataset there are 30 subjects, and each subject has only one session. In a session, there are seven types of emotions: joy, funny, neutral, sad, fear, disgust, and anger. Each type of emotion has 4 related film clips. Therefore, there are 28 trials per session. Each trial consists of 120 samples and there are a total of 3360 samples in one session.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Protocol",
      "text": "To fully evaluate our model, two types of experiments are implemented: subject-dependent and subject-independent experiment. For the subject-dependent experiment, the training data and testing data are obtained from different EEG trials of the same subject. For the subject-independent experiment, the training data and testing data are obtained from different subjects.\n\nFor the subject-dependent experiment, the same experimental protocol is applied as in  [9]    [19]  [25]  [63] . That is, for the SEED dataset, the EEG data of the first nine trials are used in each session as training data and the remaining six trials in the session as testing data for each subject. For the SEED-IV dataset, the first sixteen trials of the session are used for each subject as training data and the remaining eight trials as testing data. For the MPED dataset, the EEG data of the first twenty-one trials in the session are adopted for the training data and the remaining seven trials in this session are the testing data for each subject.\n\nFor the subject-independent experiment, the leave-onesubject-out (LOSO) cross-validation strategy is used in  [25]  [65] for each subject. Namely, one subject's EEG emotion data constituted the testing data, and the remaining subjects' EEG emotion data constituted the training data. The process continued until all subjects' EEG emotion data are tested once. Note: For the subject-dependent experiment, we calculate the average accuracy based on the results of all the sessions. While for the subject-independent experiment, we calculate the average accuracy based on the results of all the subjects. -indicates the experiment results are not reported on that dataset. Note: For the subject-dependent experiment, we calculate the average accuracy based on the results of all the sessions. While for the subject-independent experiment, we calculate the average accuracy based on the results of all the subjects.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Details",
      "text": "In the experiments, the released differential entropy (DE) in SEED and SEED-IV, and the short-time Fourier transform (STFT) in MPED are feed into the model as input. The size of the input X is 62 × 5; the output dimensions of each electrode is 32; and K = 2, that is, the graph convolution aggregated the information of the second-order neighbors.\n\nIn particular, GMSS is implemented by pytorch on a Nvidia 3080 GPU. The model is trained using the Adam optimizer with a batch size of 100. The learning rate is 0.001, and the weight decay rate is 8e-5. The mean accuracy (ACC) and standard deviation (STD) are employed as evaluation criteria in all datasets. The code of GMSS can be found at https://github.com/CHEN-XDU/GMSS.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Unsupervised Mode",
      "text": "In the upstream task, the model is trained by self-supervised pretext tasks, consisting of two jigsaw puzzle tasks and one contrastive learning task. In the downstream task, the frozen feature extractor is applied and a linear classifier is used to evaluate the performance of GMSS. We compared GMSS with two self-supervised EEG emotion recognition methods SSL-EEG  [54]  and SeqCLR  [55] . In addition, since there are few methods based on self-supervised EEG emotion recognition and the code is not released, we also compared with some popular self-supervised methods in other fields such as DeepCluster  [61] , MoCo  [49] , SwAV  [48] , SimCLR  [50]  and SimSiam  [51] . These methods are reproduced and maintain the experimental protocol consistent with GMSS. For a fair comparison, all of these methods of other fields adopted the same feature extraction operation as GMSS. To fit the EEG emotion recognition task, MoCo, (2) Subject-independent experimental results SwAV, SimCLR and SimSiam adopted the same data augmented as GMSS. The experimental results are shown in Table  2 . Concretely, GMSS improves the accuracy by 5.86%, 8.52%, 2.02%, 8.51%, 4.34%, and 2.66% compared with the existing SOTA methods in the subject-dependent and subject-independent experiments on SEED, SEED-IV, and MPED datasets, respectively. Especially compared with MoCo, SwAV, SimCLR, SimSiam and SeqCLR which are also contrastive learning-based methods, GMSS achieves better results. This is attributed to GMSS having more positive and negative pairs (We set M = 8), and two more pretext tasks, that is, spatial and frequency jigsaw puzzle tasks, which are helpful in learning more discriminant and general EEG emotion representation. In summary, from the results of Table  2 , in the unsupervised mode, it is observed that GMSS achieves an acceptable results without labels, making it more relevant to practical applications.\n\nTo better understand the confusion matrix of GMSS in recognizing different emotions, the unsupervised confusion matrices of all the experiments are displayed in Fig.  6 . There are two observations:\n\n(1) For the subject-dependent experiment shown in Fig.  6 (1), it is observed that happy is the easiest emotion recognized by SEED dataset. This is also observed in the results of the SEED-IV dataset. For MPED, which contains seven emotions, GMSS shows its superiority when identifying funny, neutral, fear, and anger. In addition, we can find joy is most easily confused with neutral. This may be because joy is more difficult to induce than other emotions. (2) From the results of the subject-independent task shown in Fig.  6 (2), for SEED, it is obvious that the accuracy of the happy emotion is much higher than neutral and sad, which is similar to the observation in Fig.  6 (1). With SEED-IV, we can notice that neutral emotion achieves the highestaccuracy since other emotions such as neutral lead to confusion. For MPED, funny, neutral, and sad emotions are much easier to recognize. It should be noted that, in the cross-subject task, the focus is on the sad emotion, which is difficult to identify from our observation.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Supervised Mode",
      "text": "In this section, a joint-training strategy is adopted. Based on the self-supervised training approaches, ground-truth emotion labels are used to train the feature extractor simultaneously. To evaluate the advantages of GMSS, the experiments conducted were the same as those of other methods, including linear support vector machine (SVM)  [62] , dynamical graph convolutional neural network (DGCNN)  [24] , regularized graph neural network (RGNN)  [20] , domain adversarial neural networks (DANN)  [37] , bi-hemisphere domain adversarial neural network (BiDANN)  [18] , attention-long short-term memory (A-LSTM)  [63] , and bi-hemispheric discrepancy model for EEG emotion recognition (BiHDM)  [25] . All these methods are representative of previous studies on emotion recognition. Their results are directly quoted or reproduced from the literature to ensure a convincing comparison with the proposed method, and are summarized in Table  3 .  For the subject-dependent experiments, in Table  3 , it is observed that GMSS attains the best performance on three public EEG emotional datasets compared with all aforementioned methods above. In particular, the results on SEED and SEED-IV, with GMSS are 2.24% and 7% higher than those of the most advanced method RGNN. Meanwhile, it is also observed that GMSS achieves a performance very close to BiHDM on MPED, that is, 40.16% vs. 40.34%. This is because BiHDM is trained not only on the labeled training data but also on the unlabeled testing data. However, the GMSS is trained only on the training data. For a fair comparison, the domain discriminator of BiHDM is ablated and the experiments are conducted on the same input data as GMSS, which is denoted as BiHDM w/o DA. The experimental results show that GMSS improves the classification accuracy by 1.61% compared with BiHDM w/o DA. Furthermore, GMSS outperforms the BiHDM by 3.36% and 12.02% on SEED and SEED-IV datasets, respectively. These results verify that GMSS has a better discrimination capability under subject-dependent experiments. Additionally, our GMSS has a considerable running speed. On the SEED dataset of subject-dependent experiments, the average training time and average testing time for one epoch are 3762.7ms and 331.39ms respectively. Subject-independent experiment are also performed. It is observed that GMSS achieves the SOTA performance on SEED and MPED, which is 1.12% and 0.22% higher than the previous best method BiHDM, respectively. Moreover, GMSS achieves a performance close to that of RGNN on SEED-IV, that is, 73.48% vs. 73.84%, respectively. However, while RGNN removes its node-wise domain adversarial training component (NodeDAT), that is, training with the labeled training data as well as without the unlabeled testing data, denoted as RGNN w/o DA, the accuracy of RGNN w/o DA is 1.83% lower than that of GMSS. Furthermore, GMSS outperforms RGNN by 1.22% on the SEED dataset. In addition, compared with these advanced methods training without the unlabeled testing data, that is, BiHDM w/o DA and RGNN w/o DA, GMSS is 4.6%, 1.83%, and 1.06% higher, respectively. This indicates that our model can extract more general data representations for different subjects. Besides, compared with all baselines on all datasets and both experimental protocols, GMSS achieves the lowest standard deviation in accuracy, indicating the excellent discrimination and generalization capability of our model. We argue that the main reason can be attributed to the multitask framework and self-supervised learning tasks.\n\nSimilar to the unsupervised mode, the confusion matrices of all experiments are also applied in the supervised mode to better understand the confusion of GMSS in recognizing different emotions as shown in Fig.  7 . There are two observations:\n\n(1) For the results of subject-dependent EEG emotion recognition experiment in Fig.  7 (1), the classification accuracy for the three emotions is approximately 90% for the SEED dataset. In particular, for happy, the accuracy is above 95%. The happy and neutral emotions are easier to recognize than the sad emotion. For SEED-IV, which contains four emotions, we can notice that the accuracy of all emotions is above 80%.\n\nFor MPED, which is a complex dataset that consists of seven types of emotions, it is observed that funny and neutral emotions are much easier to recognize than other emotions. Moreover, for negative emotions, fear and anger are easier to recognize than sad and disgust. (2) From the results of the subject-independent EEG emotion recognition experiment, for SEED, the happy emotion is much easier to be recognize than neutral and sad emotions. For SEED-IV, neutral and fear emotions are much easier to recognize. For MPED, which is a hard seven classification task, only funny, neutral and fear achieve acceptable results, which suggests that researchers should pay attention to joy, sad, disgust and anger in cross-subject emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "In this section, the representations of the visualization and ablation studies are presented.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Representation Visualization",
      "text": "To verify the discriminating ability of GMSS, the features obtained by GMSS on the MPED dataset are visualized. Fig.  8  shows the representation visualization of the subject-dependent experiment in supervised mode using tdistributed stochastic neighbor embedding (t-SNE)  [66]  on the MPED dataset. As shown in Fig.  8 (1), it is difficult to separate the different classes from the original EEG data. However, for the learned EEG representation in Fig.  8 (2), for the same emotion clusters, there are clear borders between different emotions, which verify that GMSS can discriminate features for EEG emotion recognition. Moreover, it is observed that the funny is more distinguishable than other emotions. This may be because funny induced more easily.\n\nIn addition, comparing with Fig.  8 (1) and Fig.  8 (2), it is observed that GMSS has the potential to clarify the borders of various emotions and brings the same emotions closer together in feature space.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "To assess the contribution of each essential pretext task in our model, experiments are conducted with the ablated GMSS models in both unsupervised and supervised modes. The ablation research verifies the influence of each pretext task and the combination of multiple tasks on the performance of EEG emotion recognition. In Table  4 , the results are presented for the subject-dependent experiments in both unsupervised and supervised modes. In the unsupervised mode, GMSS-S, -F, and -C denote that the only spatial jigsaw puzzle task, frequency jigsaw puzzle task, and contrastive learning task are taken into consideration in the ablation model. Furthermore, GMSS-SF, -SC, -FC denote the spatial and frequency jigsaw puzzle tasks, spatial jigsaw puzzle and contrastive learning tasks, frequency jigsaw puzzle and contrastive learning tasks respectively taken into consideration by the ablation model, simultaneously. Similarly, in the supervised mode, GMSS-F, -C, -SF, -SC, and -FC represent the same ablation methods but are trained on the groundtruth emotion labels instead.\n\nIn the case of one self-supervised pretext task, GMSS-S achieves the best performance on four out of six results. This indicates that the spatial jigsaw puzzle task is extremely helpful in improving the discrimination of EEG emotional signals. Moreover, GMSS-F achieves the best performance on two out of six results, which implies that the frequency jigsaw puzzle task is helpful as well. The above results demonstrate that only one jigsaw puzzle task could improve the ability to distinguish EEG emotion signals. In the case of two tasks, GMSS-SF achieves the best performance on all datasets except MPED in the supervised mode, which is slightly lower than that of GMSS-SC. This further proves the effectiveness of the jigsaw puzzle task. In addition, compared with the corresponding results of only one task, the combination of the two tasks improve the accuracy of emotion recognition. This indicates that the three selfsupervised tasks that were proposed are relevant and can promote model learning and more discriminative emotional representation. Furthermore, we can see that GMSS adopts all pretext tasks, achieving the best performance. This proves the effectiveness of our graph-based multi-task self-supervised learning framework.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Parameter Analysis -Chebyshev Filter Size",
      "text": "As a hyper-parameter, Chebyshv filter size K, namely, Korder neighbor, will impact the performance of EEG emotion recognition. Thus, in this section, we conduct additional experiment to analyze the results of different Chebyshv filter size K on SEED dataset. Here we set K = 1, 2, ..., 10 separately. And the results are shown in Fig.  9 . It is obvious that GMSS achieves the best performance when K = 2 . When K is greater than 2, the performance of the model has a relatively noticeable downward trend. When K is greater than 4, it tends to be stable gradually. We attribute the decline to the influence of over-smoothing.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a graph-based multi-task self-supervised learning model is proposed for EEG emotion recognition. Our model is inspired by the multi-task learning theory and self-supervised learning theory, which combines different self-supervised tasks to improve model generalization and the ability to recognize EEG emotional signals. Several selfsupervised tasks assist in improving the resilience of the model to emotion noise labels. The spatial pattern of EEG emotion signals is studied through the spatial jigsaw puzzle task. To reveal the intrinsic frequency bands for EEG emotion recognition, the frequency jigsaw puzzle task is employed, and the feature space is further standardized by the contrastive learning tasks. The experimental results validate the effectiveness of the proposed model. In future work, multi-task self-supervised learning will be further investigated to explore how to further improve EEG emotion recognition.",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of GMSS. In the unsupervised training mode, for the upstream task, the original graph data are not used to train the network. For",
      "page": 4
    },
    {
      "caption": "Figure 1: Three self-supervised tasks",
      "page": 5
    },
    {
      "caption": "Figure 2: Spatial jigsaw puzzle. The 62 electrodes are divided into 10",
      "page": 5
    },
    {
      "caption": "Figure 3: Frequency jigsaw puzzle. The frequency jigsaw puzzle transforms",
      "page": 5
    },
    {
      "caption": "Figure 3: , the energy features of the EEG",
      "page": 5
    },
    {
      "caption": "Figure 4: Contrastive learning. The original data applied with spatial trans-",
      "page": 6
    },
    {
      "caption": "Figure 4: To ensure that positive",
      "page": 6
    },
    {
      "caption": "Figure 5: EEG graph structure and adjacency matrix A construction.",
      "page": 7
    },
    {
      "caption": "Figure 5: , an undirected graph G(V, E) is em-",
      "page": 7
    },
    {
      "caption": "Figure 6: Confusion matrices in unsupervised mode. (a)-(c) and (d)-(f)",
      "page": 9
    },
    {
      "caption": "Figure 6: (1), it is observed that happy is the easiest emotion",
      "page": 9
    },
    {
      "caption": "Figure 6: (2), for SEED, it is obvious that the",
      "page": 9
    },
    {
      "caption": "Figure 6: (1). With SEED-IV, we can notice that",
      "page": 9
    },
    {
      "caption": "Figure 7: Confusion matrices in supervised mode. (a)-(c) and (d)-(f) are the",
      "page": 10
    },
    {
      "caption": "Figure 7: There are two",
      "page": 10
    },
    {
      "caption": "Figure 7: (1), the classiﬁcation",
      "page": 10
    },
    {
      "caption": "Figure 8: shows the representation visualization of the",
      "page": 11
    },
    {
      "caption": "Figure 8: (1), it is difﬁcult to",
      "page": 11
    },
    {
      "caption": "Figure 8: (1) and Fig. 8(2), it is",
      "page": 11
    },
    {
      "caption": "Figure 9: It is obvious",
      "page": 11
    },
    {
      "caption": "Figure 8: t-SNE visualization based on original EEG emotion data and discriminative representations learned by GMSS. (a)-(e) are the distributions",
      "page": 12
    },
    {
      "caption": "Figure 9: Experiment results based on different Chebyshv ﬁlter sizes.",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Brain region": "Pre-Frontal",
          "Electrode name": "AF3, FP1, FPZ, FP2, AF4"
        },
        {
          "Brain region": "Frontal",
          "Electrode name": "F1, FZ, F2, FC1, FCZ, FC2"
        },
        {
          "Brain region": "Left Frontal",
          "Electrode name": "F7, F5, F3, FT7, FC5, FC3"
        },
        {
          "Brain region": "Right Frontal",
          "Electrode name": "F4, F6, F8, FC4, FC6, FT8"
        },
        {
          "Brain region": "Left Temporal",
          "Electrode name": "T7, C5, C3, TP7, CP5, CP3"
        },
        {
          "Brain region": "Right Temporal",
          "Electrode name": "C4, C6, T8, CP4, CP6, TP8"
        },
        {
          "Brain region": "Central",
          "Electrode name": "C1, CZ, C2, CP1, CPZ,\nCP2, P1, PZ, P2"
        },
        {
          "Brain region": "Left Parietal",
          "Electrode name": "P7, P5, P3, PO7, PO5, CB1"
        },
        {
          "Brain region": "Right parietal",
          "Electrode name": "P4, P6, P8, PO6, PO8, CB2"
        },
        {
          "Brain region": "Occipital",
          "Electrode name": "PO3, POZ, PO4, O1, OZ, O2"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "R Dolan"
      ],
      "year": "2002",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Neural correlates of social and nonsocial emotions: An fmri study",
      "authors": [
        "J Britton",
        "K Phan",
        "S Taylor",
        "R Welsh",
        "K Berridge",
        "I Liberzon"
      ],
      "year": "2006",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "4",
      "title": "Real-time eeg-based emotion recognition and its applications",
      "authors": [
        "Y Liu",
        "O Sourina",
        "M Nguyen"
      ],
      "year": "2011",
      "venue": "Transactions on Computational Science XII"
    },
    {
      "citation_id": "5",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Computer-aided diagnosis of depression using eeg signals",
      "authors": [
        "U Acharya",
        "V Sudarshan",
        "H Adeli",
        "J Santhosh",
        "J Koh",
        "A Adeli"
      ],
      "year": "2015",
      "venue": "European Neurology"
    },
    {
      "citation_id": "7",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "9",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Y Roy",
        "H Banville",
        "I Albuquerque",
        "A Gramfort",
        "T Falk",
        "J Faubert"
      ],
      "year": "2019",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "11",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human Brain Mapping"
    },
    {
      "citation_id": "12",
      "title": "Eeg-based spatio-temporal convolutional neural network for driver fatigue evaluation",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "C Mu",
        "Q Cai",
        "W Dang",
        "S Zuo"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "13",
      "title": "Cascade and parallel convolutional recurrent neural networks on eeg-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots",
        "B Benatallah"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "15",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "16",
      "title": "Phase-locking value based graph convolutional neural networks for emotion recognition",
      "authors": [
        "Z Wang",
        "Y Tong",
        "X Heng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "A review on nonlinear methods using electroencephalographic recordings for emotion recognition",
      "authors": [
        "B García-Martínez",
        "A Martinez-Rodrigo",
        "R Alcaraz",
        "A Fernández-Caballero"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "20",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "S Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "22",
      "title": "A review of classification algorithms for eeg-based brain-computer interfaces",
      "authors": [
        "F Lotte",
        "M Congedo",
        "A Lécuyer",
        "F Lamarche",
        "B Arnaldi"
      ],
      "year": "2007",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "23",
      "title": "From regional to global brain: a novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "26",
      "title": "Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction",
      "authors": [
        "Z Li",
        "Z Cui",
        "S Wu",
        "X Zhang",
        "L Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "27",
      "title": "Learning attention-based embeddings for relation prediction in knowledge graphs",
      "authors": [
        "D Nathani",
        "J Chauhan",
        "C Sharma",
        "M Kaul"
      ],
      "year": "2019",
      "venue": "Learning attention-based embeddings for relation prediction in knowledge graphs",
      "arxiv": "arXiv:1906.01195"
    },
    {
      "citation_id": "28",
      "title": "Metapathguided heterogeneous graph neural network for intent recommendation",
      "authors": [
        "S Fan",
        "J Zhu",
        "X Han",
        "C Shi",
        "L Hu",
        "B Ma",
        "Y Li"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "29",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Z Wu",
        "S Pan",
        "F Chen",
        "G Long",
        "C Zhang",
        "S Philip"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "30",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "arxiv": "arXiv:1606.09375"
    },
    {
      "citation_id": "31",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "32",
      "title": "Graph neural networks with convolutional arma filters",
      "authors": [
        "F Bianchi",
        "D Grattarola",
        "L Livi",
        "C Alippi"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Improving graph neural network expressivity via subgraph isomorphism counting",
      "authors": [
        "G Bouritsas",
        "F Frasca",
        "S Zafeiriou",
        "M Bronstein"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "On inductivetransductive learning with graph neural networks",
      "authors": [
        "G Ciano",
        "A Rossi",
        "M Bianchini",
        "F Scarselli"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Deep constraintbased propagation in graph neural networks",
      "authors": [
        "M Tiezzi",
        "G Marra",
        "S Melacci",
        "M Maggini"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
      "authors": [
        "I Kokkinos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "A Kendall",
        "Y Gal",
        "R Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "authors": [
        "R Collobert",
        "J Weston"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers",
      "authors": [
        "J.-T Huang",
        "J Li",
        "D Yu",
        "L Deng",
        "Y Gong"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "41",
      "title": "Multi-task learning as multi-objective optimization",
      "authors": [
        "O Sener",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "Multi-task learning as multi-objective optimization",
      "arxiv": "arXiv:1810.04650"
    },
    {
      "citation_id": "42",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
      "authors": [
        "M Noroozi",
        "P Favaro"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Selfsupervised learning with fully convolutional networks",
      "authors": [
        "Z Yang",
        "H Yu",
        "Y He",
        "Z.-H Mao",
        "A Mian"
      ],
      "year": "2020",
      "venue": "Selfsupervised learning with fully convolutional networks",
      "arxiv": "arXiv:2012.10017"
    },
    {
      "citation_id": "45",
      "title": "Domain generalization by solving jigsaw puzzles",
      "authors": [
        "F Carlucci",
        "A D'innocente",
        "S Bucci",
        "B Caputo",
        "T Tommasi"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "S Gidaris",
        "P Singh",
        "N Komodakis"
      ],
      "year": "2018",
      "venue": "Unsupervised representation learning by predicting image rotations",
      "arxiv": "arXiv:1803.07728"
    },
    {
      "citation_id": "47",
      "title": "Deep clustering for unsupervised learning of visual features",
      "authors": [
        "M Caron",
        "P Bojanowski",
        "A Joulin",
        "M Douze"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "M Caron",
        "I Misra",
        "J Mairal",
        "P Goyal",
        "P Bojanowski",
        "A Joulin"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "49",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "51",
      "title": "Exploring simple siamese representation learning",
      "authors": [
        "X Chen",
        "K He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Temporal contrastive graph for self-supervised video representation learning",
      "authors": [
        "Y Liu",
        "K Wang",
        "H Lan",
        "L Lin"
      ],
      "year": "2021",
      "venue": "Temporal contrastive graph for self-supervised video representation learning",
      "arxiv": "arXiv:2101.00820"
    },
    {
      "citation_id": "53",
      "title": "Ms2l: Multi-task selfsupervised learning for skeleton based action recognition",
      "authors": [
        "L Lin",
        "S Song",
        "W Yang",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "A novel solution for eeg-based emotion recognition",
      "authors": [
        "Z Xie",
        "M Zhou",
        "H Sun"
      ],
      "year": "2021",
      "venue": "2021 IEEE 21st International Conference on Communication Technology"
    },
    {
      "citation_id": "55",
      "title": "Contrastive representation learning for electroencephalogram classification",
      "authors": [
        "M Mohsenvand",
        "M Izadi",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "56",
      "title": "The five percent electrode system for high-resolution eeg and erp measurements",
      "authors": [
        "R Oostenveld",
        "P Praamstra"
      ],
      "year": "2001",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "57",
      "title": "The brain basis of emotion: a meta-analytic review",
      "authors": [
        "K Lindquist",
        "T Wager",
        "H Kober",
        "E Bliss-Moreau",
        "L Barrett"
      ],
      "year": "2012",
      "venue": "The Behavioral and Brain Sciences"
    },
    {
      "citation_id": "58",
      "title": "Regional brain activity in emotion: A framework for understanding cognition in depresion",
      "authors": [
        "W Heller",
        "J Nitscke"
      ],
      "year": "1997",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "59",
      "title": "Affective style, psychopathology, and resilience: brain mechanisms and plasticity",
      "authors": [
        "R Davidson"
      ],
      "year": "2000",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "60",
      "title": "Spectral networks and locally connected networks on graphs",
      "authors": [
        "J Bruna",
        "W Zaremba",
        "A Szlam",
        "Y Lecun"
      ],
      "year": "2013",
      "venue": "Spectral networks and locally connected networks on graphs",
      "arxiv": "arXiv:1312.6203"
    },
    {
      "citation_id": "61",
      "title": "Deep clustering for unsupervised learning of visual features",
      "authors": [
        "M Caron",
        "P Bojanowski",
        "A Joulin",
        "M Douze"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "62",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "63",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "64",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "65",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}