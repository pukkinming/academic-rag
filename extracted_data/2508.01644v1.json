{
  "paper_id": "2508.01644v1",
  "title": "Drkf: Decoupled Representations With Knowledge Fusion For Multimodal Emotion Recognition",
  "published": "2025-08-03T08:05:57Z",
  "authors": [
    "Peiyuan Jiang",
    "Yao Liu",
    "Qiao Liu",
    "Zongshun Zhang",
    "Jiaye Yang",
    "Lu Liu",
    "Daibing Yao"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "Contrastive learning",
    "Mutual information",
    "Multimodal fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) aims to identify emotional states by integrating and analyzing information from multiple modalities. However, inherent modality heterogeneity and inconsistencies in emotional cues remain key challenges that hinder performance. To address these issues, we propose a Decoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF consists of two main modules: an Optimized Representation Learning (ORL) Module and a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual information estimation method with progressive modality augmentation to decouple task-relevant shared representations and modality-specific features while mitigating modality heterogeneity. KF includes a lightweight self-attentionbased Fusion Encoder (FE) that identifies the dominant modality and integrates emotional information from other modalities to enhance the fused representation. To handle potential errors from incorrect dominant modality selection under emotionally inconsistent conditions, we introduce an Emotion Discrimination Submodule (ED), which enforces the fused representation to retain discriminative cues of emotional inconsistency. This ensures that even if the FE selects an inappropriate dominant modality, the Emotion Classification Submodule (EC) can still make accurate predictions",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition based on speech and text is essential for human-computer interaction (HCI)  [1] . The MER's fundamental concept is to acquire modality representations and subsequently fuse them  [2, 3] . In representation learning, contrastive learning-based methods have been widely applied to various multimodal tasks  [4] [5] [6] . These methods rely on the multi-view redundancy assumption, which states that the shared information across different modalities can sufficiently capture the critical features required for downstream tasks  [7, 8] .\n\nAlthough multimodal emotion representation learning has made significant progress under this assumption, it does not always hold in broader real-world multimodal scenarios. In multi-view nonredundant scenarios, the information contained in each modality is not necessarily relevant to the downstream task. To address this issue, existing studies  [9] [10] [11] [12]  leverage techniques such as adversarial learning, parameter sharing, and subspace learning to decouple modality-specific and shared features.\n\nThe aforementioned methods are capable of extracting modalityspecific and shared features. However, they are unable to ensure that the learned representations are pertinent to the tasks.  [13]  adopts an information-theoretic perspective to describe the modality-specific and shared information pertaining to a given task. A Contrastive Mutual Information Estimation (CMIE) method has been introduced to optimize modality-specific and shared representations that are applicable to the task  [14] . In such methods, neural networks (NNs) are typically employed to determine mutual information scores between modalities  [15, 16] . However, the efficacy of representation learning can be compromised by the instability of these approaches when there is a substantial distributional disparity between task labels and input modalities.\n\nIn multimodal representation fusion, early methods primarily relied on tensor fusion and simple feature concatenation  [17, 18] . Recent advances have introduced cross-attention mechanisms to better model semantic dependencies and task-oriented alignment across modalities. However, these mechanisms can still suffer from the introduction of redundant noise and increased modeling ambiguity, particularly when emotion-related information is inconsistently conveyed across different modalities  [19, 20] .\n\nIn this work, we aim to address two key challenges in MER: the difficulty of extracting and aligning task-relevant information across heterogeneous modalities-stemming from their inherent representational differences-and the inconsistencies in emotional cues conveyed by different modalities, as illustrated in Fig.  1 . To tackle these challenges, we propose a Decoupled Representations with Knowledge Fusion Method (DRKF) for MER. Our model consists of two modules: the Optimized Representation Learning Module (ORL), inspired by  [21] , indirectly aligns the input modalities with the label distribution through progressive modality augmentation learning, thereby overcoming the challenge of mutual information estimation in the CMIE method caused by modality-label distribution discrepancies. Following the ORL, the Knowledge Fusion Module (KF) consists of a Fusion Encoder (FE), an Emotion Classification Submodule (EC), and an Emotion Discrimination Submodule (ED). The FE, based on self-attention mechanism, identifies the dominant modality of the current sample and integrates complementary emotional information from other modalities to enhance the fused representation. Under emotionally inconsistent conditions, the ED further constrains the fused representation to retain discriminative cues regarding intermodal emotional discrepancies, thereby mitigating potential errors caused by incorrect dominant modality selection. Finally, the EC takes the fused representation as input to perform the emotion classification task. Through their collaborative design, these three components enable more robust and adaptable multimodal emotion recognition.\n\nâ€¢ We introduce an optimized representation learning module, which learns an optimal enhanced modality to guide the alignment of distributions across modalities as well as between modalities and labels, thereby facilitating more effective representation decoupling.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Representation Learning",
      "text": "Bengio  [22]  emphasized that the performance of machine learning models heavily depends on the selection of input features. Different feature representations can entangle varying underlying explanatory factors, potentially obscuring their distinct contributions to the learning process. Multimodal emotion representations can be categorized into feature engineering-based representations and deep neural network-based representations. The former relies on expert knowledge, including acoustic features extracted using openSMILE  [23]  and textual features from emotion lexicons  [24]  and syntactic structures  [25] , while the latter leverages deep learning to automatically extract high-level features. Representative models include BERT  [26] , RoBERTa  [27] , wav2vec 2.0  [28] , and WavLM  [29] . Building on deep representations, multimodal emotion representation learning can be further categorized into single-stream and multi-stream models. Single-stream models use a shared encoder to learn joint representations within a unified latent space, whereas multi-stream models maintain separate pathways for each modality and integrate them later to capture cross-modal interactions  [30] . While multi-stream architectures effectively preserve modalityspecific information and enhance cross-modal interactions, they can also introduce irrelevant noise, hindering optimal fusion performance. Decoupled representation serves as a key mechanism to filter out irrelevant information and improve fusion quality  [9] .\n\nTo enhance the effectiveness of decoupled representations, recent studies have incorporated contrastive learning to reduce intermodal distributional discrepancies and achieve decoupling of shared representations across modalities  [31, 32] . Additionally, some approaches employ subspace mapping techniques, adversarial learning, and orthogonality constraints to extract task-relevant modalityspecific information  [10, 11, 33] , while others leverage informationtheoretic methods to quantify the task relevance of private information, further improving the interpretability and discriminative power of learned representations  [14, 34, 35] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Fusion",
      "text": "Morency et al.  [36]  identified five core challenges in multimodal learning: representation learning, modality conversion, modality alignment, co-learning, and modality fusion. Among these, modality fusion is essential for cross-modal knowledge integration. Existing fusion strategies can be broadly categorized into feature-level, decision-level, and interaction-based fusion. Feature-level fusion combines features from different modalities, such as through concatenation  [37]  or time-scale-aware integration  [38] , but increases the classifier's burden in handling redundancy and modality misalignment. Decision-level fusion  [39]  integrates modality-specific predictions using methods like ensemble learning, weighted averaging, or voting, but often overlooks fine-grained interactions crucial for tasks like emotion recognition. Interaction-based fusion learns cross-modal relationships through attention mechanisms or latent space alignment. For instance,  [40]  proposed a multi-hop attention mechanism, allowing textual tokens to iteratively query audio features, thus enhancing fusion expressiveness.\n\nAlthough attention-based fusion strategies are effective, they require task-specific queries to adapt to dataset variations, limiting unified multimodal modeling. To address this,  [19, 20]  proposed a bidirectional cross-attention mechanism to improve adaptability and generalization across datasets. However, while bidirectional cross-attention has proven effective, it may introduce noise when dealing with emotionally inconsistent modalities, leading to model confusion and performance degradation.  (1) The ORL Module comprises three components: Modality Encoding (ME), Progressive Augmentation (PA), and Decoupled Representations (DR). The ME integrates an acoustic encoder and a semantic encoder to extract modality-specific embeddings. The acoustic encoder, based on the pre-trained wav2vec2 model 1  , transforms raw audio into acoustic embeddings, while the semantic encoder, leveraging the pre-trained RoBERTa model 2  , encodes raw text into semantic embeddings. The PE employs two identically structured residual autoencoder networks, each consisting of five residual autoencoder blocks with six linear layers per block. The purpose of this component is to learn the optimal feature augmentation for the acoustic and semantic modalities. The DR includes contrastive training and emotion modality (Emodality) shuffling mechanisms. The contrastive training method eliminates task-irrelevant modality noise and facilitates the learning of decoupled representations. The Emodality shuffling mechanism restructures optimized modality pairs for downstream processing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "(2) The KF Module comprises three components: the Fusion Encoder (FE), Emotion Classification Submodule (EC) and the Emotion Discrimination Submodule (ED). The FE is a lightweight, selfattention-based encoder that identifies the dominant modality and integrates supplementary emotional information from other modalities. To address potential errors caused by incorrect dominant modality selection under emotionally inconsistent conditions, the ED enforces the fused representation to retain discriminative cues related to intermodal emotional discrepancies. This mechanism ensures that, even when the FE fails to select the optimal modality, the EC can still make accurate predictions by leveraging the preserved inconsistency information. Both ED and EC are implemented as two independent multilayer perceptrons (MLPs). 3.3.1 The Decoupled Representations. We employed an informationtheoretic-based decoupled representation approach to filter out taskirrelevant modality information and optimize task-relevant representations, including modality-shared information ğ‘† and modalityspecific information ğ‘ƒ. The task-relevant modality information can be expressed by the following formula:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Orl Module",
      "text": "Where, ğ¼ (ğ‘‹ 1 , ğ‘‹ 2 ; ğ‘Œ ) is the mutual information between the task variable ğ‘Œ and the modalities ğ‘‹ 1 and ğ‘‹ 2 .\n\n) Where, ğ‘† is the task-relevant modality-shared mutual information, ğ‘ƒ 1 and ğ‘ƒ 2 are the task-relevant modality-specific mutual information within each modality.\n\nWhere, ğ¼ (ğ‘‹ 1 ; ğ‘‹ 2 ) represents the total mutual information between the two modalities ğ‘‹ 1 and ğ‘‹ 2 , ğ¼ (ğ‘‹ 1 ; ğ‘‹ 2 | ğ‘Œ ) represents the conditional mutual information between ğ‘‹ 1 and ğ‘‹ 2 given the task ğ‘Œ , reflecting task-irrelevant modality-shared mutual information.\n\nCalculating mutual information, as shown in the formula above, requires a closed-form density function and a log-density ratio between the joint and marginal distributions in a manageable form. However, in real-world machine learning tasks, we only have access to samples from the joint distribution, making direct computation of mutual information difficult and forcing us to rely on approximation methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3.3.2",
      "text": "The Progressive Augmentation. To address the challenge of directly computing mutual information, we introduce contrastive mutual information estimation, formulated as follows:\n\nIn the above equations, ğ‘€ represents the batch size during training. The expectation operator E is taken over the joint distribution of positive and negative sample pairs, with or without conditioning on the label ğ‘¦. The function ğ‘“ (â€¢, â€¢) measures correlation between inputs. Positive samples (ğ‘¥ 1 , ğ‘¥ 2 ) or (ğ‘¥ 1 , ğ‘¥ 2 , ğ‘¦) share semantic alignment, while negative samples ğ‘¥ - 2 or (ğ‘¥ - 2 , ğ‘¦) are drawn from other instances in the batch.\n\nAlthough contrastive mutual information estimation circumvents the challenges of direct computation, it still faces a critical limitation: the representation gap between modalities and task labels further increases the difficulty of mutual information estimation. As shown in Eq. (  7 ), ğ‘¥ 1 and ğ‘¥ 2 follow continuous distributions in their respective feature spaces, whereas ğ‘¦ is discrete. This modality-label distribution mismatch makes score estimation more challenging. To tackle these challenges, we propose a progressive modality augmentation strategy that guides the alignment between modalities and between modality and label distributions by iteratively learning the optimal augmented modality. The optimal augmented modality is defined as follows:\n\nâ€¢ Optimal augmented modality: When ğ¼ (ğ‘‹ ; ğ‘Œ ) = ğ¼ (ğ‘‹ ; X1 ), X1\n\nis the optimal unimodal augmentation of ğ‘‹ , which implies that the only information shared between ğ‘‹ and X1 is taskrelevant, and that ğ‘‹ and X1 lie within the same subspace.\n\nOur proposed progressive augmentation strategy, as illustrated in Fig.  3 , is designed to learn the optimal augmented modality. Unlike traditional static feature augmentation methods, our proposed progressive augmentation strategy is a dynamic optimization approach based on original modality features, enabling a stepwise optimization process to achieve optimal modality augmentation. To guide the learning of the augmented features, we introduce two carefully designed constraints. First, by constraining the distribution discrepancy between the augmented modality and the original modality, we ensure that the augmented features reside in the same subspace as the original modality features, thus reducing modality heterogeneity. Second, by minimizing the discrepancy between the augmented modality and the task label distribution, we ensure that the augmented features are aligned with the task label distribution to the greatest extent. Finally, we control the model's overall performance by adjusting the weights of these two constraints.\n\nThe process of progressive augmentation can be described by the following equations:\n\nWe utilize a residual autoencoder ğ‘“ ğ´ğ¸ to learn the optimal unimodal augmentation Tğ‘ ğ‘’ğ‘ , Tğ‘ğ‘™ğ‘  for the text feature sequence.\n\nWhere ğ‘‡ ğ‘ ğ‘’ğ‘ âˆˆ R ğ‘‘ ğ‘› Ã—ğ‘‘ ğ‘§ denotes the feature sequence output of the text encoder, with ğ‘‘ ğ‘› as the sequence length and ğ‘‘ ğ‘§ as the dimensionality of each sequence element. Tğ‘ğ‘™ğ‘  âˆˆ R ğ‘‘ ğ‘§ represents the global feature vector obtained by average pooling the sequence feature vector.\n\nSimilarly, the optimal unimodal augmentation for the speech feature sequence is calculated as follows:\n\nWhere ğ‘† ğ‘ ğ‘’ğ‘ âˆˆ R ğ‘‘ ğ‘› Ã—ğ‘‘ ğ‘§ denotes the feature sequence output of the speech encoder, with ğ‘‘ ğ‘› as the sequence length and ğ‘‘ ğ‘§ as the dimensionality of each sequence element. Sğ‘ğ‘™ğ‘  âˆˆ R ğ‘‘ ğ‘§ represents the global feature vector obtained by average pooling the sequence feature vector.\n\nTo ensure that the augmented modality remains in the same subspace as the original modality, we use the Mean Squared Error (MSE) loss to constrain the learning space of the augmented modality. Meanwhile, the Kullback-Leibler divergence (KLD) loss is employed to enforce the augmented modality to learn the distribution of the task labels. The calculation formula is as follows:\n\nHere, ğ‘† ğ‘– ğ‘ ğ‘’ğ‘ and ğ‘‡ ğ‘– ğ‘ ğ‘’ğ‘ represent the original feature sequence vectors of the ğ‘–-th sample in the speech and text modalities, respectively. Sğ‘– ğ‘ ğ‘’ğ‘ and T ğ‘– ğ‘ ğ‘’ğ‘ represent the augmented feature sequence vectors. ğ‘€ is the batch size, and ğ¶ denotes the total number of emotion categories. The true label of the ğ‘–-th sample in the fusion modality for category ğ‘ is denoted as ğ‘¦ ğ‘ ğ‘– , while Å·ğ‘ ğ‘– represents the predicted probability for the same sample and category. L ğ‘ refers to the final augmentation loss, and ğ›¼ is a hyperparameter.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contrastive Mutual Information Estimation.",
      "text": "The maximum task-relevant modality mutual information can be transformed into minimizing the negative of contrastive mutual information. Therefore, the objective function for our decoupled representation learning method is defined as follows:\n\nIn the above formulas, ğ‘”(â€¢) represents a projection function implemented via an MLP; sim(â€¢, â€¢) represents the cosine similarity; L ğ‘€ğ¼ ğ‘† ğ‘– and L ğ‘€ğ¼ ğ‘‡ ğ‘– denote the intra-modality CMIE objective functions, while L ğ‘€ğ¼ ğ‘† ğ‘– &ğ‘‡ ğ‘– corresponds to the intermodality CMIE objective function.\n\nFinally, our CMIE optimization objective L ğ‘ is defined as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Kf Module",
      "text": "3.4.1 The Fusion Encoder. To enable the proposed knowledge fusion module to effectively model the complex interactions between different modalities, we applied a specialized concatenation process to the multimodal input sequences, as shown in Equation  18 .\n\nWhere, ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â€¢) denotes the concatenation function, ğ¶ ğ‘ğ‘™ğ‘  and ğ¶ ğ‘ ğ‘’ğ‘ represent the classification token vector and separator token vector, respectively. ğ¶ ğ‘ğ‘™ğ‘  is designed to aggregate global information from the input features during the modality fusion process, while ğ¶ ğ‘ ğ‘’ğ‘ acts as a boundary to distinguish between the two modalities, facilitating the ED in learning modality-consistent information. Finally, ğ‘“ ğ¹ ğ¸ (â€¢) processes the concatenated sequence to generate the fused feature vector ğ‘‹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "3.4.2",
      "text": "The Emotion Discrimination Submodule. The ED is implemented as an MLP, trained on data generated through the Emodality shuffling process in the ORL module. Specifically, for each batch of data, samples from different modalities are randomly combined to create new speech-text pairs. If the original emotion labels of both modalities in a newly formed pair match, the emotional information is considered consistent (assigned a label of 1). Conversely, if the labels do not match, the emotional information is deemed inconsistent (assigned a label of 0). The optimization function for this module is as follows:\n\nWhere, ğ‘€ is the batch size, ğ‘Œ ğ‘– is the true label of the ğ‘–-th sample in the binary emotion decoupling task, and Å¶ğ‘– represents the predicted probability of the ğ‘–-th sample output.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3.4.3",
      "text": "The Emotion Classification Submodule. We employ an independent MLP to perform multimodal emotion classification. Specifically, the EC takes as input the correctly matched sample pairs generated by the Emodality shuffling process. The calculation processes for the emotion classification loss L ğ‘“ is presented in Equation  20 .\n\nWhere, ğ‘€ is the batch size, ğ¶ is the number of emotion categories, ğ‘¦ ğ‘ ğ‘– is the true label of the ğ‘–-th sample in the fusion modality for Finally, our objective function, denoted as L, is formally defined in Equation  21 .\n\nWhere ğ›½, ğ›¾, and ğ›¿ are hyperparameters.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Settings 4.1 Datasets",
      "text": "We validated our proposed model on three publicly available multimodal datasets, IEMOCAP  [50] , MELD  [51]  and M3ED  [52] . Specifically, IEMOCAP is a recorded dialogue dataset with emotion labels including anger, happiness, sadness, frustration, excitement, fear, surprise, disgust, and others. To ensure consistency with previous research, we focused on four emotion categories: happiness, sadness, anger, and neutral, where excitement was merged into the happiness category, resulting in a total of 5,531 samples. The experiments followed a five-fold leave-one-session-out strategy, Using Unweighted Accuracy (ACC), Weighted Accuracy (WACC), and their average (Avg) to evaluate model performance. MELD is a challenging multi-party conversation dataset, annotated with seven emotion labels. Unlike IEMOCAP, this dataset is divided into training, development, and test sets, providing a standardized training and evaluation strategy for models. WACC, Weighted F1 score (WF1) and Avg were used to assess the performance of the models on this dataset. M3ED is the first Chinese multi-label emotion dialogue dataset. The utterance-level emotion labels include seven categories: happiness, surprise, sadness, disgust, anger, fear, and neutral. Following previous studies, we used Precision, Recall, ACC, Micro-F1 (F1) score and the Avg as evaluation metrics to assess model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Our model is implemented using the PyTorch framework, with AdamW as the optimizer, a learning rate of 1e-5, and a batch size of 4. The output dimension of the projection head function in contrastive learning is set to 1024, and the multimodal fusion layer has 8 attention heads. The values of the loss function hyperparameters are set to 0.2, 0.2, 1.0, and 0.2, respectively. Our training is conducted on a Linux system with an A100 GPU, for a total of 100 epochs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Models",
      "text": "To validate the effectiveness of the proposed method, we compared ORKF with the current advanced baseline methods. The baselines used to evaluate ORKF across different datasets are as follows. It is important to note that some baseline models are evaluated on multiple datasets, and we provide their descriptions only when they are mentioned for the first time.\n\nCompared Methods for IEMOCAP Dataset: GBAN  [41]  with gated attention fusion; DIMMN  [53]  using dynamic memory interaction; MSER-CADF  [43]  with cross-attention fusion; MCFN  [44]  employing dual-stream temporal-spatial modeling; SAMS  [45]  aligning semantics across modalities; LLMSER  [46]  enhancing prompts in language models; KS-Transformer  [47]  using pre-trained feature extraction and early fusion; KBCAM  [48]  incorporating Bayesian attention with external knowledge; DBT  [49]  utilizing dual-branch Transformer with fine-tuning fusion. Compared Methods for MELD Dataset: MCSCAN  [54]  with parallel cross/self-attention; DIMMN  [53]  with dynamic memory integration; SACMA  [55]  integrating speaker-aware emotion recognition; SMCN  [56]  self-guided modality alignment; RMERCT  [57]  using Transformer-based cross-modal fusion; SMIN  [58]  semi-supervised multimodal learning; HiMul-LGG  [59]  hierarchical decision fusion strategy. Compared Methods for M3ED Dataset: MSCNN-SPU  [60]  integrating multi-scale CNN with statistical pooling; M-TLEAF  [61]  using bidirectional GRU and Transformer fusion; CARAT  [62]  employing contrastive feature reconstruction and aggregation.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "As shown in Table  1 , the proposed ORKF method achieves the best overall performance on the IEMOCAP dataset, with an ACC of 80.7%, a WACC of 79.9%, and an average (Avg) of 80.3%. In terms of Notes: N/A indicates that the metric value was not provided in the original paper. â€¡ denotes that the average value was calculated from known experimental results due to irreproducibility. To further validate the performance of ORKF, we evaluated the model on the MELD dataset. The experimental results are presented in Table  2 .\n\nAccording to the experimental comparison results in Table  2 , ORKF demonstrates superior performance even on the highly imbalanced MELD dataset. Specifically, ORKF achieves a WACC of 66.7, a WF1 of 65.4, and an average score of 66.0, all of which represent the best results among the compared methods. Although ORKF achieved SOTA performance on both the IEMOCAP and MELD datasets, it should be noted that these datasets are English datasets with single-label annotations, where task-relevant information may primarily rely on shared mutual information. To further validate the model's performance, we introduced the Chinese multi-label emotion recognition dataset M3ED for testing.\n\nAs shown in Table  3 , even on the more complex multi-label Chinese emotion recognition dataset, ORKF demonstrates strong performance, achieving an F1 score of 52.0 and an average score of 51.7, reaching SOTA results.\n\nOverall, the comparative experimental results demonstrate that ORKF effectively integrates information from multiple modalities, achieving robust emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the effectiveness of the proposed strategy, we conducted ablation experiments on the IEMOCAP, MELD, and M3ED datasets, with the results presented in Table  4 .\n\nAs shown in Table  4 , the comparison between the first and fourth rows demonstrates that removing the ED leads to a noticeable performance drop across all three datasets, highlighting its critical role in helping the fusion module learn effective joint representations. The comparison between the second and third rows further validates the effectiveness of the proposed Progressive Contrastive Mutual Information Estimation (PCMI) approach in enhancing decoupled representation learning and improving overall model performance.\n\nTo further verify the effectiveness of the proposed decoupled representation learning strategy based on PCMI, we visualized the learned embeddings on the test set (session 2) of the IEMOCAP dataset using t-distributed Stochastic Neighbor Embedding (t-SNE), as shown in Figs.  4a-4c . From the figures, it can be observed that the   Finally, the comparison between the fifth and sixth rows shows that the proposed self-attention-based fusion encoder outperforms the traditional cross-attention fusion encoder, offering more substantial gains in emotion recognition accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "To address the prevalent challenges of modality heterogeneity and emotional inconsistency across modalities in MER tasks, we propose a novel framework named Decoupled Representations with Knowledge Fusion (DRKF). The framework consists of two core modules: the Optimized Representation Learning (ORL) module and the Knowledge Fusion (KF) module. Specifically, the ORL module aims to decouple task-relevant modality-shared and modalityspecific information while reducing inter-modality heterogeneity, thereby facilitating more effective multimodal fusion. The KF module is designed to learn a fusion representation that is sensitive to emotional discrepancies across modalities, which enhances the model's robustness in scenarios where emotional cues from different modalities are not aligned. Extensive experiments on three widely used benchmark datasets for multimodal emotion recognition demonstrate that DRKF outperforms several state-of-the-art models across multiple evaluation metrics, exhibiting strong performance and generalization capabilities.\n\nDespite the promising results achieved by the proposed DRKF model on bimodal emotion recognition tasks, certain limitations remain. The current evaluation is limited to the audio-text bimodal setting, and has not yet been extended to trimodal or higher-order multimodal fusion scenarios. In future work, we plan to further explore the adaptability and scalability of DRKF in more complex multimodal input settings, such as those involving video, speech, and text, to better address the demands of real-world multimodal emotion recognition applications.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of Modality-specific Emotions and True",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the Proposed DRKF Framework. It consists of two components: the ORL Module, which improves",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the proposed DRKF. It consists of two key compo-",
      "page": 3
    },
    {
      "caption": "Figure 3: Decoupled Representations Learning Flowchart.",
      "page": 4
    },
    {
      "caption": "Figure 3: , is designed to learn the optimal augmented modality. Un-",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparison of Emotion Category Distributions: PCMI vs. CMIE and Baseline.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": ""
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Peiyuan Jiang"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "School of Computer Science and"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Engineering, University of Electronic"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Science and Technology of China"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Chengdu, Sichuan, China"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "darcy981020@gmail.com"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Zongshun Zhang"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "School of Computer Science and"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Engineering, University of Electronic"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Science and Technology of China"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "Chengdu, Sichuan, China"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": "202421081411@std.uestc.edu.cn"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": ""
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": ""
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": ""
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chengdu, Sichuan, China": "357497551@qq.com"
        },
        {
          "Chengdu, Sichuan, China": "Abstract"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "Multimodal emotion recognition (MER) aims to identify emotional"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "states by integrating and analyzing information from multiple"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "modalities. However, inherent modality heterogeneity and incon-"
        },
        {
          "Chengdu, Sichuan, China": "sistencies in emotional cues remain key challenges that hinder"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "performance. To address these issues, we propose a Decoupled"
        },
        {
          "Chengdu, Sichuan, China": "Representations with Knowledge Fusion (DRKF) method for MER."
        },
        {
          "Chengdu, Sichuan, China": "DRKF consists of\ntwo main modules: an Optimized Representa-"
        },
        {
          "Chengdu, Sichuan, China": "tion Learning (ORL) Module and a Knowledge Fusion (KF) Module."
        },
        {
          "Chengdu, Sichuan, China": "ORL employs a contrastive mutual information estimation method"
        },
        {
          "Chengdu, Sichuan, China": "with progressive modality augmentation to decouple task-relevant"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "shared representations and modality-specific features while mitigat-"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "ing modality heterogeneity. KF includes a lightweight self-attention-"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "based Fusion Encoder (FE) that identifies the dominant modality and"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "integrates emotional information from other modalities to enhance"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "the fused representation. To handle potential errors from incor-"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "rect dominant modality selection under emotionally inconsistent"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "conditions, we introduce an Emotion Discrimination Submodule"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "(ED), which enforces the fused representation to retain discrimi-"
        },
        {
          "Chengdu, Sichuan, China": "native cues of emotional inconsistency. This ensures that even if"
        },
        {
          "Chengdu, Sichuan, China": "the FE selects an inappropriate dominant modality, the Emotion"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "Classification Submodule (EC) can still make accurate predictions"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "*Corresponding author: Yao Liu."
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "Permission to make digital or hard copies of all or part of this work for personal or"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "classroom use is granted without fee provided that copies are not made or distributed"
        },
        {
          "Chengdu, Sichuan, China": "for profit or commercial advantage and that copies bear this notice and the full citation"
        },
        {
          "Chengdu, Sichuan, China": "on the first page. Copyrights for components of this work owned by others than the"
        },
        {
          "Chengdu, Sichuan, China": ""
        },
        {
          "Chengdu, Sichuan, China": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or"
        },
        {
          "Chengdu, Sichuan, China": "republish, to post on servers or to redistribute to lists, requires prior specific permission"
        },
        {
          "Chengdu, Sichuan, China": "and/or a fee. Request permissions from permissions@acm.org."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "estimation in the CMIE method caused by modality-label distri-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "bution discrepancies. Following the ORL, the Knowledge Fusion"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Module (KF) consists of a Fusion Encoder (FE), an Emotion Classifi-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "cation Submodule (EC), and an Emotion Discrimination Submodule"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "(ED). The FE, based on self-attention mechanism,\nidentifies the"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "dominant modality of the current sample and integrates comple-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "mentary emotional information from other modalities to enhance"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "the fused representation. Under emotionally inconsistent condi-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "tions, the ED further constrains the fused representation to retain"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "discriminative cues regarding intermodal emotional discrepancies,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "thereby mitigating potential errors caused by incorrect dominant"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "modality selection. Finally, the EC takes the fused representation"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "as input to perform the emotion classification task. Through their"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "collaborative design, these three components enable more robust"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Figure 1: Illustration of Modality-specific Emotions and True",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "and adaptable multimodal emotion recognition."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Labels in a Multi-view Non-redundant Scenario.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "â€¢ We introduce an optimized representation learning mod-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "as input to perform the emotion classification task. Through their": "collaborative design, these three components enable more robust"
        },
        {
          "as input to perform the emotion classification task. Through their": "and adaptable multimodal emotion recognition."
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "â€¢ We introduce an optimized representation learning mod-"
        },
        {
          "as input to perform the emotion classification task. Through their": "ule, which learns an optimal enhanced modality to guide"
        },
        {
          "as input to perform the emotion classification task. Through their": "the alignment of distributions across modalities as well as"
        },
        {
          "as input to perform the emotion classification task. Through their": "between modalities and labels, thereby facilitating more ef-"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "fective representation decoupling."
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "â€¢ We introduce a Knowledge Fusion Module that leverages"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "collaborative learning to integrate fusion encoding, emo-"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "tion consistency discrimination, and emotion classification,"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "ensuring reliable emotion recognition."
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "â€¢ Extensive experiments on three benchmark datasets demon-"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "strated that the proposed DRKF framework surpasses state-"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "of-the-art methods."
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "2\nRelated Work"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "2.1\nRepresentation Learning"
        },
        {
          "as input to perform the emotion classification task. Through their": ""
        },
        {
          "as input to perform the emotion classification task. Through their": "Bengio [22] emphasized that the performance of machine learning"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion classification.": "representations across modalities [31, 32]. Additionally, some ap-"
        },
        {
          "emotion classification.": "proaches employ subspace mapping techniques, adversarial learn-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "ing, and orthogonality constraints to extract task-relevant modality-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "specific information [10, 11, 33], while others leverage information-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "theoretic methods to quantify the task relevance of private infor-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "mation, further improving the interpretability and discriminative"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "power of learned representations [14, 34, 35]."
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "2.2\nModality Fusion"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "Morency et al. [36] identified five core challenges in multimodal"
        },
        {
          "emotion classification.": "learning: representation learning, modality conversion, modality"
        },
        {
          "emotion classification.": "alignment, co-learning, and modality fusion. Among these, modality"
        },
        {
          "emotion classification.": "fusion is essential for cross-modal knowledge integration. Exist-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "ing fusion strategies can be broadly categorized into feature-level,"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "decision-level, and interaction-based fusion. Feature-level fusion"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "combines features from different modalities, such as through con-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "catenation [37] or time-scale-aware integration [38], but increases"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "the classifierâ€™s burden in handling redundancy and modality mis-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "alignment. Decision-level fusion [39] integrates modality-specific"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "predictions using methods like ensemble learning, weighted av-"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "eraging, or voting, but often overlooks fine-grained interactions"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "crucial for tasks like emotion recognition. Interaction-based fusion"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "learns cross-modal relationships through attention mechanisms"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "or latent space alignment. For instance, [40] proposed a multi-hop"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "attention mechanism, allowing textual tokens to iteratively query"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "audio features, thus enhancing fusion expressiveness."
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "Although attention-based fusion strategies are effective, they"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "require task-specific queries to adapt to dataset variations, limiting"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "unified multimodal modeling. To address this, [19, 20] proposed a"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "bidirectional cross-attention mechanism to improve adaptability"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "and generalization across datasets. However, while bidirectional"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "cross-attention has proven effective, it may introduce noise when"
        },
        {
          "emotion classification.": "dealing with emotionally inconsistent modalities, leading to model"
        },
        {
          "emotion classification.": ""
        },
        {
          "emotion classification.": "confusion and performance degradation."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(2) The KF Module comprises three components: the Fusion",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "conditional mutual information between ğ‘‹1 and ğ‘‹2 given the task"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Encoder (FE), Emotion Classification Submodule (EC) and the Emo-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "ğ‘Œ , reflecting task-irrelevant modality-shared mutual information."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tion Discrimination Submodule (ED). The FE is a lightweight, self-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Calculating mutual information, as shown in the formula above,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "attention-based encoder that identifies the dominant modality and",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "requires a closed-form density function and a log-density ratio"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "integrates supplementary emotional information from other modal-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "between the joint and marginal distributions in a manageable form."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ities. To address potential errors caused by incorrect dominant",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "However, in real-world machine learning tasks, we only have access"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "modality selection under emotionally inconsistent conditions, the",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "to samples from the joint distribution, making direct computation of"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ED enforces the fused representation to retain discriminative cues",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "mutual information difficult and forcing us to rely on approximation"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "related to intermodal emotional discrepancies. This mechanism en-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "methods."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "sures that, even when the FE fails to select the optimal modality, the",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "3.3.2\nThe Progressive Augmentation. To address the challenge of"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "EC can still make accurate predictions by leveraging the preserved",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "directly computing mutual information, we introduce contrastive"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "inconsistency information. Both ED and EC are implemented as",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "mutual information estimation, formulated as follows:"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "two independent multilayer perceptrons (MLPs).",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.3\nThe ORL Module",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "exp ğ‘“\n(ğ‘¥1, ğ‘¥2)"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "(6)\nlog\nğ¼ (ğ‘‹1; ğ‘‹2) = Eğ‘¥1, ğ‘¥2, ğ‘¥ âˆ’"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nï£¹ï£ºï£ºï£ºï£ºï£ºï£»\n(cid:16)\n(cid:17)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "exp(sim(ğ‘§ğ‘–\nğ‘†, ğ‘§ğ‘–\nğ‘‡ )/ğœ)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the augmented features are aligned with the task label distribu-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(16)\nLğ‘€ğ¼ğ‘†ğ‘– &ğ‘‡ğ‘– = âˆ’ log"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(cid:205)ğ‘€"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tion to the greatest extent. Finally, we control the modelâ€™s overall\nğ‘†, ğ‘§ğ‘˜\nğ‘‡ )/ğœ)\nğ‘˜=1 exp(sim(ğ‘§ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "performance by adjusting the weights of these two constraints."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "In the above formulas, ğ‘”(Â·) represents a projection function im-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "The process of progressive augmentation can be described by"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "plemented via an MLP; sim(Â·, Â·) represents the cosine similarity;"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the following equations:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Lğ‘€ğ¼ğ‘†ğ‘–\nand Lğ‘€ğ¼ğ‘‡ğ‘– denote the intra-modality CMIE objective func-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "We utilize a residual autoencoder ğ‘“ğ´ğ¸ to learn the optimal uni-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "corresponds to the intermodality CMIE ob-\ntions, while Lğ‘€ğ¼ğ‘†ğ‘– &ğ‘‡ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘‡ğ‘ğ‘™ğ‘ \nmodal augmentation Ëœğ‘‡ğ‘ ğ‘’ğ‘,\nfor the text feature sequence."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "jective function."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘‡ğ‘ ğ‘’ğ‘ = ğ‘“ğ´ğ¸ (ğ‘‡ğ‘ ğ‘’ğ‘),\nğ‘‡ğ‘ğ‘™ğ‘  = avg( Ëœğ‘‡ğ‘ ğ‘’ğ‘)\n(8)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Finally, our CMIE optimization objective Lğ‘ is defined as follows:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Where ğ‘‡ğ‘ ğ‘’ğ‘ âˆˆ Rğ‘‘ğ‘› Ã—ğ‘‘ğ‘§ denotes the feature sequence output of"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(cid:16)\n(cid:17)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "1 ğ‘€\nğ‘€âˆ‘ï¸ ğ‘–\nLğ‘ =\nthe text encoder, with ğ‘‘ğ‘› as the sequence length and ğ‘‘ğ‘§ as the"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(17)\nLğ‘€ğ¼ğ‘†ğ‘–\n+ Lğ‘€ğ¼ğ‘‡ğ‘–\n+ Lğ‘€ğ¼ğ‘†ğ‘– &ğ‘‡ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "=1\nğ‘‡ğ‘ğ‘™ğ‘  âˆˆ Rğ‘‘ğ‘§\ndimensionality of each sequence element.\nrepresents"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the global feature vector obtained by average pooling the sequence"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.4\nThe KF Module"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "feature vector."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.4.1\nThe Fusion Encoder. To enable the proposed knowledge fu-\nSimilarly, the optimal unimodal augmentation for the speech"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "sion module to effectively model the complex interactions between\nfeature sequence is calculated as follows:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "different modalities, we applied a specialized concatenation process"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘†ğ‘ ğ‘’ğ‘ = ğ‘“ğ´ğ¸ (ğ‘†ğ‘ ğ‘’ğ‘),\nğ‘†ğ‘ğ‘™ğ‘  = avg( Ëœğ‘†ğ‘ ğ‘’ğ‘)\n(9)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "to the multimodal input sequences, as shown in Equation 18."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Where ğ‘†ğ‘ ğ‘’ğ‘ âˆˆ Rğ‘‘ğ‘› Ã—ğ‘‘ğ‘§ denotes the feature sequence output of"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘‹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› = ğ‘“ğ¹ ğ¸ (cid:0)Concat (cid:0)ğ¶ğ‘ğ‘™ğ‘ , ğ‘†ğ‘ ğ‘’ğ‘, ğ¶ğ‘ ğ‘’ğ‘,ğ‘‡ğ‘ ğ‘’ğ‘, ğ¶ğ‘ ğ‘’ğ‘ (cid:1)(cid:1)\n(18)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the speech encoder, with ğ‘‘ğ‘› as the sequence length and ğ‘‘ğ‘§ as the"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘†ğ‘ğ‘™ğ‘  âˆˆ Rğ‘‘ğ‘§\ndimensionality of each sequence element.\nrepresents\nWhere, ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (Â·) denotes the concatenation function, ğ¶ğ‘ğ‘™ğ‘  and"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the global feature vector obtained by average pooling the sequence\nğ¶ğ‘ ğ‘’ğ‘ represent the classification token vector and separator token"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "feature vector.\nvector, respectively. ğ¶ğ‘ğ‘™ğ‘  is designed to aggregate global information"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "To ensure that\nthe augmented modality remains in the same\nfrom the input features during the modality fusion process, while"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "subspace as the original modality, we use the Mean Squared Er-\nğ¶ğ‘ ğ‘’ğ‘ acts as a boundary to distinguish between the two modalities,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ror (MSE) loss to constrain the learning space of the augmented\nfacilitating the ED in learning modality-consistent\ninformation."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "modality. Meanwhile, the Kullback-Leibler divergence (KLD) loss\nFinally, ğ‘“ğ¹ ğ¸ (Â·) processes the concatenated sequence to generate the"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "is employed to enforce the augmented modality to learn the distri-\nfused feature vector ğ‘‹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "bution of the task labels. The calculation formula is as follows:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.4.2\nThe Emotion Discrimination Submodule. The ED is imple-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2\n2(cid:21)\n(cid:20)(cid:13)\n1"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "mented as an MLP, trained on data generated through the Emodality\nğ‘†ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘€âˆ‘ï¸ ğ‘–\n(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n+\nLğ‘€ğ‘†ğ¸ =\n(10)\nğ‘ ğ‘’ğ‘ âˆ’ Ëœğ‘†ğ‘–\nğ‘ ğ‘’ğ‘\nğ‘ ğ‘’ğ‘ âˆ’ Ëœğ‘‡ ğ‘–\nğ‘ ğ‘’ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2ğ‘€\nshuffling process in the ORL module. Specifically, for each batch"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "=1"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "of data, samples from different modalities are randomly combined"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘¦ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘€âˆ‘ï¸ ğ‘–\nğ¶âˆ‘ï¸ ğ‘\nto create new speech-text pairs. If the original emotion labels of\nğ‘¦ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "1 ğ‘€\nLğ¾ğ¿ğ· =\n(11)\nlog\nğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘¦ğ‘\nË†"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘–\nboth modalities in a newly formed pair match, the emotional infor-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "=1\n=1"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "mation is considered consistent (assigned a label of 1). Conversely,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Lğ‘ = ğ›¼ Â· Lğ‘€ğ‘†ğ¸ + Lğ¾ğ¿ğ·\n(12)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "if the labels do not match, the emotional\ninformation is deemed"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Here, ğ‘†ğ‘–\nğ‘ ğ‘’ğ‘ and ğ‘‡ ğ‘–\nğ‘ ğ‘’ğ‘ represent the original feature sequence vec-\ninconsistent (assigned a label of 0). The optimization function for"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tors of the ğ‘–-th sample in the speech and text modalities, respectively.\nthis module is as follows:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘ ğ‘’ğ‘ and Ëœğ‘‡ ğ‘–\nğ‘ ğ‘’ğ‘ represent the augmented feature sequence vectors."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘€ 2\nğ‘€ is the batch size, and ğ¶ denotes the total number of emotion"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "1 ğ‘€\nâˆ‘ï¸ ğ‘–\n(cid:2)ğ‘Œğ‘–\ncategories. The true label of the ğ‘–-th sample in the fusion modality\nLğ‘ = âˆ’\nlog Ë†ğ‘Œğ‘– + (1 âˆ’ ğ‘Œğ‘– ) log(1 âˆ’ Ë†ğ‘Œğ‘– )(cid:3)\n(19)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "=1\nfor category ğ‘ is denoted as ğ‘¦ğ‘\nrepresents the predicted\nğ‘– , while Ë†ğ‘¦ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "probability for the same sample and category. Lğ‘ refers to the final\nWhere, ğ‘€ is the batch size, ğ‘Œğ‘–\nis the true label of the ğ‘–-th sample in"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "augmentation loss, and ğ›¼ is a hyperparameter.\nthe binary emotion decoupling task, and Ë†ğ‘Œğ‘– represents the predicted"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "probability of the ğ‘–-th sample output."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.3.3\nContrastive Mutual Information Estimation. The maximum"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "task-relevant modality mutual\ninformation can be transformed"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "3.4.3\nThe Emotion Classification Submodule. We employ an inde-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "into minimizing the negative of contrastive mutual\ninformation."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "pendent MLP to perform multimodal emotion classification. Specif-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Therefore, the objective function for our decoupled representation"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ically, the EC takes as input the correctly matched sample pairs gen-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "learning method is defined as follows:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "erated by the Emodality shuffling process. The calculation processes"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘§ğ‘–\nğ‘§ğ‘–\nğ‘§ğ‘–\nfor the emotion classification loss Lğ‘“"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "is presented in Equation 20.\nËœ\nËœ\n(13)\nğ‘† = ğ‘”(ğ‘†ğ‘–\nğ‘† = ğ‘”( Ëœğ‘†ğ‘–\nğ‘‡ = ğ‘”(ğ‘‡ ğ‘–\nğ‘‡ = ğ‘”( Ëœğ‘‡ ğ‘–\nğ‘ğ‘™ğ‘  ),\nğ‘ğ‘™ğ‘  ), ğ‘§ğ‘–\nğ‘ğ‘™ğ‘  ),\nğ‘ğ‘™ğ‘  )"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘§ğ‘–\nexp(sim(ğ‘§ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Ëœ\nğ‘†,\nğ‘† )/ğœ)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘€âˆ‘ï¸ ğ‘–\nğ¶âˆ‘ï¸ ğ‘\nğ‘¦ğ‘\n(14)\nlog Ë†ğ‘¦ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "1 ğ‘€\nLğ‘“ = âˆ’\n(20)\nLğ‘€ğ¼ğ‘†ğ‘– = âˆ’ log"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘–\n(cid:205)2ğ‘€"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Ëœ\n[ğ‘˜ â‰  ğ‘–] exp(sim(ğ‘§ğ‘–\nğ‘†,\nğ‘† )/ğœ)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘˜=1\n=1\n=1"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘§ğ‘–\nexp(sim(ğ‘§ğ‘–"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Ëœ\nğ‘‡ ,\nğ‘‡ )/ğœ)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Where, ğ‘€ is the batch size, ğ¶ is the number of emotion categories,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(15)\nLğ‘€ğ¼ğ‘‡ğ‘– = âˆ’ log"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(cid:205)2ğ‘€\nğ‘¦ğ‘"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "is the true label of the ğ‘–-th sample in the fusion modality for\nËœ\n[ğ‘˜ â‰  ğ‘–] exp(sim(ğ‘§ğ‘–\nğ‘–\nğ‘‡ ,\nğ‘‡ )/ğœ)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ğ‘˜=1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Models",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "WACC (%)"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "GBAN [41]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "72.4"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "MSER-MVAM [42]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "75.4"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "MSER-CADF [43]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "76.5"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "MCFN [44]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "76.0"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "SAMS [45]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "76.6"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "LLMSER [46]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "78.1"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "KS-Transformer [47]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "74.3"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "KBCAM [48]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "75.5"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "DBT [49]",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "77.8"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Ours(ORKF)",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "79.9"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Î”Sota",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "â†‘2.30"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "improvement of our proposed method compared to the second-best model. (â†‘) indicates an improvement over the second-best performance,"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "where higher values are better. (â†“) indicates a decrease relative to the best performance, where lower values are better."
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "category ğ‘, and Ë†ğ‘¦ğ‘\nrepresents the predicted probability of the ğ‘–-th"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "sample in the fusion modality for category ğ‘."
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "Finally, our objective function, denoted as L, is formally defined"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "in Equation 21."
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "L = Lğ‘ + ğ›½ Â· Lğ‘ + ğ›¾ Â· Lğ‘“ + ğ›¿ Â· Lğ‘\n(21)"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "Where ğ›½, ğ›¾, and ğ›¿ are hyperparameters."
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "4\nExperiment Settings"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "4.1\nDatasets"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "We validated our proposed model on three publicly available multi-"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "modal datasets, IEMOCAP [50], MELD [51] and M3ED [52]. Specif-"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "ically, IEMOCAP is a recorded dialogue dataset with emotion labels"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "including anger, happiness, sadness, frustration, excitement, fear,"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "surprise, disgust, and others. To ensure consistency with previ-"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "ous research, we focused on four emotion categories: happiness,"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "sadness, anger, and neutral, where excitement was merged into"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "the happiness category, resulting in a total of 5,531 samples. The"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "experiments followed a five-fold leave-one-session-out strategy,"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "Using Unweighted Accuracy (ACC), Weighted Accuracy (WACC),"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "and their average (Avg)\nto evaluate model performance. MELD"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "is a challenging multi-party conversation dataset, annotated with"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "seven emotion labels. Unlike IEMOCAP, this dataset is divided into"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "training, development, and test sets, providing a standardized train-"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "ing and evaluation strategy for models. WACC, Weighted F1 score"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "(WF1) and Avg were used to assess the performance of the models"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "on this dataset. M3ED is the first Chinese multi-label emotion di-"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "alogue dataset. The utterance-level emotion labels include seven"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "categories: happiness, surprise, sadness, disgust, anger, fear, and"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "neutral. Following previous studies, we used Precision, Recall, ACC,"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "Micro-F1 (F1) score and the Avg as evaluation metrics to assess"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "model performance."
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "4.2\nImplementation Details"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": ""
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "Our model\nis implemented using the PyTorch framework, with"
        },
        {
          "Notes: Bold values indicate the best performance. Underlined values denote the second-best performance. Î”Sota represents the relative": "AdamW as the optimizer, a learning rate of 1e-5, and a batch size"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: , even on the more complex multi-label Tofurtherverifytheeffectivenessoftheproposeddecoupled",
      "data": [
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": ""
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "Models",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Avg (%)"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "MCSCAN [54]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "59.2â€¡"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "DIMMN [53]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "59.6"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "SACMA [55]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "60.8"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "MCFN [44]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "63.3"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "SMCN [56]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "63.6"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "SAMS [45]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "64.0"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "RMERCT [57]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "63.5"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "SMIN [58]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "65.0"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "HiMul-LGG [59]",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "65.6"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "Ours(ORKF)",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "66.0"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "Î”Sota",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "â†‘0.60"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition": "Notes: N/A indicates that the metric value was not provided in the original paper. â€¡ denotes that the average value was calculated from",
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , even on the more complex multi-label Tofurtherverifytheeffectivenessoftheproposeddecoupled",
      "data": [
        {
          "known experimental results due to irreproducibility.": ""
        },
        {
          "known experimental results due to irreproducibility.": "Models"
        },
        {
          "known experimental results due to irreproducibility.": "MSCNN-SPU â€  [60]"
        },
        {
          "known experimental results due to irreproducibility.": "M-TLEAF â€  [61]"
        },
        {
          "known experimental results due to irreproducibility.": "MCFN â€  [44]"
        },
        {
          "known experimental results due to irreproducibility.": "SAMS â€  [45]"
        },
        {
          "known experimental results due to irreproducibility.": "CARATâ€  [62]"
        },
        {
          "known experimental results due to irreproducibility.": "Ours(ORKF)"
        },
        {
          "known experimental results due to irreproducibility.": "Î”Sota"
        },
        {
          "known experimental results due to irreproducibility.": "Notes: â€  The results are obtained through our own reproduction experiments."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , even on the more complex multi-label Tofurtherverifytheeffectivenessoftheproposeddecoupled",
      "data": [
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "ACC and Avg, ORKF achieves relative improvements of approxi-"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "mately 2.28% and 2.55%, respectively, compared to the second-best"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "method DBT (ACC of 78.9%, Avg of 78.3%). For the WACC met-"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "ric, ORKF shows a relative improvement of about 2.30% over the"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "second-best method LLMSER (WACC of 78.1%)."
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "To further validate the performance of ORKF, we evaluated the"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "model on the MELD dataset. The experimental results are presented"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "in Table 2."
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "According to the experimental comparison results in Table 2,"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "ORKF demonstrates superior performance even on the highly im-"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "balanced MELD dataset. Specifically, ORKF achieves a WACC of"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "66.7, a WF1 of 65.4, and an average score of 66.0, all of which repre-"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "sent the best results among the compared methods. Although ORKF"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "achieved SOTA performance on both the IEMOCAP and MELD"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "datasets, it should be noted that these datasets are English datasets"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "with single-label annotations, where task-relevant information may"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "primarily rely on shared mutual information. To further validate"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "the modelâ€™s performance, we introduced the Chinese multi-label"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "emotion recognition dataset M3ED for testing."
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "As shown in Table 3, even on the more complex multi-label"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "Chinese emotion recognition dataset, ORKF demonstrates strong"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "performance, achieving an F1 score of 52.0 and an average score of"
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": ""
        },
        {
          "Notes: â€  The results are obtained through our own reproduction experiments.": "51.7, reaching SOTA results."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": "ğ¸ğ·"
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": "âœ“"
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": "âœ“"
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": "âœ“"
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        },
        {
          "Table 4: Results for Strategy Analysis": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "Figure 4: Comparison of Emotion Category Distributions: PCMI vs. CMIE and Baseline.",
          "gories after Using PCMI Method": ""
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "decoupled representation learning strategy based on PCMI effec-",
          "gories after Using PCMI Method": "to emotional discrepancies across modalities, which enhances the"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "tively facilitates the learning of well-structured and discriminative",
          "gories after Using PCMI Method": "modelâ€™s robustness in scenarios where emotional cues from dif-"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "representations.",
          "gories after Using PCMI Method": "ferent modalities are not aligned. Extensive experiments on three"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "Finally, the comparison between the fifth and sixth rows shows",
          "gories after Using PCMI Method": "widely used benchmark datasets for multimodal emotion recogni-"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "that the proposed self-attention-based fusion encoder outperforms",
          "gories after Using PCMI Method": "tion demonstrate that DRKF outperforms several state-of-the-art"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "the traditional cross-attention fusion encoder, offering more sub-",
          "gories after Using PCMI Method": "models across multiple evaluation metrics, exhibiting strong per-"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "stantial gains in emotion recognition accuracy.",
          "gories after Using PCMI Method": "formance and generalization capabilities."
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "",
          "gories after Using PCMI Method": "Despite the promising results achieved by the proposed DRKF"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "",
          "gories after Using PCMI Method": "model on bimodal emotion recognition tasks, certain limitations"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "5\nConclusion",
          "gories after Using PCMI Method": ""
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "",
          "gories after Using PCMI Method": "remain. The current evaluation is limited to the audio-text bimodal"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "To address the prevalent challenges of modality heterogeneity and",
          "gories after Using PCMI Method": "setting, and has not yet been extended to trimodal or higher-order"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "emotional inconsistency across modalities in MER tasks, we pro-",
          "gories after Using PCMI Method": "multimodal fusion scenarios. In future work, we plan to further"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "pose a novel framework named Decoupled Representations with",
          "gories after Using PCMI Method": "explore the adaptability and scalability of DRKF in more complex"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "Knowledge Fusion (DRKF). The framework consists of two core",
          "gories after Using PCMI Method": "multimodal input settings, such as those involving video, speech,"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "modules: the Optimized Representation Learning (ORL) module",
          "gories after Using PCMI Method": "and text, to better address the demands of real-world multimodal"
        },
        {
          "gories before Using PCMI Method\ngories after Using CMIE": "and the Knowledge Fusion (KF) module. Specifically, the ORL mod-",
          "gories after Using PCMI Method": "emotion recognition applications."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "References"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[1] Andreas Triantafyllopoulos, BjÃ¶rn W. Schuller, GÃ¶kÃ§e Ä°ymen, Metin Sezgin, Xi-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "angheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu, Silvan Mertes, Elisabeth"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "AndrÃ©, Ruibo Fu, and Jianhua Tao. 2023. An Overview of Affective Speech Synthe-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "sis and Conversion in the Deep Learning Era. Proc. IEEE 111, 10 (2023), 1355â€“1381."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "doi:10.1109/JPROC.2023.3250266"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[2]\nJingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin. 2021. MMGCN: Multi-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "modal Fusion via Deep Graph Convolution Network for Emotion Recognition"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "in Conversation. In Proceedings of the 59th Annual Meeting of the Association for"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Computational Linguistics and the 11th International Joint Conference on Natural"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "5666â€“5675. doi:10.18653/v1/2021.acl-long.440"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[3] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Morency. 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Processing, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "for Computational Linguistics, Copenhagen, Denmark, 1103â€“1114. doi:10.18653/"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "v1/D17-1115"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[4] Alec Radford,\nJong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "From Natural Language Supervision.\nIn Proceedings of\nthe 38th International"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Conference on Machine Learning (Proceedings of Machine Learning Research,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, Virtual Event, 8748â€“8763."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "https://proceedings.mlr.press/v139/radford21a.html"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[5]\nSimon Jenni, Alexander Black, and John Collomosse. 2023. Audio-Visual Con-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "trastive Learning with Temporal Self-Supervision. In Proceedings of the Thirty-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Seventh AAAI Conference on Artificial\nIntelligence and Thirty-Fifth Conference"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Educational Advances in Artificial Intelligence (AAAIâ€™23/IAAIâ€™23/EAAIâ€™23). AAAI"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Press, Washington, DC, USA, Article 898, 9 pages. doi:10.1609/aaai.v37i7.25967"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[6] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Yin Cui, and Boqing Gong. 2021.\nVATT: Transformers for Multimodal Self-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Supervised Learning from Raw Video, Audio and Text. In Advances in Neural"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., Red Hook,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "NY, USA, 24206â€“24221.\nhttps://proceedings.neurips.cc/paper_files/paper/2021/"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "file/cb3213ada48302953cb0f166464ab356-Paper.pdf"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[7] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. 2021. Contrastive"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "learning, multi-view redundancy, and linear models. In Proceedings of the 32nd"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "International Conference on Algorithmic Learning Theory (Proceedings of Machine"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Learning Research, Vol. 132), Vitaly Feldman, Katrina Ligett, and Sivan Sabato"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "(Eds.). PMLR, Virtual Event, 1179â€“1206.\nhttps://proceedings.mlr.press/v132/"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "tosh21a.html"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[8] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Morency. 2021.\nSelf-supervised Learning from a Multi-view Perspective.\nIn"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "International Conference on Learning Representations (ICLR). OpenReview, Virtual"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Event, 10 pages.\nhttps://openreview.net/forum?id=-bdp_8Itjwp"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[9] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "2022. Disentangled Representation Learning for Multimodal Emotion Recogni-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "tion. In Proceedings of the 30th ACM International Conference on Multimedia (MM"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "â€™22)\n(Lisboa, Portugal) (MM â€™22). Association for Computing Machinery, New"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "York, NY, USA, 1642â€“1651. doi:10.1145/3503161.3547754"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[10]\nYi Zhang, Mingyuan Chen, Jundong Shen, and Chongjun Wang. 2022. Tailor Ver-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "satile Multi-Modal Learning for Multi-Label Emotion Recognition. In Proceedings"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "of the AAAI Conference on Artificial Intelligence, Vol. 36. AAAI Press, Vancouver,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "British Columbia, Canada, 9100â€“9108."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[11] Haoqin Sun, Shiwan Zhao, Xuechen Wang, Wenjia Zeng, Yong Chen, and Yong"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Qin. 2024. Fine-Grained Disentangled Representation Learning For Multimodal"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Emotion Recognition. In ICASSP 2024 - 2024 IEEE International Conference on"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Acoustics, Speech and Signal Processing (ICASSP). IEEE, Seoul, South Korea, 11051â€“"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "11055. doi:10.1109/ICASSP48485.2024.10447667"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "[12] Xulong Du, Xingnan Zhang, Dandan Wang, Yingying Xu, Zhiyuan Wu, Shiqing"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Zhang, Xiaoming Zhao, Jun Yu, and Liangliang Lou. 2024.\nIntegrating Repre-"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "sentation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Multimodal Emotion Recognition. In Proceedings of the 2024 Joint International"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Conference on Computational Linguistics, Language Resources and Evaluation"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "(LREC-COLING 2024). European Language Resources Association (ELRA), Torino,"
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": "Italy, 9120â€“9130."
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        },
        {
          "Foundation of Sichuan, China (Award No. 2024NSFSC0496).": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[13] Zhixiang Shen, Shuo Wang, and Zhao Kang. 2024.\nBeyond Redundancy:\nAcknowledgments"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Information-aware Unsupervised Multiplex Graph Structure Learning. In Ad-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "This work was supported by the National Natural Science Foun-\nvances in Neural\nInformation Processing Systems. Curran Associates, Inc., Red"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Hook, NY, USA, 10 pages.\ndation of China (Award No. U22B2061) and the Natural Science"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[14]\nPaul Pu Liang, Zihao Deng, Martin Q. Ma,\nJames Y. Zou, Louis-Philippe"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Foundation of Sichuan, China (Award No. 2024NSFSC0496)."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Morency,\nand Ruslan Salakhutdinov. 2023.\nFactorized Contrastive Learn-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ing: Going Beyond Multi-view Redundancy.\nIn Advances\nin Neural\nInfor-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "mation Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko,\nReferences"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., Red Hook, NY,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[1] Andreas Triantafyllopoulos, BjÃ¶rn W. Schuller, GÃ¶kÃ§e Ä°ymen, Metin Sezgin, Xi-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "USA, 32971â€“32998.\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "angheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu, Silvan Mertes, Elisabeth"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "6818dcc65fdf3cbd4b05770fb957803e-Paper-Conference.pdf"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "AndrÃ©, Ruibo Fu, and Jianhua Tao. 2023. An Overview of Affective Speech Synthe-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[15] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Ar-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "sis and Conversion in the Deep Learning Era. Proc. IEEE 111, 10 (2023), 1355â€“1381."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "mand Joulin. 2020. Unsupervised Learning of Visual Features by Contrasting Clus-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "doi:10.1109/JPROC.2023.3250266"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ter Assignments. In Advances in Neural Information Processing Systems, Vol. 33."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[2]\nJingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin. 2021. MMGCN: Multi-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Curran Associates, Inc., Red Hook, NY, USA, 9912â€“9924.\nhttps://proceedings."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "modal Fusion via Deep Graph Convolution Network for Emotion Recognition"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "neurips.cc/paper/2020/file/f1748d6b0fd9c3c0b67a5f43a715af2b-Paper.pdf"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "in Conversation. In Proceedings of the 59th Annual Meeting of the Association for"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[16]\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. 2019. Learning Rep-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Computational Linguistics and the 11th International Joint Conference on Natural"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "resentations by Maximizing Mutual Information Across Views. In Advances in"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Language Processing (Volume 1: Long Papers), Chengqing Zong, Fei Xia, Wenjie"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "F. dâ€™ AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc.,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "5666â€“5675. doi:10.18653/v1/2021.acl-long.440"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Red Hook, NY, USA, 15509â€“15520.\nhttps://proceedings.neurips.cc/paper_files/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[3] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Morency. 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[17] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Morency. 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Processing, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "for Computational Linguistics, Copenhagen, Denmark, 1103â€“1114. doi:10.18653/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Processing, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "v1/D17-1115"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "for Computational Linguistics, Copenhagen, Denmark, 1103â€“1114. doi:10.18653/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[4] Alec Radford,\nJong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "v1/D17-1115"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[18]\nJoosung Lee and Wooin Lee. 2022. CoMPM: Context Modeling with Speakerâ€™s"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Pre-trained Memory Tracking for Emotion Recognition in Conversation.\nIn"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "From Natural Language Supervision.\nIn Proceedings of\nthe 38th International"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Proceedings of the 2022 Conference of the North American Chapter of the Association"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Conference on Machine Learning (Proceedings of Machine Learning Research,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "for Computational Linguistics: Human Language Technologies, Marine Carpuat,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, Virtual Event, 8748â€“8763."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "https://proceedings.mlr.press/v139/radford21a.html"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "for Computational Linguistics, Seattle, United States, 5669â€“5679. doi:10.18653/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[5]\nSimon Jenni, Alexander Black, and John Collomosse. 2023. Audio-Visual Con-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "v1/2022.naacl-main.416"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "trastive Learning with Temporal Self-Supervision. In Proceedings of the Thirty-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[19] Yuntao Shou, Huan Liu, Xiangyong Cao, Deyu Meng, and Bo Dong. 2025. A"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Seventh AAAI Conference on Artificial\nIntelligence and Thirty-Fifth Conference"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Low-Rank Matching Attention Based Cross-Modal Feature Fusion Method for"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Conversational Emotion Recognition.\nIEEE Transactions on Affective Computing"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Educational Advances in Artificial Intelligence (AAAIâ€™23/IAAIâ€™23/EAAIâ€™23). AAAI"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "16, 2 (2025), 1177â€“1189. doi:10.1109/TAFFC.2024.3498443"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Press, Washington, DC, USA, Article 898, 9 pages. doi:10.1609/aaai.v37i7.25967"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[20] Tao Shi and Shao-Lun Huang. 2023. MultiEMO: An Attention-Based Correlation-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[6] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Aware Multimodal Fusion Framework for Emotion Recognition in Conversations."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Yin Cui, and Boqing Gong. 2021.\nVATT: Transformers for Multimodal Self-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "In Proceedings of the 61st Annual Meeting of the Association for Computational"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Supervised Learning from Raw Video, Audio and Text. In Advances in Neural"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Okazaki\n(Eds.). Association for Computational Linguistics, Toronto, Canada,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., Red Hook,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "14752â€“14766. doi:10.18653/v1/2023.acl-long.824"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "NY, USA, 24206â€“24221.\nhttps://proceedings.neurips.cc/paper_files/paper/2021/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[21] Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "file/cb3213ada48302953cb0f166464ab356-Paper.pdf"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Yi Xu, Belinda Zeng, and Trishul Chilimbi. 2023. Understanding and Constructing"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[7] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. 2021. Contrastive"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Latent Modality Structures in Multi-Modal Representation Learning. In Proceed-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "learning, multi-view redundancy, and linear models. In Proceedings of the 32nd"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "International Conference on Algorithmic Learning Theory (Proceedings of Machine"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(CVPR). IEEE, Vancouver, BC, Canada, 7661â€“7671."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Learning Research, Vol. 132), Vitaly Feldman, Katrina Ligett, and Sivan Sabato"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Representation\n[22] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(Eds.). PMLR, Virtual Event, 1179â€“1206.\nhttps://proceedings.mlr.press/v132/"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "learning: A review and new perspectives.\nIEEE transactions on pattern analysis"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tosh21a.html"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "and machine intelligence 35, 8 (2013), 1798â€“1828."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[8] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Florian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller. 2010. Opensmile: The Munich\n[23]"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Morency. 2021.\nSelf-supervised Learning from a Multi-view Perspective.\nIn"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Versatile and Fast Open-Source Audio Feature Extractor. In Proceedings of the"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "International Conference on Learning Representations (ICLR). OpenReview, Virtual"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "18th ACM International Conference on Multimedia (MM â€™10) (Firenze, Italy) (MM"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Event, 10 pages.\nhttps://openreview.net/forum?id=-bdp_8Itjwp"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "â€™10). Association for Computing Machinery, New York, NY, USA, 1459â€“1462."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[9] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "doi:10.1145/1873951.1874246"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2022. Disentangled Representation Learning for Multimodal Emotion Recogni-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[24]\nFlavio Carvalho, Gabriel Santos, and Gustavo Paiva Guedes. 2018. AffectPT-br:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tion. In Proceedings of the 30th ACM International Conference on Multimedia (MM"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "An Affective Lexicon Based on LIWC 2015. In 2018 37th International Conference"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "â€™22)\n(Lisboa, Portugal) (MM â€™22). Association for Computing Machinery, New"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "of the Chilean Computer Science Society (SCCC). IEEE, Puerto Varas, Chile, 1â€“5."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "York, NY, USA, 1642â€“1651. doi:10.1145/3503161.3547754"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "doi:10.1109/SCCC.2018.8705251"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[10]\nYi Zhang, Mingyuan Chen, Jundong Shen, and Chongjun Wang. 2022. Tailor Ver-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[25]\nJamilu Awwalu, Azuraliza Abu Bakar, and Mohd Ridzwan Yaakub. 2019. Hybrid N-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "satile Multi-Modal Learning for Multi-Label Emotion Recognition. In Proceedings"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "gram model using NaÃ¯ve Bayes for classification of political sentiments on Twitter."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "of the AAAI Conference on Artificial Intelligence, Vol. 36. AAAI Press, Vancouver,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Neural Computing and Applications 31 (2019), 9207â€“9220. doi:10.1007/s00521-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "British Columbia, Canada, 9100â€“9108."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "019-04248-z"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[11] Haoqin Sun, Shiwan Zhao, Xuechen Wang, Wenjia Zeng, Yong Chen, and Yong"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[26] Xiangyu Qin, Zhiyu Wu, Tingting Zhang, Yanran Li, Jian Luan, Bin Wang, Li"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Qin. 2024. Fine-Grained Disentangled Representation Learning For Multimodal"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Wang, and Jinshi Cui. 2023. BERT-ERC: Fine-Tuning BERT Is Enough for Emotion"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Emotion Recognition. In ICASSP 2024 - 2024 IEEE International Conference on"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Recognition in Conversation. In Proceedings of the Thirty-Seventh AAAI Confer-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Acoustics, Speech and Signal Processing (ICASSP). IEEE, Seoul, South Korea, 11051â€“"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ence on Artificial Intelligence (AAAI-23), Vol. 37. AAAI Press, Washington, DC,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "11055. doi:10.1109/ICASSP48485.2024.10447667"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "USA, 13492â€“13500. doi:10.1609/aaai.v37i11.26582"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[12] Xulong Du, Xingnan Zhang, Dandan Wang, Yingying Xu, Zhiyuan Wu, Shiqing"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[27]\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Zhang, Xiaoming Zhao, Jun Yu, and Liangliang Lou. 2024.\nIntegrating Repre-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa:"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "sentation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Multimodal Emotion Recognition. In Proceedings of the 2024 Joint International"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "https://arxiv.org/abs/1907.11692"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Conference on Computational Linguistics, Language Resources and Evaluation"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[28] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(LREC-COLING 2024). European Language Resources Association (ELRA), Torino,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Represen-"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Italy, 9120â€“9130."
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tations.\nIn Advances in Neural\nInformation Processing Systems, H. Larochelle,"
        },
        {
          "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland": "M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Inc., Red Hook, NY, USA, 12449â€“12460.\nhttps://proceedings.neurips.cc/paper_",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Circuits and Systems for Video Technology 33, 9 (2023), 5318â€“5329. doi:10.1109/"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "TCSVT.2023.3247822"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[29]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[46]\nJennifer Santoso, Kenkichi Ishizuka, and Taiichi Hashimoto. 2024. Large Lan-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "guage Model-Based Emotional Speech Annotation Using Context and Acoustic"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Feature for Speech Emotion Recognition. In ICASSP 2024 - 2024 IEEE International"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Furu Wei. 2022. WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Seoul, South"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Speech Processing.\nIEEE Journal of Selected Topics in Signal Processing 16, 6 (2022),",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Korea, 11026â€“11030. doi:10.1109/ICASSP48485.2024.10448316"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "1505â€“1518. doi:10.1109/JSTSP.2022.3188113",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[47] Weidong Chen, Xiaofeng Xing, Xiangmin Xu, Jichen Yang, and Jianxin Pang. 2022."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[30] Ronghao Lin and Haifeng Hu. 2024. Adapt and explore: Multimodal mixup for",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Key-Sparse Transformer for Multimodal Speech Emotion Recognition. In ICASSP"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "representation learning.\ndoi:10.1016/j.\nInformation Fusion 105 (2024), 102216.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "inffus.2023.102216",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "(ICASSP). IEEE, Singapore, 6897â€“6901. doi:10.1109/ICASSP43922.2022.9746598"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[31] Andrey Guzhov, Federico Raue, JÃ¶rn Hees, and Andreas Dengel. 2022. AudioCLIP:",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[48] Zihan Zhao, Yu Wang, and Yanfeng Wang. 2023. Knowledge-aware Bayesian"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Extending CLIP to Image, Text and Audio. In ICASSP 2022 - 2022 IEEE International",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Co-attention for Multimodal Emotion Recognition. In ICASSP 2023 - 2023 IEEE"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Singapore,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "976â€“980. doi:10.1109/ICASSP43922.2022.9747631",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Rhodes Island, Greece, 1â€“5."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[32] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[49] Yufan Yi, Yan Tian, Cong He, Yajing Fan, Xinli Hu, and Yiping Xu. 2023. DBT:"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Feature Learning via Non-Parametric Instance Discrimination. In Proceedings of",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "multimodal emotion recognition based on dual-branch transformer. The Journal"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "of Supercomputing 79, 8 (2023), 8611â€“8633. doi:10.1007/s11227-022-05001-5"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Salt Lake City, UT, USA, 3733â€“3742.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[50] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[33] Yu-An Chung, Wei-Hung Weng, Schrasing Tong, and James Glass. 2018. Un-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "supervised Cross-Modal Alignment of Speech and Text Embedding Spaces. In",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "IEMOCAP:\nInteractive emotional dyadic motion capture database.\nLanguage"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Advances in Neural Information Processing Systems, Vol. 31. Curran Associates,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "resources and evaluation 42 (2008), 335â€“359. doi:10.1007/s10579-008-9076-6"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Inc., Red Hook, NY, USA, 7354â€“7365.\nhttps://proceedings.neurips.cc/paper_files/",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[51]\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "paper/2018/file/f3ce96dfe0061d0e6e105b0b70e5aafb-Paper.pdf",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Cambria, and Rada Mihalcea. 2019. MELD: A Multimodal Multi-Party Dataset for"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[34] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Emotion Recognition in Conversations. In Proceedings of the 57th Annual Meeting"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Yoshua Bengio, Aaron Courville, and Devon Hjelm. 2018. Mutual\nInforma-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "of the Association for Computational Linguistics, Anna Korhonen, David Traum,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "tion Neural Estimation. In Proceedings of the 35th International Conference on",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "and LluÃ­s MÃ rquez (Eds.). Association for Computational Linguistics, Florence,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Machine Learning (Proceedings of Machine Learning Research, Vol. 80), Jennifer",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Italy, 527â€“536. doi:10.18653/v1/P19-1050"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Dy and Andreas Krause (Eds.). PMLR, Stockholm, Sweden, 531â€“540.\nhttps:",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin Jin, Xinchao Wang,\n[52]"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "//proceedings.mlr.press/v80/belghazi18a.html",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "and Haizhou Li. 2022. M3ED: Multi-modal Multi-scene Multi-label Emotional"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[35]\nPengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Dialogue Database. In Proceedings of the 60th Annual Meeting of the Association for"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Carin. 2020. CLUB: A Contrastive Log-Ratio Upper Bound of Mutual Information.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "In Proceedings of the 37th International Conference on Machine Learning (ICMLâ€™20).",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "JMLR.org, Virtual Event, Article 166, 10 pages.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Dublin, Ireland, 5699â€“5710. doi:10.18653/v1/2022.acl-long.391"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[36] Tadas BaltruÅ¡aitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multi-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[53]\nJintao Wen, Dazhi Jiang, Geng Tu, Cheng Liu, and Erik Cambria. 2023. Dynamic"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "modal Machine Learning: A Survey and Taxonomy.\nIEEE Transactions on Pattern",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "interactive multiview memory network for emotion recognition in conversation."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Analysis and Machine Intelligence 41, 2 (2019), 423â€“443. doi:10.1109/TPAMI.2018.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Information Fusion 91 (2023), 123â€“133. doi:10.1016/j.inffus.2022.10.009"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2798607",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Licai Sun, Bin Liu, Jianhua Tao, and Zheng Lian. 2021. Multimodal Cross-and\n[54]"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[37] Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, and Roger",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Self-Attention Network for Speech Emotion Recognition. In ICASSP 2021 - 2021"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Zimmermann. 2018.\nICON: Interactive Conversational Memory Network for",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Multimodal Emotion Detection. In Proceedings of the 2018 Conference on Empirical",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "IEEE, Toronto, ON, Canada, 4275â€“4279. doi:10.1109/ICASSP39728.2021.9413816"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hock-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[55]\nLili Guo, Yikang Song, and Shifei Ding. 2024. Speaker-aware cognitive network"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "enmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computational Linguistics,",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "with cross-modal attention for multimodal emotion recognition in conversation."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Brussels, Belgium, 2594â€“2604. doi:10.18653/v1/D18-1280",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Knowledge-Based Systems 296 (2024), 111969. doi:10.1016/j.knosys.2024.111969"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[38]\nEfthymios Georgiou, Charilaos Papaioannou, and Alexandros Potamianos. 2019.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[56] Mixiao Hou, Zheng Zhang, and Guangming Lu. 2022. Multi-Modal Emotion"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Deep Hierarchical Fusion with Application in Sentiment Analysis. In Proceedings",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Recognition with Self-Guided Modality Calibration. In ICASSP 2022 - 2022 IEEE"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "https://api.semanticscholar.\nof Interspeech 2019. ISCA, Graz, Austria, 3302â€“3306.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "org/CorpusID:202736442",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Singapore, 4688â€“4692. doi:10.1109/ICASSP43922.2022.9747859"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[39] Qiuju Zhang, Hongtao Zhang, Keming Zhou, and Le Zhang. 2023. Developing a",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[57] Baijun Xie, Mariia Sidulova, and Chung Hyuk Park. 2021. Robust Multimodal"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Physiological Signal-Based, Mean Threshold and Decision-Level Fusion Algo-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Emotion Recognition from Conversation with Transformer-Based Crossmodality"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "rithm (PMD) for Emotion Recognition. Tsinghua Science and Technology 28, 4",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Fusion. Sensors 21, 14, Article 4913 (2021), 16 pages. doi:10.3390/s21144913"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(2023), 673â€“685. doi:10.26599/TST.2022.9010038",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[58] Zheng Lian, Bin Liu, and Jianhua Tao. 2023. SMIN: Semi-Supervised Multi-Modal"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[40]\nSeunghyun Yoon, Seokhyun Byun, Subhadeep Dey, and Kyomin Jung. 2019.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Interaction Network for Conversational Emotion Recognition.\nIEEE Transactions"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Speech Emotion Recognition Using Multi-hop Attention Mechanism. In ICASSP",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "on Affective Computing 14, 3 (2023), 2415â€“2429. doi:10.1109/TAFFC.2022.3141237"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[59] Changzeng Fu, Fengkui Qian, Kaifeng Su, Yikai Su, Ze Wang, Jiaqi Shi, Zhigang"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "(ICASSP). IEEE, Brighton, United Kingdom, 2822â€“2826. doi:10.1109/ICASSP.2019.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Liu, Chaoran Liu, and Carlos Toshinori Ishi. 2025. HiMul-LGG: A hierarchical"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "8683483",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "decision fusion-based localâ€“global graph neural network for multimodal emotion"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[41]\nPengfei Liu, Kun Li, and Helen Meng. 2020. Group Gated Fusion on Attention-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "recognition in conversation. Neural Networks 181 (2025), 106764. doi:10.1016/j."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Based Bidirectional Alignment\nfor Multimodal Emotion Recognition.\nIn Pro-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "neunet.2024.106764"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "ISCA, Shanghai, China, 379â€“383.\ndoi:10.21437/\nceedings of\nInterspeech 2020.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[60] Zixuan Peng, Yu Lu, Shengfeng Pan, and Yunfeng Liu. 2021. Efficient Speech"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Interspeech.2020-2067",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Emotion Recognition Using Multi-Scale CNN and Attention. In ICASSP 2021 - 2021"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[42]\nLin Feng, Lu-Yao Liu, Sheng-Lan Liu, Jian Zhou, Han-Qing Yang, and Jie Yang.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2023. Multimodal speech emotion recognition based on multi-scale MFCCs and",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "IEEE, Toronto, ON, Canada, 3020â€“3024. doi:10.1109/ICASSP39728.2021.9414286"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "multi-view attention mechanism. Multimedia Tools Appl. 82, 19 (March 2023),",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[61]\nSoumya Dutta and Sriram Ganapathy. 2022. Multimodal Transformer with"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "28917â€“28935. doi:10.1007/s11042-023-14600-0",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Learnable Frontend and Self Attention for Emotion Recognition. In ICASSP 2022"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[43] Mustaqeem Khan, Wail Gueaieb, Abdulmotaleb El Saddik, and Soonil Kwon. 2024.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "MSER: Multimodal speech emotion recognition using cross-attention with deep",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "(ICASSP). IEEE, Singapore, 6917â€“6921. doi:10.1109/ICASSP43922.2022.9747723"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "fusion. Expert Systems with Applications 245 (2024), 122946. doi:10.1016/j.eswa.",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "[62] Cheng Peng, Ke Chen, Lidan Shou, and Gang Chen. 2024. CARAT: Contrastive"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "2023.122946",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "and Yang Li.\n2023.\nA Dual Attention-based Modality-\n[44] Xiaoheng Zhang",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Recognition. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Collaborative Fusion Network for Emotion Recognition. In Proceedings of Inter-",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "speech 2023. ISCA, Dublin, Ireland, 1468â€“1472. doi:10.21437/Interspeech.2023-523",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intel-"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "[45] Mixiao Hou, Zheng Zhang, Chang Liu, and Guangming Lu. 2023.\nSemantic",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "ligence (AAAIâ€™24/IAAIâ€™24/EAAIâ€™24). AAAI Press, Vancouver, BC, Canada, Article"
        },
        {
          "MM â€™25, October 27â€“31, 2025, Dublin, Ireland": "Alignment Network for Multi-Modal Emotion Recognition.\nIEEE Transactions on",
          "Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, & Daibing Yao.": "1626, 9 pages. doi:10.1609/aaai.v38i13.29374"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
      "authors": [
        "Andreas Triantafyllopoulos",
        "BjÃ¶rn Schuller",
        "GÃ¶kÃ§e Ä°ymen",
        "Metin Sezgin",
        "Xiangheng He",
        "Zijiang Yang",
        "Panagiotis Tzirakis",
        "Shuo Liu",
        "Silvan Mertes",
        "Elisabeth AndrÃ©",
        "Ruibo Fu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proc. IEEE",
      "doi": "10.1109/JPROC.2023.3250266"
    },
    {
      "citation_id": "2",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "3",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "4",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "5",
      "title": "Audio-Visual Contrastive Learning with Temporal Self-Supervision",
      "authors": [
        "Simon Jenni",
        "Alexander Black",
        "John Collomosse"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence (AAAI'23/IAAI'23/EAAI'23)",
      "doi": "10.1609/aaai.v37i7.25967"
    },
    {
      "citation_id": "6",
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "authors": [
        "Hassan Akbari",
        "Liangzhe Yuan",
        "Rui Qian",
        "Wei-Hong Chuang",
        "Shih-Fu Chang",
        "Yin Cui",
        "Boqing Gong"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Contrastive learning, multi-view redundancy, and linear models",
      "authors": [
        "Christopher Tosh",
        "Akshay Krishnamurthy",
        "Daniel Hsu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 32nd International Conference on Algorithmic Learning Theory (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "Self-supervised Learning from a Multi-view Perspective",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Yue Wu",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations (ICLR). OpenReview, Virtual Event, 10 pages"
    },
    {
      "citation_id": "9",
      "title": "Disentangled Representation Learning for Multimodal Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (MM '22",
      "doi": "10.1145/3503161.3547754"
    },
    {
      "citation_id": "10",
      "title": "Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition",
      "authors": [
        "Yi Zhang",
        "Mingyuan Chen",
        "Jundong Shen",
        "Chongjun Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Fine-Grained Disentangled Representation Learning For Multimodal Emotion Recognition",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Xuechen Wang",
        "Wenjia Zeng",
        "Yong Chen",
        "Yong Qin"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP48485.2024.10447667"
    },
    {
      "citation_id": "12",
      "title": "Integrating Representation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based Multimodal Emotion Recognition",
      "authors": [
        "Xulong Du",
        "Xingnan Zhang",
        "Dandan Wang",
        "Yingying Xu",
        "Zhiyuan Wu",
        "Shiqing Zhang",
        "Xiaoming Zhao",
        "Jun Yu",
        "Liangliang Lou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "13",
      "title": "Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning",
      "authors": [
        "Zhixiang Shen",
        "Shuo Wang",
        "Zhao Kang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy",
      "authors": [
        "Paul Pu Liang",
        "Zihao Deng",
        "Martin Ma",
        "James Zou",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
      "authors": [
        "Mathilde Caron",
        "Ishan Misra",
        "Julien Mairal",
        "Priya Goyal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Learning Representations by Maximizing Mutual Information Across Views",
      "authors": [
        "Philip Bachman",
        "Devon Hjelm",
        "William Buchwalter"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "18",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.416"
    },
    {
      "citation_id": "19",
      "title": "A Low-Rank Matching Attention Based Cross-Modal Feature Fusion Method for Conversational Emotion Recognition",
      "authors": [
        "Yuntao Shou",
        "Huan Liu",
        "Xiangyong Cao",
        "Deyu Meng",
        "Bo Dong"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3498443"
    },
    {
      "citation_id": "20",
      "title": "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.824"
    },
    {
      "citation_id": "21",
      "title": "Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning",
      "authors": [
        "Qian Jiang",
        "Changyou Chen",
        "Han Zhao",
        "Liqun Chen",
        "Qing Ping",
        "Son Dinh Tran",
        "Yi Xu",
        "Belinda Zeng",
        "Trishul Chilimbi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "Opensmile: The Munich Versatile and Fast Open-Source Audio Feature Extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia (MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "24",
      "title": "AffectPT-br: An Affective Lexicon Based on LIWC 2015",
      "authors": [
        "Flavio Carvalho",
        "Gabriel Santos",
        "Gustavo Guedes"
      ],
      "year": "2018",
      "venue": "2018 37th International Conference of the Chilean Computer Science Society (SCCC)",
      "doi": "10.1109/SCCC.2018.8705251"
    },
    {
      "citation_id": "25",
      "title": "Hybrid Ngram model using NaÃ¯ve Bayes for classification of political sentiments on Twitter",
      "authors": [
        "Jamilu Awwalu",
        "Azuraliza Abu Bakar",
        "Mohd Ridzwan"
      ],
      "year": "2019",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-019-04248-z"
    },
    {
      "citation_id": "26",
      "title": "BERT-ERC: Fine-Tuning BERT Is Enough for Emotion Recognition in Conversation",
      "authors": [
        "Zhiyu Xiangyu Qin",
        "Tingting Wu",
        "Yanran Zhang",
        "Jian Li",
        "Bin Luan",
        "Li Wang",
        "Jinshi Wang",
        "Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)",
      "doi": "10.1609/aaai.v37i11.26582"
    },
    {
      "citation_id": "27",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692[cs.CL"
    },
    {
      "citation_id": "28",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3188113"
    },
    {
      "citation_id": "30",
      "title": "Adapt and explore: Multimodal mixup for representation learning",
      "authors": [
        "Ronghao Lin",
        "Haifeng Hu"
      ],
      "year": "2024",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.102216"
    },
    {
      "citation_id": "31",
      "title": "AudioCLIP: Extending CLIP to Image, Text and Audio",
      "authors": [
        "Andrey Guzhov",
        "Federico Raue",
        "JÃ¶rn Hees",
        "Andreas Dengel"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9747631"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised Feature Learning via Non-Parametric Instance Discrimination",
      "authors": [
        "Zhirong Wu",
        "Yuanjun Xiong",
        "Stella Yu",
        "Dahua Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces",
      "authors": [
        "Yu-An Chung",
        "Wei-Hung Weng",
        "Schrasing Tong",
        "James Glass"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Mutual Information Neural Estimation",
      "authors": [
        "Mohamed Ishmael Belghazi",
        "Aristide Baratin",
        "Sai Rajeshwar",
        "Sherjil Ozair",
        "Yoshua Bengio",
        "Aaron Courville",
        "Devon Hjelm"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "CLUB: A Contrastive Log-Ratio Upper Bound of Mutual Information",
      "authors": [
        "Pengyu Cheng",
        "Weituo Hao",
        "Shuyang Dai",
        "Jiachang Liu",
        "Zhe Gan",
        "Lawrence Carin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning (ICML'20)"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2018.2798607"
    },
    {
      "citation_id": "37",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "38",
      "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
      "authors": [
        "Efthymios Georgiou",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech 2019"
    },
    {
      "citation_id": "39",
      "title": "Developing a Physiological Signal-Based, Mean Threshold and Decision-Level Fusion Algorithm (PMD) for Emotion Recognition",
      "authors": [
        "Qiuju Zhang",
        "Hongtao Zhang",
        "Keming Zhou",
        "Le Zhang"
      ],
      "year": "2023",
      "venue": "Tsinghua Science and Technology",
      "doi": "10.26599/TST.2022.9010038"
    },
    {
      "citation_id": "40",
      "title": "Speech Emotion Recognition Using Multi-hop Attention Mechanism",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Subhadeep Dey",
        "Kyomin Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2019.8683483"
    },
    {
      "citation_id": "41",
      "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
      "authors": [
        "Pengfei Liu",
        "Kun Li",
        "Helen Meng"
      ],
      "year": "2020",
      "venue": "Proceedings of Interspeech 2020",
      "doi": "10.21437/Interspeech.2020-2067"
    },
    {
      "citation_id": "42",
      "title": "Multimodal speech emotion recognition based on multi-scale MFCCs and multi-view attention mechanism",
      "authors": [
        "Lin Feng",
        "Lu-Yao Liu",
        "Sheng-Lan Liu",
        "Jian Zhou",
        "Han-Qing Yang",
        "Jie Yang"
      ],
      "year": "2023",
      "venue": "Multimedia Tools Appl",
      "doi": "10.1007/s11042-023-14600-0"
    },
    {
      "citation_id": "43",
      "title": "MSER: Multimodal speech emotion recognition using cross-attention with deep fusion",
      "authors": [
        "Mustaqeem Khan",
        "Wail Gueaieb"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2023.122946"
    },
    {
      "citation_id": "44",
      "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "year": "2023",
      "venue": "Proceedings of Interspeech 2023",
      "doi": "10.21437/Interspeech.2023-523"
    },
    {
      "citation_id": "45",
      "title": "Semantic Alignment Network for Multi-Modal Emotion Recognition",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Chang Liu",
        "Guangming Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2023.3247822"
    },
    {
      "citation_id": "46",
      "title": "Large Language Model-Based Emotional Speech Annotation Using Context and Acoustic Feature for Speech Emotion Recognition",
      "authors": [
        "Jennifer Santoso",
        "Kenkichi Ishizuka",
        "Taiichi Hashimoto"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP48485.2024.10448316"
    },
    {
      "citation_id": "47",
      "title": "Key-Sparse Transformer for Multimodal Speech Emotion Recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofeng Xing",
        "Xiangmin Xu",
        "Jichen Yang",
        "Jianxin Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9746598"
    },
    {
      "citation_id": "48",
      "title": "Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition",
      "authors": [
        "Zihan Zhao",
        "Yu Wang",
        "Yanfeng Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "DBT: multimodal emotion recognition based on dual-branch transformer",
      "authors": [
        "Yufan Yi",
        "Yan Tian",
        "Cong He",
        "Yajing Fan",
        "Xinli Hu",
        "Yiping Xu"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing",
      "doi": "10.1007/s11227-022-05001-5"
    },
    {
      "citation_id": "50",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "51",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "52",
      "title": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.391"
    },
    {
      "citation_id": "53",
      "title": "Dynamic interactive multiview memory network for emotion recognition in conversation",
      "authors": [
        "Jintao Wen",
        "Dazhi Jiang",
        "Geng Tu",
        "Cheng Liu",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2022.10.009"
    },
    {
      "citation_id": "54",
      "title": "Multimodal Cross-and Self-Attention Network for Speech Emotion Recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP39728.2021.9413816"
    },
    {
      "citation_id": "55",
      "title": "Speaker-aware cognitive network with cross-modal attention for multimodal emotion recognition in conversation",
      "authors": [
        "Lili Guo",
        "Yikang Song",
        "Shifei Ding"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2024.111969"
    },
    {
      "citation_id": "56",
      "title": "Multi-Modal Emotion Recognition with Self-Guided Modality Calibration",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9747859"
    },
    {
      "citation_id": "57",
      "title": "Robust Multimodal Emotion Recognition from Conversation with Transformer-Based Crossmodality Fusion",
      "authors": [
        "Baijun Xie",
        "Mariia Sidulova",
        "Chung Hyuk"
      ],
      "year": "2021",
      "venue": "Sensors",
      "doi": "10.3390/s21144913"
    },
    {
      "citation_id": "58",
      "title": "SMIN: Semi-Supervised Multi-Modal Interaction Network for Conversational Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3141237"
    },
    {
      "citation_id": "59",
      "title": "HiMul-LGG: A hierarchical decision fusion-based local-global graph neural network for multimodal emotion recognition in conversation",
      "authors": [
        "Changzeng Fu",
        "Fengkui Qian",
        "Kaifeng Su",
        "Yikai Su",
        "Ze Wang",
        "Jiaqi Shi",
        "Zhigang Liu",
        "Chaoran Liu",
        "Carlos Toshinori"
      ],
      "year": "2025",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2024.106764"
    },
    {
      "citation_id": "60",
      "title": "Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention",
      "authors": [
        "Zixuan Peng",
        "Yu Lu",
        "Shengfeng Pan",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP39728.2021.9414286"
    },
    {
      "citation_id": "61",
      "title": "Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9747723"
    },
    {
      "citation_id": "62",
      "title": "CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition",
      "authors": [
        "Cheng Peng",
        "Ke Chen",
        "Lidan Shou",
        "Gang Chen"
      ],
      "year": "1626",
      "venue": "Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI'24/IAAI'24/EAAI'24)",
      "doi": "10.1609/aaai.v38i13.29374"
    }
  ]
}