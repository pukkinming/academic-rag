{
  "paper_id": "2408.02976v3",
  "title": "Empathy Level Alignment Via Reinforcement Learning For Empathetic Response Generation",
  "published": "2024-08-06T06:16:00Z",
  "authors": [
    "Hui Ma",
    "Bo Zhang",
    "Bo Xu",
    "Jian Wang",
    "Hongfei Lin",
    "Xiao Sun"
  ],
  "keywords": [
    "Empathetic response generation",
    "empathy communication mechanism",
    "reinforcement learning",
    "reward function"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Empathetic response generation, aiming to understand the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Traditional approaches typically employ maximum likelihood estimation as the optimization objective during training, yet fail to align the empathy levels between generated and target responses. To this end, we propose an empathetic response generation framework using reinforcement learning (EmpRL). The framework develops an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. EmpRL utilizes the pre-trained T5 model as the generator and further fine-tunes it to initialize the policy. To align the empathy levels between generated and target responses within a given context, an empathy reward function containing three empathy communication mechanisms-emotional reaction, interpretation, and exploration-is constructed using pre-designed and pre-trained empathy identifiers. During reinforcement learning training, the proximal policy optimization algorithm is used to fine-tune the policy, enabling the generation of empathetic responses. Both automatic and human evaluations demonstrate that the proposed EmpRL framework significantly improves the quality of generated responses, enhances the similarity in empathy levels between generated and target responses, and produces empathetic responses covering both affective and cognitive aspects.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Empathetic Response Generation",
      "text": "In recent years, empathetic response generation has gained significant attention. Unlike emotional response generation, which focuses on producing responses with specific emotions  [18] -  [21] , the task of empathetic response generation aims to respond in an empathetic manner. Existing works are mainly divided into two categories.\n\nThe former involves detecting and utilizing the user's emotion. MoEL  [11]  employs different Transformer decoders to compute response representations for each possible emotion, which are then softly combined to generate the final response. Majumder et al.  [12]  proposed that empathetic responses often mimic the user's emotions to some degree, and introduced MIME to generate both mimicking and nonmimicking representations. EmpDG  [22]  leverages coarsegrained dialogue-level and fine-grained token-level emotions to generate empathetic responses, while introducing interaction discriminators to interact with user feedback. KEMP  [13]  integrates commonsense knowledge and emotional lexical knowledge to understand and express emotions. The latter considers both affective and cognitive empathy. Buliding upon GPT-2  [23] , CoMAE  [14]  hierarchically models three factors of empathy expression: communication mechanism, dialogue act, and emotion. CEM  [15]  leverages external commonsense knowledge to acquire comprehensive information about the user's situation and feelings. CASE  [16]  integrates two heterogeneous graphs involving commonsense and concept knowledge, and introduces a two-level strategy to align cognition and affection.\n\nDue to the remarkable capabilities of large language models (LLMs) in natural language generation tasks, several studies have explored the potential of LLMs for generating empathetic responses. Loh et al.  [24]  proposed a simple instructional prompt to investigate the ability of five LLMs to generate empathetic responses. Qian et al.  [25]  conducted an empirical investigation into the performance of LLMs and proposed three targeted methods for improvement. Chen et al.  [26]  developed a multi-turn empathetic conversation dataset and finetuned ChatGLM to improve its ability to generate empathetic responses.\n\nHowever, existing approaches fail to explicitly align the empathy levels between generated responses and target responses. To ensure that the empathy levels of the generated responses align more closely with the target responses, we develop an effective empathy reward function and use RL training to generate empathetic responses.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Reinforcement Learning For Sequence Generation",
      "text": "RL  [27]  trains the agent using rewards derived from interactions with the environment, primarily to solve sequential decision-making problems. With the advancement of deep RL, sequence generation using RL has received widespread attention. To alleviate the exposure bias problem in generative models, Ranzato et al.  [28]  proposed mixed incremental crossentropy reinforce (MIXER), which leverages RL to optimize evaluation metrics such as BLEU and ROUGE. Rennie et al.  [29]  introduced self-critical sequence training (SCST), utilizing CIDEr metric as the reward function, which outperforms MIXER. Ziegler et al.  [30]  asked human annotators to provide feedback on various answers generated by the language model to train a reward model. Le et al.  [31]  introduced CodeRL framework for program synthesis tasks, leveraging RL to enhance the code generation capabilities of pre-trained language models.\n\nIn the field of dialogue generation, Li et al.  [32]  devised three distinct reward functions to evaluate the informativity, coherence, and ease of answering of generated responses, and employed the policy gradient method to enhance sequence-tosequence models. Zhang et al.  [33]  introduced three coherence models to compute the similarity between the dialogue context and the generated response, and incorporated them as reward functions. Saleh et al.  [34]  utilized hierarchical RL to model utterance-level rewards, improving the flexibility of dialogue models in learning long-term conversational rewards. Shin et al.  [35]  emphasized the importance of sentiment lookahead and designed three distinct sentiment look-ahead reward functions to encourage more empathetic responses. Su et al.  [36]  integrated both cognitive and affective aspects of empathy using RL to enhance the perceptual and emotional expression abilities.\n\nAchieving human-like expressions of empathy is a crucial factor in empathetic response generation. In this work, we use RL to align the empathy levels between generated and target responses within a given context, enhancing the quality of generated responses to encompass both affective and cognitive empathy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology A. Task Definition",
      "text": "The task of empathetic response generation aims to develop a dialogue model that acts as a listener and generates empathetic responses. Formally, let\n\nbelong to the speaker, while even-numbered utterances (u 2 , u 4 , • • • , u N -2 ) correspond to the listener. The goal is to generate the next response y = [y 1 , y 2 , • • • , y T ], which is fluent, coherent with the context, and empathetic to the speaker's situation and feelings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Overview",
      "text": "The architecture of our proposed EmpRL framework is illustrated in Fig.  2 . EmpRL utilizes the pre-trained T5 model as a generator, which is fine-tuned to initialize the policy. Subsequently, an empathy reward function is designed, incorporating three empathy communication mechanisms, namely emotional reaction, interpretation, and exploration. This function aligns the empathy levels between the generated responses and the target responses within a given context. To prevent the policy from deviating excessively from the fine-tuned generator, which could lead to incoherent responses, a KL penalty term is integrated into the empathy reward. Finally, the PPO algorithm is employed to further train the policy, enabling it to generate empathetic responses that encompass both affective and cognitive aspects.\n\nIn the following sections, we first introduce the generator for policy initialization and the empathy identifier for calculating",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Generator",
      "text": "The pre-trained T5 model  [37]  serves as the backbone for generating responses. We further train the model using full fine-tuning, and the fine-tuned generator is described as follows:\n\nwhere c represents the context and ŷ is the generated response.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Empathy Identifier",
      "text": "To establish an effective empathy reward in EmpRL, we propose an empathy identifier that identifies the empathy level of a response within its context. Fig.  3  gives the structure of the empathy identifier. The identifier first utilizes two independently pre-trained T5 encoders  [37]  to encode the context and the response, respectively. Then, the encoded response, serving as the query, and the encoded context, serving as the key and value, are input into a single-head attention mechanism  [38] , followed by a residual connection  [39] , to generate a context-aware representation of the response. Finally, the representation is fed into a max-pooling operation and a linear layer to produce the predicted label, i.e., empathy level.\n\nTo train the empathy identifier, we use a publicly available dataset-the Mental Health Subreddits dataset  [17] . The dataset 1 is derived from threads posted on 55 subreddits that focus on mental health, containing 3k <seek post, response post> pairs. For each pair, three empathy communication mechanisms (i.e., emotional reaction, interpretation, and exploration) are individually labeled as no, weak, or strong, indicating the empathy level. The annotators are crowdworkers trained using the EPITOME framework  [17]  to ensure high-quality annotations. The statistics of the dataset are presented in Table  I .\n\nThree empathy identifiers are trained, each tailored to a specific communication mechanism. We set aside 20% of the dataset for validation, and Fig.  4  shows the validation results. We find that all three identifiers achieve excellent performance, demonstrating the feasibility of the proposed empathy identifier architecture.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Emprl Framework",
      "text": "The proposed EmpRL framework treats empathetic response generation as a sequential decision-making problem and leverages RL to maximize the expected reward for generating responses. Below, we provide a detailed introduction to the various components included in the framework.\n\nState: The state is the dialogue context along with the generated response. The state at time t, denoted as s t = (ŷ <t , c), comprises the dialogue context c and the tokens generated prior to time t.\n\n1 https://github.com/behavioral-data/Empathy-Mental-Health Action: The action is the generated response. The action at time t is denoted as a t = ŷt , which corresponds to the token generated at time t.\n\nPolicy: The policy network π θ generates empathetic responses based on the context, where θ represents the learnable parameters. At time t, the policy is π θ (a t |s t ) = π θ (ŷ t |ŷ <t , c), which predicts the current token using the dialogue context and previously generated tokens. EmpRL initializes the policy π θ using the fine-tuned generator ρ.\n\nReward Function: As shown in Fig.  2 , the reward function consists of two components: empathy reward and KL penalty. The empathy reward aligns the empathy levels between generated and target responses, and KL penalty prevents the policy from deviating far from the generator.\n\n(1) Empathy Reward: The emotional reaction reflects affective empathy, while interpretation and exploration represent cognitive empathy. To ensure that empathy levels of the generated responses align more closely with the target responses, we develop an empathy reward function that integrates the three empathy communication mechanisms:\n\nwhere y, ŷ, and c denote the target response, generated response, and context, respectively; l i and li represent the empathy levels of the target response and generated response within the context, respectively, as determined by the pretrained empathy identifier EmpathyIdentifier i ; and i corresponds to a communication mechanism, which belongs to emotion reaction, interpretation, or exploration. EmpRL treats l i and li as the gold and predicted labels, respectively. The cross-entropy loss between them is computed as the loss for communication mechanism i. By summing the losses across three different communication mechanisms, we obtain the empathy level loss L emp (ŷ, y, c). Finally, we use Eq. (  2 ) to product the empathy reward R emp (ŷ, y, c).\n\n(2) KL Penalty: To prevent the policy from deviating excessively from the fine-tuned generator, which could lead to incoherent responses, a KL-divergence penalty term is incorporated into the empathy reward. The KL penalty at time t is:\n\nThe final reward vector R (ŷ, y, c) ∈ R T is defined as:\n\nwhere r t denotes the reward at time t, T represents the length of the generated response, and β is the coefficient for the KL penalty.\n\nBecause the empathy reward R emp (ŷ, y, c) is only available once the response is fully generated, it does not contribute to the rewards at any time before T .\n\nAdvantage Function: According to generalized advantage estimator  [40] , the advantage function for the state-action pair (s t , a t ) at time t is defined as:\n\nwhere r t denotes the reward at time t, V π (s t ) represents the value function for state s t , and γ, λ ∈ [0, 1] are the discount factor and adjustment factor, respectively. In EmpRL, the policy and value function share a common network architecture. Consequently, an additional trainable token-level value head (V-Head), implemented as a linear layer, is directly added on top of the generator's hidden states to compute the value function.\n\nObjective: The objective of EmpRL is to maximize the expected reward of generated responses:\n\nTo reduce the variability of predictions, the maximization of the expected reward is reformulated as the maximization of the expected advantage:\n\nOptimization Method: We employ the PPO algorithm  [41]  to train the policy π θ , and the total loss function is defined as:\n\nwhere L CLIP θ is the clipped surrogate policy objective function, and L V F θ is the value function squared error term; hyperparameters α and ǫ represent the weights for the linear combination of losses and the clipping range of proximal policy ratio, respectively; and c t π (θ) = π θ (at|st) π θ old (at|st) denotes the ratio of the new policy to the old policy.\n\nMinimizing total loss function involves simultaneously maximizing the clipped surrogate policy objective and minimizing the value error. Maximizing the clipped surrogate policy objective aims to maximize the expected reward, where the surrogate policy objective calculates the ratio between the current and the old policy and then constrains the update step within an appropriate range. This constraint prevents large variations during policy updates, leading to more stable optimization. On the other hand, minimizing the token-level value estimation, which is based on the difference between values of the new policy and the estimated dense returns of the old policy, further enhances the stability.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label: Excited",
      "text": "Situation: When I was little and I found out that we were going to the amusement park.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conversation:",
      "text": "Speaker: When I was a little child my parents told me that we were going to the amusement park and it made me so happy.\n\nListener: I bet! That sounds like fun. Speaker: It was fun for sure, all the rides and the food, it brings back great memories.\n\nListener: What sweet memories to hold on to.\n\nFig.  5 : An example from the EmpatheticDialogues dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Settings",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset",
      "text": "We conduct experiments on the EmpatheticDialogues dataset  [10] . EmpatheticDialogues 2  is a large-scale multi-turn dialogue dataset containing 24, 850 open-domain empathetic conversations between speakers and listeners. An example is illustrated in Fig.  5 . Each conversation is grounded in a situation, which is a description provided by the speaker derived from an emotion label (the dataset includes 32 uniformly distributed emotion labels). During the conversation, the speaker talks about the situation, while the listener responds empathetically. Our EmpRL framework generates responses exclusively based on context information, without relying on emotion labels or situation descriptions.\n\nWe also use the Persona-based Empathetic Conversation (PEC) dataset  [42]  to evaluate EmpRL. PEC 3  is a real-world, persona-grounded empathetic dialogue dataset collected from two subreddits, happy and offmychest on Reddit. Since our task focuses on dyadic conversations and does not incorporate persona information, multi-party conversations are excluded, and persona-related details are disregarded. Furthermore, we ensure that each dialogue context corresponds to a single response. After preprocessing, the dataset contains 56, 808, 7, 115, and 7, 131 samples for training, validation, and testing, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We implement the proposed EmpRL 4  using Transformers library  [43] . The implementation comprises two stages: finetuning the generator and further training it using RL.\n\nGenerator Fine-tuning: We first train the T5-base model using full fine-tuning. During training, we employ the AdamW optimizer  [44] , with an initial learning rate of 1.0e -4 and the batch size of 8. During inference, we set the maximum decoding step to 30 and use the topk-topp  [45]  sampling strategy, where k = 20, p = 1.0, and temperature τ = 0.9.\n\nRL Training: We employ the AdamW optimizer during RL training. The number of steps for training the generator using PPO is set to 1, 600, with the learning rate of 1.0e -5 and the batch size of 32. The hyperparameters for the KL penalty coefficient β, discount factor γ, adjustment factor λ, linear combination weight between losses α, and proximal policy ratio clipping range ǫ are set to 0.2, 1.0, 0.95, 0.1, and 0.2, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Baselines",
      "text": "We compare the proposed EmpRL with the following empathetic models.\n\nMoEL  [11] : It utilizes different Transformer decoders to compute response representations for each possible user emotion, and then softly combines these representations to generate responses.\n\nMIME  [12] : A Transformer-based model that takes into account emotion grouping and emotion mimicry. It first generates both mimicking and non-mimicking representations, and subsequently fuses these representations to produce responses.\n\nEmpDG  [22] : A multi-resolution adversarial framework comprising an empathetic generator and interactive discriminators. The generator leverages coarse-and fine-grained emotions to generate responses, while discriminators interact with user feedback.\n\nKEMP  [13] : A knowledge-aware model that integrates external commonsense knowledge and emotional lexicon knowledge to enhance its ability to explicitly perceive and express emotions.\n\nThe models presented above primarily focus on affective empathy, whereas the following baselines consider both affective and cognitive dimensions.\n\nCEM  [15] : A Transformer-based model that utilizes COMET  [46]  to generate various types of commonsense knowledge, thereby enhancing the understanding of user's situation and feelings.\n\nSEEK  [47] : A serial encoding and emotion-knowledge interaction framework that predicts emotion-intent characteristics of responses and models a bi-directional interactive selection process between commonsense knowledge and emotions.\n\nCASE  [16] : It constructs two heterogeneous graphs involving commonsense and conceptual knowledge, and aligns coarse-and fine-grained cognition and affection through a twolevel strategy.\n\nIn addition, we also conduct comparisons with open-source and API-based language models. T5  [37] : We use T5-base as the backbone of our EmpRL framework, which contains 220 million parameters. The pretrained model is further trained using full fine-tuning.\n\nLlama3  [48] : We adopt LLama3-8B, an LLM developed by Meta AI, and fine-tune it using LoRA  [49]  to generate responses.\n\nChatGPT 5  : We utilize the \"gpt-3.5-turbo\" model from the OpenAI ChatGPT API and employ a 2-shot prompt setting to generate responses.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "We evaluate the performance of EmpRL and baselines using both automatic and human evaluations. Human evaluation consists of human ratings and human A/B tests.\n\nAutomatic Evaluation: We employ the following metrics:\n\n• Perplexity (PPL): PPL is the exponent of the crossentropy and evaluates the overall quality of generated responses. A lower score represents more fluent and natural responses. • Distinct-1/2 (Dist-1/2) [50]: Dist-1/2 measures the diversity of generated responses by calculating the proportion of distinct unigrams/bigrams. Higher scores indicate greater diversity in the responses. • Empathy F1-score (Emp-F1): To evaluate the similarity in empathy levels between generated and target responses within a given context, we design the Emp-F1 evaluation metric. Specifically, our pre-trained empathy identifiers are employed to assess the empathy levels for three communication mechanisms in each <context, target response> pair, treating these as gold labels. Meanwhile, empathy levels for the same three communication mechanisms are identified in each <context, generated response> pair and treated as predicted labels. The weighted average F1-scores for each empathy mechanism are then calculated separately. Finally, the average of these three weighted F1-scores yields the Emp-F1 score, which measures the similarity in empathy levels.\n\nHuman Evaluation: We randomly sample 100 dialogue contexts along with their corresponding responses generated by EmpRL and baseline models. Three graduate students with expertise in natural language processing and affective computing from our research team are recruited as human evaluators.\n\nTo ensure consistency and reliability, all evaluators receive detailed instructions and complete a brief practice session before assessments. This preparatory phase familiarizes them with the evaluation process and align their interpretations of the evaluation criteria. Human evaluations are conducted under a double-blind setup to mitigate potential biases. The specific instructions for both the human rating and human A/B test are outlined below.\n\nHuman Rating: The quality of generated responses is assessed based on three criteria: empathy, relevance, and fluency. Ratings for each criterion are on a scale from 1 to 5, where higher scores indicate better performance (1: poor, 2: marginal, 3: moderate, 4: good, 5: excellent). The average score from three evaluators is used as the final rating.\n\nHuman A/B Test: Given a dialogue context and two candidate responses (A and B), please select the preferred response through a comprehensive assessment of their overall quality. This assessment should take into account multiple dimensions, including empathetic expression, contextual relevance, and linguistic fluency. If both responses are deemed equally effective, a \"Tie\" option is available. The final result is determined by majority vote. In cases where the three evaluators provide conflicting results, a fourth evaluator is introduced. The proportions of \"Win\", \"Lose\", and \"Tie\" outcomes for EmpRL, in comparison to the baselines, are reported.\n\nFor both human evaluations, we use Fleiss' Kappa (κ)  [51]  to measure the inter-evaluator agreement.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Automatic Evaluation Results",
      "text": "The automatic evaluation results on EmpatheticDialogues are presented in Table  II . We conduct EmpRL with 5 random seeds and report the mean scores along with 95% confidence intervals. From the table, we conclude that: (1) Models such as CASE, SEEK, and CEM, which incorporate both affective and cognitive empathy, outperform models that focus solely on affective empathy, such as MoEL, MIME, and KEMP. This validates that considering both types of empathy leads to better response generation.  (2)  In terms of PPL, EmpRL shows a reduction of 22.69 compared to the state-of-the-art baseline CASE, indicating that leveraging pre-trained language models can generate more fluent responses. Llama3 achieves a lower PPL primarily because it is an LLM with more extensive training datasets and parameters. (3) Regarding Dist-1/2, EmpRL performs comparably to Llama3 and ChatGPT, demonstrating that the proposed EmpRL is capable of generating diverse responses. (4) In terms of Emp-F1, EmpRL achieves the highest score of 69.43%. This result indicates that its responses exhibit a greater similarity in empathy levels to the target responses, further validating the effectiveness of EmpRL.\n\nWe also conduct experiments on the PEC dataset  6  . As shown in Table  III , we obtain similar results to those on the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Human Evaluation Results",
      "text": "Human Rating Results: Table  IV  presents human rating results for EmpRL and baselines. First, CASE achieves the highest performance among the task-related baselines, demonstrating the effectiveness of incorporating both affective and cognitive empathy. Second, EmpRL significantly outperforms the task-related baselines across all aspects, indicating that the proposed framework effectively enhances the quality of generated responses. Third, EmpRL surpasses Llama3 in terms of empathy, while both models achieve comparable results in relevance and fluency, further validating EmpRL's capability to generate more empathetic responses. Finally, ChatGPT outperforms EmpRL, primarily for the following reasons: ChatGPT benefits from more diverse training datasets, which enable it to possess a broader knowledge base; it is initialized with GPT-3.5, which has a much large number of parameters; and it leverages reinforcement learning from human feedback, which aids in aligning with complex human values. The Fleiss' Kappa scores are above 0.2 across all models, indicating fair agreement (κ ∈ (0.2, 0.4]) among the evaluators. Due to the subjective nature of the evaluation, where different evaluators have varying criteria for giving scores from 1 to 5, the results of inter-evaluator agreement are not particularly high. Nonetheless, the scores still demonstrate a certain level of agreement.   V . Consistent with the human ratings, Em-pRL outperforms task-related baselines and Llama3, demonstrating the effectiveness of the proposed RL-based framework. Furthermore, EmpRL achieves a \"Win\" or \"Tie\" ratio of 54.0% compared to ChatGPT. This finding indicates that although EmpRL does not surpass ChatGPT, it remains competitive in the majority of generated responses, further demonstrating the superiority of EmpRL. It is worth noting that all Kappa scores exceed 0.4, indicating moderate agreement (κ ∈ (0.4, 0.6]) among the evaluators, which reflects a degree of reliability in the evaluations. However, notable discrepancies remain, primarily due to the inherent complexity of the task and the subjectivity of human judgment. Empathy is a complex and multifaceted concept, and both understanding and expressing it pose significant challenges, not only for dialogue systems but also for humans. Thus, addressing the task of empathetic response generation, with a focus on empathy factors, as well as developing evaluation methods that ensure consistency and reliability, requires further exploration.\n\nOn the other hand, as observed by Zhao et al.  [52] , Chat-GPT often repeats the pattern of \"emotional restatement + information expansion\" when expressing empathy. To further validate this observation, we perform a manual inspection of the responses generated by ChatGPT and EmpRL for human evaluation samples, assessing whether their responses exhibit this repetitive pattern. The results, including the repetition proportion (defined as the percentage of responses containing repetitive patterns) and the non-repetition proportion, are presented in Fig.  6 . We find that the proportion of ChatGPT repeating this pattern is as high as 47.0%. This repetitive pattern may cause users to experience monotony and a lack of freshness. Therefore, enhancing personalized empathy capabilities requires greater attention.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Ablation Study",
      "text": "To investigate the effectiveness of different components in our EmpRL framework, we conduct ablation experiments. We consider the following three settings:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Discussion",
      "text": "Analysis on RL Training: During the RL training, we present the trends of the total training loss and reward score on the EmpatheticDialogues dataset to assess the performance of the training process. The trends 8 are depicted in Fig.  7 .\n\nAs training progresses, we observe a gradual decrease in the total loss, while the reward score shows a more significant increase. Both metrics stabilize after approximately 1, 600 steps. This indicates the successful training and convergence 7 Removing the KL penalty term leads to a significant increase in PPL as the number of training steps grows. Therefore, the result of \"w/o KL Penalty\" is obtained by fine-tuning the generator with PPO for 600 steps. 8 The smoothed results are illustrated.    [53] . DailyDialog 9  is a high-quality chit-chat dataset containing human conversations about daily life. We assess ChatGPT and our trained models (i.e., T5, Llama3, and EmpRL) on the DailyDialog test set. Additionally, we present the results of several models reported in CTRLStruct  [54]  for comparison. BLEU-1/2  [55]  and ROUGE-L  [56]  are used to evaluate model performance.\n\nTable  VII  presents the experimental results. First, it is evident that EmpRL shows some gaps when compared to Llama3 and ChatGPT, which is understandable given their advanced natural language generation capabilities. Second, EmpRL outperforms Seq2Seq, BART, and CTRLStruct in terms of BLEU-2 and ROUGE-L, validating the effectiveness of our model in open-domain dialogues. Finally, EmpRL achieves results comparable to T5, indicating that EmpRL is not overly fine-tuned for empathy alone but still retains certain general response generation capabilities.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "E. Case Study",
      "text": "Table VIII shows several responses generated by EmpRL and baseline models. In Case 1, task-related baselines, namely MoEL, CEM, and CASE, demonstrate a limited understanding of the speaker's utterance, resulting in responses that lack contextual relevance. Meanwhile, the response generated by Llama3 is deficient in empathy. In contrast, both ChatGPT and EmpRL effectively comprehend the speaker's situation and feelings, generating empathetic responses. Notably, Em-pRL's response expresses empathy in both affective (\"That's wonderful.\") and cognitive (\"It's always good to be able to reconnect to family and friends.\") aspects. In Case 2 and 3, MoEL, CEM, and CASE again fail to generate high-quality empathetic responses, whereas the performance of Llama3, ChatGPT, and EmpRL is significantly superior.\n\nFrom these examples, it can be seen that while ChatGPT is capable of generating high-quality responses, it typically follows a pattern of \"emotional restatement + information expansion\" to express empathy. For instance, ChatGPT responds with \"Yes, it's sad to see such a well-known brand go.\" followed by \"I hope the employees were able to find new jobs.\" in Case 2. In contrast, our proposed EmpRL framework directly understands the speaker's situation and feelings, generating an empathetic response that includes both affective (\"Ouch! You must be really sorry.\") and cognitive (\"Is there another place you can take the kids to?\") dimensions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose an EmpRL framework for empathetic response generation. The framework defines an empathy reward function and maximizes the expected reward using the PPO algorithm to align the empathy levels between generated and target responses within a given context. The empathy reward function, which incorporates emotional reaction, interpretation, and exploration communication mechanisms, is constructed utilizing pre-designed and pre-trained empathy identifiers. This reward aligns the empathy levels of generated responses more closely with target responses. For model evaluation, we introduce an Emp-F1 metric to measure the similarity in empathy levels between generated and target responses. Automatic and human evaluations demonstrate that EmpRL enhances the quality of generated responses, producing more empathetic responses that encompass both affective and cognitive dimensions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Limitations And Future Work",
      "text": "The main limitation of our work is that we only validate the effectiveness of aligning the empathy levels between responses generated by T5 and human responses. Our framework, however, is also applicable to other language models. In future work, we will extend this framework to LLMs with the goal of enhancing their empathetic capabilities. Besides, our designed reward function is a single-turn reward, which does not take into account empathy consistency across multi-turn dialogues. In the future, we plan to develop a multi-turn (i.e., trajectorylevel) reward and adopt multi-turn RL that not only aligns empathy levels at the individual turn level but also penalizes sudden shifts in empathy, encouraging empathy consistency throughout the dialogue. Furthermore, it would be promising to integrate the retrieval-augmented generation method into our framework. We will create an empathetic dialogue retrieval database to store context-response pairs along with their corresponding empathy levels, and design an efficient retrieval mechanism to further enhance the generation of empathetic responses by retrieving relevant dialogue samples.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of two types of dialogue responses.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates examples of both empathetic and non-",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed EmpRL.",
      "page": 3
    },
    {
      "caption": "Figure 2: EmpRL utilizes the pre-trained T5 model",
      "page": 3
    },
    {
      "caption": "Figure 3: The architecture of Empathy Identiﬁer.",
      "page": 3
    },
    {
      "caption": "Figure 3: gives the structure of",
      "page": 3
    },
    {
      "caption": "Figure 4: Results of empathy identiﬁers on Mental Health",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the validation",
      "page": 4
    },
    {
      "caption": "Figure 2: , the reward function",
      "page": 4
    },
    {
      "caption": "Figure 5: An example from the EmpatheticDialogues dataset.",
      "page": 5
    },
    {
      "caption": "Figure 5: Each conversation is grounded in a situ-",
      "page": 5
    },
    {
      "caption": "Figure 6: We ﬁnd that the proportion of ChatGPT",
      "page": 8
    },
    {
      "caption": "Figure 6: Proportions of repetitive and non-repetitive patterns",
      "page": 8
    },
    {
      "caption": "Figure 7: As training progresses, we observe a gradual decrease in the",
      "page": 8
    },
    {
      "caption": "Figure 7: Trends in the total training loss and reward score during",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "83.79"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label: Excited": "Situation: When I was little and I found out that we were going to the amusement park."
        },
        {
          "Label: Excited": "Conversation:"
        },
        {
          "Label: Excited": "Speaker: When I was a little child my parents told me that we were going to the amusement\npark and it made me so happy."
        },
        {
          "Label: Excited": "Listener: I bet! That sounds like fun."
        },
        {
          "Label: Excited": "Speaker: It was fun for sure, all the rides and the food, it brings back great memories."
        },
        {
          "Label: Excited": "Listener: What sweet memories to hold on to."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on dialogue systems: Recent advances and new frontiers",
      "authors": [
        "H Chen",
        "X Liu",
        "D Yin",
        "J Tang"
      ],
      "year": "2017",
      "venue": "ACM SIGKDD Explorations Newsletter"
    },
    {
      "citation_id": "2",
      "title": "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
      "authors": [
        "X Gu",
        "K Yoo",
        "J.-W Ha"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Hierarchical prediction and adversarial learning for conditional response generation",
      "authors": [
        "Y Li",
        "R Zhang",
        "W Li",
        "Z Cao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "4",
      "title": "Towards efficient dialogue pre-training with transferable and interpretable latent structure",
      "authors": [
        "X Zhao",
        "L Liu",
        "T Fu",
        "S Shi",
        "D Zhao",
        "R Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "5",
      "title": "End-to-end dialogue generation using a single encoder and a decoder cascade with a multidimension attention mechanism",
      "authors": [
        "B Belainine",
        "F Sadat",
        "M Boukadoum"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "6",
      "title": "Less is more: Mitigate spurious correlations for open-domain dialogue response generation models by causal discovery",
      "authors": [
        "T Feng",
        "L Qu",
        "G Haffari"
      ],
      "year": "2023",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Measuring individual differences in empathy: Evidence for a multidimensional approach",
      "authors": [
        "M Davis"
      ],
      "year": "1983",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "8",
      "title": "Empathy: A review of the concept",
      "authors": [
        "B Cuff",
        "S Brown",
        "L Taylor",
        "D Howat"
      ],
      "year": "2016",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "9",
      "title": "Empathetic conversational systems: A review of current advances, gaps, and opportunities",
      "authors": [
        "A Raamkumar",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Z Lin",
        "A Madotto",
        "J Shin",
        "P Xu",
        "P Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Mime: Mimicking emotions for empathetic response generation",
      "authors": [
        "N Majumder",
        "P Hong",
        "S Peng",
        "J Lu",
        "D Ghosal",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Knowledge bridging for empathetic dialogue generation",
      "authors": [
        "Q Li",
        "P Li",
        "Z Ren",
        "P Ren",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Comae: A multifactor hierarchical framework for empathetic response generation",
      "authors": [
        "C Zheng",
        "Y Liu",
        "W Chen",
        "Y Leng",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "15",
      "title": "Cem: Commonsense-aware empathetic response generation",
      "authors": [
        "S Sabour",
        "C Zheng",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation",
      "authors": [
        "J Zhou",
        "C Zheng",
        "B Wang",
        "Z Zhang",
        "M Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "A computational approach to understanding empathy expressed in text-based nental health support",
      "authors": [
        "A Sharma",
        "A Miner",
        "D Atkins",
        "T Althoff"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32nd AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Affectdriven dialog generation",
      "authors": [
        "P Colombo",
        "W Witon",
        "A Modi",
        "J Kennedy",
        "M Kapadia"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "Cdl: Curriculum dual learning for emotioncontrollable response generation",
      "authors": [
        "L Shen",
        "Y Feng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Dual learning for conversational emotion recognition and emotional response generation",
      "authors": [
        "S Zhang",
        "H Hu",
        "S Xing"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Empdg: Multiresolution interactive empathetic dialogue generation",
      "authors": [
        "Q Li",
        "H Chen",
        "Z Ren",
        "P Ren",
        "Z Tu",
        "Z Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "24",
      "title": "Harnessing large language models' empathetic response generation capabilities for online mental health counselling support",
      "authors": [
        "S Loh",
        "A Raamkumar"
      ],
      "year": "2023",
      "venue": "Harnessing large language models' empathetic response generation capabilities for online mental health counselling support",
      "arxiv": "arXiv:2310.08017"
    },
    {
      "citation_id": "25",
      "title": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements",
      "authors": [
        "Y Qian",
        "W Zhang",
        "T Liu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "26",
      "title": "Soulchat: Improving llms' empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations",
      "authors": [
        "Y Chen",
        "X Xing",
        "J Lin",
        "H Zheng",
        "Z Wang",
        "Q Liu",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "27",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction"
    },
    {
      "citation_id": "28",
      "title": "Sequence level training with recurrent neural networks",
      "authors": [
        "M Ranzato",
        "S Chopra",
        "M Auli",
        "W Zaremba"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "Selfcritical sequence training for image captioning",
      "authors": [
        "S Rennie",
        "E Marcheret",
        "Y Mroueh",
        "J Ross",
        "V Goel"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Fine-tuning language models from human preferences",
      "authors": [
        "D Ziegler",
        "N Stiennon",
        "J Wu",
        "T Brown",
        "A Radford",
        "D Amodei",
        "P Christiano",
        "G Irving"
      ],
      "year": "2019",
      "venue": "Fine-tuning language models from human preferences",
      "arxiv": "arXiv:1909.08593"
    },
    {
      "citation_id": "31",
      "title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
      "authors": [
        "H Le",
        "Y Wang",
        "A Gotmare",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Deep reinforcement learning for dialogue generation",
      "authors": [
        "J Li",
        "W Monroe",
        "A Ritter",
        "D Jurafsky",
        "M Galley",
        "J Gao"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Reinforcing coherence for sequence to sequence model in dialogue generation",
      "authors": [
        "H Zhang",
        "Y Lan",
        "J Guo",
        "J Xu",
        "X Cheng"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Hierarchical reinforcement learning for open-domain dialog",
      "authors": [
        "A Saleh",
        "N Jaques",
        "A Ghandeharioun",
        "J Shen",
        "R Picard"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Generating empathetic responses by looking ahead the user's sentiment",
      "authors": [
        "J Shin",
        "P Xu",
        "A Madotto",
        "P Fung"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Rlca: Reinforcement learning model integrating cognition and affection for empathetic response generation",
      "authors": [
        "Y Su",
        "H Bian",
        "B Fan",
        "B Lian",
        "C Zhang",
        "B Zhang",
        "R Huang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "37",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "38",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "39",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Highdimensional continuous control using generalized advantage estimation",
      "authors": [
        "J Schulman",
        "P Moritz",
        "S Levine",
        "M Jordan",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations"
    },
    {
      "citation_id": "41",
      "title": "Proximal policy optimization algorithms",
      "authors": [
        "J Schulman",
        "F Wolski",
        "P Dhariwal",
        "A Radford",
        "O Klimov"
      ],
      "year": "2017",
      "venue": "Proximal policy optimization algorithms",
      "arxiv": "arXiv:1707.06347"
    },
    {
      "citation_id": "42",
      "title": "Towards personabased empathetic conversational models",
      "authors": [
        "P Zhong",
        "C Zhang",
        "H Wang",
        "Y Liu",
        "C Miao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "43",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "44",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "45",
      "title": "The curious case of neural text degeneration",
      "authors": [
        "A Holtzman",
        "J Buys",
        "L Du",
        "M Forbes",
        "Y Choi"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "46",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Empathetic dialogue generation via sensitive emotion recognition and sensible knowledge selection",
      "authors": [
        "L Wang",
        "J Li",
        "Z Lin",
        "F Meng",
        "C Yang",
        "W Wang",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "Empathetic dialogue generation via sensitive emotion recognition and sensible knowledge selection"
    },
    {
      "citation_id": "48",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "49",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "A diversitypromoting objective function for neural conversation models",
      "authors": [
        "J Li",
        "M Galley",
        "C Brockett",
        "J Gao",
        "W Dolan"
      ],
      "year": "2016",
      "venue": "A diversitypromoting objective function for neural conversation models"
    },
    {
      "citation_id": "51",
      "title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "authors": [
        "J Fleiss",
        "J Cohen"
      ],
      "year": "1973",
      "venue": "Educational and psychological measurement"
    },
    {
      "citation_id": "52",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "53",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "54",
      "title": "Ctrlstruct: Dialogue structure learning for open-domain response generation",
      "authors": [
        "C Yin",
        "P Li",
        "Z Ren"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "55",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "56",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    }
  ]
}