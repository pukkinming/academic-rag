{
  "paper_id": "2402.01355v2",
  "title": "Findingemo: An Image Dataset For Emotion Recognition In The Wild",
  "published": "2024-02-02T12:22:41Z",
  "authors": [
    "Laurent Mertens",
    "Elahe' Yargholi",
    "Hans Op de Beeck",
    "Jan Van den Stock",
    "Joost Vennekens"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code. Preprint. Under review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Computer vision has known an explosive growth over the past decade, most notably due to the resurgence of Artificial Neural Networks (ANNs). For many vision-related tasks, computer models have been developed that match or exceed human performance, e.g., image classification  [1]  and mammographic screening  [2] . Many of these tasks, however, are relatively simplistic in nature: detecting the absence or presence of an object, or naming an item in the picture. When it comes to more complex tasks, Artificial Intelligence (AI) still has a long way to go. Affective Computing  [3] , a field that combines disciplines such as computer science and cognitive psychology to study human affect and attempt to make computers understand emotions, is an example of such a complex problem. This paper is concerned in particular with the subtask of Emotion Recognition, i.e., building AI models to recognize the emotional state of individuals, in our case from pictures. This problem has many applications, ranging from psychology  [4] , to human-computer interaction  [5] , to robotics  [6] . It is, however, complex: in the field of psychology, the concept of what an emotion is exactly is heavily debated  [7, 8, 9] , resulting in several ways of describing emotions, either by means of continuous dimensions  [10, 11] , or by means of labels, with different competing label classification schemes existing  [12, 13, 14] .\n\nThe application of computer vision techniques toward Emotion Recognition has historically largely focused on detecting emotions from human facial expressions, with the problem still being actively investigated  [15, 16, 17, 18, 19, 20, 21] . However, the importance of context in emotion recognition is increasingly being acknowledged in psychology  [22, 23] . This led to the release of the computer vision dataset EMOTIC  [24] , presenting photos of people in natural settings, rather than face-focused close-ups, and leading the way to more complex ANN systems that attempt to combine multiple information streams extracted from these images  [25, 26, 27] . Nevertheless, even these more recent efforts focus on the emotional state of one particular individual within the picture. In this paper, we present the FindingEmo dataset, which is the first to target higher-order social cognition. Each image in the dataset depicts multiple people in a specific social setting, and has been annotated for the overall emotional content of the entire scene, instead of focusing on a single individual. We hope this data can be used by AI practitioners and psychologists alike to further the understanding of Emotion Recognition, and more broadly, Social Cognition. This is a complex process, consisting of many layers. Consider, e.g., the photograph depicted in Figure  1 . Looking only at the bride's face, one could easily assume she is very sad, or even distressed. Taking also her wedding gown into account, a positive setting is suddenly suggested; perhaps her tears are tears of joy? Only when looking at the full picture does it become clear that the bride is overcome with emotion in a positive way, as conveyed by the setting, the groom reading a prepared text and the clearly supportive bystanders. Thus, full understanding of the bride's emotional state requires the full scene, including the groom and the solemnly smiling bystanders. This example illustrates how Social Cognition involves detection of relevant elements, extracting relations among these and attributing meaning to construct a coherent whole.\n\nThe source code for the scraper and annotation interface used to create the dataset are available from our dedicated repository  1  , together with the URLs of the annotated images and their corresponding annotations. To mitigate the issue of broken URLs, we provide multiple URLs for a same image whenever possible, and are continuously expanding the set of images for which multiple URLs are provided (about 10k so far). For copyright reasons, we do not share the images themselves. More information with regard to legal compliance can be found in §A.2.\n\nThe data collection process was approved by the KU Leuven Ethics Committee.\n\nThe remainder of the paper is structured as follows. In Section 2 the data collection process and dataset are described in detail. Next, baseline results for emotion classification and valence and arousal regression problems based on popular ImageNet ANN architectures, as well as Visual Transformers CLIP and DINOv2, are presented in Section 3. We build upon this by investigating the effect of merging the features and predictions of several models in Section 4. Finally, we conclude with a discussion in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset Description",
      "text": "The dataset is split into a publicly released set of annotations for 25,869 unique images, and a privately kept set of 1,525 images  2  . Each image depicts multiple people in various, naturalistic, social settings. We follow Emotic  [24]  in creating a training (=our public) set with one annotation per image, and a test (=our private) set with multiple annotations per image. In total, 655 participants-a short description of whom can be found in §A.8-contributed annotations. In what follows, we list the most important annotation dimensions; for a full list, see §A.3.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Valence And Arousal",
      "text": "We used Russell's continuous Valence and Arousal dimensions  [10] , with integer scales [-3, -2, . . . , 3] for Valence and [0, 1, . . . , 6] for Arousal. Arousal was named \"Intensity\" in our annotation interface, as we felt \"Arousal\" might carry a sexual connotation for some users. Emotion Users had to pick an emotion from Plutchik's discrete Wheel of Emotions (PWoE)  [13] , shown in Figure  2 . We opted for this particular emotion classification scheme as it strikes a balance between the more limited and sometimes contested Ekman's 6  [12] , and the more expansive, and potentially more confusing, Geneva Emotion Wheel  [14] . It defines 24 primary emotions, grouped into 8 groups of 3, with emotions within a group differing in intensity. It is depicted as a flower with the 24 emotions organized in 8 leaves and 3 concentric rings. Each leaf represents a group of 3, with opposite leaves representing opposite emotions. The rings represent the intensity levels, from most intense at the center to least intense at the outside. An additional advantage of PWoE is that one can easily opt to use all 24 emotions, or instead limit oneself to the 8 groups, allowing some granularity control. We refer to these choices as \"Emo24\" and \"Emo8\" respectively, and refer to the groups as \"emotion leaves\".",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Positioning Versus Existing Datasets",
      "text": "Although research in automated Emotion Recognition has been gaining in popularity over the years, progress is still hampered by a lack of data. Earlier work tended to focus solely on recognizing emotions from faces. In their recent review paper,  [28]  list no less than 21 publicly available datasets of facial images for this purpose, typically annotated with Ekman's 6, potentially extended with a \"neutral\" category, or custom defined emotion categories. Some of the more popular such datasets, like JAFFE  [29]  and CK+  [30] , make use of a limited number of actors (10 for JAFFE, 123 for CK+) who were instructed to act out a certain emotion, resulting in caricatural emotional expressions.\n\nPublicly available datasets going beyond the face are few in number. First, there is EMOTIC  [24] , a 23,571 image dataset depicting people in the wild, and with natural expressiveness. An explicit goal of EMOTIC is to take context into account when assessing a person's emotional state. One or more individual subjects are delineated by a bounding box in each picture for a total of 34,320 subjects, each annotated for Valence, Arousal, Dominance and one of 26 custom defined emotion categories. CAER-S is a dataset of 70,000 stills taken from 79 TV shows. The stills were extracted from 13,201 video clips that were annotated for Ekman's 6 + neutral. Each still contains at least one visible face. The aim of the dataset is to allow augmenting facial emotion recognition with contextual features. Similar to EMOTIC, there is HECO, a dataset of 9,385 images taken from previously released Human-Object Interaction datasets, films and the internet. Like EMOTIC, 19,781 individual subjects were annotated in the pictures for Valence, Arousal, Dominance, 8 discrete emotion categories comprised of Ekman's 6 + Excitement and Peace, and two novel dimensions, Self-assurance and Catharsis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Creation Process",
      "text": "The creation of the dataset was split into two phases. The first phase focused on gathering a large set of images, prioritizing quantity over quality. The second phase consisted of collecting the annotations. We present a brief summary of both phases here, and refer to §A.4 for more details.\n\nPhase 1 Images were gathered using a custom built image scraper that generates random search queries, each consisting of three terms selected from predefined lists of, respectively, emotions, social settings/environments, and age groups of people (e.g., 'adults', 'seniors', etc.). For each query, the first N results were retrieved, filtered and downloaded. In total 1,041,105 images were collected.\n\nPhase 2 Annotations were gathered using a custom web interface (see §A.5 for a screenshot). Annotators were recruited through the Prolific 3  platform, and first required to agree to an Informed Consent clause, followed by detailed instructions (see §A.6 for a copy). To monitor the process closely, we performed many (51, to be exact) runs, each with a limited number (around 10 to 15) of participants. For each run, the Prolific user selection criteria were the same: fluent English speaker, (self-reported) neurotypical, and a 50/50 split male/female. Candidates were informed of a total expected task duration of 1h, and offered a £10 reward. Analysis of the durations (see §A.7) show our time estimation to be fair. In total, data collection costs were £10k, including fees and taxes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotator Grading And Annotator Overlap",
      "text": "To assess the reliability of annotators, we used a set of 5 fixed images, referred to as \"fixed overlap images\", chosen specifically for being unambiguous.  4  For each image, a default annotation was defined consisting of the \"keep/reject\" choice (4 keeps, 1 reject), Valence (value range), Arousal (value range) and Emotion (emotion leaf). This results in 4 datapoints per image, or 20 datapoints in total. Annotators' submissions for these images were compared to the reference, earning 1/20 point per matching datapoint, resulting in a final \"overlap score\" s ∈ [0, 1]. Users with s >= 0.8 were automatically accepted. An alternative score s alt was computed which ignored those overlap images whose reference value was \"keep\", but were annotated as \"reject\". The reason for this is that it quickly became clear that despite the system providing a \"Skip\" option in case users rather not annotate a certain image, some chose to \"reject\" these images instead. Also, one of the \"keep\" images shows a bit of text, which users were instructed to reject. Some users were more strict than others in applying this rule.\n\nWe defined a system parameter p R that controls when overlap images (i.e., images already annotated by others) are shown to users. For each new image request, an overlap image is served with probability p R , starting with the 5 fixed overlap images, in a fixed sequence. Once these are annotated, the system serves other, non-fixed, already annotated images. At first, these were randomly chosen from all annotated images, but this resulted in too many images with only 2 annotations. Hence, we created a process that limits the pool of images to choose from, and attempts to strive for 5 annotations per (non-fixed) overlap image. Using this system, we obtained a dataset with 80.9/19.1 split single label/multi-label annotations. These multi-label images make up the private set. Detailed interannotator statistics on this private set are reported in §A.9, indicating that for 26.2% of the images, all annotators agreed on the emotion leaf, while for 46.6% of the images two labels were given.\n\nOut of these two-label annotations, 42.8% refer to adjacent emotion leafs. Annotators agree less on Arousal (average min-max difference of 2.7 ± 1.4) than on Valence (average min-max difference of 1.8 ± 1.2). Importantly, average Valence disagreement plateaus close to 2 with increasing number of annotations per image, while a linearely increasing trend is apparent for Arousal.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Statistics And Observations",
      "text": "This section presents statistics for the 8 leaves of PWoE. For the full 24 emotions, see §A.10. Figure  3  shows the distribution of annotations per emotion leaf. An imbalance is obvious, with in particular \"joy\" and \"anticipation\" being overrepresented, and \"surprise\" and \"disgust\" heavily underrepresented, despite an added balancing mechanism (see §A.4.2). A similar imbalance is found in popular facial expression datasets, such as FER2013  [32]  (only 600 \"disgust\" images versus nearly 5,000 for other Ekman's 6 labels) and AffectNet  [33]  (134,915 \"happy\" faces, 25,959 \"sad\" faces, 14,590 \"surprise\" faces, 4,303 \"disgust\" faces). Although EMOTIC  [24]  uses custom emotion labels, making a one-to-one comparison more difficult, it is also heavily skewed towards positive labels (top 3: \"engagement\", \"happiness\" and \"anticipation\"; bottom 3: \"aversion\", \"pain\" and \"embarassement\"). Compared to these other datasets, ours exhibits less imbalance. In Table  2 , we group average annotation values for Arousal, Valence and Ambiguity. As expected, perceived \"negative\" emotions (\"fear\", \"sadness\", \"disgust\" and \"anger\") have a negative average Valence, with the inverse being true for \"positive\" emotions (\"joy\", \"trust\"). Somewhat undecided are \"surprise\" and \"anticipation\", which can go either way. The highest Arousal values are reserved for \"anger, \"sadness\" and \"fear\". We hypothesize the unexpectedly high Arousal value for \"sadness\" might be due to naming this dimension \"Intensity\" in our interface; although a grieving person is generally considered to have low arousal, the emotion of sadness itself is felt intensely. Further analysis on the full emotion set reported in §A.10 verifies that also at this more fine-grained level, annotations conform to expectations, with Arousal levels increasing along with the intensity level of the PWoE ring, and Valence levels analogously increasing for \"positive\" and decreasing for \"negative\" emotions.  Figure  4  shows the association between Arousal and Valence annotations, indicating as expected a collinearity between higher Arousal values and the extremes of the Valence range.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Model Results",
      "text": "Baseline results are obtained by applying transfer learning to popular ImageNet-based ANN architectures AlexNet  [34] , VGG16  [35] , ResNet 18, 50 and 101  [36]  and DenseNet 161  [37] .  5  For each, we use the default PyTorch implementations and weights, and replace the last layer with a new output layer that matches the chosen task (see below). Only this last layer is trained. We do the same experiment for some of these same architectures trained from scratch on the Places365 dataset  [38] , using the official PyTorch models. We also consider EmoNet  [39] , a model for labeling images with one out of 20 custom emotion labels reflecting the emotion elicited in the observer, obtained by applying transfer learning to AlexNet and trained on a private database. In this case, we first process the image with EmoNet, and then send the resulting 20-feature vector through a new linear layer. We use the EmoNet PyTorch port by the main author  6  . Lastly, we also use Visual Transformer models CLIP  [40]  (ViT-B/32) and DINOv2  [41]  (ViT-B/14 distilled with registers)  7  , using both models to obtain embeddings for input images, and like with EmoNet, use these as input to a single linear layer.\n\nWe distinguish three tasks: Emo8 classification, where we predict one of the 8 primary emotions defined by the emotion leaves of PWoE; Arousal regression, where we predict the numerical arousal value; Valence regression, where we predict the numerical valence value. For classification, we apply a softmax to the output of the final layer. Target values for regression problems are reduced to the range [0, 1] using an appropriate linear rescaling. Hence, we apply a sigmoid function to the model output. Network outputs are transformed back to the original problem domain by using the inverse scaling.\n\nPreprocessing for ImageNet models consisted in scaling images to an 800x600 resolution, keeping the original ratio and centering and padding with black borders where necessary, followed by normalization using default ImageNet pixel means and standard deviations. For Places365 and EmoNet models, we followed the preprocessing steps described in the respective papers. For CLIP, we use the default preprocessing chain that comes with the model, and for DINOv2 we use the same preprocessing as for the ImageNet models, but with a rescaling to 798x602.\n\nFor each task, and each model, we trained 10 models per starting learning rate lr 0 and per loss function L.\n\nFor classification, we used lr 0 ∈ [10 -1 , 10 -2 , 10 -3 , 10 -4 ] and L ∈ [CrossEntropyLoss, UnbalancedCrossEntropyLoss]; for regression we used lr 0 ∈ [10 -3 , 10 -4 , 10 -5 , 10 -6 , 10 -7 ] and L ∈ [MSELoss, WeightedMSELoss]. UnbalancedCrossEn-tropyLoss is a novel extension of the traditional CrossEntropyLoss, created to allow giving different weights to different misclassifications. WeightedMSELoss is a natural extension of MSELoss that takes into account class imbalance. Full technical details for both can be found in §A.11.\n\nAll experiments use the public dataset, Adam loss with default PyTorch parameter values, and the custom lr update rule lr e = lr0 / √ (e//3)+1, with lr e the learning rate at epoch e. By virtue of the floor division (//), this means we update the learning rate once every 3 epochs. The data was randomly split 80/20 train/test, making sure that each target label was also split according to this same rule.\n\nReported metrics are: for classification, Average Precision (AP)-as computed using the scikit-learn package-and Weighted F1 (W.F1); for regression, Mean Average Error (MAE) computed in the original problem domain, and Spearman Rank Correlation (S.R). Training stopped when either the epoch with the best loss (or the best W.F1 score for classification) on the test set lies 6 epochs behind the current epoch, or 250 epochs were reached, with the corresponding best model put forward as the final trained model. Only results for the (lr 0 , L)-combination yielding the best average Weighted F1 or Mean Average Error performance over the corresponding 10 models are reported.\n\nAll our experiments were implemented in Python using PyTorch, and split over an Intel Xeon W-2145 workstation with 32GB RAM and two nVidia GeForce RTX 3060 GPUs with 12GB VRAM, and an Intel i7-12800HX laptop with 32GB RAM and an nVidia GeForce RTX 3060 Laptop GPU with 12GB VRAM. Test results are plotted in Figure  5 , with the graph for train data, and tables containing the numerical results grouped in §A.12. In order to speed up training, we buffered model activations whenever possible.  8 Apparent from these results is that these are hard problems. ImageNet-trained models slightly outperform their Places365-trained counterparts. This suggests that the natural object features extracted from the ImageNet dataset are more salient toward emotion recognition than are place-related features. In 9 out of 13 cases, our UnbalancedCrossEntropyLoss has the edge over regular CrossEntropyLoss. Predicting Arousal appears more difficult than predicting Valence, which aligns with lesser annotator agreement for Arousal than Valence, as analyzed in §A.9. As for the architectures, VGG is a clear winner, with ResNet second. Although twice as large, ResNet101 performs very similar to ResNet50. The larger depth of the DenseNet model does not translate in better performance. A breakdown of model performance per Emo8 class can be found in §A.12, showing overall best performance on 'Joy' and 'Anger'. Worst performance is registered for 'Surprise' and 'Disgust', which perhaps not surprisingly are also the emotions for which the least annotations are available.\n\nInterestingly, as explored in §A.14, when a model deviates from the target Emo8 annotation there is a strong tendency toward \"nearby\" emotions. Most often this is the adjacent leaf, with more distant leaves increasingly more unlikely. This behavior is reminiscent of the kind of disagreements we find among our human annotators.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Beyond The Baseline",
      "text": "To build upon the baseline established in §3, we built multi-stream models by applying the popular technique of late fusion  [24, 25, 26, 27] . This section reports results for Emo8 classification; the analogous discussion for Arousal and Valence regression can be found in §A.13.\n\nWe consider the following streams for combinations: Emo8 predictions: for each considered architecture, we trained an Emo8 model, and took the predictions from this model as an 8-feature vector; Baseline features: we take the model features from the penultimate layer. Vector size depends on the architecture; EmoNet predictions: applying the model gives us a 20-feature vector (see §3); YoLo v3 trained on Open Images + Facial Emotion Recognition (OIToFER): we apply YoLo v3  9  [42], using LightNet  [43] , to each image and extract the detected \"Human face\" regions with probablity p > 0.005. We then apply the FER2013-trained ResNet18 model by X. Yuan 10  to the extracted faces, resulting in a 7-feature vector per face. We generate two 7-feature vectors from this, one containing the vector averages, the other the standard deviations, and concatenate both to obtain a final 14-feature vector; Places365 ResNet18 predictions: applying the ResNet18 model trained on the Places365 dataset gives us a 365-feature vector per image; Places365 ResNet18 features: we take the model activations from the penultimate layer, giving us a 512-feature vector.\n\nThe experimental setup is identical to §3, except that for time considerations, we only consider CrossEntropyLoss.  11  The test results for Emo8 classification are shown in Figure  6 . Training results, as well as numerical training and test results, are included in §A.  13 . A first observation is that improving upon the baseline appears non-trivial; except for VGG16, the obtained gains are modest. Second, the highest gains clearly come from adding facial emotion features. Third, even though adding EmoNet and OIToFER features separately has a positive effect for VGG16, adding both together does not result in a compounded improvement. Fourth, the added dimensionality of concatening features instead of predictions in the case of Places365 does not result in markedly different results, in some cases even leading to worse results. Finally, not a single stream combination resulted in improved performance for CLIP and DINOv2, with the best VGG16 results nearing CLIP/DINOv2 performance.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Findings The analysis of our dataset shows the annotations to conform to expectations, with Valence and Arousal values following the expected trends. Furthermore, when annotators disagree on the emotion label, they tend to choose nearby emotions in PWoE nonetheless. Our experiments show that, for the Emo8 prediction task on our dataset, modern ViT models do not seem to really outperform older CNN architectures, with VGG16 even (slightly) outperforming DINOv2 when both baselines are augmented with Facial Emotion features. For Arousal and Valence prediction however, the ViT models are clearly superior.\n\nLimitations 1) While images in our private set have multiple annotations, we have followed the approach of Emotic  [24]  and gathered only a single annotation per image in our public set. This choice has allowed us to gather a larger data set, but may cause concerns about reliability. These concerns are alleviated by the clear tendency observed on the private set toward similar emotions in case of multiple labels ( §A.9), combined with trained models exhibiting this same tendency to strongly favor nearby emotion leaves when deviating from the annotation ( §A.14). In short: the models trained using single annotations showed similar statistics to the human multi-label annotations.\n\n2) Concerning potential biases in the images themselves, as they were scraped from the internet the dataset inherits the same biases the internet exhibits. In particular, we have not performed any analysis concerning potential representation issues. As such, there is an unverified possibility that models trained on our dataset wrongly associate \"negative\" emotions more strongly with certain minority groups. 3) Since legal issues (see §A.2) prevent us from sharing the actual images, we had to resort to sharing URLs. While URLs can break, we mitigate this risk by offering multiple different URLs for the same image where possible.\n\nImpact Statement This paper presents work whose goal is to advance the field of Machine Learning, Psychology and Psychiatry. Our own interest lies with non-commercial applications with respect to the understanding of Emotion Recognition and Social Cognition in individuals, and how these can be affected by neurological conditions. In particular, we hope that our (future) work will be of help in assisting people with impaired Social Cognition to navigate life.\n\nNevertheless, the data, and possible future Machine Learning advances inspired by it, could very well lead to commercial (e.g., personalized ads tailored to one's mood) and surveillance (e.g., general crowd monitoring, detection of aggression within crowds, etc.) applications that we strongly feel warrant a public debate with regard to their desirability, and even legality.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "We present FindingEmo, a dataset of 25k image annotations for Emotion Recognition that goes beyond the traditional focus on faces or single individuals, and is the first to target higherorder social cognition. The dataset creation process has been discussed in detail, and the annotations have been shown to align with expectations. Baseline results are presented for Emotion, Arousal and Valence prediction, as well as first steps to go beyond the baseline. These results show the dataset to be complex, and the tasks hard, with even modern models like CLIP and DINOv2 struggling. This suggests that in order to solve these tasks, novel Machine Learning roads might need to be explored.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Our Annotation Interface And Code For Model Training Are Made Open Source.",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Appendix A.1 Dataset Logo",
      "text": "The logo of the dataset is depicted in Figure  7 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.2 Legal Compliance",
      "text": "Concerning the legal status of the dataset, two question arise: 1) are we allowed to share URLs to (potentially) copyrighted content, and 2) are we allowed to use (potentially) copyrighted material to train our models?\n\nWith regard to 1, we verified this with copyright experts at our institute who assured us that this is legal. With regard to 2, we point to Title II, Article 3, \"Text and data mining for the purposes of scientific research\", of the so-called InfoSoc Directive  12  , which provides an exception to copyright obligations for (members of) research organisations. As members of KU Leuven, we fall under this law. If you are not a member of a European research or cultural heritage institution, you will need to check with your local regulation whether or not you have the right to use this material for research purposes.\n\nWe are the rightful owners of the annotations, so no potential copyright issues arise for this data. We expressly distribute the dataset under a non-commercial CC BY-NC-SA 4.0 license.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.3 Additional Annotation Dimensions",
      "text": "These are the remaining annotation dimensions that were not mentioned in the main text for brevity.\n\nAge group Users had to tick one or more boxes from \"Children\", \"Youth\", \"Young Adults\", \"Adults\" and \"Seniors\", indicating the age groups present in the image. Deciding factor(s) for emotion Users had to tick one or more boxes from \"Neutral\", \"Body language\", \"Conflict context vs. person\", \"Facial expression\" and \"Staging\", indicating what prompted them to choose for a particular emotion. Ambiguity Lastly, users could indicate by means of an integer scale [0, 1, . . . , 6] how ambiguous the emotional content exhibited by the entire photograph was, or alternatively, how much difficulty they had in annotating the picture.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.4 Details Of The Dataset Creation Process",
      "text": "This section describes in more detail the two phases in the dataset creation process introduced in §2.2.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.4.1 Phase 1: Gathering Images",
      "text": "Phase 1 consisted in building a customized, Python-based DuckDuckGo  13  image scraper, programmed to generate random image search queries as follows. Three sets of keywords were defined: one containing a diverse set of emotions; one containing social settings and environments (e.g., 'birthday', 'workplace', etc.); and one referring to humans (e.g., 'people', 'adults', 'youngsters', etc.).  14  By taking all possible combinations of the elements in these sets, the system generated a multitude of queries, such as, e.g., \"happy youngsters birthday\". The first N results were then retrieved and filtered to exclude a number of manually blacklisted domains (e.g., stock photography providers) and by image size. Query results that passed the filtering steps were downloaded.\n\nWe started with N = 500 and image width 800px < w < 1600px, and later extended this to N = 1000 and 800px < w < 3200. Obviously, not all downloaded images satisfied our criterion of depicting multiple people in a natural setting. Hence, as a further filtering step, one of the authors annotated 3097 images as either \"keep\" (useful) or \"reject\" (no use). These images were used in a random 80/20 split to train a CNN to perform the same task, achieving an accuracy of 77.6%. This model was used to further filter downloaded images, in particular to identify spurious images such as, e.g., drawings, images with lots of text, etc.: if the CNN labeled the downloaded image as \"reject\", the image was discarded. If the downloaded image was labeled as \"keep\", it entered the pool of images that could be selected for annotation.\n\nIn total 1,041,105 images were collected.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "A.4.2 Phase 2: Gathering Annotations",
      "text": "The annotations were gathered using a custom web interface written in Python, HTML and JavaScript. Annotators were recruited through the Prolific platform. For this, a job would be created, which we refer to as a \"run\", to which users could subscribe. After doing so, they received a URL that allowed them to log on to our system and, after agreeing to an Informed Consent clause, perform the annotations. First, users were presented with detailed instructions, a copy of which are provided in §A.6, after which the data collection proper began. To be able to monitor the process closely, and to cope with hardware limitations of our server, we opted to only perform runs with a limited number of participants, most often 10 or 15. For each run, the Prolific user selection criteria were the same: fluent English speaker, (self-reported) neurotypical, and a 50/50 split male/female. In total, annotations were collected over 51 runs. Candidates were informed of an expected task duration of 1h, including reading the instructions, and offered a £10 reward. Analysis of the durations (see §A.7) show our time estimation to be fair. We spent a total of £10k, which includes annotators whose contributions were filtered out, and most importantly, Prolific fees and taxes.\n\nA screenshot of the interface is included in §A.5. The interface presents users with images on the left side, and dimensions to annotate on the right side. At the top left, users are presented with two buttons: one to skip an image if they so wish, and one to save the current annotation and move on to the next image.\n\nUpon being presented an image, the first choice users needed to make was, just like the filtering CNN, whether to \"keep\" or \"reject\" the image, according to the provided instructions. Essentially, users were asked to reject images that contained no people, were watermarked, were of bad quality, etc. If users opted to \"reject\" an image, no further annotation was needed. This step was needed to further filter images that passed through the CNN. If the choice was \"reject\", no further action (besides saving) was required. Optionally, users could choose to select one of several tags indicating why they opted to reject the image from \"Bad quality photo\", \"Copyright\", \"Watermark\", \"No interaction\", \"No people\", \"Text\" and \"Not applicable\". Each user was asked to annotate 50 \"keep\" images; \"rejects\" did not count towards the total goal. Despite this, some users still performed full annotations on images they rejected. If users opted to \"keep\" the image, they were expected to annotate all other dimensions as well.\n\nAlthough the frontend (i.e., user interface) remained essentially unchanged, the backend underwent some changes as annotations were collected, and some lessons were learned, which we discuss here.\n\nInitial iteration Initially, an image was randomly selected from the corpus, and processed by an updated \"keep/reject\" CNN (see §A.4.1) with an accuracy of 83.6%. If the \"keep\" probability p k was < 0.75, a new random image would be selected and tested, until one was found with p k ≥ 0.75. If this image had already been annotated, the process would start over, until a valid image was found, which would then be shown to the annotator.\n\nSecond iteration At first, the annotating of all dimensions was not enforced; users could select the \"keep\" checkbox, save the annotation without annotating anything else, and move on to the next image. Most did their job diligently, but nevertheless we opted to update the interface to require all dimensions be annotated in case of a \"keep\", before the \"Save\" option became available. This frequently prompted messages from users complaining the \"Save\" option was not available to them. A further update explained this to users who prematurely clicked on the \"Save\" button.\n\nThird iteration Over the course of the first few thousand annotations, it became clear that two emotion leaves were particularly overrepresented, namely \"joy\" and \"anticipation\", respectively accounting for 35.9% and 23.0% of all annotations by the time of Run 9. In an attempt to counter this, we came up with the following system.\n\nBesides the \"keep/reject\" CNN, we trained a second CNN to predict the Emo8 label. We then first computed all \"keep/reject\" predictions for all images in the corpus, and followed this up by predicting Emo8 labels for all \"keep\"-labeled images. Upon starting the annotation server, these predictions are loaded into memory. When selecting an image to show to a user, first an emotion label is chosen, with odds inversely proportional to the number of images that were tagged (by the CNN) with a certain label. Second, out of all images tagged with this label, one that had not previously been annotated by an annotator would be chosen. The CNN used to make the predictions was retrained at several steps along the annotation gathering process. Using this system, we managed to decrease \"joy\" down to 28.4%, and up \"sadness\" from 6.3% to 10.5%.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.5 Annotation Interface",
      "text": "A screenshot of the annotation interface is shown in Figure  8 .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.6 Copy Of The Annotator Instructions",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Welcome",
      "text": "It is recommended to set your browser to \"full-screen\" mode. Typically, this mode can be toggled by using the 'F11' key. This interface was designed for screen resolutions with a width of 1920 pixels. In case your screen has a higher/lower resolution, the interface should automatically resize itself so as to fully fit on your screen, but this might come at the price of reduced image sharpness.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Thank You For Your Willingness To Participate In This Annotation Task!",
      "text": "In this experiment, you will be expected to annotate 50 \"good\" images, i.e., annotated as \"Keep\", after which you will receive a URL that will direct you to the Prolific completion page for this task. Please take the time to read these annotation instructions before continuing.\n\nNote that if for any reason you get logged out at some point, you should be able to log back in using the same URL provided to you by Prolific, and pick up right where you left.\n\nWe want to build a database of photographs with an emotional content. You will be shown randomly selected images from a large corpus, and we ask you to evaluate photographs regarding 2 consecutive issues.\n\nFirst, regardless of the emotional content, all photographs should adhere to the following criteria:\n\n• Each photograph must display a realistic situation, e.g., no drawings, no watermark, no fantasy content (i.e., digitally manipulated photos), no horror, etc.\n\n• The formal quality of the photograph should be sufficient, i.e., no fuzzy/blurry photographs. • Each picture must display at least 2 people that are clearly visible. Alternatively, if only one person is shown, but this person is clearly a part of a larger context, the image can also be suitable. • The main feature of the photograph must not consist of a textual element. For instance, if a cardboard displaying 'stop racism' is a central feature of the picture, the picture is not suitable.\n\nIf an image does not adhere to each of these criteria, or you are not certain, please rate it as not suitable by choosing the \"Reject\" option. Else, mark it as \"Keep\", in which case all other dimensions, except for \"tags\", need to be annotated before you can proceed! Even if you want to keep the default value of a slider, you still need to click the slider first.\n\nImages can further be described by a number of tags:\n\n• Bad quality photo: when a picture is too blocky/blurry.\n\n• Copyright: a copyright, contrary to a watermark, is not repeated but appears only once. Typically, this leads to the picture being rejected, unless possibly the copyright is only small in size and could be cropped out without losing the essence of the picture. • Watermark: a watermark is a specific pattern, typically containing the name of the copyright holder, that is repeated over an entire image. • No interaction: the people in the picture don't have a direct interaction.\n\n• No people: the picture does not depict any people.\n\n• Text: the image contains a lot of text, either typeset on top of it, or present on, e.g., banners held by subjects depicted in the picture. If the text is typeset, this is disqualifying (i.e., the picture is rejected). If the text is present in the picture itself, it is disqualifying if it is too prominent. Use your own discretion to determine what is \"too prominent\" and what is not. A good rule of thumb is: if your attention is immediately drawn to the textual elements when viewing the picture, then it is too prominent and the picture is disqualified. • Not Applicable: typically used for images that are actually a collage of more than one photo, or that are rejected but don't fit any of the other tags.\n\nIf a photograph is not rated as suitable (i.e., \"Reject\"), no further assessment is required; click \"Save\" to proceed to the next paragraph. Else, for \"Keep\" or \"Uncertain\" photos, you are also expected to annotate the age group of the main participants in the picture. These labels are of course not clear cut; feel free to use your own discretion as to which label applies best.\n\nSecond, we want you to focus on the emotional labelling of the photographs. Concretely, we ask you to annotate the image on a number of dimensions\n\nWe ask you to indicate the emotional characteristic of the ENTIRE SCENE displayed in the photograph, independent of your own political/religious/sexual orientation. So a black lives matter protest is typically negative (= the participants are not happy) independent of whether you support BLM. Specifically, we ask you to rate the valence (\"Negative/Positive\") of the overall emotional gist of the photograph on a 7-point Likert scale from negative (-3) over neutral (0) to positive (+3), and also the intensity, ranging from not intense at all (0) to very intense (6) by using the appropriate sliders.\n\nWe also ask to indicate an emotional label by means of a mouse click on an emotion wheel called \"Plutchik's Wheel of Emotions\". If you can't find the perfect emotional label then you choose the 'next best thing', i.e., the one that reflects it most. In case no particular emotion fits, i.e., the participants all display a neutral expression, you can opt to select no emotion, although such cases are expected to be rare. For a more detailed description of each emotion depicted in this wheel, see, e.g., https://www.6seconds.org/2020/08/11/plutchik-wheel-emotions/. Additional info for each emotion will be displayed when hovering over its corresponding cell.\n\nPlease also rate how straightforward the emotional content that is exhibited by the entire photograph is using the scale indicated with \"Ambiguity\". For instance, if there are approximately as much emotionally positive as emotionally negative cues in the photograph, the emotional content would not be clear  (6) , while only positive cues or only negative cues would result in a very high clarity (0).\n\nFinally, the options under the \"Deciding factor(s) for emotion\" header ask which aspects of the photo influenced you most when assessing the emotion, i.e., facial expressions, bodily expressions, the type of interaction ('Staging') among the persons (e.g., fighting, dancing, talking), type of context (e.g., wedding, funeral, protest, etc.), objects in the photograph (e.g., gun, chocolate) or a possible conflict between context and person(s) (i.e., somebody exuberantly laughing at a funeral). If none of these apply, and/or the emotion is rather neutral, the \"Neutral\" tag can be used, although just as for the emotion case, we expect these occasions to be rare.\n\nIf for some reason you would rather not annotate the current image being served to you, you can press the \"Skip\" button to be served a new picture and have the annotation interface be reset, without your current settings being saved.\n\nIf on the other hand you are happy with your current annotation, press \"Save\" to let it be saved and move on to the next image. If this button is greyed out, this means you have not yet annotated all necessary dimensions. Once you have reached the required number of annotations, you will automatically get to see the URL that will direct you to the Prolific completion page for this task.\n\nAt the top of this screen, you can see your annotation statistics: \"Rejected/Accepted\" = how many images you marked \"Reject\" and \"Keep\" respectively, and \"Left\" = number of \"Keep\" images left to annotate.\n\nYou can always check these instructions again whilst annotating by clicking the -icon next to each criterium. (Click once more to close the infobox again.)",
      "page_start": 14,
      "page_end": 17
    },
    {
      "section_name": "A.7 Task Duration Analysis",
      "text": "A histogram of time taken per annotator to complete the task is shown in Figure  9 . These are the durations as reported by Prolific. An important remark to make is that for Prolific users, the clock starts ticking once they subscribe to a job. By default, per the Prolific rules, for a job expected to take 1h users are allowed a maximum of 140 minutes to complete the job. It appears that many users subscribe to a job, and then leave their browser tab open for a while before starting the job proper. (Some never start, leading to a time-out.) Taking this into account, the shown distribution is a \"pessimistic\" picture, including many idled minutes. The average time taken per user, including users that were ultimately filtered out of the dataset, was 64 ± 27 minutes. With all of the above in mind, we conclude our alloted time was fair. A small negative correlation manifests between the task completion time and the annotator score (SpearmanR= -0.122, p = 0.002 for s, SpearmanR= -0.086, p = 0.029 for s alt ).",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.8 Annotator Statistics",
      "text": "Annotations were collected from 655 annotators. Prolific provided us with anonymized personal data, except for 1 user. Not all datapoints are available for all users.\n\nOf the annotators, 337 are male, 317 are female, and 1 unknown. 651 annotators were spread over 49 countries, with country for the remaining 4 unknown. Most popular were South Africa (176 annotators), Poland (127 annotators) and Portugal (104 annotators). From there, numbers drop rapidly, with follow-up Greece accounting for only 32 annotators. The full distribution of annotators per country is shown in Figure  10 . The age distribution of the 653 users who shared that info is shown in Figure  11 , indicating a large bias towards the early 20's.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.9 Inter-Annotator Agreement",
      "text": "Recall from §2 that we hold a private set of 1,525 images that have each been annotated by multiple users  15  , amounting to a combined 6115 annotations. Table  3  shows how many images have been annotated by N different annotators. Of these, 1294 images have a majority of \"Keep\" annotations, 137 are mainly \"Reject\" and 94 are undecided.\n\nWe do not report the often used Cohen's Kappa and/or Krippendorf's Alpha scores, as these metrics are only meaningful when most pairs of annotators have both annotated a substantial set of shared images. In our case, however, by design, the number of images that have been annotated by any two annotators is low (1 or 2 at most, and very often zero). As such, we feel these metrics are not applicable. We explicitly opted to have a large number of annotators annotate a small number of images each, in order to have the annotations better be a reflection of \"the population at large\", rather than of a few annotators. Focusing on the 1294 \"Keep\" images, Figure  12  shows how many images have been annotated with N different emotion labels, both for Emo8 and Emo24 labels. For 26.2% of images, all annotators chose the same emotion leaf, and 46.6% were annotated with 2 different Emo8 labels. For the finegrained Emo24 labels, 80.1% of images have been annotated with a maximum of 3 different labels.  Turning to the question of how different the separate emotion labels for a same image are, Figure  13  shows the distribution of maximum distance between labels, for images annotated with more than one label. The distances for 24 emotions are computed by also giving an ordinal to each emotion within a leaf, as shown in Figure  14 . No less than 42.8% of the times an image has been annotated with more than one Emo8 label, those labels represent adjacent emotion leaves, while in 15.5% of the cases they represented opposite leaves, most often the paris (\"anger\", \"fear\") and (\"anticipation\", \"surprise\").\n\nFigure  14:  Plutchik's Wheel of Emotions: ordinals of emotions. The outer numbers represent the ordinal of the leaf, the numbers within the upper central leaf the ordinals of the emotions within a leaf. E.g., \"joy\" = 0.66 and \"boredom\" = 5.33. The distance between them then becomes 3.33, being the sum of the distance between the leaves (3) and the \"intra-leaf\" distance (0.33).\n\nTo get a better idea of what Emo8 labels often appear together, we focused on images with 2 Emo8 labels, and plotted how often each emotion pair occurs. The result is shown in Figure  15 , demonstrating the pairs (\"Joy\", \"Anticipation\"), (\"Joy\", \"Trust) and (\"Aniticipation\", \"Trust\") make up the bulk of the pairs. As for opposite emotions, the pairs (\"Anticipation\", \"Surprise\") and (\"Anger\", \"Fear\") appear markedly more ofthen than (\"Joy\", \"Sadness\") and (\"Disgust\", \"Trust\"). To analyze the Arousal and Valence values, we compute the maximum distance between annotated values for both dimensions over all \"keep\" images. For Arousal, the average maximum distance is 2.7 ± 1.4, while for Valence this is 1.8 ± 1.2. This suggests that people agree much more on the Valence dimension, than they do on the Arousal dimension. This is confirmed when we compute the average maximum distance values as a function of the number of annotations for a given image, the result of which is shown in Figure  16 . For Arousal, a clear increasing maximum distance trend is visible with a stable standard deviation, going from ±1.75 to more than 4. For Valence annotations on the other hand, the maximum distance appears to plateau at close to 2.",
      "page_start": 18,
      "page_end": 21
    },
    {
      "section_name": "A.10 Extra Dataset Analysis",
      "text": "Histograms showing the distribution of Arousal, Valence and Ambiguity annotation values for the public dataset are depicted in Figure  17 . Annotation statistics per Emo24 emotion are collected in Table  4 . The table is made up of three rows, each row corresponding to a ring in Plutchik's Wheel of Emotions, from the top row corresponding to the outer (least intense) ring, to the bottom row corresponding to the inner (most intense) ring. The annotations follow this ordering, with average Arousal annotations consistently increasing from least to most intense emotion ring. Valence annotations follow suit, either increasing for positive emotions, or decreasing for negative emotions. The sole exception to this rule is center ring \"Disgust\" having a slightly lower average Valence rating (-1.62) than the inner ring \"Loathing\" (-1.57).",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "A.11 Unbalancedcrossentropyloss And Weightedmseloss",
      "text": "Table  5  compares baseline results obtained using CrossEntropyLoss vs. UnbalancedCrossEntropyLoss for Emo8 classification, and MSELoss vs. WeightedMSELoss for Arousal/Valence regression. In what follows, we detail the workings of UnbalancedCrossEntropyLoss and WeightedMSELoss. We observe that UnbalancedCrossEntropyLoss presents a clear benefit over CrossEntropyLoss for the classification problem under consideration, while WeightedMSELoss typically does not manage to positively influence model performance for regression problems.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "A.11.1 Unbalancedcrossentropyloss",
      "text": "As stated in the main text, UnbalancedCrossEntropyLoss (L UCE ) allows to give different weights to different misclassifications. E.g., it allows to penalize classifying a \"joy\" as a \"sadness\" image heavier than classifying it as \"anticipation\". It is defined as\n\nwith t the target class with predicted probability p t , h the class with the highest predicted probability p h , w t the weight of the target class, and w t,h the weight for misclassifying a sample of class t as class h. In case t = h, this reverts to regular CrossEntropyLoss.\n\nTo be able to use UnbalancedCrossEntropy loss, a distance needs to be defined between each pair of output classes. For the Emo8 task, we use the shortest number of leaves between two emotions. E.g., the distance between \"joy\" and \"surprise\" is 3, and the distance between \"joy\" and \"anger\" is 2.\n\nThe class weight w i for class i was computed according to\n\nwith N the total number of samples, N c the number of classes and N i the number of samples of class i.\n\nFinally, the weight w i,j for misclassifying a sample from class i as class j was computed as\n\nwith w i the weight for class i, w j the weight for class j and d i,j the distance between classes i and j.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "A.11.2 Weightedmseloss",
      "text": "WeightedMSELoss (L WMSE ) is a natural extension of the standard MSELoss to include class weights. Its mathematical formulation reads\n\nwith N the number of samples, w i the class weights as defined in Eq.2 and o i and t i the network output and target value for sample i respectively.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "A.12 Additional Baseline Results",
      "text": "Baseline results for train data are depicted in Figure  18 . Numerical baseline results on the train and test sets have been grouped in Tables  6  and 7  respectively. A breakdown per Emo8 for train and test sets is shown in Tables  8  and 9  respectively. For per class results, no Average Precision scores are reported, as we did not collect these.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "A.13 Additional Beyond Baseline Results",
      "text": "The   A numerical comparison of the baseline to the (overall best performing) \"Baseline+OIToFER\" model for all three tasks is included in Table  10 . From this, it is apparent that obtainable gains are architecture-dependent, with the VGG16 and ResNet50 architectures obtaining most gains and the DenseNet161, CLIP and DINOv2 architectures barely improving, regardless of task. Obtained gains are highest for the Emo8 task (absolute 13.7% AP gain, or relative 59%, for VGG16). Gains for the Arousal and Valence tasks are somewhat less straightforward to compare, as changes in MAE and Spearman R do not always agree. Compare, e.g., for ResNet50, for the Valence task an absolute 0.121 MAE and 0.120 Spearman R improvement, or relative 9.0% and 22.5% respectively, to an absolute 0.039 MAE and 0.087 Spearman R improvement, or relative 2.9% and 38% respectively for the Arousal task.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "A.14 A Note On The Fuzziness Of Emotion Recognition",
      "text": "As shown in §A.9, if there is disagreement between annotators concerning the emotion depicted in an image, then it is typically among similar emotions. So, although annotators often disagree, the different labels provided for a same image are far from random, instead showing clear tendencies toward a specific region of the emotion spectrum.\n\nThis fuzzyiness in assigned labels is a feature of human psychology. Emotion recognition is hard, nuanced, and multidimensional. With more raters, one would obtain a distribution of responses, but still no perfect agreement. To compound this issue, the estimate of the distribution per image would be poor unless one has many raters per image (tens of raters for tens of thousands of images!). This is, for many reasons not least of which financially, highly impractical.\n\nHaving one rater per image gives uncertainty if one is interested in a single image, but the average performance across many images is still meaningful. To demonstrate this, we perform the following experiment: for several architectures, we train an Emo8 prediction model on our dataset, and once trained, we let the model make predictions for each image in the training set. We train 5 models per lr 0 ∈ [10 -1 , 10 -2 , 10 -3 , 10 -4 ] using CrossEntropyLoss, and keep the one with the highest Weighted F1 score as the winner. For these models, we list in Table  11  how often the annotated emotion was ranked N (out of 8), and in Table  12  we show the distance between the top predicted and annotated emotions. Except for AlexNet, all other models show a nice downward sloping behavior as either the rank (Table  11 ) or distance (Table  12 ) increases. In other words, the \"mistakes\" made by these models are clearly not random, but show behavior that is similar to those observed in the human annotators. This confirms that even with a single annotation per image, already valuable results and insights can be obtained.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "A.15 Author Responsibility Statement",
      "text": "We, the authors, confirm that we bear all responsibility in case of any violation of rights during the collection of the data or other work, and that we will take appropriate action if and when needed, e.g., to remove data with such issues. We also confirm the licenses provided with the data and code associated with this work: an MIT license for all code; a CC BY-NC-SA 4.0 license for the dataset (concretely, the list of URLs and the annotations).\n\nIn particular, and as clearly and explicitly stated on our repository (under \"Legal Compliance and Privacy\"), we invite any rightful copyright holders or persons depicted in any of the images that do not want their work/likeness to be used within the context of this dataset to contact us, so that we can remove that specific material from the dataset.",
      "page_start": 31,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: Plutchik’s Wheel of Emotions.",
      "page": 3
    },
    {
      "caption": "Figure 2: We opted for this",
      "page": 3
    },
    {
      "caption": "Figure 3: Distribution of Emotion annotations for",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the distribution of annotations",
      "page": 5
    },
    {
      "caption": "Figure 4: Association between Valence and",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the association between Arousal and Valence annotations, indicating as expected a",
      "page": 5
    },
    {
      "caption": "Figure 5: Test data baseline performance on the Emo8 classification and Arousal and Valence",
      "page": 6
    },
    {
      "caption": "Figure 5: , with the graph for train data, and tables containing",
      "page": 7
    },
    {
      "caption": "Figure 6: Test data results for extensions beyond the baseline by applying late fusion with Facial",
      "page": 8
    },
    {
      "caption": "Figure 6: . Training",
      "page": 8
    },
    {
      "caption": "Figure 7: Figure 7: Logo for the FindingEmo dataset.",
      "page": 12
    },
    {
      "caption": "Figure 8: A screenshot of the annotation interface. Displayed photo by David Shankbone, source: WikiMedia.",
      "page": 15
    },
    {
      "caption": "Figure 9: These are the",
      "page": 17
    },
    {
      "caption": "Figure 9: Distribution of minutes taken to complete the task. The plot does not include 7 outliers.",
      "page": 17
    },
    {
      "caption": "Figure 10: The age distribution of the 653 users who shared that info is shown in",
      "page": 18
    },
    {
      "caption": "Figure 11: , indicating a large bias towards the early 20’s.",
      "page": 18
    },
    {
      "caption": "Figure 10: Distribution of country of origin of 651 annotators.",
      "page": 18
    },
    {
      "caption": "Figure 11: Distribution of age of 653 annotators.",
      "page": 18
    },
    {
      "caption": "Figure 12: shows how many images have been annotated with",
      "page": 19
    },
    {
      "caption": "Figure 12: Number of images with N different Emo8 and Emo24 labels. The y-axis is shared between",
      "page": 19
    },
    {
      "caption": "Figure 13: Number of images with a maximum distance D between their Emo8 and Emo24 labels,",
      "page": 19
    },
    {
      "caption": "Figure 13: shows the distribution of maximum distance between labels, for images annotated with more than one",
      "page": 19
    },
    {
      "caption": "Figure 14: No less than 42.8% of the times an image has been annotated with more",
      "page": 20
    },
    {
      "caption": "Figure 14: Plutchik’s Wheel of Emotions: ordinals of emotions. The outer numbers represent the",
      "page": 20
    },
    {
      "caption": "Figure 15: Prevalence of Emo8 label pairs for images annotated with 2 labels. The bigger the disc,",
      "page": 20
    },
    {
      "caption": "Figure 16: For Arousal, a clear increasing maximum distance trend is visible with",
      "page": 21
    },
    {
      "caption": "Figure 16: Distribution of maximum distance between Arousal and Valence annotations as a function",
      "page": 21
    },
    {
      "caption": "Figure 17: Annotation statistics per Emo24 emotion are collected",
      "page": 21
    },
    {
      "caption": "Figure 17: Distribution of Arousal, Valence and Ambiguity annotations. The y-axis is shared between",
      "page": 21
    },
    {
      "caption": "Figure 18: Numerical baseline results on the train and",
      "page": 23
    },
    {
      "caption": "Figure 19: Barcharts",
      "page": 23
    },
    {
      "caption": "Figure 18: Train data baseline classification performance on the Emo8 classification and",
      "page": 24
    },
    {
      "caption": "Figure 19: Training data results for extensions beyond the ImageNet baseline by applying late",
      "page": 30
    },
    {
      "caption": "Figure 20: Arousal regression results for extensions beyond the baseline by applying late fusion",
      "page": 31
    },
    {
      "caption": "Figure 21: Valence regression results for extensions beyond the baseline by applying late fusion",
      "page": 33
    }
  ],
  "tables": [
    {
      "caption": "Table 1: groups these dataset descriptions, together with ours, for easy comparison.",
      "page": 3
    },
    {
      "caption": "Table 1: Comparison of relevant datasets. “V/A/D” indicates which of the Valence, Arousal and",
      "page": 4
    },
    {
      "caption": "Table 2: , we group average annotation values",
      "page": 5
    },
    {
      "caption": "Table 2: Average Arousal, Valence and Ambiguity annotation values for the public set, per Plutchik",
      "page": 5
    },
    {
      "caption": "Table 3: shows how many images have been",
      "page": 18
    },
    {
      "caption": "Table 3: Number of images (“# imgs.”) that have been annotated by N different annotators (“# ants.”).",
      "page": 19
    },
    {
      "caption": "Table 4: The table is made up of three rows, each row corresponding to a ring in Plutchik’s",
      "page": 21
    },
    {
      "caption": "Table 4: Average Arousal, Valence and Ambiguity annotation values for the public set, per emotion. Emotions are grouped per “ring” in Plutchik’s Wheel of",
      "page": 22
    },
    {
      "caption": "Table 5: compares baseline results obtained using CrossEntropyLoss vs. UnbalancedCrossEntropyLoss",
      "page": 23
    },
    {
      "caption": "Table 5: Loss comparison for Emo8 classification and Arousal/Valence regression tasks, comparing test results for baseline models. Performance metrics format:",
      "page": 25
    },
    {
      "caption": "Table 6: Training results: Emo8 classification and Arousal/Valence regression performance for baseline models. Performance metrics format: .xxxyy should be read",
      "page": 26
    },
    {
      "caption": "Table 7: Test results: Emo8 classification and Arousal/Valence regression performance for baseline models. Performance metrics format: .xxxyy should be read as",
      "page": 27
    },
    {
      "caption": "Table 8: Training results: Emo8 Recall, Precision and F1 metrics per emotion leaf for baseline models.",
      "page": 28
    },
    {
      "caption": "Table 9: Test results: Emo8 Recall, Precision and F1 metrics per emotion leaf for baseline models.",
      "page": 29
    },
    {
      "caption": "Table 10: From this, it is apparent that obtainable gains are",
      "page": 30
    },
    {
      "caption": "Table 11: how often the annotated emotion was",
      "page": 30
    },
    {
      "caption": "Table 12: we show the distance between the top predicted and annotated",
      "page": 30
    },
    {
      "caption": "Table 11: ) or distance (Table 12) increases. In other words, the “mistakes” made by these",
      "page": 30
    },
    {
      "caption": "Table 10: Baseline vs. +OIToFER: A comparison of Emo8 classification and Arousal/Valence",
      "page": 32
    },
    {
      "caption": "Table 11: Percentage of times, with respect to the full training set, the annotated emotion was ranked",
      "page": 32
    },
    {
      "caption": "Table 12: Percentage of times, with respect to the full training set, the distance between the annotated",
      "page": 32
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification"
    },
    {
      "citation_id": "2",
      "title": "International evaluation of an ai system for breast cancer screening",
      "authors": [
        "S Mckinney",
        "M Sieniek",
        "V Godbole",
        "J Godwin",
        "N Antropova",
        "H Ashrafian",
        "T Back",
        "M Chesus",
        "G Corrado",
        "A Darzi",
        "M Etemadi",
        "F Garcia-Vicente",
        "F Gilbert",
        "M Halling-Brown",
        "D Hassabis",
        "S Jansen",
        "A Karthikesalingam",
        "C Kelly",
        "D King",
        "J Ledsam",
        "D Melnick",
        "H Mostofi",
        "L Peng",
        "J Reicher",
        "B Romera-Paredes",
        "R Sidebottom",
        "M Suleyman",
        "D Tse",
        "K Young",
        "J Fauw",
        "S Shetty"
      ],
      "year": "2020",
      "venue": "Nature (London)"
    },
    {
      "citation_id": "3",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "5",
      "title": "Emotions as computations",
      "authors": [
        "A Emanuel",
        "E Eldar"
      ],
      "year": "2023",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "7",
      "title": "Discrete emotions or dimensions? the role of valence focus and arousal focus",
      "authors": [
        "L Barrett"
      ],
      "year": "1998",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "8",
      "title": "Do discrete emotions exist?",
      "authors": [
        "L Barrett",
        "M Gendron",
        "Y.-M Huang"
      ],
      "year": "2009",
      "venue": "Philosophical Psychology"
    },
    {
      "citation_id": "9",
      "title": "On the importance of both dimensional and discrete models of emotion",
      "authors": [
        "E Harmon-Jones",
        "C Harmon-Jones",
        "E Summerell"
      ],
      "year": "2017",
      "venue": "Behavioral Sciences"
    },
    {
      "citation_id": "10",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology: A Journal for Diverse Perspectives on Diverse Psychological Issues"
    },
    {
      "citation_id": "12",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1970",
      "venue": "California Mental Health Research Digest"
    },
    {
      "citation_id": "13",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "A general psychoevolutionary theory of emotion"
    },
    {
      "citation_id": "14",
      "title": "What are emotions? and how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "SOCIAL SCIENCE INFORMATION SUR LES SCIENCES SOCIALES"
    },
    {
      "citation_id": "15",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "16",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "17",
      "title": "Robust facial expression recognition via compressive sensing",
      "authors": [
        "S Zhang",
        "X Zhao",
        "B Lei"
      ],
      "year": "2012",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "Joint pose and expression modeling for facial expression recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Iexpressnet: Facial expression recognition with incremental classes",
      "authors": [
        "J Zhu",
        "B Luo",
        "S Zhao",
        "S Ying",
        "X Zhao",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "MM 2020 -Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Facial emotion recognition with noisy multi-task annotations",
      "authors": [
        "S Zhang",
        "Z Huang",
        "D Paudel",
        "L Van Gool"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition with noisy multi-task annotations"
    },
    {
      "citation_id": "21",
      "title": "A study on computer vision for facial emotion recognition",
      "authors": [
        "Z Huang",
        "C Chiang",
        "J Chen",
        "Y Chen",
        "H Chung",
        "Y Cai",
        "H Hsu"
      ],
      "year": "2023",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "22",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "23",
      "title": "Beyond the face: how context modulates emotion processing in frontotemporal dementia subtypes",
      "authors": [
        "F Kumfor",
        "A Ibañez",
        "R Hutchings",
        "J Hazelton",
        "J Hodges",
        "O Piguet"
      ],
      "year": "2018",
      "venue": "Brain"
    },
    {
      "citation_id": "24",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "arxiv": "arXiv:2003.13401"
    },
    {
      "citation_id": "25",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Emosec: Emotion recognition from scene context",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "D Yang",
        "S Huang",
        "S Wang",
        "Y Liu",
        "P Zhai",
        "L Su",
        "M Li",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Computer Vision -ECCV 2022"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "29",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings -3rd IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "31",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "32",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Neural networks"
    },
    {
      "citation_id": "33",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "35",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "36",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Emotion schemas are embedded in the human visual system",
      "authors": [
        "P Kragel",
        "M Reddan",
        "K Labar",
        "T Wager"
      ],
      "year": "2019",
      "venue": "Science Advances"
    },
    {
      "citation_id": "40",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision"
    },
    {
      "citation_id": "41",
      "title": "Vision transformers need registers",
      "authors": [
        "T Darcet",
        "M Oquab",
        "J Mairal",
        "P Bojanowski"
      ],
      "year": "2023",
      "venue": "Vision transformers need registers"
    },
    {
      "citation_id": "42",
      "title": "Yolov3: An incremental improvement",
      "authors": [
        "J Redmon",
        "A Farhadi"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "43",
      "title": "Lightnet: Building blocks to recreate darknet networks in pytorch",
      "authors": [
        "T Ophoff"
      ],
      "year": "2018",
      "venue": "Lightnet: Building blocks to recreate darknet networks in pytorch"
    }
  ]
}