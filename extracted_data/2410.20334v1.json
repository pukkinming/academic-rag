{
  "paper_id": "2410.20334v1",
  "title": "Improving Speech-Based Emotion Recognition With Contextual Utterance Analysis And Llms",
  "published": "2024-10-27T04:23:34Z",
  "authors": [
    "Enshi Zhang",
    "Christian Poellabauer"
  ],
  "keywords": [
    "speech emotion recognition",
    "automatic speech recognition",
    "large language models",
    "foundation models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) focuses on identifying emotional states from spoken language. The 2024 IEEE SLT-GenSEC Challenge on Post Automatic Speech Recognition (ASR) Emotion Recognition 1 tasks participants to explore the capabilities of large language models (LLMs) for emotion recognition using only text data. We propose a novel approach that first refines all available transcriptions to ensure data reliability. We then segment each complete conversation into smaller dialogues and use these dialogues as context to predict the emotion of the target utterance within the dialogue. Finally, we investigated different context lengths and prompting techniques to improve prediction accuracy. Our best submission exceeded the baseline by 20% in unweighted accuracy, achieving the best performance in the challenge. All our experiments' codes, prediction results, and log files are publicly available. 2",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "One of the key tasks in natural language processing (NLP) and affective computing is Speech-based Emotion Recognition (SER), which focuses on identifying emotional cues in spoken language  [1] . Developing advanced algorithms to detect emotions in speech has numerous applications, particularly in the healthcare field. For example, previous work has shown that patients with mental health conditions often exhibit specific emotional patterns in their speech  [2, 3, 4] , which provides opportunities for speech-based psychological assessments and diagnosis.\n\nCreating an effective SER system requires a large amount of labeled emotional speech data. While there is plenty of speech data available, much of it lacks labels. The typical method to label this data is through human annotation, but this process has drawbacks. Emotions are subjective and can lead to biased labels. Even when using domain experts or consensus among multiple annotators to improve reliability, the process is still time-consuming and costly  [5, 6, 7] . Large language models (LLMs) offer another solution by automatically annotating emotional speech. Trained on extensive text datasets, LLMs are well-suited for tasks involving ASR  [8]  and emotion recognition.\n\nIn this paper, we adhere to the requirements and motivations outlined in the GenSEC Task 3  [9]  to investigate the performance of large language models (LLMs) in emotion recognition using only the post-ASR text from the IEMOCAP dataset  [10] . By analyzing the curated datasets, we aim to showcase the advantages of our approach for this challenge based on the following attributes:\n\n• ASR refinement: Due to significant variations in output transcriptions from different ASR models, it is necessary to combine, filter, and refine these outputs to obtain the most informative transcription possible as input data.\n\n• Dialogues as context: Unlike the baseline approach, we observed that each session (conversation) consists of smaller dialogue fragments (scripts). This characteristic is present in both the training and testing datasets. Therefore, using these smaller scripts as context to predict the emotion of the target utterance could be more efficient due to relevance and coherence.\n\n• Prompt structure: LLMs can consider the entire conversation as context, unlike conventional methods that often treat each sentence separately. While increasing context length could enhance performance, it is essential to determine the optimal length to avoid resource waste. Additionally, the structure of the prompts given to the model -comprising background information, context, current sentences, and tasks -plays a significant role. Exploring prompt engineering techniques can help identify the most effective approach for enhancing prediction accuracy.\n\nFig.  1 . The workflow of our approach.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Extensive prior research has explored the capabilities of LLMs for emotion recognition, often using a multimodal approach. Starting with text data for annotation and downstream SER tasks, many studies found that integrating acoustic features improves performance. For instance, the work in  [11]  benchmarked SER performance using ASR transcripts with varying Word Error Rates (WERs) and explored fusion techniques. They discovered that SER can tolerate relatively low WER, especially in real-world scenarios, and that bimodal SER with transcripts containing about 10% errors can perform comparably to those with ground-truth text, particularly with Whisper models. They also proposed an ASR error-robust framework integrating ASR error correction and modality-gated fusion. The work in  [12]  investigated using foundational models to automate SER from transcription and annotation. They used these models for emotion annotation, combining outputs from multiple LLMs and incorporating human feedback to reduce bias. Their experiments showed that using both speech and text led to higher accuracy compared to using speech alone. Additionally, LLM-generated transcriptions, such as those from Whisper-large V2, were competitive with humantranscribed ground truth. The highest prediction accuracy was achieved by combining LLM outputs and incorporating human feedback to enhance zero-shot capabilities. Similarly, the work in  [1]  demonstrated that LLMs could effectively annotate emotional speech through prompted transcriptions. Their SER experiments using these annotations achieved competitive results compared to ground truth labels. They also explored providing both conversation text and acoustic features to LLMs for annotations. In  [5] , the authors examined the potential of LLMs to annotate large volumes of speech data. By evaluating single-shot and few-shot scenarios using the IEMOCAP, MSP-IMPROV, and MELD datasets, they observed performance variability in SER. They developed a technique using ChatGPT to annotate publicly available data for training data augmentation, which improved performance on downstream SER tasks.\n\nThere are also studies that focus exclusively on text-based methods. The work in  [13]  investigated LLMs for annotating emotional speech data to reduce the human burden. They extracted text transcripts using ASR, then fed the transcriptions into pre-trained LLMs to infer weak emotion labels using an engineered prompt and a predetermined, finer-grained emotion taxonomy. These labels were used to pre-train an SER model via weakly-supervised learning, significantly improving SER performance on unlabeled data and enhancing label efficiency. In  [14] , the authors evaluated the prompting and sensitivity of the foundation model ChatGPT to determine how different prompting templates and sampling sensitivity affect the correctness and helpfulness of the responses. They conducted three affective computing tasks: sentiment analysis, toxicity detection, and sarcasm detection. By evaluating various prompting techniques, they demonstrated that straightforward prompts and assigning expert identity yield near-best performances. They also highlighted that irrelevant expertise or misaligned incentives can severely harm results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "For this emotion recognition task, we use the IEMOCAP dataset. It contains dialogues from 10 actors (5 male and 5 female) divided into five extensive sessions to convey target emotions. The complete dataset includes audio of both scripted and spontaneous dialogue scenarios, visual data capturing facial expressions and hand movements, and transcribed text. The emotions are labeled discretely as happy, anger, sadness, frustration, and neutral, as well as categorically on scales of valence, arousal, and dominance. Human transcribers and annotators have labeled and validated these as ground truth. For this challenge, we use only the post-ASR texts to predict emotions for each utterance, with the target output emotion being neutral, sad, happy, or angry.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Before starting our analysis, we thoroughly reviewed the challenge rules and closely examined both the training and evaluation data provided. The organizers had already curated the training and testing data in JSON format for direct use. We identified several key findings that informed our proposed methods. We discuss each of these findings in detail below (also see Figure  1 ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transcription Refinement",
      "text": "By examining several data entries in the training and testing datasets, we observed significant variation in the transcriptions provided by different ASR models. Table  1  shows a sample data entry from the training set, which includes the ground truth transcription. As illustrated, some models provide extremely limited information. Inspired by previous work  [15] , we conducted a Word Error Rate (WER) analysis on the training dataset for this challenge (Table  2 ), revealing inconsistencies across the models. To address this issue, we designed a straightforward algorithm (Algorithm 1), that, for each data entry, filtered out any transcriptions that are too short to provide enough information. If all transcriptions are short, we kept them all. Next, we refined the remaining transcriptions using ChatGPT 3.5. This refinement process selects the most coherent and logically structured transcription. If no clear choice is possible, we select the longest transcription, as it likely contains the most information. This process generates a new key for each data entry, which we call 'ensemble.' We expect this approach to improve data quality for subsequent predictions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Script-Based",
      "text": "Both the training and testing data are based on five main sessions (conversations). The baseline proposed by the organiz- Table  3 . The various prompts tested on the training dataset. Each prompt includes background information, context information (previous utterances up to the defined context length), the current sentence (current speaker and current sentence), and the task (what to predict and the rules to follow).\n\n3, we would consider utterances 01-05-02, 01-05-01, and 01-04-20 as context. However, 01-04-20 belongs to the previous script, which could contain irrelevant context and potentially harmful to the prediction's accuracy. Therefore, we limit the context to previous utterances within the same script. In this case, we would only use 01-05-02 and 01-05-01 as context, even with context length of 3. It ensures that all utterances within the same script will not interfere with predictions in the next script.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prompting",
      "text": "Following the baseline method, we utilize ChatGPT through the official API 3 . ChatGPT's sensitivity to prompt variability can lead to ambiguous and erroneous results with slight changes to the prompt content. This is particularly challenging because human emotions in short dialogues, like those in our training and evaluation datasets, often exhibit significant variability  [5] . To determine the most effective prompting techniques for this task, we conducted extensive experiments 3 https://platform.openai.com/docs/api-reference/ introduction inspired by previous research  [5, 14, 1] . We explored the impact of mentioning subject-matter expertise in the prompts and tested the effects of mentioning irrelevant expertise or incentives to provide correct responses. Additionally, we included additional details to structure the answer and utilized a step-by-step approach known as Chain-of-Thought (CoT). Another approach combined extra details, step-by-step thinking, and mentioned penalties for incorrect answers.\n\nThe prompt templates are in Table  3 . Each prompt includes background information, context (previous sentences up to the defined context length), the current sentence (current speaker and current sentence), and the task (predictions and requirements). This suite of prompts allows for a comprehensive evaluation of how prompt design influences the performance of LLMs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Strategy",
      "text": "In this challenge, only the training data contains accurate transcriptions and emotion labels. The performance on the testing dataset will only be available after submitting the predicted   4 . Summary of all methods tested. Methods highlighted in blue were applied to the test set. The method highlighted in blue and bold is our best performing one, utilizing GPT-4.0, while all other methods, including the baseline, are based on GPT-3.5-turbo. labels. Given the limited number of allowed submissions, we first conducted all our experiments on the training set. Initially, here is our plan: after the first submission, we will assess whether the accuracy improves by switching from 'whispertiny' to 'w2v2960largeself'. If so, we are moving in the right direction by improving WER on input data. Then, we can try other methods that have higher unweighted accuracy (UA) than the approach used in the first submission.\n\nHowever, a few considerations came to mind. First, the WER may differ between the train and test set. From the training set, we have observed significant variations in transcriptions from ASR models across data entries. This may also hold true for the test set. Therefore, we choose to use the methods that takes 'ensemble' as input for subsequent submissions, even though it lowered the UA between experiments 2 and 4 and between 3 and 5. Second, each data entry in the train and test set slightly differs in how each conversation was divided into smaller 'scripts.' For instance, in the training set, the data entries with id 'Ses01F script01 1 F000' and 'Ses01F script01 2 F000' are both considered under script01, even though they belong to different subsets of the same script. In the test set, the data entry have ID such as 'Ses01Z 01 F000', which clearly belong to script 01, so 'Ses01Z 02 F000' belongs to the second script. There are no further subsets. This difference could impact our prediction because our approach processes each script individually rather than the more nuanced subsets of the script. This may also explain why the prediction only slightly improved when we switched from a session-based to a script-based approach between experiments 2 and 3 and 4 and 5. Based on these considerations, we use the method that takes 'ensemble' as input for subsequent submissions, even though it lowered the UA between experiments 2 and 4 and between 3 and 5. Based on these considerations, we use the 'ensemble' transcription as input and a script-based approach for subsequent experiments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Metrics",
      "text": "For evaluation, we used unweighted accuracy (UA), as required by the challenge. Additionally, since we have the ground truth for the training data, we employed the F1 score to assess the model's performance in predicting different emotional labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "Table  4  presents all the methods we tested, including the one used to reproduce the baseline method. In total, we conducted 13 experiments. Each method consists of three components: the transcription used, the prompt and context length, and whether the entire session or a shorter script was used as context. In our first submission, the only difference from the given baseline was the transcription used. We selected the transcription from 'w2v2960largeself', which had the lowest WER among all options. This improved all metrics on the training set and already outperformed the baseline on the testing set. Recognizing the importance of accurate transcrip-tion, we adopted the transcription refinement approach (Section 4.1), which further improved our results.\n\nNext, we applied the script-based approach (Section 4.2), which slightly enhanced performance. Combining this with the ensemble approach, we confirmed that the script method outperformed the session-based approach, so we adhered to the script approach.\n\nWe then focused on prompting techniques. Initially, we found that increasing context length improved performance. Keeping other prompt components the same, we tested context lengths of 3 (baseline), 5, 10, and 15. There was a steady increase in accuracy until 10, with no significant improvement from 10 to 15. Thus, we settled on using a context length of 10. Following the prompting techniques discussed in Section 4.3, we conducted four more experiments. However, they all performed worse compared to the baseline prompt.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "In our experiments, we found that the best results were obtained by using refined transcription with a baseline prompt of context length 10 and using the script as context to predict the emotion of the target utterance. Based on the WER analysis, we chose the ASR output by 'w2v960largeself', which had the lowest overall WER of 0.19, compared to the 'whispertiny' with an overall WER of 0.44. The unweighted accuracy increased from 0.446 to 0.511 on the training set. The results improved further when we used an 'ensemble' approach to filter, combine, and refine all available transcriptions, emphasizing the importance of WER in the task of SER.\n\nSwitching from a session-based approach to a scriptbased approach has resulted in a slight improvement in our prediction accuracy between experiments 2 and 3, as well as between experiments 4 and 5. We anticipate that this change will have a greater impact on the testing set since it does not have subsets of scripts like the training set data.\n\nThe role of context length in prediction performance is vital. Simply increasing the context length from 3 to 5 and then from 5 to 10 significantly improved accuracy. This finding helps us understand context length's impact on prediction accuracy. It also highlighted LLMs' ability to use longer context for making predictions, compared to the conventional method that treats each sentence in a sequence of conversations separately. However, it is essential to notice that performance does not improve when we increase context length from 10 to 15. The reasons could be due to the data structure (short dialogues in general). For instance, this could change when working on monologues or reading-based datasets. It is essential to be aware of the limitations of LLMs when using extended context to make predictions.\n\nBy assigning different roles, incentives, and penalty information, we explored different prompting techniques to see if LLMs are more sensitive to a specific prompt. However, the simple baseline prompt performed better. This result differed from previous approaches  [14]  investigating different prompt effectiveness on sentiment analysis.\n\nFrom the F1 scores on training sets, we found that utterances with the emotion label 'happy' are the hardest to predict compared to others. Anger is the easiest to predict. Models have difficulty distinguishing between neutral and sad and between neutral and happy. These emotional shifts during a conversation are still challenging for LLMs to detect through text alone.\n\nIn our latest experiment, we utilized the GPT 4.0 for the first time due to limited resources available. By applying our best method on the test set, the unweighted accuracy improved from 0.694 to 0.752. From the feedback after submission, we observed an improvement in predictions for all emotion labels except for the 'happy' emotion. In comparison to the GPT 3.5 turbo model, the GPT 4.0 model tends to classify more 'happy' instances as 'neutral.' Despite this, the overall performance of the GPT 4.0 model is much better.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "SER is an essential area of study that has significantly contributed to various multimodal analyses. In this challenge, participants were required to use post-ASR transcriptions and LLMs to predict the emotion conveyed in each utterance, categorizing each as neutral, sad, happy, or angry. The dataset was organized by the order of the session (conversation).\n\nIn our approach, we applied a refinement algorithm to all ASR transcriptions to select the most informative one as input. We also identified that each conversation could be broken down into smaller dialogues (scripts). Using these scripts as context, we predicted the emotion of the target utterance. We explored using different context lengths and prompting techniques in addition to these two strategies to improve prediction accuracy further. We developed a method that achieved higher unweighted accuracy than the baseline using the Chat-GPT 3.5 turbo model. The result was further enhanced when we applied the same method to ChatGPT 4.0.\n\nThe results confirmed that LLMs can effectively consider more extended contexts preceding the current sentence to predict emotions, unlike conventional methods that often treat each sentence separately. Additionally, we found that the emotion of anger is the easiest for LLMs to predict accurately while distinguishing other emotions, particularly happy and neutral, can be more challenging.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The workflow of our approach.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Summary of all methods tested. Methods highlighted in blue were applied to the test set. The method highlighted",
      "data": [
        {
          "Methods": "",
          "F1 score (train)": "Neutral\nSadness\nHappy\nAnger",
          "UA (train)": "",
          "UA (test)": ""
        },
        {
          "Methods": "1. whispertiny + baseline 3 + session-based (baseline)\n2. w2v2960largeself + baseline 3 + session-based\n3. w2v2960largeself + baseline 3 + script-based",
          "F1 score (train)": "0.44\n0.37\n0.23\n0.61\n0.43\n0.50\n0.40\n0.66\n0.43\n0.50\n0.40\n0.67",
          "UA (train)": "0.446\n0.511\n0.514",
          "UA (test)": "0.551\n0.575\n–"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Large language model-based emotional speech annotation using context and acoustic feature for speech emotion recognition",
      "authors": [
        "Jennifer Santoso",
        "Kenkichi Ishizuka",
        "Taiichi Hashimoto"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Speech-based automatic depression detection via biomarkers identification and artificial intelligence approaches",
      "authors": [
        "Fuxiang Tao"
      ],
      "year": "2024",
      "venue": "Speech-based automatic depression detection via biomarkers identification and artificial intelligence approaches"
    },
    {
      "citation_id": "4",
      "title": "Topic modeling based multi-modal depression detection",
      "authors": [
        "Yuan Gong",
        "Christian Poellabauer"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th annual workshop on Audio/Visual emotion challenge"
    },
    {
      "citation_id": "5",
      "title": "Language and mental health: Measures of emotion dynamics from text as linguistic biosocial markers",
      "authors": [
        "Daniela Teodorescu",
        "Tiffany Cheng",
        "Alona Fyshe",
        "Saif M Mohammad"
      ],
      "year": "2023",
      "venue": "Language and mental health: Measures of emotion dynamics from text as linguistic biosocial markers",
      "arxiv": "arXiv:2310.17369"
    },
    {
      "citation_id": "6",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "Siddique Latif",
        "Muhammad Usama",
        "Ibrahim Malik",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "arxiv": "arXiv:2307.06090"
    },
    {
      "citation_id": "7",
      "title": "Listen, think, and understand",
      "authors": [
        "Yuan Gong",
        "Hongyin Luo",
        "Alexander Liu",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": "2023",
      "venue": "Listen, think, and understand",
      "arxiv": "arXiv:2305.10790"
    },
    {
      "citation_id": "8",
      "title": "The mersa dataset and a transformer-based approach for speech emotion recognition",
      "authors": [
        "Enshi Zhang",
        "Rafael Trujillo",
        "Christian Poellabauer"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Automatic speech recognition",
      "authors": [
        "Dong Yu",
        "Lin Deng"
      ],
      "year": "2016",
      "venue": "Automatic speech recognition"
    },
    {
      "citation_id": "10",
      "title": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "authors": [
        "Chao-Han Huck",
        "Taejin Yang",
        "Yuan Park",
        "Yuanchao Gong",
        "Zhehuai Li",
        "Yen-Ting Chen",
        "Chen Lin",
        "Yuchen Chen",
        "Kunal Hu",
        "Piotr Dhawan",
        "Żelasko"
      ],
      "year": "2024",
      "venue": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques",
      "arxiv": "arXiv:2406.08353"
    },
    {
      "citation_id": "13",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Lanser: Language-model supported speech emotion recognition",
      "authors": [
        "Taesik Gong",
        "Josh Belanich",
        "Krishna Somandepalli",
        "Arsha Nagrani",
        "Brian Eoff",
        "Brendan Jou"
      ],
      "year": "2023",
      "venue": "Lanser: Language-model supported speech emotion recognition",
      "arxiv": "arXiv:2309.03978"
    },
    {
      "citation_id": "15",
      "title": "On prompt sensitivity of chatgpt in affective computing",
      "authors": [
        "M Mostafa",
        "Björn Amin",
        "Schuller"
      ],
      "year": "2024",
      "venue": "On prompt sensitivity of chatgpt in affective computing",
      "arxiv": "arXiv:2403.14006"
    },
    {
      "citation_id": "16",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}