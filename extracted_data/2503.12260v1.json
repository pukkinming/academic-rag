{
  "paper_id": "2503.12260v1",
  "title": "Enhancing Facial Expression Recognition Through Dual-Direction Attention Mixed Feature Networks And Clip: Application To 8Th Abaw Challenge",
  "published": "2025-03-15T21:03:03Z",
  "authors": [
    "Josep Cabacas-Maso",
    "Elena Ortega-Beltrán",
    "Ismael Benito-Altamirano",
    "Carles Ventura"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present our contribution to the 8th ABAW challenge at CVPR 2025, where we tackle valence-arousal estimation, emotion recognition, and facial action unit detection as three independent challenges. Our approach leverages the well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all three tasks, achieving results that surpass the proposed baselines. Additionally, we explore the use of CLIP for the emotion recognition challenge as an additional experiment. We provide insights into the architectural choices that contribute to the strong performance of our methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotion recognition has emerged as a pivotal area of research within affective computing, driven by its potential applications in fields ranging from human-computer interaction to psychological research and clinical diagnostics. Since Ekman's classification of human expression faces into emotions  [6] , many studies have emerged in recent years. Calvo et al.  [3] , Baltrusaities et al.  [1] , or Kaya et al.  [8]  laid the foundational framework for understanding facial expressions as a window into emotional states. In contemporary research, Liu et al.  [26]  and Kim et al.  [9]  continued to refine and expand these methodologies, by synthesizing insights from cognitive psychology, computer vision, and machine learning, researchers have made significant strides in enhancing the accuracy and applicability of facial emotion recognition systems. In addition, the integration of the dimensions of valence and arousal  [31, 32]  added depth to the interpretation of emotional states, allowing more nuanced insight into human affective experiences.\n\nAction unit detection  [5, 27]  complemented these efforts by parsing facial expressions into discrete muscle movements, facilitating a finer-grained analysis of emotional expressions across cultures and contexts. Such advancements not only improved the reliability of automated emotion recognition systems, but also opened the possibility to personalize affective computing applications in fields such as mental health monitoring  [28]  or user experience design  [33] .\n\nTo tackle all these challenges, researchers have explored innovative architectures such as the DDAMFN (Dual-Direction Attention Mixed Feature Network)  [34] . This novel approach integrates attention mechanisms  [7]  and mixed feature extraction  [4] , enhancing the network's ability to capture intricate details within facial expressions.\n\nThere is an increasing need to develop machines capable of understanding and appropriately responding to human emotions in real-world, day-to-day applications. Addressing this challenge, a series of competitions titled Affective Behavior Analysis in-the-Wild (ABAW) has been organized  [10-16, 19-25, 32] . For the 8th ABAW  [17, 18]  challenge at CVPR 2025, six competitions were introduced: Valence-Arousal (VA) Estimation, Expression (EXPR) Recognition, Action Unit (AU) Detection, Compound Expression (CE) Recognition, Emotional Mimicry Intensity (EMI) Estimation, and Ambivalence/Hesitancy (AH) Recognition. In this work, we present our approach to the VA Estimation, EXPR Recognition, and AU Detection challenges, where we adapted the DDAMFN architecture for each challenge and additionally leveraged the CLIP embedding space to enhance emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Curation",
      "text": "For the 8th ABAW challenge, the organization provided the following datasets: • For VA Estimation: An augmented version of the Aff-Wild2 database  [24]  will be used. This audiovisual (A/V), in-the-wild dataset consists of 594 videos, totaling approximately 3 million frames, with annotations for valence and arousal. As outlined in the dataset guidelines, we preprocessed the data by filtering out frames that contained annotation values outside the specified acceptable ranges. Specifically, any frames with valence/arousal values of -5, expression values of -1, or action unit (AU) values of -1 were excluded from consideration in our analysis. This process ensured that only frames with valid annotations were retained for model training and evaluation. The filtering criteria and the number of frames removed are summarized in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Annotation Range Invalid",
      "text": "Valence/Arousal [0, 1] -5 Expressions {0, 1} -1 Action Units (AUs) {0, 1} -1\n\nWe applied rigorous filtering to the train-validation splits across three tasks: VA estimation, expression recognition, and AU detection. A summary of the dataset before and after filtering is shown in Table  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2. Network Architecture",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2.2.1. Dual-Direction Attention Mixed Feature Network",
      "text": "For the 8th ABAW challenge, we adapted the Dual-Direction Attention Mixed Feature Network (DDAMFN)  [34]  for three separate tasks. Each task has its own fully-connected layer at the end of the network: one for valence-arousal prediction with 2 output units, another for emotion recognition with 8 output units, and a third for action unit prediction with 12 output units.   1  shows a diagram of the network, which features a base MobileFaceNet (MFN)  [4]  architecture for feature extraction, followed by a Dual-Direction Attention (DDA) module-with two attention heads-and a Global Depthwise Convolution (GDConv) layer. The output of the GD-Conv layer is reshaped and fed into the corresponding fullyconnected layers for each individual task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contrastive Language-Image Pretraining",
      "text": "In addition to leveraging the Dual-Direction Attention Mixed Feature Network (DDAMFN) for the 8th ABAW challenge, we also employed the CLIP (Contrastive Language-Image Pretraining) model. CLIP  [29]  is a widely recognized Vision-Language model trained on millions of image-text pairs, where the text consists of image captions, using a contrastive learning approach. In essence, CLIP learns two types of embeddings: one for images and one for text. The model is trained to map matching image-text pairs to similar points in the embedding space, based on cosine similarity, while simultaneously pushing apart the embeddings of mismatched image-text pairs.\n\nWe followed the approach of  [2] , where they built on the CLIP embedding space by adding two fully connected layers on top of the image encoder to allow deformations of the embedding space. We then used a Contrastive loss with cosine similarity with a collection of text prompts. During training, both the CLIP image and text encoders are frozen, preserving the learned features while adapting the embedding space for sentiment analysis tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ddamfn",
      "text": "Initially, the DDAMFN pretrained model was obtained from the stock versions available in the source code repository of the original work, which had been trained on the AffectNet-  8 dataset  [34] . In our experiments, we preserved the pretrained weights for the feature extraction layers, attention mechanisms, and the GDConv layer, as these components had already been optimized for extracting meaningful features from the input data. However, the fully-connected layers were reinitialized with random weights to facilitate the adaptation of the model to our specific task.\n\nTo enhance the model's performance across various challenges, we introduced a custom classifier for each individual task. These classifiers were designed to address the unique requirements of each challenge, allowing for more specialized learning. Each classifier was trained independently and separately, with the goal of adapting the pretrained feature extraction and attention components to the specific characteristics of the new tasks. This strategy ensured that the model could benefit from the strong generalization capabilities of the DDAMFN architecture while still tailoring its output to the specific demands of each challenge.\n\nBy training each custom classifier separately, we were able to achieve a fine-tuned balance between leveraging pretrained knowledge and optimizing the model for each individual task, leading to improved overall performance in the experiments.\n\nLoss functions were calculated following these criteria:\n\n• For valence-arousal prediction, the loss function was calculated using the Concordance Correlation Coefficient (CCC). CCC is a measure that evaluates the agreement between two time series by assessing both their precision (how well the observations agree) and accuracy (how well the observations match the true values). • For emotion recognition, was cross-entropy, which is commonly employed in classification tasks to measure the difference between the predicted probability distribution and the true distribution. • For action unit (AU) detection, the binary cross-entropy loss was used, which is suitable for binary classification tasks, measuring the difference between the predicted probability and the actual binary outcome for each action unit.\n\nFurthermore, for the Action Unit task, a global threshold of 0.5 was initially tested across all AUs, followed by individual optimization for each AU  [30] .\n\nHowever, after this initial round of experiments, we observed that the model could benefit from further improvements in capturing the temporal coherence between consecutive frames. To address this, we decided to replace the fully connected layers with a Long Short-Term Memory (LSTM) network. The LSTM was specifically chosen for its ability to retain and model long-term dependencies in sequential data, enabling the model to maintain temporal coherence across frames.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Clip Contrastive",
      "text": "Following the work done in  [2]  we used the same architecture and loss. For the CLIP Contrastive model (Fig.  2 ), we introduced two fully connected layers on top of the image encoder to allow adjustments to the embedding space. We then used Contrastive loss with cosine similarity against a set of text prompts. During training, both the CLIP image and text encoders are kept frozen. A unique prompt was generated for each emotion based on the structure \"a face showing emotion.\"\n\nIn the CLIP Contrastive architecture, we added a fully connected layer with 512 units and ReLU activation on top of the CLIP image encoder, followed by another fully connected layer with 512 units and a linear activation function before the loss function. The linear activation function was selected to retain negative activations, as the original CLIPtrained embedding space contains both negative and positive values.\n\nTo train the model, we employed the same contrastive loss function used in the original CLIP approach  [29] . Specifically, given a batch of N image-text pairs (I i , T i ), i = 1, . . . , N , the losses for the image and text are computed as follows:\n\n)\n\nwhere ⟨I ei , T ej ⟩ is the cosine similarity between the embedding vectors of the i-th image sample I ei and the j-th text sample T ej , and ⟨I ei , T ei ⟩ is the cosine similarity between the i-th image sample and its corresponding text caption. The contrastive loss L CO is computed as:\n\nThis loss drives the model to enhance the similarity between the embedding vector of each sample and its associated text description while reducing the similarity between the embedding vectors of each sample and the other samples in the batch. As we did previously in the experiments using DDAMFN, we have decided to replace the fully connected layers with an LSTM to maintain temporal coherence.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The metrics evaluated for each challenge are the Concordance Correlation Coefficient (CCC) for valence, arousal, and their combination (valence-arousal), F1 score for emotion classification and F1 score for action unit (AU) detection:\n\nTable  3  presents the performance metrics for each challenge across the different architectures tested. The metrics include the combined CCC V A (Valence-Arousal), F1 Expr (expression classification), F1 AU (action unit classification), and F1 AU opt (optimized action unit classification after threshold optimization).\n\n• The baseline model shows the lowest performance across all tasks, with a CCC V A of 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "The performance of the models can be attributed to several key architectural choices and design considerations. The DDAMFN+Fc model demonstrated a significant improvement over the baseline in emotional valence-arousal prediction, achieving a CCC V A of 0.416. However, its performance in expression classification (F1 Expr = 0.249) and action unit classification (F1 AU = 0.332) remained relatively lower compared to the other models, indicating that the model struggles with fine-grained expression and action unit recognition. This may be due to the limitations of the fully connected layers in capturing the complex spatial and temporal features required for these tasks. The architecture appears to perform better with simpler, less dynamic tasks like valence-arousal prediction, where temporal and spatial dependencies are less crucial.\n\nIn contrast, the DDAMFN+LSTM model performed the best overall, achieving the highest CCC V A (0.479), F1 AU (0.411), and F1 AU opt (0.451). The integration of LSTM layers in this model allowed it to capture the temporal dependencies inherent in facial expressions and emotional states, providing a distinct advantage for both action unit classification and valence-arousal prediction. The LSTM's ability to maintain memory of previous frames is crucial in tasks involving dynamic data, such as emotion recognition, where the progression of emotional states over time plays a critical role. This temporal awareness, when combined with DDAMFN's feature extraction capabilities, enabled the DDAMFN+LSTM model to excel at both the prediction of emotional valence and arousal as well as the classification of facial action units, where subtle changes in facial expressions are key.\n\nThe CLIP+Fc and CLIP+LSTM models showed promising results, particularly in the domain of expression classification. The CLIP+LSTM model, with an F1 Expr of 0.336, outperformed the other models in this task, demonstrating its strong ability to capture the nuances of facial expressions. This success can be attributed to the CLIP model's capability to process both visual and contextual information, combined with the LSTM's ability to capture temporal dependencies. This synergy enabled the model to recognize how facial expressions evolve over time, making it especially well-suited for the task of expression classification, where understanding transitions between facial states is crucial. Moreover, CLIP models, known for their robust performance in image-based tasks, likely enhanced the model's ability to interpret diverse facial expressions and complex visual features, leading to a notable improvement in expression classification accuracy.\n\nIn summary, the superior performance of the DDAMFN+LSTM and CLIP+LSTM models can largely be attributed to their ability to handle both temporal and spatial features effectively. These models performed best in tasks that required an understanding of the dynamics of emotional valence-arousal and the subtlety of facial expressions. The LSTM's strength in learning temporal patterns, combined with advanced feature extraction methods like those employed by DDAMFN and CLIP, provided these models with a significant advantage. This highlights the importance of incorporating temporal modeling and dynamic feature extraction in architectures designed for complex tasks such as emotion recognition and facial action unit classification.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a diagram of the network, which features",
      "page": 2
    },
    {
      "caption": "Figure 1: Our DDAMFN [34] architecture for the 8th ABAW challenge: MobileFaceNet (MFN) for feature extraction (grey), Dual-",
      "page": 3
    },
    {
      "caption": "Figure 2: CLIP Architecture Overview: In this diagram, the architecture is divided into several key components. The visual path, high-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "jcabacas, eortegabeltran, ibenitoal, cventuraroy\n@uoc.edu"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "{\n}"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "by parsing facial expressions\ninto discrete muscle move-\nAbstract"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "ments,\nfacilitating\na finer-grained\nanalysis\nof\nemotional"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "We present our contribution to the 8th ABAW challenge at\nexpressions across cultures and contexts.\nSuch advance-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "CVPR 2025, where we tackle valence-arousal estimation,\nments not only improved the reliability of automated emo-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "emotion recognition,\nand facial action unit detection as\ntion recognition systems, but also opened the possibility to"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "three independent challenges. Our approach leverages the\npersonalize affective computing applications in fields such"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "well-known Dual-Direction Attention Mixed Feature Net-\nas mental health monitoring [28] or user experience de-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "work (DDAMFN) for all\nthree tasks, achieving results that\nsign [33]."
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "surpass the proposed baselines.\nAdditionally, we explore"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "the use of CLIP for the emotion recognition challenge as an"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "additional experiment. We provide insights into the archi-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "To tackle all these challenges, researchers have explored"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "tectural choices that contribute to the strong performance"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "innovative\narchitectures\nsuch\nas\nthe DDAMFN (Dual-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "of our methods."
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "Direction Attention Mixed Feature Network)\n[34].\nThis"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "novel\napproach integrates\nattention mechanisms\n[7]\nand"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "mixed feature extraction [4], enhancing the network’s abil-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "1. Introduction"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "ity to capture intricate details within facial expressions."
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "Facial emotion recognition has emerged as a pivotal area"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "of research within affective computing, driven by its poten-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "tial applications in fields ranging from human-computer in-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "teraction to psychological research and clinical diagnostics.\nThere is an increasing need to develop machines capa-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "Since Ekman’s classification of human expression faces into\nble of understanding and appropriately responding to hu-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "emotions [6], many studies have emerged in recent years.\nman emotions in real-world, day-to-day applications. Ad-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "Calvo et al.\n[3], Baltrusaities et al.\n[1], or Kaya et al.\n[8]\ndressing this challenge, a series of competitions titled Af-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "laid the foundational\nframework for understanding facial\nfective Behavior Analysis\nin-the-Wild (ABAW) has been"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "expressions as a window into emotional states.\nIn contem-\norganized [10–16, 19–25, 32].\nFor\nthe 8th ABAW [17,"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "porary research, Liu et al. [26] and Kim et al. [9] continued\n18] challenge at CVPR 2025,\nsix competitions were in-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "to refine and expand these methodologies, by synthesizing\ntroduced:\nValence-Arousal\n(VA) Estimation, Expression"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "insights from cognitive psychology, computer vision, and\n(EXPR) Recognition, Action Unit\n(AU) Detection, Com-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "machine learning, researchers have made significant strides\npound Expression (CE) Recognition, Emotional Mimicry"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "in enhancing the accuracy and applicability of facial emo-\nIntensity\n(EMI) Estimation,\nand Ambivalence/Hesitancy"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "tion recognition systems.\nIn addition, the integration of the\n(AH) Recognition.\nIn this work, we present our approach"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "dimensions of valence and arousal [31, 32] added depth to\nto the VA Estimation, EXPR Recognition, and AU Detec-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "the interpretation of emotional\nstates,\nallowing more nu-\ntion challenges, where we adapted the DDAMFN architec-"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "anced insight into human affective experiences.\nture for each challenge and additionally leveraged the CLIP"
        },
        {
          "Universitat de Barcelona, 08028 Barcelona, Spain": "embedding space to enhance emotion recognition.\nAction unit detection [5, 27] complemented these efforts"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "connected layers that bridge both paths. Finally,\nthe similarity outputs,"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "between the image and text inputs."
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "8 dataset\n[34].\nIn our experiments, we preserved the pre-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "trained weights for\nthe feature extraction layers, attention"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "mechanisms, and the GDConv layer, as these components"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "had already been optimized for extracting meaningful fea-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "tures from the input data. However, the fully-connected lay-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "ers were reinitialized with random weights to facilitate the"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "adaptation of the model to our specific task."
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "To enhance the model’s performance across various chal-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "lenges, we introduced a custom classifier for each individual"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "task. These classifiers were designed to address the unique"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "requirements of each challenge, allowing for more special-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "ized learning. Each classifier was trained independently and"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "separately, with the goal of adapting the pretrained feature"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "extraction and attention components to the specific charac-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "teristics of\nthe new tasks.\nThis\nstrategy ensured that\nthe"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "model could benefit from the strong generalization capabil-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "ities of\nthe DDAMFN architecture while still\ntailoring its"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "output to the specific demands of each challenge."
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": ""
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "By training each custom classifier separately, we were"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "able to achieve a fine-tuned balance between leveraging pre-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "trained knowledge and optimizing the model for each indi-"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "vidual task, leading to improved overall performance in the"
        },
        {
          "lighted in orange, processes the image input. The text path, shown in green, processes the textual input. The white areas represent the fully": "experiments."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Performance metrics for each challenge. Table shows",
      "data": [
        {
          "the fully connected layers with a Long Short-Term Mem-": "ory (LSTM) network. The LSTM was specifically chosen",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "The metrics evaluated for each challenge are the Concor-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "for\nits ability to retain and model\nlong-term dependencies",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "dance Correlation Coefficient\n(CCC)\nfor valence, arousal,"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "in sequential data, enabling the model to maintain temporal",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "and their combination (valence-arousal), F1 score for emo-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "coherence across frames.",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "tion classification and F1 score for action unit (AU) detec-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "3.2. CLIP Contrastive",
          "4. Results": "tion:"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "Following the work done in [2] we used the same architec-",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "CCCV + CCCA"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "P =\n(4)"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "ture and loss. For the CLIP Contrastive model (Fig. 2), we",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "2"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "introduced two fully connected layers on top of the image",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "F1Expr"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "encoder to allow adjustments to the embedding space. We",
          "4. Results": "P =\n(5)"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "8"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "then used Contrastive loss with cosine similarity against a",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "F1AU"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "set of text prompts. During training, both the CLIP image",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "P =\n(6)"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "and text encoders are kept\nfrozen. A unique prompt was",
          "4. Results": "12"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "generated for each emotion based on the structure ”a face",
          "4. Results": "Table 3 presents the performance metrics for each chal-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "showing emotion.”",
          "4. Results": "lenge across the different architectures tested. The metrics"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "In the CLIP Contrastive architecture, we added a fully",
          "4. Results": "include the combined CCCV A (Valence-Arousal), F1Expr"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "connected layer with 512 units and ReLU activation on top",
          "4. Results": "(expression\nclassification),\n(action\nunit\nclassifica-\nF1AU"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "of the CLIP image encoder, followed by another fully con-",
          "4. Results": "(optimized action unit classification af-\ntion), and F1AU opt"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "nected layer with 512 units and a linear activation function",
          "4. Results": "ter threshold optimization)."
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "before the loss function. The linear activation function was",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "• The baseline model shows the lowest performance across"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "selected to retain negative activations, as the original CLIP-",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "all\ntasks, with a CCCV A of 0.22, F1Expr of 0.25, and"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "trained embedding space contains both negative and posi-",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "F1AU of 0.39. No F1AU opt value is reported for the base-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "tive values.",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "line."
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "To\ntrain\nthe model,\nwe\nemployed\nthe\nsame\ncon-",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "• The DDAMFN+Fc model\nimproves upon the baseline,"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "trastive loss\nfunction used in the original CLIP approach",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "However,\nachieving a CCCV A of 0.416.\nthe F1Expr"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "[29].\nSpecifically,\ngiven a batch of N image-text pairs",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "and F1AU values (0.249 and 0.332, respectively) remain"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "(Ii, Ti), i = 1, . . . , N , the losses for the image and text are",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "The F1AU opt"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "computed as follows:",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "score after threshold optimization is 0.362."
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "• The DDAMFN+LSTM model achieves the highest per-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "formance\noverall, with\nthe\n0.479,\nhighest CCCV A of"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "exp(\n)\nIei, Tei",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "log",
          "4. Results": "F1AU of 0.411, and F1AU opt of 0.451. These results indi-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "1 N\nN(cid:88) i\n(1)\n⟨\n⟩\nLimg =",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "(cid:80)N",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "−\n)",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "j=1 exp(\n=1",
          "4. Results": "cate that\nthis architecture performs best\nin terms of both"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "⟨\n⟩",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "emotional valence-arousal prediction and action unit clas-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "sification."
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "• The CLIP+Fc and CLIP+LSTM models provided promis-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "Tei, Iei",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "1 N\nN(cid:88) i\nlog\n(2)\n⟨\n⟩\nLtext =",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "(cid:80)N",
          "4. Results": "ing\nresults\nin\nexpression\nclassification.\nSpecifically,"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "−\n)",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "j=1 exp(\n=1",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "⟨\n⟩",
          "4. Results": "CLIP+LSTM achieved a F1Expr of 0.336, which is the"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "highest among the rest of\nthe architectures, demonstrat-"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "where\nis the cosine similarity between the em-\nIei, Tej",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "⟨\n⟩",
          "4. Results": "ing strong performance in this task."
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "bedding vectors of\nthe i-th image sample Iei and the j-th",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "is the cosine similarity be-\ntext sample Tej, and\nIei, Tei",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "⟨\n⟩",
          "4. Results": "Table 3.\nPerformance metrics for each challenge.\nTable shows"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "tween the i-th image sample and its corresponding text cap-",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "F1Expr and F1AU already normalized by the number of classes for"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "tion. The contrastive loss LCO is computed as:",
          "4. Results": ""
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "each classification task. CCCAV is the combined CCC for valence"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "",
          "4. Results": "and arousal. F1AU opt is the result after optimizing thresholds"
        },
        {
          "the fully connected layers with a Long Short-Term Mem-": "Limg + Ltext",
          "4. Results": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "References"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[1] Tadas Baltrusaitis, Amir Zadeh, Yagmur Lim,\nand Louis-"
        },
        {
          "action unit classification.": "Philippe Morency. Openface 2.0: Facial behavior analysis"
        },
        {
          "action unit classification.": "in IEEE Winter Conference on Applications of Com-\ntoolkit."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "puter Vision (WACV), 2018."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[2] Cristina Bustos, Carles Civit, Brian Du, Albert Sol´e-Ribalta,"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "and Agata Lapedriza. On the use of vision-language mod-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "els for visual sentiment analysis:\na study on clip.\nIn 2023"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "11th International Conference on Affective Computing and"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "Intelligent Interaction (ACII), pages 1–8, 2023."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[3] Manuel G Calvo and Lauri Nummenmaa. Faces and feelings:"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "The universal recognition of emotions. Annu. Rev. Psychol.,"
        },
        {
          "action unit classification.": "67:451–471, 2016."
        },
        {
          "action unit classification.": "[4]\nSheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobile-"
        },
        {
          "action unit classification.": "facenets: Efficient cnns for accurate real-time face verifica-"
        },
        {
          "action unit classification.": "tion on mobile devices.\nIn Biometric Recognition: 13th Chi-"
        },
        {
          "action unit classification.": "nese Conference, CCBR 2018, Urumqi, China, August 11-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "12, 2018, Proceedings 13, pages 428–438. Springer, 2018."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[5]\nPaul Ekman. Darwin and facial expression: A century of re-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "search in review. General Psychology, 7(2):121–125, 2003."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[6]\nPaul Ekman and Wallace V Friesen. Facial action coding sys-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "tem: A technique for the measurement of facial movement."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "Consulting Psychologists Press, 1978."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[7] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate at-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "tention for efficient mobile network design.\nIn Proceedings"
        },
        {
          "action unit classification.": "of the IEEE/CVF conference on computer vision and pattern"
        },
        {
          "action unit classification.": "recognition, pages 13713–13722, 2021."
        },
        {
          "action unit classification.": "[8] H¨useyin Kaya and Emre G¨urpinar.\nExploring deep convo-"
        },
        {
          "action unit classification.": "lutional neural networks\nfor\nfacial action unit\nrecognition."
        },
        {
          "action unit classification.": "Neurocomputing, 387:368–380, 2020."
        },
        {
          "action unit classification.": "[9]\nJihye Kim, Seung-Chan Lee, Jong-Soo Kim, and Chee Sun"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "Yoo. A survey on facial emotion recognition: Approaches,"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "databases, and challenges. Pattern Recognition Letters, 153:"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "84–92, 2023."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[10] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "pression\nrecognition,\naction\nunit\ndetection & multi-task"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "ference on Computer Vision and Pattern Recognition, pages"
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "2328–2336, 2022."
        },
        {
          "action unit classification.": ""
        },
        {
          "action unit classification.": "[11] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &"
        },
        {
          "action unit classification.": "multi-task learning challenges.\nIn European Conference on"
        },
        {
          "action unit classification.": "Computer Vision, pages 157–172. Springer, 2023."
        },
        {
          "action unit classification.": "[12] Dimitrios Kollias. Multi-label compound expression recog-"
        },
        {
          "action unit classification.": "nition:\nC-expr database & network.\nIn Proceedings of"
        },
        {
          "action unit classification.": "the IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "action unit classification.": "Recognition, pages 5589–5598, 2023."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "of\nemotional\nvalence-arousal\nand\nthe\nsubtlety\nof\nfacial"
        },
        {
          "5. Conclusion": "The performance of the models can be attributed to several",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "expressions.\nThe LSTM’s\nstrength in learning temporal"
        },
        {
          "5. Conclusion": "key architectural choices and design considerations.\nThe",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "patterns, combined with advanced feature extraction meth-"
        },
        {
          "5. Conclusion": "DDAMFN+Fc model demonstrated a significant\nimprove-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "ods like those employed by DDAMFN and CLIP, provided"
        },
        {
          "5. Conclusion": "ment over\nthe baseline in emotional valence-arousal pre-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "these models with a significant advantage. This highlights"
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "the\nimportance of\nincorporating temporal modeling and"
        },
        {
          "5. Conclusion": "its per-\ndiction, achieving a CCCV A of 0.416. However,",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "dynamic\nfeature\nextraction in architectures designed for"
        },
        {
          "5. Conclusion": "formance in expression classification (F1Expr = 0.249) and",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "complex\ntasks\nsuch\nas\nemotion\nrecognition\nand\nfacial"
        },
        {
          "5. Conclusion": "action unit\n= 0.332)\nremained rela-\nclassification (F1AU",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "action unit classification."
        },
        {
          "5. Conclusion": "tively lower compared to the other models,\nindicating that",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "the model struggles with fine-grained expression and action",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "References"
        },
        {
          "5. Conclusion": "unit recognition. This may be due to the limitations of the",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "fully connected layers in capturing the complex spatial and",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[1] Tadas Baltrusaitis, Amir Zadeh, Yagmur Lim,\nand Louis-"
        },
        {
          "5. Conclusion": "temporal features required for these tasks. The architecture",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Philippe Morency. Openface 2.0: Facial behavior analysis"
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "in IEEE Winter Conference on Applications of Com-\ntoolkit."
        },
        {
          "5. Conclusion": "appears to perform better with simpler,\nless dynamic tasks",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "puter Vision (WACV), 2018."
        },
        {
          "5. Conclusion": "like valence-arousal prediction, where temporal and spatial",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[2] Cristina Bustos, Carles Civit, Brian Du, Albert Sol´e-Ribalta,"
        },
        {
          "5. Conclusion": "dependencies are less crucial.",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "and Agata Lapedriza. On the use of vision-language mod-"
        },
        {
          "5. Conclusion": "In contrast, the DDAMFN+LSTM model performed the",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "els for visual sentiment analysis:\na study on clip.\nIn 2023"
        },
        {
          "5. Conclusion": "best overall, achieving the highest CCCV A (0.479), F1AU",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "11th International Conference on Affective Computing and"
        },
        {
          "5. Conclusion": "(0.451).\nThe integration of LSTM\n(0.411), and F1AU opt",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Intelligent Interaction (ACII), pages 1–8, 2023."
        },
        {
          "5. Conclusion": "layers in this model allowed it\nto capture the temporal de-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[3] Manuel G Calvo and Lauri Nummenmaa. Faces and feelings:"
        },
        {
          "5. Conclusion": "pendencies\ninherent\nin facial\nexpressions\nand emotional",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "The universal recognition of emotions. Annu. Rev. Psychol.,"
        },
        {
          "5. Conclusion": "states, providing a distinct advantage for both action unit",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "67:451–471, 2016."
        },
        {
          "5. Conclusion": "classification and valence-arousal prediction. The LSTM’s",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[4]\nSheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mobile-"
        },
        {
          "5. Conclusion": "ability to maintain memory of previous\nframes\nis crucial",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "facenets: Efficient cnns for accurate real-time face verifica-"
        },
        {
          "5. Conclusion": "in tasks involving dynamic data, such as emotion recogni-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "tion on mobile devices.\nIn Biometric Recognition: 13th Chi-"
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "nese Conference, CCBR 2018, Urumqi, China, August 11-"
        },
        {
          "5. Conclusion": "tion, where the progression of emotional states over\ntime",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "12, 2018, Proceedings 13, pages 428–438. Springer, 2018."
        },
        {
          "5. Conclusion": "plays a critical role. This temporal awareness, when com-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[5]\nPaul Ekman. Darwin and facial expression: A century of re-"
        },
        {
          "5. Conclusion": "bined with DDAMFN’s feature extraction capabilities, en-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "search in review. General Psychology, 7(2):121–125, 2003."
        },
        {
          "5. Conclusion": "abled the DDAMFN+LSTM model to excel at both the pre-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[6]\nPaul Ekman and Wallace V Friesen. Facial action coding sys-"
        },
        {
          "5. Conclusion": "diction of emotional valence and arousal as well as the clas-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "tem: A technique for the measurement of facial movement."
        },
        {
          "5. Conclusion": "sification of facial action units, where subtle changes in fa-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Consulting Psychologists Press, 1978."
        },
        {
          "5. Conclusion": "cial expressions are key.",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[7] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate at-"
        },
        {
          "5. Conclusion": "The CLIP+Fc and CLIP+LSTM models showed promis-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "tention for efficient mobile network design.\nIn Proceedings"
        },
        {
          "5. Conclusion": "ing results, particularly in the domain of expression classifi-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "of the IEEE/CVF conference on computer vision and pattern"
        },
        {
          "5. Conclusion": "cation. The CLIP+LSTM model, with an F1Expr of 0.336,",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "recognition, pages 13713–13722, 2021."
        },
        {
          "5. Conclusion": "outperformed the other models in this task, demonstrating",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[8] H¨useyin Kaya and Emre G¨urpinar.\nExploring deep convo-"
        },
        {
          "5. Conclusion": "its\nstrong ability to capture the nuances of\nfacial expres-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "lutional neural networks\nfor\nfacial action unit\nrecognition."
        },
        {
          "5. Conclusion": "sions. This success can be attributed to the CLIP model’s",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Neurocomputing, 387:368–380, 2020."
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[9]\nJihye Kim, Seung-Chan Lee, Jong-Soo Kim, and Chee Sun"
        },
        {
          "5. Conclusion": "capability to process both visual and contextual\ninforma-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Yoo. A survey on facial emotion recognition: Approaches,"
        },
        {
          "5. Conclusion": "tion, combined with the LSTM’s ability to capture temporal",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "databases, and challenges. Pattern Recognition Letters, 153:"
        },
        {
          "5. Conclusion": "dependencies. This synergy enabled the model to recognize",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "84–92, 2023."
        },
        {
          "5. Conclusion": "how facial expressions evolve over\ntime, making it espe-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[10] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-"
        },
        {
          "5. Conclusion": "cially well-suited for\nthe task of expression classification,",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "pression\nrecognition,\naction\nunit\ndetection & multi-task"
        },
        {
          "5. Conclusion": "where understanding transitions between facial states is cru-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of"
        },
        {
          "5. Conclusion": "cial. Moreover, CLIP models, known for their robust per-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "ference on Computer Vision and Pattern Recognition, pages"
        },
        {
          "5. Conclusion": "formance in image-based tasks, likely enhanced the model’s",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "2328–2336, 2022."
        },
        {
          "5. Conclusion": "ability to interpret diverse facial expressions and complex",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": ""
        },
        {
          "5. Conclusion": "",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[11] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &"
        },
        {
          "5. Conclusion": "visual features, leading to a notable improvement in expres-",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "multi-task learning challenges.\nIn European Conference on"
        },
        {
          "5. Conclusion": "sion classification accuracy.",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Computer Vision, pages 157–172. Springer, 2023."
        },
        {
          "5. Conclusion": "In\nsummary,\nthe\nsuperior\nperformance\nof\nthe",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "[12] Dimitrios Kollias. Multi-label compound expression recog-"
        },
        {
          "5. Conclusion": "DDAMFN+LSTM and CLIP+LSTM models\ncan largely",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "nition:\nC-expr database & network.\nIn Proceedings of"
        },
        {
          "5. Conclusion": "be attributed to their ability to handle both temporal and",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "the IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "5. Conclusion": "spatial\nfeatures effectively.\nThese models performed best",
          "in tasks\nthat\nrequired an understanding of\nthe dynamics": "Recognition, pages 5589–5598, 2023."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "action unit\nrecognition: Aff-wild2, multi-task learning and",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Dhall, Shreya Ghosh, Chunchang Shao,\nand Guanyu Hu."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arcface. arXiv preprint arXiv:1910.04855, 2019.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "7th abaw competition: Multi-task learning and compound"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[14] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "expression recognition.\narXiv preprint arXiv:2407.03835,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "in-the-wild: Valence-arousal, expressions, action units and a",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "2024."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "unified framework. arXiv preprint arXiv:2103.15792, 2021.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[26] Xiaoying Liu, Xianhua Song, and Zheng-Hua Tan.\nDeep"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[15] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "learning for emotion recognition: A comprehensive review."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tive behavior in the second abaw2 competition.\nIn Proceed-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Neurocomputing, 440:167–180, 2021."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "ings of\nthe IEEE/CVF International Conference on Com-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[27]\nPatrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "puter Vision, pages 3652–3660, 2021.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Zara Ambadar,\nand Iain Matthews.\nThe\nextended cohn-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[16] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "kanade dataset (ck+): A complete dataset for action unit and"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "affective behavior\nin the first abaw 2020 competition.\nIn",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "in IEEE Conference on Com-\nemotion-specified expression."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "2020\n15th\nIEEE International Conference\non Automatic",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "puter Vision and Pattern Recognition Workshops (CVPRW),"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Face and Gesture Recognition (FG 2020)(FG), pages 794–",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "2010."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "800, .",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[28] Rosalind W Picard, Elias Vyzas, and Jennifer Healey.\nTo-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[17] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen,\nIrene",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "ward machine emotional\nintelligence: Analysis of affective"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Kotsia, UK Cogitat, Eric Granger, Marco Pedersoli, Simon",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "IEEE Transactions on Pattern Analysis\nphysiological states."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Bacon, Alice Baird, Chunchang Shao, et al. Advancements",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "and Machine Intelligence, 23(10):1175–1191, 2020."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "in affective and behavior analysis: The 8th abaw workshop",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "and competition.\n.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[18] Dimitrios Kollias, Panagiotis Tzirakis, Alan S. Cowen, Ste-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Learning transferable visual models from natural\nlanguage"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "fanos Zafeiriou,\nIrene Kotsia, Eric Granger, Marco Peder-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "the 38th International Con-\nsupervision.\nIn Proceedings of"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "soli, Simon L. Bacon, Alice Baird, Chris Gagne, Chun-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "ference on Machine Learning,\npages 8748–8763. PMLR,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "chang Shao, Guanyu Hu, Soufiane Belharbi, and Muham-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "2021."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "mad Haseeb Aslam. Advancements in Affective and Behav-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[30] Andrey V. Savchenko.\nFrame-level prediction of facial ex-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "ior Analysis: The 8th ABAW Workshop and Competition.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "pressions, valence, arousal and action units for mobile de-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "”2025”.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "vices, 2022."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[19] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[31] Mohammad Soleymani, David Garcia, Brendan Jou,\nand"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Bj¨orn Schuller. Deep learning for affective computing: Text-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arXiv preprint\nfect and action units\nin a single network.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "IEEE Trans-\nbased emotion recognition in decision support."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arXiv:1910.11111, 2019.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "actions on Affective Computing, 8(1):17–37, 2017."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[20] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[32]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "in-the-wild: Aff-wild database and challenge, deep architec-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tures, and beyond. International Journal of Computer Vision,",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "pages 1–23, 2019.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "IEEE, 2017."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[21] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[33] Zhihong Zeng, Maja Pantic, Glenn I Roisman, and Thomas S"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Zafeiriou.\nDistribution matching for heterogeneous multi-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Huang.\nA survey of affect\nrecognition methods: Audio,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "IEEE Transactions\nvisual,\nand spontaneous\nexpressions."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "arXiv:2105.03790, 2021.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "on Pattern Analysis and Machine Intelligence, 31(1):39–58,"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[22] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "2019."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "[34]\nSaining Zhang, Yuhang Zhang, Ye Zhang, Yufei Wang, and"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "mation, expression recognition, action unit detection & emo-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "Zhigang Song. A dual-direction attention mixed feature net-"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tional reaction intensity estimation challenges.\nIn Proceed-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "work for facial expression recognition. Electronics, 12(17):"
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "ings of\nthe IEEE/CVF Conference on Computer Vision and",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": "3595, 2023."
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Pattern Recognition, pages 5888–5897, 2023.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[23] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Zafeiriou. Distribution matching for multi-task learning of",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "classification tasks:\na large-scale study on faces & beyond.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "the AAAI Conference on Artificial Intelli-\nIn Proceedings of",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "gence, pages 2813–2821, 2024.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "[24] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "fanos Zafeiriou,\nIrene Kotsia, Alice Baird, Chris Gagne,",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "Chunchang Shao, and Guanyu Hu. The 6th affective behav-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "ior analysis in-the-wild (abaw) competition.\nIn Proceedings",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "of\nthe IEEE/CVF Conference on Computer Vision and Pat-",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        },
        {
          "[13] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,": "tern Recognition, pages 4587–4598, 2024.",
          "[25] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Yagmur Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "2",
      "title": "On the use of vision-language models for visual sentiment analysis: a study on clip",
      "authors": [
        "Cristina Bustos",
        "Carles Civit",
        "Brian Du",
        "Albert Solé-Ribalta",
        "Agata Lapedriza"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "3",
      "title": "Faces and feelings: The universal recognition of emotions",
      "authors": [
        "G Manuel",
        "Lauri Calvo",
        "Nummenmaa"
      ],
      "year": "2016",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "4",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "Sheng Chen",
        "Yang Liu",
        "Xiang Gao",
        "Zhen Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition: 13th Chinese Conference"
    },
    {
      "citation_id": "5",
      "title": "Darwin and facial expression: A century of research in review",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2003",
      "venue": "General Psychology"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "7",
      "title": "Coordinate attention for efficient mobile network design",
      "authors": [
        "Qibin Hou",
        "Daquan Zhou",
        "Jiashi Feng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "8",
      "title": "Exploring deep convolutional neural networks for facial action unit recognition",
      "authors": [
        "Hüseyin Kaya",
        "Emre Gürpinar"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "A survey on facial emotion recognition: Approaches, databases, and challenges",
      "authors": [
        "Jihye Kim",
        "Seung-Chan Lee",
        "Jong-Soo Kim",
        "Chee Sun"
      ],
      "year": "2023",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "10",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "14",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "17",
      "title": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Irene Kotsia",
        "Eric Cogitat",
        "Marco Granger",
        "Simon Pedersoli",
        "Alice Bacon",
        "Chunchang Baird",
        "Shao"
      ],
      "venue": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition"
    },
    {
      "citation_id": "18",
      "title": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Eric Granger",
        "Marco Pedersoli",
        "Simon Bacon",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2025",
      "venue": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition"
    },
    {
      "citation_id": "19",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "20",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "22",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "26",
      "title": "Deep learning for emotion recognition: A comprehensive review",
      "authors": [
        "Xiaoying Liu",
        "Xianhua Song",
        "Zheng-Hua Tan"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "28",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological states",
      "authors": [
        "Rosalind Picard",
        "Elias Vyzas",
        "Jennifer Healey"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices"
    },
    {
      "citation_id": "31",
      "title": "Deep learning for affective computing: Textbased emotion recognition in decision support",
      "authors": [
        "Mohammad Soleymani",
        "David Garcia",
        "Brendan Jou",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "33",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "Saining Zhang",
        "Yuhang Zhang",
        "Ye Zhang",
        "Yufei Wang",
        "Zhigang Song"
      ],
      "year": "2023",
      "venue": "Electronics"
    }
  ]
}