{
  "paper_id": "2001.07739v3",
  "title": "Emopain Challenge 2020: Multimodal Pain Evaluation From Facial And Bodily Expressions",
  "published": "2020-01-21T19:09:08Z",
  "authors": [
    "Joy O. Egede",
    "Siyang Song",
    "Temitayo A. Olugbade",
    "Chongyang Wang",
    "Amanda Williams",
    "Hongying Meng",
    "Min Aung",
    "Nicholas D. Lane",
    "Michel Valstar",
    "Nadia Bianchi-Berthouze"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The EmoPain 2020 Challenge is the first international competition aimed at creating a uniform platform for the comparison of multi-modal machine learning and multimedia processing methods of chronic pain assessment from human expressive behaviour, and also the identification of pain-related behaviours. The objective of the challenge is to promote research in the development of assistive technologies that help improve the quality of life for people with chronic pain via real-time monitoring and feedback to help manage their condition and remain physically active. The challenge also aims to encourage the use of the relatively underutilised, albeit vital bodily expression signals for automatic pain and painrelated emotion recognition. This paper presents a description of the challenge, competition guidelines, bench-marking dataset, and the baseline systems' architecture and performance on the Challenge's three sub-tasks: pain estimation from facial expressions, pain recognition from multimodal movement, and protective movement behaviour detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The EmoPain 2020 Challenge 1 is the first international competition in automatic pain recognition aimed at benchmarking the performance of machine learning methods designed to recognise or quantify chronic pain from behavioural-face and body-cues, and also recognise pain-related movement behaviours. Chronic pain (CP) is a widespread distressing problem that not only restricts body activities but significantly impacts on the mental, psychological, social and economic status of people with chronic pain. A 2016 study  [1]  showed that over 40% of the UK population are affected by chronic pain with this number going up to 62% for people over 75 years. A similar study for the United States puts the former figure at 25%  [2] . Beyond the individual, CP has dire consequences on socio-economic growth and development. Amongst other medical conditions, chronic pain was responsible for most medical consultations and costs the US approximately $560 billion dollars each year  [2] . The escalating socio-economic costs of CP, as well as its detrimental effect on the quality of life of individuals and their families, buttress the urgent need for efficient chronic pain interventions.\n\nTechnological interventions present a plausible solution, but the first step towards a workable system requires accurate identification and interpretation of pain-associated expressions and behaviours. Consequently, technology-driven methods (see survey in  [3] ) utilising clinically certified behavioural and physiological pain indicators for pain assessment have been proposed within the machine learning and computer vision research community. Although machineassisted pain assessment methods have advanced considerably, their practical application has been constrained by datarelated and design issues.\n\nOne major problem is that there are few publicly accessible pain datasets that meet requirements for effectively training such predictive systems. Secondly, pain expression is multi-faceted, yet there is an over-reliance on unimodal clues, particularly the face, whereas body movements are critical to effective chronic pain assessment  [4] . Although facial expressions give a good indication of affect intensity, without the body context, its discriminative property of affective states diminishes  [5] . In contrast, pain-related movement behaviour provides more information about the distress level of a pain stimulus (physical activity) and what form of support is required  [6] ,  [4] . Thus, pain literature  [7] ,  [4]  strongly advocates the use of multiple, rather than isolated behavioural cues for pain assessment. Lastly, existing benchmarking pain corpus  [8] ,  [9]  predominantly feature pain expressions induced in constrained environments and by nonthreatening stimuli which are not fully representative of realworld distressing physical activities encountered by people with chronic pain; whereas, for technological interventions to be beneficial, it should be developed on data which represent the everyday body functions of the target population. Also, some of these datasets  [9] ,  [10]  provide only unidimensional-facial cues-behavioural chronic pain characterisations.\n\nThe EmoPain 2020 challenge aims to address the above gaps by creating a platform to foster multi-modal automatic pain recognition research within the machine learning community. The challenge is based on the multi-modal EmoPain dataset, which for the first time, is opened up to the community in a competition framework to benchmark automated pain assessment methods. The EmoPain dataset  [7]  consists of audiovisual, motion data and muscle activity captured from chronic lower back pain (CLBP) and healthy participants engaged in both instructor-led and self-directed physical exercises which replicate everyday body functions. Utilising the visual and movement data dimensions, the EmoPain 2020 challenge presents three pain recognition tasks: (i) Pain Estimation from Facial Expressions Task, (ii) Pain Recognition from Multimodal Movement Task and (iii) Multimodal Movement Behaviour Classification Task.\n\nParticipants could choose to compete in all or some of the tasks. Data for each task is split into training, validation and a held-out test partition. To ensure a fair comparison, participants were given the same training and validation data to develop their algorithms/models, which was then sent to the organisers for evaluation on the held-out test set. Participants did not have access to the test data partition. Papers accompanying the challenge submissions were presented at the FG2020 International Workshop on Automated Assessment of Pain.\n\nThe rest of the paper is organized as follows: Section II discusses relevant work in automatic pain recognition; Section III gives a full description of the EmoPain dataset and the three sub-tasks as well as the metrics used for ranking participants' submissions; Section IV describes the baseline features and models developed for each task, and the results obtained. Lastly, section V summarises the contributions and concludes the work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section describes current approaches to automatic pain recognition with a focus on pain-associated face and body expression synthesis, processing, analysis and interpretation. Relevant pain literature will be discussed in three groups building on the challenge's task categorisation. An extended survey is provided in  [3] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Automatic Pain Detection Based On Facial Expressions",
      "text": "The face is a key medium for communicating pain in human interactions, particularly when pain expression is not actively suppressed by the individual. Facial expressions of pain have been shown to have distinctive properties from other basic emotions  [11] ,  [12] , lending credence to its pertinence to pain recognition. Due to its relative ease of accessibility and utilisation in daily social interaction, faces have been explored extensively for automatic pain recognition. Early work based on facial actions was limited to binary classification of face images into pain or no pain  [13] ,  [14]  or distinguishing real pain from posed pain  [15] . However, this outcome was not adequate for clinical applications as evidenced by the selfreport pain assessment scales  [16]  which aim to quantify pain rather than identify its occurrence. Consequently, recent studies moved on to estimating pain levels from facial expressions using either a multi-class classification set-up  [17]  or regression framework  [18] ,  [19] ,  [20] . This shift was also propelled by the introduction of pain datasets  [8] ,  [9]  which provide discrete pain annotations of face images.\n\nMost of these studies  [20] ,  [19]  predict pain on the 16-point Prckachin and Solomon Pain Intensity (PSPI)  [21]  scale or a condensed version  [17] , while others  [22] ,  [23]  focus on recognising observer reported or patients' self-reported pain ranging from two to five pain levels.\n\nTo discriminate pain expressions, face shape and appearance descriptors have been widely employed due to their proven effectiveness in facial expressions analysis. Appearance features encode facial deformations due to expressions (e.g., wrinkles) while shape features describe the spatial localisation of facial components (i.e., eyes, mouth and nose). In terms of facial features used, previous work on pain recognition can be classified into three: (i) handcrafted feature methods  [22] ,  [17] ,  [13] ,  [20] , (ii) data-learned feature methods  [24] ,  [25]  and (iii) hybrid-feature methods  [19] ,  [26] . Handcrafted facial descriptors are statistical measures computed from a face image using human-designed algorithms. Commonly used features in this category include gradients features  [22] , Gabor features  [15]  , Active Appearance Models (AAM)  [13] ,  [17] , Local Binary Patterns (LBP)  [20] , facial landmarks and associated distance metrics  [22]  amongst others. Data-learned features are offshoots of neural network applications to pain recognition and are automatically generated within the network. Hybrid features, on the other hand, are an integration of traditional and data-learned features and have been shown to significantly improve the predictive ability of recognition models on small datasets  [27] .\n\nAlthough pain recognition from faces has witnessed tremendous progress, there is still ample scope for improvement. Current work has concentrated on facial data collected in constrained, ideal settings where several video cameras are positioned at strategic positions to capture face images from all possible angles. Thus, captured images are usually high resolution, near frontal and unobstructed faces, whereas this is not always the case in typical everyday settings, e.g., performing rehabilitation exercise at home. Another open challenge is insufficient data representation for higher pain levels in existing pain corpus, which limits the performance of recognition models on these pain classes  [27] . Hence, novel methods that make the most of existing data, and more focus on the creation of representative chronic pain facial data are required.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Automatic Pain Detection Based On Bodily Expressions",
      "text": "Despite findings in  [4]  that the body may be more expressive of pain experience than the face or vocal modality, which are more dependent on social context, it has not been as widely explored for automatic detection of pain levels as the face. Most of the early studies  [28] ,  [29]  and a number of more recent work  [30] ,  [31]  focused on discrimination between people with chronic pain and those without. Other studies have similarly investigated differentiation between two levels of pain  [32] ,  [33] . One exception is  [34]  where 11 levels of pain were detected. While studies such as  [35] ,  [36]  have also gone beyond binary classification, unlike the afore-mentioned, they are based on experimentally-induced pain which is transient and not usually perceived a threat  [37] .\n\nThe bodily expressions used in the investigations carried out in these studies have typically depended on the pain location and the activity being performed. For example, in the work of  [31] , automatic detection of knee pain was based on gait characteristics and ground force reaction during walking tasks. Similarly, the automatic detection of neck pain in  [30]  used neck movements measured while participants performed neck exercises. For low back pain, where participants are usually being assessed during physical activities involving the trunk, features of trunk  [28] ,  [29] ,  [32] , spine  [34] , knee  [29] , and hip  [29]  movement, corresponding back muscle activity, and force and centre of gravity  [29]  have been used for pain (level) detection.\n\nAnother work in the area related to body movement is the one of Rivas et al.  [38] . In their work, the authors explore the use of hand pressure and joystick manipulation to detect stroke patients' pain level by personalising the model to each patient by using data from 10 different sessions. In  [39] , the authors extend the work by combining multiple modalities (hand pressure, gesture and facial expressions) to investigate the relationship between affective states and pain during rehabilitation. Again, individual models are built by taking advantage of the multiple sessions.\n\nIn a recent study  [40]  on automatic discrimination between healthy participants, low-level pain, and high-level pain based on complete movement instances in the EmoPain dataset, we explored features of the trunk, knee, head/neck, and arm movements computed from full-body positional data as well as features from shoulder and lower back muscle activity. We used two separate sets of features for trunk flexion and sit-to-stand movements respectively, given the considerable differences in the temporality of the two movements and the anatomical regions recruited in performing them. We additionally built a separate model for each movement type for this reason and especially to manage the limited data size available. For full and forward trunk flexion, we extracted the range of trunk and neck movement, the amount of unsteadiness in arm movement, and the time and amplitude of high-to-low muscle activity change; for sitto-stand, we extracted range of trunk and neck movement, knee and pelvic angles at the point of buttocks lift, speed and duration of the lift phase, and the time of high-to-low muscle activity change and muscle activity range. We obtained 0.90 F1 score (0.90 accuracy) on average, over the three classes and three movement types, based on leave-one-subject-out cross-validation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Automatic Detection Of Protective Movement Behaviour",
      "text": "Aside from the pain estimation on bodily expressions, the movement behaviour presented therein is informative not only of pain level but also of the emotional state and engagement level of people with chronic lower back pain (CLBP). Specifically, the protective behaviour, e.g., hesitation, guarding, stiffness, the use of support and bracing  [41] , expression of fear or low-efficacy of movements, is currently adopted by physiotherapists in tailoring their feedback and interventions  [42] ,  [43] . As the rehabilitation for CLBP people is moving towards self-management outside the hospital, researchers started to work on the establishment of a virtual physiotherapist, where the first step is about the automatic detection of protective behaviour. Early studies in this direction mainly focused on feature-engineering methods to extract discriminative features from motion capture (MoCap) and surface electromyographic (sEMG) data with shallow classifiers like Random Forests and Support Vector Machine applied on top of them  [7] ,  [44] ,  [45] . To name a few, features used include the range of joint angle, the mean of the angular velocity and the mean of the upperenvelope of the sEMG data. One limitation of these works is the lack of generalisability across different types of movement. Recently, efforts are also seen in using deep learning for the detection of protective behaviour. A comparison of different vanilla neural networks is provided in  [46] , while some data augmentation techniques were also explored. The result achieved is much higher than previous feature-based methods, on the data pooled from different movement types. Later on, a collaboration of LSTM network with attention mechanism is presented in  [47] , where better and explainable results are reported. However, challenges still exist, such as the dependence on the pre-segmented activity sequences which is not able to provide real-time encouragements and feedback, and the lack of exploitation of the bio-mechanical nature of MoCap and sEMG data especially, resulting from the traversal data processing strategy.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Challenge Description",
      "text": "This section describes the data collection protocol for the benchmark data (EmoPain database), the Challenge's tasks, task data partitioning, and proposes real-world applications of each task to clinical pain management.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Emopain Dataset",
      "text": "The EmoPain dataset  [7]  provided for the challenge originally comprised of audiovisual, motion-capture and muscle activity data, collected from 18 CLBP and 22 healthy participants. Here need to note that, the real number of participants provided for each challenge task differs. Each participant went through at least one trial of the data collection, either the normal or the difficult trial. Within a trial, the participant performs a sequence of activities, namely oneleg-stand, reach-forward, stand-to-sit, sit-to-stand and benddown. These activities are connected by transition activities, like standing still, sitting still and self-preparation. In the difficult trial, participant has to follow instructions set by the experimenter and carry a 2Kg weight in each hand during the performance of reach-forward and bend-down. There are no such limitations in the normal trial.\n\nFor the facial expression video, several sets of features are extracted for the challenge participant, which will be described in detail in the next section. For the body movement data, the joint angles and respective angular velocities are computed. The dataset for the challenge is split into training, validation and a held-out test partition. The participant partition are shown in Table  I . The class distribution is not considered for the partition of the dataset, but we ensure each partition has sufficient representation of healthy participants and CLBP patients' data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Challenge Tasks",
      "text": "The EmoPain Challenge consists of three main tasks namely: (i) pain estimation from facial expressions, (ii) pain recognition from multi-modal movement, and (iii) protective movement behaviour detection. Participants were expected to compete in at least one or more tasks.\n\nThe Pain Estimation from Facial Expressions Task aims to develop technology to automatically quantify pain from face images of CLBP and healthy participants performing physical activity. These technologies could potentially support real-time pain assessment for patients who are unable to self-report pain, e.g., unconscious patients, and in constrained settings, e.g., ICUs, where continuous recording of a person's face is possible. Anchoring on facial properties deemed suitable for facial expression analysis  [20] ,  [27] , data for this sub-task consists of anonymized face shape and appearance features extracted from the EmoPain video images (see details in IV-A), as well as observer pain annotations for each face image on an 11-point scale ranging from 0 (no pain) to 10 (maximum possible pain intensity).Due to data protection and ethical constraints, we did not provide the original video images.\n\nNote that the values of the original pain annotations for the face range from 0 to 1000. These labels are heavily unbalanced, as the value of most labels are zero and for some other values, only less than 10 frames have such pain level. To alleviate this problem, we re-sampled all labels into 11 bins, from 0 to 10. Specifically, the values of all original labels were divided by 100, and then allocated to the bin whose value corresponds to their integral part, e.g., a label value of 232 will be assigned to bin 2. The distribution of the final provided labels are detailed in Table  II . Participants' submissions to this task were ranked using the Concordance Correlation Coefficient (CCC)  [48]  which measures the temporal association between the model predictions and ground truth pain labels. CCC is preferred over similar measures-Pearson's CC and Spearman's CC-because it encodes precision and accuracy metrics in a single measurement and is robust to location and scale variations  [48] .\n\nThe Pain Recognition from Multimodal Movement Task aims to detect and classify levels of pain experienced by a person with chronic pain during movement activities. Technology with this capability could help a person with chronic pain more helpfully pace physical activity performance  [40] . Data for this sub-task comprises of muscle activity data, 13 joint angles and angular energies (see full description in  [47] ) captured from CLBP and healthy participants while performing physical activities. Each activity instance is accompanied by a three-class pain annotation: no pain, low pain and high pain, which will serve as ground-truth labels for the task. The submissions for this task were evaluated using F1 scores and accuracy, but final ranking was done based on Matthew Correlation Coefficient (MCC)  [49]  which better accounts for the negative classes.\n\nThe Multimodal Movement Behaviour Classification Task aims to develop technology that can detect and classify protective behaviours (e.g., rigid movement) in people with chronic pain. Such technologies could provide immediate and appropriate feedback or support to users, e.g., notifying the user to adopt a correct posture if the use of maladaptive strategy is detected  [40] ,  [46] . Data for this task consists of 13 bodily joint angular features and muscle activity for each movement frame with corresponding activity-type labels and binary protective behaviour annotations by 2 physiotherapists and 2 psychologists. For this task, macro average F1 score and the F1 score for each class (i.e. protective and nonprotective) were used for ranking participants' submissions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Baseline Features And Systems",
      "text": "In this section, we describe the features extracted from each pain expression modality, the baseline models implemented for each task, and present the results obtained from the performance evaluation of the models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Pain Estimation From Facial Expressions",
      "text": "For the pain estimation from face sub-challenge, we extracted four facial descriptors using the OpenFace 2.0 toolkit  [50] , and two deep-learned emotion-oriented feature representations  [51] . The detailed descriptions of these features are as follows:\n\n• Facial landmarks: 68 2-D and 3-D fiducial facial points.\n\n• Head pose: Pitch, yaw and roll angles.\n\n• Gaze: 3-D gaze directions.\n\n• HOG: a 4464-D Histogram of Oriented Gradients (HOG) features.\n\n• Action Unit (AU) occurrence: 18 AUs whose values are 1 (present) or 0 (absent).\n\n• AU intensities: 17 AUs whose values range from 0 to 5 (max intensity).\n\n• VGG-16 feature: 4096-D deep features extracted from the second fully-connected layers of the VGG-16 network  [52] .\n\n• ResNet-50 feature: 2048-D deep features extracted from the fully-connected layers of the ResNet-50 network.  [53] . The VGG-16 and ResNet-50 network are pretrained on the Affwild dataset  [54]  with valence and arousal labels. Although the data labels are significantly imbalanced as seen in Table  II , we do not perform any data augmentation, to enhance the reproducibility of the reported results. While the task can be solved as an 11-class classification problem, in this challenge, we treated it as a regression problem. The face baseline system employed four different feature sets: 2 hand-crafted features including geometric features (a combination of 2-D facial landmarks and gaze directions) and 4464-D HOG feature; and 2 emotion-oriented deeplearned feature sets including 4096-D VGG-16 features and 2048-D ResNet features. Note that the 2-D facial landmarks are transformed into a 136-D dimension feature vector for each frame. The training process starts with feature normalisation. For each dimension of the input feature, the training set was normalised using z-score as shown in Equation  1 .\n\nwhere µ and σ are the mean and standard deviation of the feature values over the entire training data. The obtained mean value standard deviation were then applied to normalize the validation and test set. In this sub-challenge, we trained an Artificial Neural Network (ANN) for each feature subset. The employed ANNs follow the set-up presented in  [55] , which consists of 4 fully connected hidden layers.\n\nA dropout  [56]  with probability 0.5 and a ReLU layer is placed after each fully-connected layer. RMSprop is used as the training method, while Mean Square Error (MSE) is employed as the loss function. The hyper-parameters and topology chosen for the baseline systems are shown in Table  III . These hyper-parameters were determined by grid search on validation set. The baseline results of the Pain Estimation from Faces sub-challenge are given in Table  IV . They show that amongst the single-feature models, the best correlation (CCC) on the development set results was achieved by VGG-16 feature, which also obtained good RMSE and MSE results. However, while VGG-16 feature also achieved solid performance on the test set in terms of the RMSE and MSE, its predictions are not highly correlated with the ground-truth of the test set. Instead, the combination of facial landmarks and eye gaze features produced excellent RMSE and MSE results on both development and test set, and also generated predictions with the highest correlation (PCC) to the labels in the test set. These results indicate that the pain level can be partially reflected by the geometric information of the face and eyes.\n\nThe decision-level fusion of all modalities gave the best results on both the development set (RMSE = 1.69, PCC = 0.25, CCC = 0.18) and test set (MAE = 0.91, RMSE = 1.41, PCC = 0.10, CCC = 0.06), except the MAE returned on the development set (MAE = 1.26) is slightly higher than the best one (MAE = 1.24). Based on the fusion results, we can argue that though the individual features were not very informative for pain intensity estimation when simple ANNs are used as the back-end, their fusion still seems to provide more valuable and positive information for pain estimation. Based on all results, the recognition of pain intensities from the face is still challenging when only combining existing standard hand-crafted or deep-learned features with a simple back-end. This observation opens interesting research questions about how to extract painrelated cues from complex facial expressions and emotions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Pain Classification Based On Body Movement And Muscle Activity",
      "text": "Due to the limited data size available in this task, we chose to build a single model for all movement types in the dataset so as to maximise the training data. The features that we extracted (see Table  V ) were based on findings in  [40] . We extracted range of joint angles, to characterise the range of movement across anatomical regions relevant to the movement types. We additionally computed speed of movement over all joint angles and over each movement. While it might ordinarily be valuable to compute speed separately for each joint, it was necessary for us to constrain feature dimensionality in order to further address the data   where i = 2, 3, ..., I; I = 13; t = 1, 2, ..., T; k = 1, 2, 3, 4 size limitation. Finally, we computed the range of activity for each of the four muscle groups in the sEMG data. Each data instance is made up of one or more iterations (up to 6) of a complete movement type, and so it was important to additionally incorporate the dynamics within each instance in the feature set. We addressed this by extracting the 18 above-mentioned features in 4 identicallysized non-overlapping window segments that together cover the data instance. 4 was a compromise between limiting the number of features and characterising movements which had the maximum number of repetitions. This led to 72 dimensions for the feature vector for each data instance.\n\nWe explored three main algorithms for the three-level classification of pain based on body movement and muscle activity data: Random Forest (RF)  [57] , Support Vector Machines (SVMs)  [58] , and k-Nearest Neighbours (kNN). The algorithms were evaluated using leave-one-subject-out crossvalidation, based on the challenge training set alone. The hyperparameters for the algorithms were set based on grid search using an inner validation set within each validation fold, and among: 1, 5, 10, and 50 trees for the RF, and one, square root of the total amount, and the total amount for the number of features used to split each node in the RF; 1 to 5 degrees for the polynomial SVM, Gaussian or sigmoid kernels for the SVM, and 0.001, 0.01, 0.1, 1, 10, and 100 as the box constraint size for either of the three SVMs; k between 1 and 5, and minkowski, euclidean, manhattan, or chebyshev distances for the kNN. Note that in the SVMs and kNN setup, the feature set was normalised to zero and unit variance.\n\nThe kNN, and sigmoid and Gaussian SVM, which emerged as not worse off than chance-level detection based on the cross-validation, were further evaluated in hold-out validation, with the challenge training, validation, and test sets for training, validation, and testing respectively. Table  VI  shows the data sizes across the three pain classes (healthy, low-level pain, and high-level pain) for both the leaveone-subject-out cross-validation (LOSO-CV) and the holdout validation. Table  VII  shows the F1 scores, Matthews Correlation Coefficients (MCCs)  [49] , and accuracies of the SVM, RF, and kNN, for three-level pain classification based on leave-one-subject-out cross-validation with the training set. Both the RF and polynomial SVM perform worse than chance-level detection (F1 score = 0.33; MCC = 0; accuracy",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Protective Movement Behaviour Detection",
      "text": "To leave enough space for explorations, a stacked-LSTM network adapted from  [46]  is used as the baseline for the movement behaviour detection task. The architecture stays the same, where three LSTM layers with 32 hidden units are used together with a softmax fully-connected layer for classification. The input to the network is a frame with size of NxTxD, where N is the number of samples, T is the length of timesteps and D is the dimension of features. The data used is the 13 angles and their respective square of angular velocities as well as the upper envelope of the sEMG data. As a result, the data matrix has the dimension D=30. A sliding window of 180 timesteps long and a 0.75 overlapping ratio is used to extract consecutive frames from each activity type.  groundtruth of each frame is determined by majority-voting: a frame is labelled as protective if at least half of the samples within it were coded as protective, and vice versa.\n\nThe results achieved by the stacked-LSTM network are reported in Table  IX . We can see from the result that all the frames in the validation set are detected as nonprotective. This can be due to the fact that the protective and non-protective samples included in the training set are very imbalanced, while the baseline method does not apply any technique to solve it. On the other hand, the size of the training data is still limited. The result on the test set is slightly better with some frames correctly detected as protective (F1 score of protective class=0.2465). This proved the feasibility of using deep learning for the detection of protective behavior. Except for processing the MoCap and sEMG in a traversal way that ignored the biomechanical connectivity, challenges remain on i) how to deal with the imbalance problem in the data set; ii) how to design better data augmentation approaches.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we introduced the first EmoPain 2020 Challenge on automatic pain recognition from multimodal face and body expressions based on the EMOPAIN dataset and guidelines for participation in the competition. It featured three tasks: (i) pain estimation from face shape and appearance features, (ii) pain recognition from muscle activity and joint angle statistical features, and (iii) classification of protective body movement behaviour. For each task, we described the expressive behavioural features extracted, the baseline system implementations and perfor-mance on the benchmark dataset. In this challenge, participants only received the extracted expression features rather than the video data, thus the baseline implementations do not employ feature optimisation or augmentation methods to allow for reproducibility of the results. Lastly, the baseline program code, results and participant rankings can be found on the EmoPain2020 Challenge's webpage: https://mvrjustid.github.io/EmoPainChallenge2020/.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Partitions": "Train",
          "Face Tasks": "8 CLBP and 11 HP",
          "Body Tasks": "10 CLBP and 6 HP"
        },
        {
          "Partitions": "Validation",
          "Face Tasks": "3 CLBP and 6 HP",
          "Body Tasks": "4 CLBP and 3 HP"
        },
        {
          "Partitions": "Test",
          "Face Tasks": "3 CLBP and 5 HP",
          "Body Tasks": "4 CLBP and 3 HP"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality": "FL+GAZE\nFL+GAZE\nHOG\nHOG\nVGG-16\nVGG-16\nResNet-50\nResNet-50",
          "Partition": "Valid.\nTest.\nValid.\nTest.\nValid.\nTest.\nValid.\nTest.",
          "MAE": "1.51\n1.37\n1.24\n0.93\n1.34\n0.92\n1.42\n1.14",
          "RMSE": "1.74\n1.56\n1.91\n1.61\n1.82\n1.43\n2.08\n1.74",
          "PCC": "0.04\n0.10\n0.05\n0.03\n0.24\n0.02\n-0.08\n-0.09",
          "CCC": "0.003\n0.003\n0.04\n0.02\n0.18\n0.004\n-0.04\n-0.06"
        },
        {
          "Modality": "Fusion\nFusion",
          "Partition": "Valid.\nTest.",
          "MAE": "1.26\n0.91",
          "RMSE": "1.69\n1.41",
          "PCC": "0.25\n0.10",
          "CCC": "0.18\n0.06"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Features": "Range of\njoint angle",
          "Formulae": "∆Ji = maxt Ji − mint Ji",
          "Dimension": "11"
        },
        {
          "Features": "max\nSpeed\nmin\nmean",
          "Formulae": "δJi\nmaxi maxt\nδt\nδJi\nmini mint\nδt\nδJi\nδt\nPt\nT\nPi\nI",
          "Dimension": "1\n1\n1"
        },
        {
          "Features": "Range of muscle activity",
          "Formulae": "∆Ek = maxt Ek − mint Ek",
          "Dimension": "4"
        },
        {
          "Features": "where i = 2, 3,\n..., I;\nI = 13;\nt = 1, 2,\n..., T; k = 1, 2, 3, 4",
          "Formulae": "",
          "Dimension": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm": "Sigmoid/Gaussian SVM",
          "F1 Score*": "0.41",
          "MCC*": "0.19",
          "Accuracy": "0.44"
        },
        {
          "Algorithm": "kNN",
          "F1 Score*": "0.34",
          "MCC*": "0.05",
          "Accuracy": "0.37"
        },
        {
          "Algorithm": "RF",
          "F1 Score*": "0.26",
          "MCC*": "-0.10",
          "Accuracy": "0.27"
        },
        {
          "Algorithm": "Polynomial SVM",
          "F1 Score*": "0.15",
          "MCC*": "-0.16",
          "Accuracy": "0.26"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "kNN\n(k=1, manhattan distance)": "F1 Score",
          "Sigmoid/Gaussian SVM\n(Gaussian kernel,\nbox constraint=0.1)": "F1 Score"
        },
        {
          "kNN\n(k=1, manhattan distance)": "0.39\n0.09\n0.44",
          "Sigmoid/Gaussian SVM\n(Gaussian kernel,\nbox constraint=0.1)": "0.00\n0.14\n0.00"
        },
        {
          "kNN\n(k=1, manhattan distance)": "0.31",
          "Sigmoid/Gaussian SVM\n(Gaussian kernel,\nbox constraint=0.1)": "0.34"
        },
        {
          "kNN\n(k=1, manhattan distance)": "0.35",
          "Sigmoid/Gaussian SVM\n(Gaussian kernel,\nbox constraint=0.1)": "0.07"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "stacked-LSTM",
          "Partition": "Valid",
          "Class": "Non-protective (0)",
          "Acc": "-",
          "F1 score": "0.9622"
        },
        {
          "Method": "",
          "Partition": "",
          "Class": "Protective (1)",
          "Acc": "-",
          "F1 score": "-"
        },
        {
          "Method": "",
          "Partition": "",
          "Class": "Average",
          "Acc": "0.4636",
          "F1 score": "0.4811"
        },
        {
          "Method": "",
          "Partition": "Test",
          "Class": "Non-protective (0)",
          "Acc": "-",
          "F1 score": "0.9029"
        },
        {
          "Method": "",
          "Partition": "",
          "Class": "Protective (1)",
          "Acc": "-",
          "F1 score": "0.2465"
        },
        {
          "Method": "",
          "Partition": "",
          "Class": "Average",
          "Acc": "0.828",
          "F1 score": "0.5747"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Prevalence of chronic pain in the uk: a systematic review and meta-analysis of population studies",
      "authors": [
        "A Fayaz",
        "P Croft",
        "R Langford",
        "L Donaldson",
        "G Jones"
      ],
      "year": "2016",
      "venue": "BMJ open"
    },
    {
      "citation_id": "2",
      "title": "Prevalence of chronic pain and high-impact chronic pain among adultsunited states",
      "authors": [
        "J Dahlhamer",
        "J Lucas",
        "C Zelaya",
        "R Nahin",
        "S Mackey",
        "L Debar",
        "R Kerns",
        "M Von",
        "L Korff",
        "C Porter",
        "Helmick"
      ],
      "year": "2016",
      "venue": "Morbidity and Mortality Weekly Report"
    },
    {
      "citation_id": "3",
      "title": "Automatic recognition methods supporting pain assessment: A survey",
      "authors": [
        "P Werner",
        "D Lopez-Martinez",
        "S Walter",
        "A Al-Hamadi",
        "S Gruss",
        "R Picard"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "The influence of communication goals and physical demands on different dimensions of pain behavior",
      "authors": [
        "M Sullivan",
        "P Thibault",
        "A Savard",
        "R Catchlove",
        "J Kozey",
        "W Stanish"
      ],
      "year": "2006",
      "venue": "Pain"
    },
    {
      "citation_id": "5",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "6",
      "title": "Evidence for the role of psychological factors in abnormal paraspinal activity in patients with chronic low back pain",
      "authors": [
        "P Watson",
        "C Booker",
        "C Main"
      ],
      "year": "1997",
      "venue": "J Musculoskelet Pain"
    },
    {
      "citation_id": "7",
      "title": "The automatic detection of chronic pain-related expression: requirements, challenges and the multimodal emopain dataset",
      "authors": [
        "M Aung",
        "S Kaltwang",
        "B Romera-Paredes",
        "B Martinez",
        "A Singh",
        "M Cella",
        "M Valstar",
        "H Meng",
        "A Kemp",
        "M Shafizadeh"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "P Werner",
        "A Al-Hamadi",
        "S Crawcour",
        "A Andrade",
        "G Da Silva"
      ],
      "year": "2013",
      "venue": "International Conference on Cybernetics"
    },
    {
      "citation_id": "9",
      "title": "Painful monitoring: Automatic pain monitoring using the unbc-mcmaster shoulder pain expression archive database",
      "authors": [
        "P Lucey",
        "J Cohn",
        "K Prkachin",
        "P Solomon",
        "S Chew",
        "I Matthews"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "10",
      "title": "Machine recognition and representation of neonatal facial displays of acute pain",
      "authors": [
        "S Brahnam",
        "C.-F Chuang",
        "F Shih",
        "M Slack"
      ],
      "year": "2006",
      "venue": "Artif Intell Med"
    },
    {
      "citation_id": "11",
      "title": "Pain and negative emotions in the face: judgements by health care professionals",
      "authors": [
        "J Kappesser",
        "A De C Williams"
      ],
      "year": "2002",
      "venue": "Pain"
    },
    {
      "citation_id": "12",
      "title": "Recognition and discrimination of prototypical dynamic expressions of pain and emotions",
      "authors": [
        "D Simon",
        "K Craig",
        "F Gosselin",
        "P Belin",
        "P Rainville"
      ],
      "year": "2008",
      "venue": "PAIN R"
    },
    {
      "citation_id": "13",
      "title": "The painful face-pain expression recognition using active appearance models",
      "authors": [
        "A Ashraf",
        "S Lucey",
        "J Cohn",
        "T Chen",
        "Z Ambadar",
        "K Prkachin",
        "P Solomon"
      ],
      "year": "2009",
      "venue": "Image and vision computing"
    },
    {
      "citation_id": "14",
      "title": "Machine assessment of neonatal facial expressions of acute pain",
      "authors": [
        "S Brahnam",
        "C.-F Chuang",
        "R Sexton",
        "F Shih"
      ],
      "year": "2007",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "15",
      "title": "Automatic coding of facial expressions displayed during posed and genuine pain",
      "authors": [
        "G Littlewort",
        "M Bartlett",
        "K Lee"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "16",
      "title": "Assessment of pain",
      "authors": [
        "H Breivik",
        "P Borchgrevink",
        "S Allen",
        "L Rosseland",
        "L Romundstad",
        "E Breivik",
        "G Hals",
        "A Kvarstein",
        "Stubhaug"
      ],
      "year": "2008",
      "venue": "BJA: British Journal of Anaesthesia"
    },
    {
      "citation_id": "17",
      "title": "Automatic detection of pain intensity",
      "authors": [
        "Z Hammal",
        "J Cohn"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "18",
      "title": "Pain intensity evaluation through facial action units",
      "authors": [
        "Z Zafar",
        "N Khan"
      ],
      "year": "2014",
      "venue": "2014 22nd International Conference on Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Cumulative attributes for pain intensity estimation",
      "authors": [
        "J Egede",
        "M Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Continuous Pain Intensity Estimation from Facial Expressions",
      "authors": [
        "S Kaltwang",
        "O Rudovic",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Advances in Visual Computing"
    },
    {
      "citation_id": "21",
      "title": "The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain",
      "authors": [
        "K Prkachin",
        "P Solomon"
      ],
      "year": "2008",
      "venue": "Pain"
    },
    {
      "citation_id": "22",
      "title": "Automatic pain recognition from video and biomedical signals",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "R Niese",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2014",
      "venue": "Int Conf Patt Recog"
    },
    {
      "citation_id": "23",
      "title": "Data fusion for automated pain recognition",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Traue",
        "P Werner",
        "A Al-Hamadi",
        "M Kächele",
        "F Schwenker",
        "A Andrade",
        "G Moreira"
      ],
      "year": "2015",
      "venue": "International Conference on Pervasive Computing Technologies for Healthcare"
    },
    {
      "citation_id": "24",
      "title": "Recurrent convolutional neural network regression for continuous pain intensity estimation in video",
      "authors": [
        "J Zhou",
        "X Hong",
        "F Su",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "25",
      "title": "Spatio-temporal pain recognition in cnn-based super-resolved facial images",
      "authors": [
        "M Bellantonio",
        "M Haque",
        "P Rodriguez",
        "K Nasrollahi",
        "T Telve",
        "S Escalera",
        "J Gonzalez",
        "T Moeslund",
        "P Rasti",
        "G Anbarjafari"
      ],
      "year": "2016",
      "venue": "Video Analytics. Face and Facial Expression Recognition and Audience Measurement"
    },
    {
      "citation_id": "26",
      "title": "Automatic neonatal pain estimation: An acute pain in neonates database",
      "authors": [
        "J Egede",
        "M Valstar",
        "M Torres",
        "D Sharkey"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "27",
      "title": "Fusing deep learned and handcrafted features of appearance, shape, and dynamics for automatic pain estimation",
      "authors": [
        "J Egede",
        "M Valstar",
        "B Martinez"
      ],
      "year": "2017",
      "venue": "Int Conf Automat Face & Gesture Recog"
    },
    {
      "citation_id": "28",
      "title": "Comparison of lumbar paravertebral EMG patterns in chronic low back pain patients and non-patient controls",
      "authors": [
        "D Ahern",
        "M Follick",
        "J Council",
        "N Laser-Wolston",
        "H Litchman"
      ],
      "year": "1988",
      "venue": "Pain"
    },
    {
      "citation_id": "29",
      "title": "The use of artificial neural networks to identify patients with chronic low-back pain conditions from patterns of sit-to-stand manoeuvres",
      "authors": [
        "G Gioftsos",
        "D Grieve"
      ],
      "year": "1996",
      "venue": "Clin Biomech"
    },
    {
      "citation_id": "30",
      "title": "Classification of Neck Movement Patterns Related to Whiplash-Associated Disorders Using Neural Networks",
      "authors": [
        "H Grip",
        "F Ohberg",
        "U Wiklund",
        "Y Sterner",
        "J Karlsson",
        "B Gerdle"
      ],
      "year": "2003",
      "venue": "IEEE T Inf Technol B"
    },
    {
      "citation_id": "31",
      "title": "Automatic Recognition of Gait Patterns Exhibiting Patellofemoral Pain Syndrome Using a Support Vector Machine Approach",
      "authors": [
        "D Lai",
        "P Levinger",
        "R Begg",
        "W Gilleard",
        "M Palaniswami"
      ],
      "year": "2009",
      "venue": "IEEE T Inf Technol B"
    },
    {
      "citation_id": "32",
      "title": "Classification of low back pain from dynamic motion characteristics using an artificial neural network",
      "authors": [
        "J Bishop",
        "M Szpalski",
        "S Ananthraman",
        "D Mcintyre",
        "M Pope"
      ],
      "year": "1997",
      "venue": "Spine"
    },
    {
      "citation_id": "33",
      "title": "Detecting affective states in virtual rehabilitation",
      "authors": [
        "J Rivas",
        "F Orihuela-Espina",
        "L Sucar",
        "L Palafox",
        "J Hernándezfranco",
        "N Bianchi-Berthouze"
      ],
      "year": "2015",
      "venue": "Detecting affective states in virtual rehabilitation"
    },
    {
      "citation_id": "34",
      "title": "Relationship between pain and vertebral motion in chronic low-back pain subjects",
      "authors": [
        "J Dickey",
        "M Pierrynowski",
        "D Bednar",
        "S Yang"
      ],
      "year": "2002",
      "venue": "Clin Biomech"
    },
    {
      "citation_id": "35",
      "title": "Methods for Person-Centered Continuous Pain Intensity Assessment from Bio-Physiological Channels",
      "authors": [
        "M Kachele",
        "P Thiam",
        "M Amirian",
        "F Schwenker",
        "G Palm"
      ],
      "year": "2016",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Automatic Pain Recognition with Facial Activity Descriptors",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "K Limbrecht-Ecklundt",
        "S Walter",
        "S Gruss",
        "H Traue"
      ],
      "year": "2016",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "A neurocognitive model of attention to pain: Behavioral and neuroimaging evidence",
      "authors": [
        "V Legrain",
        "S Damme",
        "C Eccleston",
        "K Davis",
        "D Seminowicz",
        "G Crombez"
      ],
      "year": "2009",
      "venue": "Pain"
    },
    {
      "citation_id": "38",
      "title": "Unobtrusive inference of affective states in virtual rehabilitation from upper limb motions: A feasibility study",
      "authors": [
        "J Rivas",
        "F Orihuela-Espina",
        "L Palafox",
        "N Berthouze",
        "M Lara",
        "J Hernndez-Franco",
        "E Sucar"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Automatic recognition of multiple affective states in virtual rehabilitation by exploiting the dependency relationships",
      "authors": [
        "J Rivas",
        "F Orihuela-Espina",
        "L Sucar",
        "A Williams",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "How Can Affect Be Detected and Represented in Technological Support for Physical Rehabilitation?",
      "authors": [
        "T Olugbade",
        "A Singh",
        "N Bianchi-Berthouze",
        "N Marquardt",
        "M Aung",
        "A Williams"
      ],
      "year": "2019",
      "venue": "Transactions on Computer-Human Interaction"
    },
    {
      "citation_id": "41",
      "title": "Development of an observation method for assessing pain behavior in chronic low back pain patients",
      "authors": [
        "F Keefe",
        "A Block"
      ],
      "year": "1982",
      "venue": "Behav Ther"
    },
    {
      "citation_id": "42",
      "title": "Fear-avoidance and its consequences in chronic musculoskeletal pain: A state of the art",
      "authors": [
        "J Vlaeyen",
        "S Linton"
      ],
      "year": "2000",
      "venue": "Pain"
    },
    {
      "citation_id": "43",
      "title": "The relationship between guarding, pain, and emotion",
      "authors": [
        "T Olugbade",
        "N Bianchi-Berthouze",
        "A Williams"
      ],
      "year": "2019",
      "venue": "PAIN Report"
    },
    {
      "citation_id": "44",
      "title": "Human observer and automatic assessment of movement related self-efficacy in chronic pain: from exercise to functional activity",
      "authors": [
        "T Olugbade",
        "N Berthouze",
        "N Marquardt",
        "A Williams"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Automatic recognition of fear avoidance behavior in chronic pain physical rehabilitation",
      "authors": [
        "M Aung",
        "N Bianchi-Berthouze",
        "P Watson",
        "A Williams"
      ],
      "year": "2014",
      "venue": "International Conference on Pervasive Computing Technologies for Healthcare"
    },
    {
      "citation_id": "46",
      "title": "Recurrent network based automatic detection of chronic pain protective behavior using mocap and semg data",
      "authors": [
        "C Wang",
        "T Olugbade",
        "A Mathur",
        "A De C Williams",
        "N Lane",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "Proceedings of the 23rd International Symposium on Wearable Computers (ISWC)"
    },
    {
      "citation_id": "47",
      "title": "Learning temporal and bodily attention in protective movement behavior detection",
      "authors": [
        "C Wang",
        "M Peng",
        "T Olugbade",
        "N Lane",
        "A Williams",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "Proceedings of the 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "48",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "49",
      "title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme",
      "authors": [
        "B Matthews"
      ],
      "year": "1975",
      "venue": "Biochimica et Biophysica Acta"
    },
    {
      "citation_id": "50",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "51",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "52",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition"
    },
    {
      "citation_id": "53",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "54",
      "title": "Deep affect prediction inthe-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Int J Comput Vis"
    },
    {
      "citation_id": "55",
      "title": "Automatic prediction of depression and anxiety from behaviour and personality attributes",
      "authors": [
        "S Jaiswal",
        "S Song",
        "M Valstar"
      ],
      "year": "2019",
      "venue": "Int Conf Affect Comput Intell Interact"
    },
    {
      "citation_id": "56",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "J Mach Learn Res"
    },
    {
      "citation_id": "57",
      "title": "Random Forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "58",
      "title": "Support-Vector Networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    }
  ]
}